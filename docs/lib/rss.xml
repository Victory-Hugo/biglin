<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[科研思路]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>科研思路</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 04 Nov 2024 12:47:07 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 04 Nov 2024 12:45:33 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>Surfer 软件教程在这：<a data-href="Surfer：克里金插值制作贴合的等值线图" href="软件\地理绘图\surfer：克里金插值制作贴合的等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：克里金插值制作贴合的等值线图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br><br>Group,Lat,Long,Frequency
id1,27.29847,105.30504,0
id2,35.60122,103.21091,0.1
id3,26.25427,107.52226,0
id4,25.05,102.72,0
id5,26.25427,107.52226,0
id6,22.55,114.1,0
id7,35.60122,103.21091,0
id8,29.57,106.56,0.015
id9,26.11,119.3,0.02
复制<br>需要准备的文件应该包括：<br>
<br>经纬度
<br>经纬度所在的单倍群频率
<br><br>
<br>
打开 Arcgis，新建一张底图。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523205502.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
添加经纬度数据<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523205535.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523205559.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
输入表：刚才准备好的频率表<br>
输出要素：默认就行。<br>
X 和 Y 分别对应经度和纬度。<br>
坐标系请选择刚才地图所属的坐标系，以保证投影点的正确。

<br>
制作等高线图<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523205932.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210004.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Z 值字段选择频率，其余的选择默认即可。如果有特殊要求可以查看相关说明书。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210233.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在环境选项卡中，需要注意，输出坐标系应该选择与底图相同的坐标系。这样才能更好地显示！

<br>
美化地图<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210343.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
软件中有很多个性化的参数可以调整来获得更好的视觉效果！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210813.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以选择适合自己的颜色和风格！

<br>
打印布局<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210853.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210921.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
添加图例来说明图层的颜色含义：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523210948.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523211036.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
最后进行导出：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240523211202.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\地理绘图\arcgis-pro：使用克里金绘制单倍群等值线图.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis Pro：使用克里金绘制单倍群等值线图.md</guid><pubDate>Wed, 31 Jul 2024 01:31:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br><br>地理坐标系统（GCS）使用纬度和经度来描述地球表面的位置。这种坐标系统是基于地球的球形模型，通常使用度（°）、分（'）和秒（"）来度量。<br>
<br>纬度（Latitude）：表示北纬或南纬，范围从赤道的0°至北极90°北纬和南极90°南纬。
<br>经度（Longitude）：表示东经或西经，范围从本初子午线的0°东经或西经至180°。
<br><br>投影坐标系统（PCS）将地球表面的三维位置转换为二维平面上的点。这种转换通常需要一个数学模型，这样可以在平面上准确地表示地理位置，适合进行测量和绘图。因此，投影坐标系统常用于制图和精确工程项目。<br>
<br>平面坐标：使用直角坐标系统（如UTM、Web Mercator等），通常以米或英尺为单位，坐标值表示为东（X）和北（Y）的偏移量。
<br>例子：北京在UTM 50N投影下的坐标可能是东坐标 448202米，北坐标 4414487米。<br>再看看这个？
<a data-href="Arcgis：深入了解坐标" href="软件\地理绘图\arcgis：深入了解坐标.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="LiThumbsUp" aria-label="LiThumbsUp" data-icon="LiThumbsUp" aria-hidden="true" style="transform: translateY(20%);"><svg xmlns="http://www.w3.org/2000/svg" width="18px" height="18px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-thumbs-up"><path d="M7 10v12"></path><path d="M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2a3.13 3.13 0 0 1 3 3.88Z"></path></svg></span>Arcgis：深入了解坐标</a>
<br><br><br><br>每个国家或地区都有各自的基准面，我们通常所说的北京54坐标系、西安80坐标系实际上指的是我国的两个大地基准面。<br>
<br>北京54坐标系：我国参照前苏联从1953年起采用克拉索夫斯基(Krassovsky)椭球体建立了我国的北京54坐标系。
<br>西安80坐标系：1978年采用国际大地测量协会推荐的1975地球椭球体（IAG75）建立了我国新的大地坐标系西安80坐标系。目前大地测量基本上仍以北京54坐标系作为参照。<br>
WGS1984基准面采用 WGS84椭球体，它是一地心坐标系，即以地心作为椭球体中心，目前 GPS 测量数据多以 WGS1984为基准。
<br>地理坐标采用下图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281613242.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>Krasovsky_1940_Albers（阿尔伯斯投影，又名“正轴等积割圆锥投影”、 “双轴纬线等积圆锥投影”，即经过两次纬线校正） 。具体参数如图：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281614217.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一、第二标准纬线分别为25°N和47°N，中央经线为105°E。<br><br>ArcGIS中没有直接提供相应参数的投影，只有自己建立。<br>
在缺失投影的数据基础上，选择ArcToolbox→数据管理工具→投影和变换→要素→投影。在输出坐标系中“新建投影坐标系”，进行下图参数。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281614059.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281617001.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281618002.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408281617070.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\地理绘图\arcgis：变换坐标系将中国地图校正.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：变换坐标系将中国地图校正.md</guid><pubDate>Thu, 29 Aug 2024 02:09:12 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arcgis：地理空间数据]]></title><description><![CDATA[ 
 <br>首先弄明白数据的格式：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754428.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>shp是一种矢量图形。
<br>dbf是一种数据库，存放了名字、位置等各种信息。
<br>tif是一种栅格数据，不能无限放大。
<br>数据的来源：<br>
<br>地理空间数据云网站。 <a rel="noopener nofollow" class="external-link" href="https://www.gscloud.cn/#page1/2" target="_blank">https://www.gscloud.cn/#page1/2</a>
<br>SRTM data search网站。 <a rel="noopener nofollow" class="external-link" href="https://www.earthdata.nasa.gov/sensors/srtm" target="_blank">https://www.earthdata.nasa.gov/sensors/srtm</a><br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754430.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>Natural earth网站。 <a rel="noopener nofollow" class="external-link" href="https://www.naturalearthdata.com/" target="_blank">https://www.naturalearthdata.com/</a>
<br>DIVA-GIS网站。 <a rel="noopener nofollow" class="external-link" href="https://www.diva-gis.org/Data" target="_blank">https://www.diva-gis.org/Data</a>
<br>国家基础地理信息数据 百度云。 <a rel="noopener nofollow" class="external-link" href="https://www.ngcc.cn/ngcc/" target="_blank">https://www.ngcc.cn/ngcc/</a>
<br><a rel="noopener nofollow" class="external-link" href="https://www.webmap.cn/commres.do?method=dataDownload" target="_blank">https://www.webmap.cn/commres.do?method=dataDownload</a>
]]></description><link>软件\地理绘图\arcgis：地理空间数据.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：地理空间数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:13 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754428.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754428.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arcgis：绘制省份人口数]]></title><description><![CDATA[ 
 <br>
<br>首先需要根据所选shp文件格式进行数据清洗。<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755373.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
打开属性表，查看其中的格式：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755374.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>按照上述格式对自己的数据进行整理。<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755375.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>链接数据和图层：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755376.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>根据值字段微调<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755377.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
完成！<br>
当然可以加上一点点小小的细节：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755378.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
]]></description><link>软件\地理绘图\arcgis：绘制省份人口数.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：绘制省份人口数.md</guid><pubDate>Wed, 31 Jul 2024 01:31:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arcgis：绘制XY数据]]></title><description><![CDATA[ 
 <br>X、Y数据在Arcgis软件中被称为经度和纬度。<br>
<br>添加数据<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755189.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>选定文件，链接字段<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755190.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>注意选择编辑来框选坐标系：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755191.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>如果要对数据进行分级，最好利用Excel，因为地图的统计数据是利用VB和PY进行的，很难用。
<br>可以在Arcgis中对数据类型进行定义：也可以在Excel中进行更改。<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755192.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
双精度=经纬度∈数字<br>
但是字符串≠数字
]]></description><link>软件\地理绘图\arcgis：绘制xy数据.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：绘制XY数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:13 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755189.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191755189.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arcgis：基础绘图]]></title><description><![CDATA[ 
 <br>通过打开目录，然后将需要的图层拖入工作区，然后在左侧的图层区进行完微调，可以更换轮廓及区域的颜色大小透明度等<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754208.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754209.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
标注每个省会城市的名字：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754210.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这样一张初稿就完成了：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754211.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
接下来切换到布局视图：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754213.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
调整大小：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754214.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
最后调整一下：<br>
<br>指北针
<br>比例尺
<br>图例：注意，图例要进行转换成图形，取消分组，可以手动更改名称很方便！<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754215.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
对于严格的中国地图，还应该插入南海的部分，这需要进行以下操作：
<br>插入数据框<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754216.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>右点击数据框的属性：设置一下不透明度等
<br>将上述图层全部复制粘贴到南海中：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754217.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754218.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>选择南海区域，选择放大镜工具，放大南海区域：<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754219.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
]]></description><link>软件\地理绘图\arcgis：基础绘图.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：基础绘图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:13 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754208.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402191754208.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Arcgis：没有ObjectID字段解决办法]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/Sunshine_20201/article/details/108928400" rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/Sunshine_20201/article/details/108928400" target="_blank">原文</a><br>若X，Y事件图层所基于的表中没有ObjectID字段。<br>
在点击【确定】时，会弹出”表没有ObjectID字段“。<br>则无法执行：<br>
<br>在地图图层中选择要素
<br>执行使用了选择集的操作，如从表导航到地图
<br>编辑图层属性
<br>对任意X，Y事件图层执行任意交互式编辑操作
<br>定义关联<br>
解决方法：（将XY数据图层另存为要素类）
<br>右击图层，点击【数据】|【导出数据】
<br>然后再立刻导入就可以了。
]]></description><link>软件\地理绘图\arcgis：没有objectid字段解决办法.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：没有ObjectID字段解决办法.md</guid><pubDate>Fri, 21 Jun 2024 07:31:14 GMT</pubDate></item><item><title><![CDATA[地理坐标系统（GCS）]]></title><description><![CDATA[ 
 <br>坐标是确定地理位置的重要参数，地球上任何一个位置可以由 经度、纬度、海拔 决定。如果我们知道了 经度和纬度 也可以查询海拔：<a data-href="python：根据经纬度获取海拔" href="软件\python\地理经纬\python：根据经纬度获取海拔.html" class="internal-link" target="_self" rel="noopener nofollow">python：根据经纬度获取海拔</a>。<br>地理坐标系统（Geographic Coordinate System, GCS）和投影坐标系统（Projected Coordinate System, PCS）是地理信息系统（GIS）中用于描述地球表面位置的两种主要坐标系统。它们的选择和使用取决于分析需求、精确度要求及地图制作的目的。<br><br>地理坐标系统使用经度（Longitude）和纬度（Latitude）来描述地球上任何位置的坐标。这些坐标是通过地球的球形或椭球形模型计算得出的。<br>
<br>经度：从本初子午线（位于格林威治，英国）向东和向西度量至180度。
<br>纬度：从赤道向北和向南度量至90度。<br>
经纬度通常用度（°）、分（'）和秒（"）来表示，或者以十进制形式表示。地理坐标系统简单直观，易于理解和使用，但由于地球的曲率，这种系统在进行面积、距离和角度的精确测量时存在误差。
<br><br>投影坐标系统则是将地球的三维表面通过数学方法投影到一个二维平面上。这种系统允许我们在平面地图上更精确地进行测量和绘图，但引入了不同类型的投影变形，如面积、形状、距离或方向的扭曲。<br>
<br>常见的投影类型：

<br>圆柱投影：如墨卡托投影，适合海洋导航，因为它能保持角度，但在高纬度地区会极大夸大面积。
<br>圆锥投影：如阿尔伯斯等角圆锥投影，适合跨越大陆的地图，能较好地保持地区间的面积比例。
<br>方位投影：如极地方位投影，常用于极地地区的地图制作。


<br><br>在我们拿到一批数据之后，在没有任何多余信息的情况下，表格中出现的经度和纬度绝大部分情况下指的是 GCS。<br><br>假如我们需要绘制全球地图，那么基本分为如下2种：<br>
<br>正轴等积方位投影（Azimuthal Equal-Area Projection），这是一种用于保持面积比例的投影方式，通常用于显示全球的对称性和保持相对区域面积。<br>
<img alt="4o28b0625501ad13015501ad2bfc0074.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408291040437.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>等角圆柱投影（Mercator Projection），这是最常见的地图投影之一。它保持了角度和形状的正确性，但会导致高纬度地区的面积失真。<br>
<img alt="世界地图1：4100万对开基础要素版线划一.JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408291041537.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>正轴等积方位投影（Azimuthal Equal-Area Projection）是一种保持面积不变的方位投影。根据投影中心点的不同位置，它可以产生不同的投影图，适用于不同的地理区域和应用。以下是几种著名的子分类：<br>
<br>
兰伯特方位投影（Lambert Azimuthal Equal-Area Projection）：

<br>这种投影是正轴等积方位投影的标准形式。通常用于绘制全球地图或展示地球上某个特定半球的面积。投影中心可以是地球的任何点，但常用于极点或赤道。


<br>
北极方位等积投影（Polar Azimuthal Equal-Area Projection）：

<br>当投影的中心点位于北极时，这种投影被称为北极方位等积投影。它常用于极地研究和极地地区的地图绘制，因为它可以很好地展示围绕极点的区域，并且保持面积的正确性。


<br>
南极方位等积投影（Antarctic Azimuthal Equal-Area Projection）：

<br>类似于北极方位投影，这种投影的中心点位于南极，适合展示南极周围的区域。它常用于南极地区的科学研究和地图制作。


<br>
赤道方位等积投影（Equatorial Azimuthal Equal-Area Projection）：

<br>如果投影中心位于赤道上，这种投影被称为赤道方位等积投影。它适用于展示赤道附近的区域，并保持这些区域的面积比例正确。


<br>
全圆投影（Global Azimuthal Equal-Area Projection）：

<br>这种形式的投影涵盖整个地球，通常用于全球地图或地球仪的平面投影版本。它的特点是在整个地球表面保持面积的正确比例，尽管形状会发生较大的变形。


<br><br>等角圆柱投影（Mercator Projection） 是一种保持角度和形状的地图投影，但它会使高纬度地区的面积显著失真。等角圆柱投影主要有几种著名的子分类，其中包括墨卡托投影。<br>
<br>墨卡托投影（Mercator Projection）
<br>
<br>标准墨卡托投影是等角圆柱投影的经典形式，由杰拉德·墨卡托（Gerardus Mercator）于1569年发明。它将地球表面投影到一个圆柱体上，再将圆柱体展开为平面。墨卡托投影在全球地图中保持了局部角度的正确性，特别适用于航海，因为它能使直线航线成为地图上的直线。
<br>
<br>横轴墨卡托投影（Transverse Mercator Projection, TM Projection）
<br>
<br>横轴墨卡托投影是墨卡托投影的一个变体，其中投影的圆柱轴与地轴垂直，而不是平行。这种投影非常适合表示南北延伸的区域，如南北走向的国家或洲际区域。高斯-克吕格投影（Gauss-Krüger Projection）就是横轴墨卡托投影的一种。
<br>
<br>伪墨卡托投影（Pseudo-Mercator Projection）
<br>
<br>伪墨卡托投影是现代网络地图（如Google Maps和OpenStreetMap）中常用的投影。虽然它保持了与标准墨卡托投影类似的角度特性，但进行了简化，使得计算更为容易。这种投影适用于电子地图的快速渲染，但在高纬度地区仍然存在明显的面积失真。
<br>
<br>等距墨卡托投影（Equidistant Cylindrical Projection, Equirectangular Projection）
<br>
<br>等距墨卡托投影（也称为等距圆柱投影）是另一种圆柱投影，保持纬线间的距离一致。虽然它与墨卡托投影不同，主要用于简单的地图制作和一些特定的应用场景，但由于它的数学简单性，它经常被误认为是墨卡托投影的变体。
<br>
<br>倾斜墨卡托投影（Oblique Mercator Projection）
<br>
<br>倾斜墨卡托投影是一种更加特殊的变体，其中投影轴相对于赤道倾斜。这种投影常用于一些需要沿对角线方向表示的区域，尤其是当地图区域是沿某个对角线方向延伸时。
<br>
<br>加权墨卡托投影（Weighted Mercator Projection）
<br>
<br>这种投影是一个实验性的变体，试图通过对高纬度地区进行加权处理，来减少传统墨卡托投影在这些区域的面积失真。然而，这种方法在实际应用中较为少见。
想看更多投影？
<a data-tooltip-position="top" aria-label="https://m.thepaper.cn/newsDetail_forward_10750264" rel="noopener nofollow" class="external-link" href="https://m.thepaper.cn/newsDetail_forward_10750264" target="_blank">科普 | 25种地图投影类型：视觉参考指南 (thepaper.cn)</a>


<br><br>对于一批没有任何条件的经纬度数据，可以直接在平面内按照 x 轴和 y 轴直接绘制散点，然后通过整体的拉伸即可得到正轴等积方位投影，不需要进行任何坐标变换。因此，可以完美地展示在正轴等积方位投影绘制地世界地图上。除了正轴等积方位投影地世界地图，其他大部分的地图都需要进行投影变换。针对中国地区的投影变换，可以查看 <a data-href="Arcgis：变换坐标系将中国地图校正" href="软件\地理绘图\arcgis：变换坐标系将中国地图校正.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🥵" aria-label="🥵" data-icon="🥵" aria-hidden="true" style="transform: translateY(0px);"></span>Arcgis：变换坐标系将中国地图校正</a><img class="emoji" draggable="false" alt="🥵" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" height="18px" style="max-width: 100%;">。]]></description><link>软件\地理绘图\arcgis：深入了解坐标.html</link><guid isPermaLink="false">软件/地理绘图/Arcgis：深入了解坐标.md</guid><pubDate>Thu, 29 Aug 2024 02:55:56 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408291040437.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408291040437.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[QGIS 导入图层]]></title><description><![CDATA[ 
 <br>QGIS是一个绘制地图的软件，使用方式和 Arcgis 差不多。<br>但是从可扩展性和可操作性来看，QGIS 比 Arcgis 更容易受到青睐。<br><br>安装并打开 QGIS 之后，可以自定义工作目录：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081923093.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以将比较常用的目录添加到收藏夹，以供访问。<br>将 shp 文件拖到 图层 就可以进行预览：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081924475.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>右侧可以更改 图层样式：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081924897.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081925775.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
：选择带有地理位置的 csv 文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081926888.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>QGIS 在这一点比 Arcgis 方便很多，不会莫名其妙要求你包含 object_ID 字段，直接拖进来就可以了。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081928604.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
坐标轴一般来说是 Beijing1954 或者 Beijing WGS1984。可以逐个尝试。<br>现在稍微可以调整散点大小形状等：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081929917.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用 标注 功能添加文字：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081930071.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>调整完之后点击  即可。]]></description><link>软件\地理绘图\qgis：绘制xy散点图.html</link><guid isPermaLink="false">软件/地理绘图/QGIS：绘制XY散点图.md</guid><pubDate>Wed, 31 Jul 2024 01:31:46 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[什么是反距离加权差值？]]></title><description><![CDATA[ 
 <br>请在看这篇教程之前首先阅读：<br>
<a data-href="Surfer：克里金插值制作贴合的等值线图" href="软件\地理绘图\surfer：克里金插值制作贴合的等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：克里金插值制作贴合的等值线图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
<a data-href="Surfer：使用z-score克里金插值等值线图" href="软件\地理绘图\surfer：使用z-score克里金插值等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🥵" aria-label="🥵" data-icon="🥵" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：使用z-score克里金插值等值线图</a><img class="emoji" draggable="false" alt="🥵" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" height="18px" style="max-width: 100%;"><br><br>反距离加权插值法（Inverse Distance Weighting, IDW）是一种常用的地理空间插值方法，主要用于根据已知点的数据估计未知点的数据。这种方法假设数据点间的影响随距离的增加而减少，即距离已知点越近的未知点，其属性值越倾向于接近这个已知点的属性值。<br>为什么使用反距离加权而不用克里金？
分情况讨论。克里金法具有外推 z 值的属性，因此某些情况下会导致不合理的情况。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408211556132.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在这样的情况下，我们偏向使用反距离加权差值法：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408211558902.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>非常简单。首先获得 经纬度 及对应的频率（推荐使用 z 值），做法参见：<a data-href="Surfer：使用z-score克里金插值等值线图" href="软件\地理绘图\surfer：使用z-score克里金插值等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🥵" aria-label="🥵" data-icon="🥵" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：使用z-score克里金插值等值线图</a><img class="emoji" draggable="false" alt="🥵" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" height="18px" style="max-width: 100%;">, <a data-href="python：根据频数求z-score" href="软件\python\数据科学与格式转换\python：根据频数求z-score.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="😮‍💨" aria-label="😮‍💨" data-icon="😮‍💨" aria-hidden="true" style="transform: translateY(0px);"></span>python：根据频数求z-score</a><img class="emoji" draggable="false" alt="😮‍💨" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f62e-200d-1f4a8.svg" height="18px" style="max-width: 100%;">。<br>
获得数据之后进行：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408211600217.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
其余的设置不变，请参见 <a data-href="Surfer：克里金插值制作贴合的等值线图" href="软件\地理绘图\surfer：克里金插值制作贴合的等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：克里金插值制作贴合的等值线图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。]]></description><link>软件\地理绘图\surfer：反距离加权插值制作贴合的等值线图.html</link><guid isPermaLink="false">软件/地理绘图/Surfer：反距离加权插值制作贴合的等值线图.md</guid><pubDate>Wed, 21 Aug 2024 08:02:01 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备]]></title><description><![CDATA[ 
 <br>Arcgis 软件教程在这：<a data-href="Arcgis Pro：使用克里金绘制单倍群等值线图" href="软件\地理绘图\arcgis-pro：使用克里金绘制单倍群等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🥵" aria-label="🥵" data-icon="🥵" aria-hidden="true" style="transform: translateY(0px);"></span>Arcgis Pro：使用克里金绘制单倍群等值线图</a><img class="emoji" draggable="false" alt="🥵" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" height="18px" style="max-width: 100%;"><br><br>
<br>首先下载 Surfer 软件，这里有百度网盘下载链接：

<br>链接:  <a rel="noopener nofollow" class="external-link" href="https://pan.baidu.com/s/1iSubREjN6j7fTxmfmIWeOw?pwd=iskw" target="_blank">https://pan.baidu.com/s/1iSubREjN6j7fTxmfmIWeOw?pwd=iskw</a> 提取码: iskw 


<br>然后下载 shp 文件，自己去找。
<br>最后可以下载这里的示例数据，简单尝试一下： 

<br><a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E5%85%8B%E9%87%8C%E9%87%91%E6%8F%92%E5%80%BC%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE.csv?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=10000001722387802&amp;Signature=FBzG6fgNJQapJOXQC%2F3SPkukFO0%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E5%85%8B%E9%87%8C%E9%87%91%E6%8F%92%E5%80%BC%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE.csv?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=10000001722387802&amp;Signature=FBzG6fgNJQapJOXQC%2F3SPkukFO0%3D" target="_blank">下载链接</a>


<br><br> <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310903283.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
点击 打开，选择一个合适的 shp 文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310909416.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
选中 shp 文件，立刻点击 导出，导出为一个 bln 文件<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310909251.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310910424.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>现在，导入 csv 数据：<br>
数据由几列组成：<br>Longitude,Latitude,value
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310911088.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
然后接着导入 bln 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310912553.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
开始制作克里金插值：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310912545.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310912804.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310913864.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里注意，请看上图，重点调整以下选项：<br>
<br>X 方向和 Y 方向：这里主要调整最终产生的克里金插值的矩形大小。如果你喜欢更大的矩形，那么就把最小值变得更小，最大值变得更大。但是这样做会让画面很模糊，因此可以增大节点数，从而提高分辨率。
<br>不分配数据的多边形文件指的是，我们可以根据 bln 文件来删除不想看到克里金插值图形的地方（例如海洋等）。在这里我们选择刚才导出的 bln 文件，然后选择 外部白化。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310913434.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
现在你得到了乱七八糟的图形。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310915458.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>将需要的图层拖动到一起，会自动进行坐标系对齐！：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310919157.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
填充等值线，选择美丽的颜色，然后按照自己的需求调整即可！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407310921367.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
最后 导出 为 PDF 即可！]]></description><link>软件\地理绘图\surfer：克里金插值制作贴合的等值线图.html</link><guid isPermaLink="false">软件/地理绘图/Surfer：克里金插值制作贴合的等值线图.md</guid><pubDate>Sat, 03 Aug 2024 13:46:47 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[计算公式]]></title><description><![CDATA[ 
 <br>什么叫做 z-score？
这是一种归一化的手段。具体请去百度。
<br><br>由于我们一般关注的是频率数据，所以在这个背景下，计算如下<a data-footref="1" href="about:blank#fn-1-a9718c06e0409e5e" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>：<br><br>
<br> 是群体中的单倍群的频率
<br> 是群体中所有单倍群的平均值
<br> 是标准差
<br><br>由于我的脚本是按照省份进行计算的，所以至少需要这几项<br>ID,Province,Haplogroup,Latitude,Longitude
1,Sichuan,D4,30,100
复制<br>请自己准备好上述数据。<br><br>这个脚本是进行累加的（会把下游单倍群累加到上游作为单倍群，所以还需要准备单倍群<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1722697016&amp;Signature=CuMBXp2%2FfK78iY3BbdmF%2Bs7fQwA%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1722697016&amp;Signature=CuMBXp2%2FfK78iY3BbdmF%2Bs7fQwA%3D" target="_blank">列表文件</a>）<br>
Y 染色体的代码更简单，请自己仿照下列写。<br>import pandas as pd

# 读取并解析单倍群列表文件
def parse_haplogroup_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        haplogroup_lines = f.readlines()

    haplogroup_tree = {}
    stack = []

    for line in haplogroup_lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
        indent_level = len(line) - len(stripped_line)
        while len(stack) &gt; 0 and stack[-1][1] &gt;= indent_level:
            stack.pop()
        if stack:
            parent = stack[-1][0]
            haplogroup_tree.setdefault(parent, []).append(stripped_line)
        stack.append((stripped_line, indent_level))

    return haplogroup_tree

# 查找特定单倍群的所有下游单倍群
def find_all_descendants(haplogroup, haplogroup_tree):
    descendants = []
    stack = [haplogroup]
    while stack:
        current = stack.pop()
        if current in haplogroup_tree:
            children = haplogroup_tree[current]
            descendants.extend(children)
            stack.extend(children)
    return descendants

# 替换下游单倍群为上游单倍群，并记录未找到的单倍群
def replace_haplogroups(data, haplogroup_name, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    found_haplogroups = set(data['Haplogroup'].unique())
    missing_haplogroups = [hap for hap in descendants if hap not in found_haplogroups]
    
    if missing_haplogroups:
        print(f"The following haplogroups were not found in the dataset: {', '.join(missing_haplogroups)}")
    
    data['Haplogroup'] = data['Haplogroup'].apply(lambda x: haplogroup_name if x in descendants else x)
    return data

# 加载数据
#file_path = '阿尔泰语系.csv'
file_path = '数据清洗整理（合并满族样本）.csv'
data = pd.read_csv(file_path)

# 加载并解析单倍群列表文件
haplogroup_file_path = '线粒体单倍群phylotree(version17).txt'
haplogroup_tree = parse_haplogroup_file(haplogroup_file_path)

# 获取单倍群在各省的数量
haplogroup_name = input("Please input your haplogroup:(eg:D4)")
data_replaced = replace_haplogroups(data, haplogroup_name, haplogroup_tree)

# 过滤指定单倍群并统计各省数量
def count_haplogroup_by_province(haplogroup_name, data, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    
    filtered_data = data[data['Haplogroup'] == haplogroup_name]
    province_counts = filtered_data['Province'].value_counts().reset_index()
    province_counts.columns = ['Province', 'Value']
    
    total_counts = data.groupby('Province').size().reset_index(name='Total')
    province_counts = province_counts.merge(total_counts, on='Province', how='left')
    
    # 获取每个省的第一次出现的纬度和经度
    province_info = data[['Province', 'Latitude', 'Longitude']].drop_duplicates(subset='Province')
    province_counts = province_counts.merge(province_info, on='Province', how='left')
    
    # 过滤Total小于200的省份
    province_counts = province_counts[province_counts['Total'] &gt;= 200]
    
    return province_counts

province_counts = count_haplogroup_by_province(haplogroup_name, data_replaced, haplogroup_tree)

countstocsv = input("是否输出数量分布文件？请回答：（YES/NO）")
if countstocsv == "YES":
    province_counts.to_csv(f"C:/Users/victo/Desktop/Distribution_{haplogroup_name}.csv",sep=",",encoding='utf-8')

# 计算z-score
province_counts['Mean'] = province_counts['Value'].mean()
province_counts['SD'] = province_counts['Value'].std()
province_counts['z'] = (province_counts['Value'] - province_counts['Mean']) / province_counts['SD']

# 重新排列列的顺序，并生成新的文件
zscore_file = province_counts[['z','Latitude', 'Longitude']]

# 保存到CSV文件
output_file_path = f"C:/Users/victo/Desktop/Distribution_{haplogroup_name}_zscore.csv"
zscore_file.to_csv(output_file_path, sep=",", encoding="utf-8", index=False)

print(f"File saved to {output_file_path}")

复制<br>现在你得到了数据：<br>z,Latitude,Longitude
1.0,23,110
复制<br>将这些数据放入软件。<br>
参照 <a data-href="Surfer：克里金插值制作贴合的等值线图" href="软件\地理绘图\surfer：克里金插值制作贴合的等值线图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Surfer：克里金插值制作贴合的等值线图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
下列改变，其它不变。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408032201430.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>
<br>
<br><a data-href="2019 MBE 河流流域塑造了汉族的母系遗传景观" href="文献及报道\文献\2024年阅读\1-6月\2019-mbe-河流流域塑造了汉族的母系遗传景观.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="😀" aria-label="😀" data-icon="😀" aria-hidden="true" style="transform: translateY(0px);"></span>2019 MBE 河流流域塑造了汉族的母系遗传景观</a><img class="emoji" draggable="false" alt="😀" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f600.svg" height="18px" style="max-width: 100%;"><a href="about:blank#fnref-1-a9718c06e0409e5e" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\地理绘图\surfer：使用z-score克里金插值等值线图.html</link><guid isPermaLink="false">软件/地理绘图/Surfer：使用z-score克里金插值等值线图.md</guid><pubDate>Sat, 03 Aug 2024 14:16:24 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[构建系统发育树的重要步骤]]></title><description><![CDATA[ 
 <br><br>如果你的序列的某些区域特别容易突变，那可能导致基于这些区域的系统发育分析结果不准确或误导。因此，我们需要选择变异较低的保守区域进行系统发育分析，以提高树的准确性和解析力。<br><br>将得到的序列提交Gblock在线服务器（<a rel="noopener nofollow" class="external-link" href="http://www.phylogeny.fr/one_task.cgi?task_type=gblocks" target="_blank">http://www.phylogeny.fr/one_task.cgi?task_type=gblocks</a>），得到保守区的序列 align_cured.fasta。<br>这个网站目前缺乏维护，已经出现了很多替代品。这里推荐Trimal软件，可以自动修剪去除不可靠的区域：<a data-href="Trimal：自动修剪序列不可靠区域" href="软件\其它生信软件\t-z\trimal：自动修剪序列不可靠区域.html" class="internal-link" target="_self" rel="noopener nofollow">Trimal：自动修剪序列不可靠区域</a>。<br><br>不能建树<br><br>可以建树<br><br>如果你的序列突变过多，导致饱和，那么就无法反映正确的系统发育关系。需要进行检测。<br><br>通常有两种方法：  <br>
<br>第一种是PAUP 软件。
<br>第二种用DAMBE 软件。<br>
这里使用第二种。DAMBE：核苷酸替代饱和度检测<br>
<br><br>序列比对的目的是获得长度一样的序列。<br><br>多种软件都可以用来比对：<br>
<br><a data-href="MEGA" href="MEGA" class="internal-link" target="_self" rel="noopener nofollow">MEGA</a>
<br><a data-href="MAFFT" href="MAFFT" class="internal-link" target="_self" rel="noopener nofollow">MAFFT</a>
<br>MUSCLE
<br>……
<br>推荐你看看这个<a data-href="序列比对（Align）软件评测" href="软件\其它生信软件\使用心得\序列比对（align）软件评测.html" class="internal-link" target="_self" rel="noopener nofollow">序列比对（Align）软件评测</a><br>
再看看这个序列比对的思考<br><br><a data-href="序列比对（Align）软件评测" href="软件\其它生信软件\使用心得\序列比对（align）软件评测.html" class="internal-link" target="_self" rel="noopener nofollow">序列比对（Align）软件评测</a><br>转载自如下：
<a data-tooltip-position="top" aria-label="https://www.yuque.com/wusheng/gw7a9p/otk3ex" rel="noopener nofollow" class="external-link" href="https://www.yuque.com/wusheng/gw7a9p/otk3ex" target="_blank">序列比对前须知(1) (yuque.com)</a>
<br><br>
<br>全局比对（global alignment）  :它是假设两条序列在整个长度上是相似的，然后从头到尾比较两条序列的最佳匹配，适合高度相关等长序列比对。不适用于发散的不同长度的序列，因为它不能识别两条序列中高度相似的局部序列。  
<br>局部比对（local alignment）  :不考虑两条序列全局相似，而是找两条序列中高度相似的局部区域而不考虑其他区域，适合包含相似模块分散的生物序列，可以找出domain（保守区）和motif。  
<br>两种策略的区别  ：假设如下两条序列Query和Subject，分别采用全局比对和局部比对比对策略，从下图红色方框里可以看出差异<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222910.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>有无Gap比对：两条序列，进行有空位比较和无空位比较的差异如下图：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222925.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>不管采用哪种策略，比对算法基本上是相似的：  <br>
<br>点阵方法：构造一个二维矩阵。很容易识别序列中高度相似的区域，它在识别染色体重复（两条相同序列，即自己和自己比）和比较两个高度相关的基因组中的基因顺序的保守性非常有用。缺点：很难构造多序列比对。
<br>动态规划方法：匹配两条序列中所有可能字符，也是构造一个二维矩阵确定最优比对方法。里面引入“空位罚分”，即代表插入和删除的空位，因为在自然进化中插入和删除的发生频率比替换相对少。而且开始一个新的空位和拓展一个已经存在的空位付出的代价是不同的（原理是一旦插入和删除发生，那么临近的一些残基很容易插入和删除），两者会有不同的罚分。序列末尾不进行罚分，因为实际中很多同源序列是不等长的，如果末端进行了空位罚分反而是不实际的结果。  
<br>启发式算法：一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例 的 一个可行解，该可行解与最优解的偏离程度一般不能被预计。  
<br><br>
<br>全局比对的动态规划算法：经典的算法是Needleman-Wunsch算法。它必须对序列从头到尾进行计算得到最高比对得分。缺点，关注全长的最大比对的缺点是找不到局部的序列相似。对于发散序列和具有不同结构域的序列，这种方法达不到最理想的比对。全局两两比对的web程序是GAP.  
<br>局部比对的动态规划算法：正常比对序列中，两条被比对序列的分离水平是不知道的，两条序列的长度也可能不同，这种情况下，识别局部相似性比去对比包含残基的整个序列更加有意义。第一个运用动态规划进行局部序列比对的算法是Smith-Waterma算法。  
<br>&nbsp;<br>
这种算法下的局部比对分为两种：<br>
1. local alignment (Smith-Waterman) with affine gap costs (Gotoh)<br>
2. local alignment with generalized affine gap costs (Altschul)<br>
全局比对中，最终结果受到选用得分矩阵的影响，而局部比对的目标是找到局部最高分。这种方法适应于对分散序列和具有来自多个不同源的区域序列，目前大多数两两比对程序基本采用局部比对策略。<br><br>
<br>动态规划算法 &nbsp;可得到最优解，但是计算量非常大，实际中很难用于多序列比对。  
<br>启发式算法（heuristic algorithm)  

<br>渐进法（progressive methods）：Clustal, T-Coffee, MUSCLE  
<br>迭代法（iterative methods）：PRRP, DIALIGN  
<br>其它算法：Partial Order Algorithm、profile HMM、meta-methods (<a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>)…<br>
注：启发式算法的计算过程也有可能利用到动态规划算法，比如Clustal采用的渐进式算法：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603223405.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br><br>&nbsp;1. 核苷酸得分矩阵  ：核苷酸得分矩阵相对简单，对匹配位置赋予一个正值或者高分，对失配位置赋予一个负值或者低分。但是这种方法不符合实际，观察显示转换（嘌呤与嘌呤或者嘧啶与嘧啶之间的替换）发生频率比颠换（嘧啶与嘌呤之间的替换）高，因此，需要一个反应不同残基替换发生频率不同的更加复杂的统计模型。<br>
2. 氨基酸的替换矩阵 ：氨基酸的替换矩阵比较复杂，某种氨基酸很容易被具有相似理化性质的其他氨基酸替换，而很难被具有不同理化性质的氨基酸替换。而不同理化性质的氨基酸替换可能导致结构和功能的缺失，这种会导致分类的替换是很少被进化所选择的。  氨基酸替换矩阵是一个20×20矩阵，它用来反映氨基酸被替换的可能性。经验上的矩阵，包括PAM矩阵和BLOSUM矩阵。  <br><br>PAM的意思是可接受的点突变，观察到的突变不会改变蛋白质的一般功能，观察到的氨基酸突变被认为是自然选择接受了的。一个PAM单位被定义为有1%的氨基酸位点发生了变化或者100氨基酸有1个发生突变，但这并不意味100次PAM后，每个氨基酸都发生变化，因为其中一些位置可能会经过多次突变，甚至可能会变回到原来的氨基酸。一个特定残基对的PAM分数是通过一个多阶段过程得到的，这个过程包括：<br>
1. 计算相对突变率。一个特定氨基酸被同源氨基酸替换的总数除以在整个比对中这种氨基酸出现的总数；  &nbsp;<br>
2. 用随机替换率对预期的氨基酸替换率进行标准化，把标准化的突变率除以特定氨基酸出现频率，然后取以10为底的对数。把结果取整后填入替换矩阵，这个矩阵就可以反映氨基酸替换的可能性。<br>
3. 对于较分散序列的高阶PAM矩阵是通过对PAM1矩阵相乘推出来的，例如，PAM80就是对PAM1矩阵自乘80次得到的。一个PAM80矩阵只相当于观察到突变率的50%，PAN250表示一致性为20%。<br>
4. 缺点：PAM矩阵构建过程中，只是之间观察了PAM1中的基于一小堆极近相近序列中的残基替换（这个要从PAM矩阵的来源说起，当时构建这个矩阵用的序列是21组非常接近的蛋白质序列），PAM矩阵对于分散序列的比对是不可靠的。  PAM矩阵数字与序列相似度的对应关系：  <br><br>注意:PAM250矩阵&nbsp;→&nbsp;对应这些用于比对的序列相似度估计在14%&nbsp;–&nbsp;27%，一般PAM250矩阵对应序列相似度在20%。  <br><br>为了弥补PAM的缺点，BLOSUM矩阵应用而生，BLOSUM矩阵与PAM矩阵的不同之处在于：用于产生矩阵的蛋白质家族及多肽链数目，BLOSUM比PAM大约多20倍。BLOSUM矩阵不用推断，而是用实际上所选序列的残基一致性的比例来构建矩阵。与PAM矩阵的阶数相反，BLOSUM矩阵阶数越低代表序列越分散。比如BLOSUM60，这个矩阵首先寻找氨基酸模式，即有意义的一段氨基酸片断（如一个结构域及其相邻的两小段氨基酸序列），分别比较相同的氨基酸模式之间氨基酸的保守性（某种氨基酸对另一种氨基酸的取代数据），然后，以所有&nbsp;60％保守性的氨基酸模式之间的比较数据为根据，产生BLOSUM60；同理，以所有80％保守性的氨基酸模式之间的比较数据为根据，产生BLOSUM80。  <br><br>&nbsp; ①PAM1矩阵是通过一个进化模型得到的，而BLOSUM矩阵完全是由观测值构成，因此，BLOSUM矩阵可能没有PAM矩阵那么强的进化上的实际意义，这也是PAM矩阵被常用来重构系统发育树的原因。<br>
②PAM矩阵对发散序列可能不符合实际。<br>
③BLOSUM矩阵是完全通过保守序列的局部比对得到的，而PAM1是对包含保守和变化区域的整个序列全局比对得到的。这是BLOSUM矩阵更加适用于搜索数据库和寻找蛋白质中保守区域的原因。一些经验上测试BLOSUM矩阵在局部比对的正确性上胜过PAM矩阵。  <br>  对于一个给定的矩阵，一个正的分数说明在一个同源序列的数据集里观察到的氨基酸替换频率比随机替换频率高；零分表示在一个同源序列的数据集里观察到的氨基酸替换频率和随机替换频率相等；负数一个同源序列的数据集里观察到的氨基酸替换频率比随机替换频率低，这通常发生在不相似残基之间。  <br><br>序列比对本身就是一个随机问题，我们要对这种随机进行检验，比对完后，会有一个P值。<br>
P值的解读：  <br>
<br>如果P值小于10-100，表明两条序列是精确匹配的；  
<br>如果10-100&lt;P&lt;10-50，表明两条序列近似匹配；  
<br>如果10-50&lt;P&lt;10-5，表明两条序列有较近的同源关系；  
<br>如果10-5&lt;P&lt;10-1，表明两条序列可能存在较远的同源关系；  
<br>如果P&gt;10-1，那么这两条序列能匹配上可能是由于随机的关系；<br>
可以使用软件PRSS，用来评价序列两两比对在在统计学上显著性。  
<br><br>同一性和同源性这两个词经常被混淆。<br>
同源性只有高低之分，没有具体数值，属于"质”的属性；<br>
同一性才有具体数值，是“量”词。同一性数值越大，同源性越高。<br><br>多序列比对有3个东西是大家基本认同的：  <br>
<br>基因编码区的比对，需要采取基于密码子方式的比对；  
<br>目前多序列比对精确度：MAFFT&gt;Muscle&gt;Clustal；  
<br>多序列比对尽管是最基础的步骤，但是会影响后续的分析。  
<br>笔者最近发现了一个有趣的现象，不过也仅仅是这次处理的数据集才遇到，这让我重新审视了序列比对这个东西---多序列比对尽可能选用精确度高的工具，且一定要仔细检查比对后的Alignment文件。<br>
因为有时，对于某些数据，某种工具和某种方法比对的结果不一定是正常的(准确的)。<br>
笔者从NCBI下载了70条某个基因的基因编码区序列（已知是高度相似的），每条序列都从ATG起始密码子开始，如下：(截图仅显示了42条）<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224023.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
选用了MEGA X里的Muscle(Codons)，也就是Muscle的基于密码子的比对方法，比对结果部分截图如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224033.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
看上去好像有哪里不对劲，竟然在起始密码子(ATG)前引入了gap，尽管引入的gap数量是3个(3的倍数），但是gap引入位置不对，比对结果不合理，或者说前面9个碱基没比对正确。<br>
换用MEGA X里的ClustalW(Codons)进行多序列比对，比对结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224058.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
还是有很多序列在起始密码子(ATG)前引入了gap，比对结果不合理，和Muscle(Codons)的结果相差无几。  <br><br>换用MEGA X里的Muscle，这次采用不基于密码子的比对方法，比对结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224121.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
嗯，结果看上去，所有序列都是以ATG起始了，而且引入的gap的数量也是3的倍数。  <br>接下来看看，这种引入gap从生物学角度来看是否合理：  <br>
以EF203067和MF094681为例，其生物学意义是，如果考虑EF203067来自于MF094681的话，MF094681在其第二个密码子ACT在A和C之间插入了3个碱基(GTA),于是产生了EF203067的前9个碱基ATGAGTACT，相比于MF094681的前6个碱基“ATGACT”而言，这种插入碱基的方式，使得EF203067在该位置只是多了一个氨基酸而并没有发生移码突变，其他序列的插入碱基方式也没有造成移码突变，而这些序列已知是高度相似的，所以这种通过插入碱基而突变的方式是合理的。尽管有一定的生物学意义，但是基因编码区不基于密码方式子比对，结果可能不太可靠。
<br>换用MEGA X里的ClustalW，也采用不基于密码子的比对方法，结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224457.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
和ClustalW基于密码子的比对方法的结果差不多，在ATG前引入了gap，比对结果不合理。<br><br>主要是数据的引起的。  由于这个插入发生的位置比较特殊，非常靠前，而比对中引入gap本身就是一个带有概率的随机事件，所以Muscle和ClustalW在尽管采取了基于密码子的方式比对，结果还是不尽人意。<br>
通过上面的Muscle(Codons)和ClustalW(Codons)比对的结果，我们可以大致推测，自然界中真实的情况可能是，有几条序列在起始密码子ATG后面，从第4个碱基开始，直接插入了3个碱基，导致他们比其他序列在这个位置多了3个碱基(即多了1个氨基酸)。  <br><br><br>碱基的<a data-href="颠换（transversion）" href="术语\颠换（transversion）.html" class="internal-link" target="_self" rel="noopener nofollow">颠换（transversion）</a>和<a data-href="转换（transition）" href="术语\转换（transition）.html" class="internal-link" target="_self" rel="noopener nofollow">转换（transition）</a>发生的概率不同，序列内的碱基频率也可能影响对系统发育的判断，所以我们需要进行检测。<br>
具体的原理如下：图解核苷酸替换模型（Nucleotide substitution models）<br><br><br>软件很多，例如jModeltest,bmodeltest等。在这里推荐：<a data-href="iqtree：寻找最优模型及分区" href="软件\iqtree\iqtree：寻找最优模型及分区.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：寻找最优模型及分区</a>。<a data-tooltip-position="top" aria-label="http://user.qzone.qq.com/58001704/blog/1369539093" rel="noopener nofollow" class="external-link" href="http://user.qzone.qq.com/58001704/blog/1369539093" target="_blank">其它的软件</a><br><br>原理：系统发育树<br><br><br>系统发育树（也称为系统发育树或进化树）是一种分支图或树，根据它们的物理或遗传特征的异同说明各种生物物种或其他实体之间的进化关系。 地球上的所有生命都属于一个单一的系统发育树，表明有一个共同的祖先。<br>在有根的系统发育树中，每个具有后代的节点代表这些后代的推断最近的共同祖先，并且在某些树中，边长可以解释为时间估计。 每个节点都称为分类单元。 内部节点通常被称为假设的分类单元，因为它们无法直接观察到。 生物信息学、系统学和系统发育学等生物医学学科发现树木很有价值。 从低级生命形式到高级生命形式（例如存在大链）的阶梯状进展的古老概念产生了生命之树的概念。 早期对“分支”系统发育树的描述包括爱德华希区柯克（1800 年代）著作《基础地质学》中描述植物和动物之间地质关系的“古生物学图表”。查尔斯·达尔文 (Charles Darwin) 在 1859 年出版的《物种起源》(On the Origin of Species) 一书中用图表描绘了一棵进化“树”。 一个多世纪后，进化生物学家继续使用树状图来说明进化，因为它们有效地传达了物种形成是通过谱系的适应性和半随机分裂发生的概念。术语系统发育或系统发育源自古希腊语单词 v (phûlon)，意思是“种族”或“血统”，以及 (génesis)，意思是“起源”或“来源”。<br><br><img alt="Part-of-a-phylogenetic-tree-Li-and-Goldman-1998" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/Part-of-a-phylogenetic-tree-Li-and-Goldman-1998.webp" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>分支：系统发育树的分支代表进化谱系或血统。 它们连接节点或发散站点。
<br>Nodes：节点代表从它分化出来的物种或群体的共同祖先。 当节点表示不可观察的共同祖先时，它们可以称为内部节点或内部分支。
<br>树叶：&nbsp;系统发育树的尖端或叶子代表现存或活着的物种或群体。 终端类群是位于分支末端的类群。
<br>Root（根）：系统发育树的根代表所有包括的物种或群体的最近的共同祖先。 通常，它被描绘在树的底部。
<br>分支长度：&nbsp;在系统发育树中，分支长度表示沿特定分支发生的进化时，遗传物质的变化的数量。 就时间（例如，数百万年）或遗传变异（例如，DNA 替换）而言，持续时间可以量化。
<br><br>
<br>系统发育距离：&nbsp;两个物种或群体之间的系统发育距离量化了它们的相关程度。 通常，它是使用遗传或形态差异来估计的。
<br>类群：分类群是构成系统发育树的生物体或物种的类别。 单个物种到更高的分类水平，例如属、科、目，甚至更大的群体。
<br>分支：氏族是系统发育树中的单系实体，由祖先及其所有后代组成。 它们具有源自共同祖先的独特特征。
<br><br><br><img alt="Figure_20_01_01-768x231" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/Figure_20_01_01-768x231.webp" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
有根和无根树 | 这两种系统发育树都显示了生命的三个领域（细菌、古细菌和真核生物）之间的关系，但 (a) 有根树试图确定各种物种何时从一个共同祖先中分化出来，而 (b) 无根树则没有。 <br><br>
<br>有根的系统发育树说明了具有共同祖先的类群之间的进化关系。
<br>根代表树中所有分类群共享的最近的祖先。 它通常位于树的底部，并作为解释分类单元之间的分支模式和进化关系的参考点。
<br>有根树揭示了进化的方向性和分歧事件的相对时间线。
<br><img alt="rooted-phylogenetic-tree" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/rooted-phylogenetic-tree.webp" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
<br>无根系统发育树描述了缺乏指定共同根的分类群之间的进化关系。
<br>无根树不是单一的根，而是将分类关系描述为一系列连接的分支和节点。 无根树无法提供有关进化方向或分歧事件的相对时间线的信息，因为它们缺少根。
<br>无根树主要用于可视化类群之间的关系，但它们也可用于推断进化模式并识别密切相关的类群或群。
<br><img alt="un-rooted-phylogenetic-tree-1" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/un-rooted-phylogenetic-tree-1.webp" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
无根树<br><br><br><img alt="cladogram869610009617971312" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/cladogram869610009617971312.webp" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>软件很多，<a data-href="RAxML-NG：使用基础" href="软件\其它生信软件\k-s\raxml-ng：使用基础.html" class="internal-link" target="_self" rel="noopener nofollow">RAxML-NG：使用基础</a>，<a data-href="iqtree：基础操作" href="软件\iqtree\iqtree：基础操作.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：基础操作</a>, <a data-href="MrBayes：贝叶斯方法建树" href="软件\beast\mrbayes：贝叶斯方法建树.html" class="internal-link" target="_self" rel="noopener nofollow">MrBayes：贝叶斯方法建树</a>]]></description><link>软件\其它生信软件\使用心得\构建系统发育树的重要步骤.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/构建系统发育树的重要步骤.canvas</guid><pubDate>Wed, 11 Sep 2024 09:44:06 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222910.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222910.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[简易的mtDNA分析流程]]></title><description><![CDATA[ 
 具体代码具体代码流程逻辑软件需求<br>你可能需要更新一下 java 的版本：<br>sudo apt update
sudo apt install openjdk-17-jdk
复制<br>如果是全基因组的 VCF 文件<br>#!/bin/bash

# 指定VCF文件所在的目录
VCF_DIR="/mnt/d/MTDNA/10K_CPGDP/VCF/Extracted_MT"
# 指定合并后输出文件的位置（包括文件名）
OUTPUT_FILE="/mnt/e/Scientifc_software/haplocheck/vcf_input/merged_output.vcf"

# 压缩所有VCF文件并生成.gz文件
for file in ${VCF_DIR}/*.vcf; do
    echo "Compressing $file with bgzip..."
    bgzip -c "$file" &gt; "${file}.gz"
    if [ $? -ne 0 ]; then
        echo "Error compressing $file"
        exit 1
    fi
done

# 为压缩后的文件生成索引
for file in ${VCF_DIR}/*.vcf.gz; do
    echo "Indexing $file..."
    bcftools index "$file"
    if [ $? -ne 0 ]; then
        echo "Error indexing $file"
        exit 1
    fi
done

# 合并所有压缩并索引过的VCF文件
echo "正在合并文件到 $OUTPUT_FILE..."
bcftools merge ${VCF_DIR}/*.vcf.gz -o "$OUTPUT_FILE"
if [ $? -ne 0 ]; then
    echo "Error during merging VCF files."
    exit 1
fi

echo "已经成功合并至 $OUTPUT_FILE."

复制<br>cd /mnt/e/Scientifc_software/Haplogrep3
/mnt/e/Scientifc_software/Haplogrep3/haplogrep3 classify \
    --input /mnt/d/MTDNA/10K_CPGDP/VCF/Extracted_MT/xks_325samples_joint_call_MT.vcf \
    --tree phylotree-rcrs@17.2 \
    --output /mnt/c/Users/victo/Desktop/10K \
    --extend-report \
    --write-fasta
复制<br><a data-href="MAFFT" href="MAFFT" class="internal-link" target="_self" rel="noopener nofollow">MAFFT</a> 软件：<a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a><br><br>#!/bin/bash

# 定义输入和输出目录
input_dir="/mnt/d/MTDNA/10K_CPGDP/VCF/Oringin"
output_dir="/mnt/d/MTDNA/10K_CPGDP/VCF/Extracted_MT"

# 创建输出目录（如果不存在）
mkdir -p "$output_dir"

# 遍历输入目录中的每个 .vcf.gz 文件
for filepath in "$input_dir"/*.vcf.gz; do
    # 获取文件名
    filename=$(basename "$filepath")
    # 定义输出文件路径
    output_path="$output_dir/${filename/.vcf.gz/_MT.vcf}"
    
    # 使用 bcftools 提取MT染色体数据并解压为 .vcf 格式
    bcftools view -r MT "$filepath" -o "$output_path"  # 未使用 -Oz, 输出为解压后的 .vcf 文件
    
    # 为解压后的 .vcf 文件创建索引
    bcftools index --force "$output_path"  # --force 标志可以强制重新生成索引
    
    echo "MT染色体数据已从 $filename 提取并解压到 $output_path，且已创建索引。"
done


复制<br>或者使用遍历子文件夹：<br>#!/bin/bash

# 定义输入和输出目录
input_dir="/mnt/d/MTDNA/10K_CPGDP/VCF/Oringin"
output_dir="/mnt/d/MTDNA/10K_CPGDP/VCF/Extracted_MT"

# 创建输出目录（如果不存在）
mkdir -p "$output_dir"

# 使用 find 命令递归查找所有 .vcf.gz 文件
find "$input_dir" -type f -name "*.vcf.gz" | while read -r filepath; do
    # 获取文件名
    filename=$(basename "$filepath")
    # 定义输出文件路径
    output_path="$output_dir/${filename/.vcf.gz/_MT.vcf}"
    
    # 使用 bcftools 提取MT染色体数据并解压为 .vcf 格式
    bcftools view -r MT "$filepath" -o "$output_path"  # 输出为解压后的 .vcf 文件
    
    # 为解压后的 .vcf 文件创建索引
    bcftools index --force "$output_path"  # --force 标志可以强制重新生成索引
    
    echo "MT染色体数据已从 $filename 提取并解压到 $output_path，且已创建索引。"
done

复制<br>注意，有时候古代 DNA 的 BAM 文件存在多个文件一个命名的情况，这是由于一个样本测序多次。
我们的解决办法是，为每个 BAM 文件单独生成 fasta 文件。因为 mtDNA 的覆盖度一般比较高。
<br>#!/bin/bash

# 指定目录
input_dir="/mnt/d/MTDNA/古代DNA/WangNatCommun2019/output_files"
output_dir="$input_dir/with_rg"

# 创建输出目录
mkdir -p "$output_dir"

# 遍历目录中的 BAM 文件
for bam_file in "$input_dir"/*.bam; do
    # 提取文件名（去掉路径和扩展名）
    file_name=$(basename "$bam_file" .bam)
    
    # 生成包含 Read Group 的新 BAM 文件
    samtools addreplacerg -r "ID:$file_name" -r "SM:$file_name" -r "PL:ILLUMINA" \
    -o "$output_dir/${file_name}_rg.bam" "$bam_file"
done

cd $output_dir

for file in "$output_dir"/*.bam; do
    samtools index "$file"
done

复制<br>
<br>GATK 软件
<br>JAVA 软件
<br>samtools 软件
<br>haplocheck 软件
<br>haplogrep 软件
<br>bcftools 软件
<br><br>请前往官网下载这些软件至 Linux 系统或者 WSL 系统<br><br><br><br><br>#!/bin/bash

# 指定搜索的根目录
root_dir="/mnt/d/MTDNA/古代DNA/ZhuMBG2024"

# 线粒体DNA的染色体名称（可以修改为实际的名称，例如 'MT', 'chrM' 等）
mt_chr="MT"

# 输出文件夹名称
output_dir="$root_dir/output_files"

# 错误日志文件路径
error_log="$output_dir/error.txt"

# 创建输出文件夹（如果不存在）
mkdir -p "$output_dir"

# 清空或者创建 error.txt 文件
&gt; "$error_log"

# 检查 samtools 是否已安装
if ! command -v samtools &amp;&gt; /dev/null
then
    echo "Error: samtools is not installed. Please install it before running this script."
    exit 1
fi

# 递归查找 .bam 文件
find "$root_dir" -name "*.bam" | while read bam_file; do
    # 检查文件是否存在且可读
    if [ ! -r "$bam_file" ]; then
        echo "Error: Cannot read BAM file: $bam_file"
        echo "$bam_file" &gt;&gt; "$error_log"
        continue
    fi
    
    # 为每个 .bam 文件生成索引（如果索引文件不存在）
    if [ ! -f "${bam_file}.bai" ]; then
        echo "Indexing BAM file: $bam_file"
        samtools index "$bam_file"
        if [ $? -ne 0 ]; then
            echo "Error: Failed to index BAM file: $bam_file"
            echo "$bam_file" &gt;&gt; "$error_log"
            continue
        fi
    else
        echo "BAM index already exists for: $bam_file"
    fi

    # 提取线粒体DNA (基于 mt_chr 变量)
    output_file="$output_dir/$(basename ${bam_file%.bam}_${mt_chr}.bam)"
    echo "Extracting mitochondrial DNA ($mt_chr) to: $output_file"
    samtools view -b "$bam_file" "$mt_chr" &gt; "$output_file"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to extract mitochondrial DNA from $bam_file"
        echo "$bam_file" &gt;&gt; "$error_log"
        continue
    fi

    # 为生成的线粒体 BAM 文件建立索引
    echo "Indexing mitochondrial BAM file: $output_file"
    samtools index "$output_file"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to index mitochondrial BAM file: $output_file"
        echo "$bam_file" &gt;&gt; "$error_log"
    fi
done

# 打印完成信息
echo "Processing complete. All output files are in $output_dir. Any errors have been logged to $error_log."

复制<br>注意，完整的步骤解析在这里：<a data-href="GATK：简易的mtDNA分析流程" href="软件\其它生信软件\使用心得\gatk：简易的mtdna分析流程.html" class="internal-link" target="_self" rel="noopener nofollow">GATK：简易的mtDNA分析流程</a><br><br><br>#!/bin/bash
# 如果mtDNA的名称改变，请注意gatk命令改变。
# 指定根目录和输出文件夹
root_dir="/mnt/d/MTDNA/古代DNA/ZhuMBG2024/output_files"
reference_genome="/mnt/e/Scientifc_software/Reference/MT.fasta"
output_dir="$root_dir/vcf_output"

# 创建输出文件夹
mkdir -p "$output_dir"

# 遍历 BAM 文件，使用 GATK HaplotypeCaller 生成 vcf 文件
for bam_file in "$root_dir"/*.bam; do
    sample_name=$(basename "$bam_file" .bam)
    output_vcf="$output_dir/${sample_name}.vcf"
    echo "正在处理bam文件 $bam_file"

    /mnt/e/Scientifc_software/gatk-4.4.0.0/gatk --java-options "-Xmx32G" HaplotypeCaller \
    -R $reference_genome \
    -I "$bam_file" \
    -O "$output_vcf" \
    -L MT \
    --sample-ploidy 1 \
    #-ERC gvcf # 如果你希望生成g.vcf才使用这个参数。 
done

echo "所有的文件已经处理完成了！"

复制<br>注意，这里需要 rCRS 参考序列<br><br>你可以在这一步制定符合自己要求的过滤措施，也可以直接交给软件默认执行，毕竟 haplocheck 软件就是为这个而生的<br>#!/bin/bash

# 定义输入文件和输出文件
input_vcf="/mnt/d/MTDNA/10K_CPGDP/VCF/Filter/filtered_simple_hs37d5_260samples_MT.vcf"
output_vcf="/mnt/d/MTDNA/10K_CPGDP/VCF/Filter/filtered_simple_hs37d5_260samples_MT_filtered.vcf"

# 使用 bcftools 进行过滤
bcftools filter \
  -i 'QUAL&gt;1000 &amp; (AF&lt;0.1 | AF&gt;0.9) &amp; INFO/DP&gt;1000 &amp; FS&lt;60 &amp; SOR&lt;3 &amp; MQ&gt;40 &amp; MQRankSum&gt;-12.5 &amp; QD&gt;2.0 &amp; InbreedingCoeff&gt;0 &amp; ReadPosRankSum&gt;-8.0' \
  -o "$output_vcf" \
  -O v \
  "$input_vcf"

# 使用 bcftools 进行过滤
bcftools filter \
  -i 'QUAL&gt;1000 &amp; (AF&lt;0.1 | AF&gt;0.9)'  \
  -o "$output_vcf" \
  -O v \
  "$input_vcf"

echo "过滤完成，结果已输出到 $output_vcf"

复制<br># 定义路径
SOFT_PATH="/mnt/e/Scientifc_software/haplocheck"
OUT_DIR="/mnt/e/Scientifc_software/haplocheck/results"

# 切换到软件路径
cd $SOFT_PATH

# 遍历目录中所有的 .vcf.gz 文件
/mnt/e/Scientifc_software/haplocheck/haplocheck --raw --out "$OUT_DIR" /mnt/d/MTDNA/10K_CPGDP/VCF/Extracted_MT/xks_325samples_joint_call_MT.vcf

复制<br><br><br><br>如果是全基因组的BAM文件如果只包含mtDNA的BAM文件参见我发现有时候合并之后的vcf
反而会导致单倍型分型错误如果需要自定义，请参照下列如果不需要自定义，直接执行过滤后的vcf执行haplocheck使用haplogrep可以直接根据vcf生成fasta序列，包含任何在vcf中出现的indel等]]></description><link>软件\其它生信软件\使用心得\简易的mtdna分析流程.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/简易的mtDNA分析流程.canvas</guid><pubDate>Mon, 04 Nov 2024 01:22:31 GMT</pubDate></item><item><title><![CDATA[<strong>一、线粒体基因组主要分析内容流程图</strong>]]></title><description><![CDATA[ 
 <br>原文链接: <a data-tooltip-position="top" aria-label="https://www.yuque.com/wusheng-s0bue/cvxne8/spy5ekikgb75z80e" rel="noopener nofollow" class="external-link" href="https://www.yuque.com/wusheng-s0bue/cvxne8/spy5ekikgb75z80e" target="_blank">线粒体基因组分析流程 (yuque.com)</a><br><br><img src="https://cdn.nlark.com/yuque/0/2024/png/40551061/1709024234747-a4d99195-63f7-48e6-b473-6de9a5d44ed7.png#averageHue=%23fdfdfc&amp;clientId=uf2cae3a5-51f1-4&amp;from=paste&amp;id=u416810a9&amp;originHeight=444&amp;originWidth=853&amp;originalType=url&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1df18701-586a-4e09-81f0-45c3df948a4&amp;title=" referrerpolicy="no-referrer"><br><br>参考序列基因组: GenBank (KY085912)<br>原始reads RUN ID: SRR5602602 (Laurus nobilis L.)<br>更多更详细的测试：ERR1917165，ERR2206741，ERR268390，ERR964904<br><br><br><br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/Kinggerm/GetOrganelle" target="_blank">https://github.com/Kinggerm/GetOrganelle</a><br>其他使用者说明：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/bab69c3889b5" target="_blank">https://www.jianshu.com/p/bab69c3889b5</a><br><a rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/m0_62678513/article/details/122813505" target="_blank">https://blog.csdn.net/m0_62678513/article/details/122813505</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/665269253" target="_blank">https://zhuanlan.zhihu.com/p/665269253</a><br>
Wget https://github.com/Kinggerm/GetOrganelleGallery/raw/master/Test/reads/Arabidopsis_simulated.1.fq.gz

  

wget https://github.com/Kinggerm/GetOrganelleGallery/raw/master/Test/reads/Arabidopsis_simulated.2.fq.gz

复制<br>参数相关：<br>
get_organelle_from_reads.py -1 Arabidopsis_simulated.1.fq.gz -2 Arabidopsis_simulated.2.fq.gz -t 1 -o Arabidopsis_simulated.plastome -F embplant_pt -R 10

复制<br>-F：ORGANELLE_TYPE<br>-t: 线程数量<br>-o: 指定输出文件夹<br>-1 -2 ：指定原始输入数据<br><br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/RemiAllio/MitoFinder" target="_blank">https://github.com/RemiAllio/MitoFinder</a><br>其他使用者说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/e623292c9cbe" target="_blank">https://www.jianshu.com/p/e623292c9cbe</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/5bcb707c095e" target="_blank">https://www.jianshu.com/p/5bcb707c095e</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/611962326" target="_blank">https://zhuanlan.zhihu.com/p/611962326</a><br>参数相关说明：<br>
#Eg.

/Bio/xpl/software/MitoFinder -j project_name -1 left_fastq.gz -2 right_fastq.gz -r reference.gb -o 5 -p 5 -m 10

复制<br>-j 任务的id号，输出的结果文件也用id号命名<br>-r 近缘物种的已经注释的线粒体基因组文件<br>-o 指遗传密码类型<br>-p 使用最大的线程数<br>-m 最大允许的内存消耗<br><br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/YanCCscu/meangs" target="_blank">https://github.com/YanCCscu/meangs</a><br>其他使用者说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/89025d63a4c8" target="_blank">https://www.jianshu.com/p/89025d63a4c8</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/12d5f74c27f6" target="_blank">https://www.jianshu.com/p/12d5f74c27f6</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/610048488" target="_blank">https://zhuanlan.zhihu.com/p/610048488</a><br>example: <a rel="noopener nofollow" class="external-link" href="https://github.com/YanCCscu/MEANGS/tree/master/example" target="_blank">https://github.com/YanCCscu/MEANGS/tree/master/example</a><br>参数相关：<br>
meangs.py -1 SRR039541.3_1.clean.fq.gz -2 SRR039541.3_2.clean.fq.gz -o HumanMito -t 16 -n 2000000 -i 300 --deepin

复制<br><br>使用软件：MitoFinder v1.4.1<br>使用说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/RemiAllio/MitoFinder" target="_blank">https://github.com/RemiAllio/MitoFinder</a><br>其他使用者说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/e623292c9cbe" target="_blank">https://www.jianshu.com/p/e623292c9cbe</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/5bcb707c095e" target="_blank">https://www.jianshu.com/p/5bcb707c095e</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/611962326" target="_blank">https://zhuanlan.zhihu.com/p/611962326</a><br>example: <a rel="noopener nofollow" class="external-link" href="https://github.com/RemiAllio/MitoFinder/tree/master/test_case" target="_blank">https://github.com/RemiAllio/MitoFinder/tree/master/test_case</a><br>相关参数：<br>
mitofinder -j Hospitalitermes_medioflavus_NCBI -a Hospitalitermes_medioflavus_NCBI.fasta -r Hospitalitermes_medioflavus_NCBI.gb -o 5 -m 10 -p 10

  

#解释

-a &nbsp;assembly 序列

-j &nbsp;任务的id号，输出的结果文件也用id号命名

-r 近缘物种的已经注释的线粒体基因组文件

-o 指遗传密码类型，例如5代表无脊椎动物线粒体的遗传密码

-p 允许Mitofinder在运行时使用的最大线程数

-m 允许使用的计算机最大存储量

复制<br><br><br>使用软件：emboss<br>使用说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/google/emboss" target="_blank">https://github.com/google/emboss</a><br>其他使用者说明书：<br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/457001773" target="_blank">https://zhuanlan.zhihu.com/p/457001773</a><br><a rel="noopener nofollow" class="external-link" href="https://indexofire.github.io/pathongs/book/C01_Common-Utility/04_emboss/" target="_blank">https://indexofire.github.io/pathongs/book/C01_Common-Utility/04_emboss/</a><br><a rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/Cassiel60/article/details/88218706" target="_blank">https://blog.csdn.net/Cassiel60/article/details/88218706</a><br><a rel="noopener nofollow" class="external-link" href="https://max.book118.com/html/2017/0522/108259246.shtm" target="_blank">https://max.book118.com/html/2017/0522/108259246.shtm</a><br>example: <a rel="noopener nofollow" class="external-link" href="https://github.com/google/emboss/tree/master/testdata" target="_blank">https://github.com/google/emboss/tree/master/testdata</a><br>相关参数：<br>
usage: embossc [-h] [--color-output {always,never,if_tty,auto}] [--import-dir IMPORT_DIRS]

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[--generate {cc}] [--output-path OUTPUT_PATH]

&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;input_file

复制<br><br>使用软件：RepeatMasker<br>软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://github.com/rmhubley/RepeatMasker" target="_blank">https://github.com/rmhubley/RepeatMasker</a><br><a rel="noopener nofollow" class="external-link" href="https://www.repeatmasker.org/RepeatMasker/" target="_blank">https://www.repeatmasker.org/RepeatMasker/</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/ffdbedae80fa" target="_blank">https://www.jianshu.com/p/ffdbedae80fa</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/668013133" target="_blank">https://zhuanlan.zhihu.com/p/668013133</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/532849330" target="_blank">https://zhuanlan.zhihu.com/p/532849330</a><br><a rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/woodcorpse/article/details/73148464" target="_blank">https://blog.csdn.net/woodcorpse/article/details/73148464</a><br><a rel="noopener nofollow" class="external-link" href="https://www.repeatmasker.org/webrepeatmaskerhelp.html" target="_blank">https://www.repeatmasker.org/webrepeatmaskerhelp.html</a><br>example:<br>
基因组获取自：https://www.ncbi.nlm.nih.gov/genome/68823?genome_assembly_id=380814

复制<br>相关参数：<br>
RepeatMasker -pa 4 -species "Fusarium tricinctum" -poly -html -gff -dir repeat1 GCA_900382705.2_FTRI.INRA104.GCA2018.2_genomic.fna

#查看帮助RepeatMasker -h

#需要要额外注意的是:

#-dir 指定的输出结果路径，须提前建立好

#-species 指定物种，否则默认比对的是人类重复序列数据库

#如果使用本地的参考库，通过 -lib 指定，替代 -species

#-s、-q、-qq 等参数可控制序列比对的灵敏度，如果你的目标物种和参考物种不是很近，可能需要提升灵敏度

复制<br><br>使用软件：raxml<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/amkozlov/wwraxml-ng" target="_blank">https://github.com/amkozlov/wwraxml-ng</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/80e2c938e050" target="_blank">https://www.jianshu.com/p/80e2c938e050</a><br><a rel="noopener nofollow" class="external-link" href="http://www.chenlianfu.com/?p=2225" target="_blank">http://www.chenlianfu.com/?p=2225</a><br><a rel="noopener nofollow" class="external-link" href="https://bin-ye.com/post/2020/01/04/raxml-%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/" target="_blank">https://bin-ye.com/post/2020/01/04/raxml-%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/170587426" target="_blank">https://zhuanlan.zhihu.com/p/170587426</a><br>example: <a rel="noopener nofollow" class="external-link" href="https://github.com/amkozlov/raxml-ng/tree/master/test/src" target="_blank">https://github.com/amkozlov/raxml-ng/tree/master/test/src</a><br>相关参数:<br>
#Eg.

raxmlHPC-PTHREADS-SSE3 -f a -m GTRGAMMA -p 12345 -x 12345 -# 10 -s 20k.phy -n chr001.raxml -T 30

  

-f :表示执行快速 Bootstrap 分析并搜索最佳得分的 ML 树。

-x : 指定一个 int 数作为随机种子

-p :指定一个随机数作为 parsimony inferences 的种子。

-# :指定 bootstrap 的次数。

-m :指定核苷酸或氨基酸替代模型。

-s :指定输入文件。phy 格式的多序列比对结果。软件包中包含一个程序来将 fasta 格式转换为 phy 格式。

-n :后缀

-T :指定多线程运行的 CPUs 。

复制<br>使用软件：IQ-TREE<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/iqtree/iqtree2" target="_blank">https://github.com/iqtree/iqtree2</a><br>其他软件说明书：<a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/408382758" target="_blank">https://zhuanlan.zhihu.com/p/408382758</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/f830282866cd" target="_blank">https://www.jianshu.com/p/f830282866cd</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/df234ca0de71" target="_blank">https://www.jianshu.com/p/df234ca0de71</a><br>example:<br><a rel="noopener nofollow" class="external-link" href="https://github.com/iqtree/iqtree2/tree/master/example" target="_blank">https://github.com/iqtree/iqtree2/tree/master/example</a><br>相关参数:<br>
iqtree -s example.phy

复制<br><br>使用软件：GIREMI &nbsp;(Generalized Identification of RNA Editing by Mutual Information)<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/zhqingit/giremi" target="_blank">https://github.com/zhqingit/giremi</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.codenong.com/jsa4285564a4a9/" target="_blank">https://www.codenong.com/jsa4285564a4a9/</a><br><a rel="noopener nofollow" class="external-link" href="https://github.com/zhqingit/giremi/issues" target="_blank">https://github.com/zhqingit/giremi/issues</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/609303893" target="_blank">https://zhuanlan.zhihu.com/p/609303893</a><br>相关参数:<br>
-f, --fasta-ref &nbsp; &nbsp;FILE &nbsp; reference genome sequence file in fasta format (NOTE: the faidx index file generated by samtools should be saved in the same directory as this fasta file)

-l, --positions &nbsp; &nbsp;FILE &nbsp; the list of all filtered SNVs after removing likely sequencing errors or SNVs due to other artifacts (see our paper for details)

-o, --output &nbsp; &nbsp; &nbsp; FILE &nbsp; write output to FILE.res

-m, --min &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INT &nbsp; &nbsp;minimal number of total reads covering candidate editing sites &nbsp;[default: 5]

-p, --paired-end &nbsp; INT &nbsp; &nbsp;1:paired-end RNA-Seq reads; 0:single-end [default: 1]

-s, --strand &nbsp; &nbsp; &nbsp; INT &nbsp; &nbsp;0:non-strand specific RNA-Seq; 1: strand-specific RNA-Seq and read 1 (first read for the paired-end reads) is sense to RNA; 2: strand-specific RNA-Seq and read 1 is anti-sense to RNA [default: 0]

复制<br>RNA编辑类型和效率。(A)每个组织中跨线粒体基因组的 RNA 编辑位点计数。(B)每个组织中 RNA 编辑位点位置的百分比。(C)每个组织中跨质体的 RNA 编辑位点的计数。(D)每个组织中 RNA 编辑效率的百分比。<br><br>使用软件：<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/krishnap25/mauve" target="_blank">https://github.com/krishnap25/mauve</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/411065032" target="_blank">https://zhuanlan.zhihu.com/p/411065032</a><br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/9df3352eb34d" target="_blank">https://www.jianshu.com/p/9df3352eb34d</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/359203720" target="_blank">https://zhuanlan.zhihu.com/p/359203720</a><br>example:<br><a rel="noopener nofollow" class="external-link" href="https://github.com/krishnap25/mauve/tree/main/examples" target="_blank">https://github.com/krishnap25/mauve/tree/main/examples</a><br><a rel="noopener nofollow" class="external-link" href="https://github.com/krishnap25/mauve/tree/main/tests" target="_blank">https://github.com/krishnap25/mauve/tree/main/tests</a><br>相关参数:<br>
#eg.

mauveAligner [options] &lt;seq1 filename&gt; &lt;sml1 filename&gt; ... &nbsp;&lt;seqN filename&gt; &lt;smlN filename&gt;

复制<br>使用软件：Mummer<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/mummer4/mummer" target="_blank">https://github.com/mummer4/mummer</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/2e184e5c15b7" target="_blank">https://www.jianshu.com/p/2e184e5c15b7</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/572010609" target="_blank">https://zhuanlan.zhihu.com/p/572010609</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/36347578" target="_blank">https://zhuanlan.zhihu.com/p/36347578</a><br>example: <a rel="noopener nofollow" class="external-link" href="https://github.com/mummer4/mummer/tree/master/examples" target="_blank">https://github.com/mummer4/mummer/tree/master/examples</a><br>相关参数:<br>
#eg

nucmer &nbsp;--prefix IRGSP1_DHX2 ~/reference/genome/IRGSP1.0/IRGSP-1.0_genome.fasta ~/reference/genome/rice_contigs/DHX2.fa

  

# 已有delta文件

dnadiff -d IRGSP1_DHX2.delta

# 未有delta文件

dnadiff IRGSP1_DHX2 ~/reference/genome/IRGSP1.0/IRGSP-1.0_genome.fasta ~/reference/genome/rice_contigs/DHX2.fa

复制<br>使用软件：circos<br>软件说明书：<a rel="noopener nofollow" class="external-link" href="https://github.com/vigsterkr/circos" target="_blank">https://github.com/vigsterkr/circos</a><br>其他软件说明书：<br><a rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/9c0d2b9d724e" target="_blank">https://www.jianshu.com/p/9c0d2b9d724e</a><br><a rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/462783428" target="_blank">https://zhuanlan.zhihu.com/p/462783428</a><br><a rel="noopener nofollow" class="external-link" href="https://github.com/node/circos-cn" target="_blank">https://github.com/node/circos-cn</a><br>example:<br>相关参数:<br>相关配置文件<br>
#指定染色体文件（绝对/相对路径+文件名）

karyotype = data/karyotype/karyotype.human.txt &nbsp;

#-----------------------------------------------------------------------------------

&lt;ideogram&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#这是定义染色体相关参数的标签，以&lt;/ideogram&gt;结尾，其中包括要设置的参数

&lt;spacing&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #定义染色体间隙宽度的标签，以&lt;/spacing&gt;，其中包括要设置的参数

default = 0.005r &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#r指的是圆的周长，设置0.5%圆的周长为间隙

#&lt;pairwise hsY;hs1&gt; &nbsp; &nbsp; &nbsp; #可以用&lt;pairwise&gt;标签特别指定某些染色体的间隙（用的是ID），因为在大多数文章中，都会留一个大间隙，来放label

#spacing = 20r &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#这里20r表示是相对default = 0.005r的20倍，也就是10%的圆的周长

#&lt;/pairwise&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#标签都要以&lt;/&gt;结尾，

&lt;/spacing&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#间隙定义结束，下面是对染色体样式的调整

radius &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 0.90r &nbsp;#轮廓的位置，这里的r指的是半径，由圆心到圆周上范围依次是0-1r，，超出部分将不再显示。

thickness &nbsp; &nbsp; &nbsp; &nbsp;= 20p &nbsp; &nbsp;#染色体整体的宽度，这里p指的是像素大小，也可以用r表示，1r=1500p

fill &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = yes &nbsp; &nbsp;#是否为染色体填充颜色，如果为yes，自动用第七列定义的颜色着色

stroke_color &nbsp; &nbsp; = dgrey &nbsp;#染色体边框的颜色，支持多种格式的输入，如：red或255,182,106

stroke_thickness = 2p &nbsp; &nbsp; #染色体边框的粗细

&lt;/ideogram&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; #定义染色体属性的标签结束

#-----------------------------------------------------------------------------------

#下面是每次都要复制粘贴上去的，他们属于circos自带的配置文件，用于调用颜色，距离，报错等信息

&lt;image&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#注意路径

&lt;&lt;include etc/image.conf&gt;&gt; #注意引用外部配置文件需要使用&lt;&lt;#&gt;&gt;

&lt;/image&gt;

&lt;&lt;include etc/colors_fonts_patterns.conf&gt;&gt;

#官方没有提到下面的文件，但是没有这个文件会报错，所以还是加上去

&lt;&lt;include etc/housekeeping.conf&gt;&gt;

复制]]></description><link>软件\其它生信软件\使用心得\线粒体基因组分析流程.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/线粒体基因组分析流程.md</guid><pubDate>Sun, 08 Sep 2024 13:55:09 GMT</pubDate><enclosure url="https://cdn.nlark.com/yuque/0/2024/png/40551061/1709024234747-a4d99195-63f7-48e6-b473-6de9a5d44ed7.png#averageHue=%23fdfdfc&amp;clientId=uf2cae3a5-51f1-4&amp;from=paste&amp;id=u416810a9&amp;originHeight=444&amp;originWidth=853&amp;originalType=url&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1df18701-586a-4e09-81f0-45c3df948a4&amp;title=" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.nlark.com/yuque/0/2024/png/40551061/1709024234747-a4d99195-63f7-48e6-b473-6de9a5d44ed7.png#averageHue=%23fdfdfc&amp;clientId=uf2cae3a5-51f1-4&amp;from=paste&amp;id=u416810a9&amp;originHeight=444&amp;originWidth=853&amp;originalType=url&amp;ratio=1.5&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u1df18701-586a-4e09-81f0-45c3df948a4&amp;title="&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[主要功能]]></title><description><![CDATA[ 
 <br>原文链接： <a data-tooltip-position="top" aria-label="https://gatk.broadinstitute.org/hc/en-us/articles/4403870837275-Mitochondrial-short-variant-discovery-SNVs-Indels" rel="noopener nofollow" class="external-link" href="https://gatk.broadinstitute.org/hc/en-us/articles/4403870837275-Mitochondrial-short-variant-discovery-SNVs-Indels" target="_blank">Mitochondrial short variant discovery (SNVs + Indels) – GATK (broadinstitute.org)</a><br><br>主要内容包括以下几个重点：<br>
<br>
线粒体基因组的挑战：线粒体基因组的环状结构、线粒体DNA插入核基因组形成的NuMTs以及低杂合子比例（heteroplasmy）给变异分析带来了困难。

<br>
变异发现流程：使用Mutect2工具，通过高灵敏度检测低杂合子变异，并采用不同的基因组断点对齐来追踪稀有线粒体变异的谱系。

<br>
预期输入：需要一个全基因组测序（WGS）的h38 CRAM或BAM文件，以及自染色体的覆盖率中位数（如果输入是CRAM文件，还需要参考的fasta文件）。

<br>
主要步骤：

<br>筛选仅保留映射到线粒体基因组的读数。
<br>将ChrM映射的BAM文件从对齐状态转换为未对齐状态。
<br>将未对齐的BAM文件与参考对齐的BAM文件和移位的参考对齐BAM文件进行合并。
<br>识别重复读数。
<br>收集BAM文件的覆盖率和性能指标。
<br>在对齐的BAM文件中调用变异。
<br>将输出的VCF文件从控制区和非控制区合并。
<br>合并输出VCF的统计文件。
<br>根据特定参数过滤变异调用。
<br>过滤掉黑名单中的位点。


<br>
工具和资源：流程中涉及的工具包括PrintReads、RevertSam、MergeBamAlignment、MarkDuplicates、CollectWgsMetrics、Mutect2、LiftoverVcf、MergeVcfs、Mutect2-MergeMutectStats、FilterMutectCalls、VariantFiltration等。此外，还提到了GATK资源包和多态性核线粒体变异（NuMTs）。

<br>
额外信息：提供了关于如何使用Mutect2调用体细胞突变以及发现体细胞短变异（SNVs + Indels）的额外信息链接。

<br>这个流程是为了解决线粒体基因组变异分析中的特定挑战，通过一系列精确的工具和步骤来提高变异检测的准确性和效率。<br><br><br>将 ChrM 映射的 BAM 文件从对齐状态转换为未对齐状态的步骤是为了解决线粒体基因组分析中的一些特定问题。这个步骤的目的和原因包括：<br>
<br>解决环状基因组的对齐问题：线粒体基因组是环状的，而参考基因组是线性的。在分析线粒体DNA时，由于其环状结构，不同的复制和断裂可能导致在参考基因组上的任意位置出现断点。这使得对齐和分析线粒体DNA变得复杂。通过将ChrM映射的BAM文件转换为未对齐状态，可以消除由于参考基因组断点位置任意性带来的问题。
<br>提高变异检测的灵敏度：在未对齐的BAM文件中，所有的读数都保留了原始的序列信息，包括插入和缺失（Indels）。这样可以在后续的分析中更准确地识别和追踪线粒体DNA中的变异，尤其是在非编码控制区域。
<br>处理NuMTs的挑战：线粒体DNA有时会插入到核基因组中，形成核线粒体DNA片段（NuMTs）。这些NuMTs可能会干扰线粒体基因组的准确映射和变异分析。通过将ChrM映射的BAM文件转换为未对齐状态，可以更容易地区分NuMTs和感兴趣的线粒体序列。
<br>允许使用不同的对齐策略：在未对齐的BAM文件中，可以采用不同的对齐策略来处理线粒体基因组的特定区域。例如，可以通过对齐到一个移位的参考序列来增加对控制区域的敏感性，从而考虑到个体间的变异。
<br>保留原始的质量和标签信息：虽然对齐信息被移除，但转换过程中会保留原始的碱基质量和对齐标签。这允许在后续的变异分析中使用这些信息，同时避免了由于对齐错误导致的假阳性变异调用。
<br>总的来说，这个步骤是为了提高线粒体变异分析的准确性和可靠性，特别是在处理环状基因组、NuMTs 以及低杂合子比例变异时。通过这种方式，研究人员可以更有效地识别和理解线粒体基因组中的变异。<br><br>将未对齐的BAM文件与参考对齐的BAM文件和移位的参考对齐BAM文件进行合并的步骤是线粒体变异发现流程中的关键环节，其作用包括：<br>
<br>增强变异检测的准确性：通过合并不同类型的BAM文件，可以确保在分析过程中考虑到所有可能的变异情况。未对齐的BAM文件包含了原始的序列信息，而参考对齐的BAM文件和移位的参考对齐BAM文件则提供了基于标准和移位参考基因组的对齐信息。这样可以更全面地识别和校验变异。
<br>处理线粒体基因组的特殊结构：线粒体基因组的环状结构和在核基因组中的插入（NuMTs）给变异检测带来了挑战。通过合并这些文件，可以更好地处理这些特殊情况，提高对线粒体特有变异的识别能力。
<br>提高对低杂合子变异的敏感性：线粒体变异通常具有低杂合子比例，这可能导致它们被误认为是测序噪声。合并不同类型的BAM文件可以提高对这些低频变异的敏感性，从而减少漏检。
<br>优化变异调用的策略：通过合并不同对齐状态下的BAM文件，可以采用更复杂的变异调用策略，比如在控制区域和非控制区域使用不同的参数和模型。这有助于更准确地区分真实变异和测序噪声。
<br>为后续分析提供更完整的数据：合并后的BAM文件包含了更全面的信息，这对于后续的变异注释、过滤和解释至关重要。这可以确保变异分析的结果更加可靠和有用。
<br>便于后续的VCF文件生成：在BAM文件合并后，可以更方便地生成和处理变异调用文件（VCF）。这一步为后续的变异分析和解释提供了一个统一和综合的数据集。
<br>总之，这一步骤是线粒体变异分析流程中不可或缺的一部分，它通过整合不同来源和状态下的数据，提高了变异检测的准确性和效率，并为后续的分析工作奠定了坚实的基础<br><br><br>samtools view -b ALL.bam chrM &gt; chrM_only.bam
复制<br><br>gatk RevertSam \
-I chrM_only.bam \
-O chrM_unaligned.bam
复制<br><br>This step merges the unaligned BAM file with the reference BAM file for the mitochondrial genome. The BAM file must also be aligned with the shifted mitochondrial BAM file. The shifted reference moves the breakpoint of the mitochondrial genome from the non-coding control region to the opposite side of the contig. This allows for sensitivity in the control region to account for variability across individuals.  <br>此步骤将未对齐的 BAM 文件与线粒体基因组的参考 BAM 文件合并。 BAM 文件还必须与移位的线粒体 BAM 文件对齐。偏移的参考将线粒体基因组的断点从非编码控制区移动到重叠群的另一侧。这允许控制区域的敏感性来解释个体之间的差异。<br>这个线粒体参考 BAM 文件从何而来？移位的 BAM 又从而何来？
我读了很多遍，觉得这个所谓的  reference BAM file 应该就是原始的 BAM 文件。
<br>如何得到移位了的 FATSA 文件呢？
可以使用 Python 来硬核解决。
<br>以下代码只需要运行一次，得到移位后的fasta即可。<br>from Bio import SeqIO
from Bio.SeqRecord import SeqRecord

# 加载原始的rCRS参考序列
with open("/home/luolintao/10K_HGDP/example/rcrs.fasta", "r") as original_fasta:
    for record in SeqIO.parse(original_fasta, "fasta"):
        sequence = record.seq

# 假设断点位置在8000
breakpoint = 8000
shifted_sequence = sequence[breakpoint:] + sequence[:breakpoint]

# 创建一个新的SeqRecord对象
new_record = SeqRecord(shifted_sequence, id=record.id, description="Shifted at position 8000")

# 保存新的移位参考序列
with open("/home/luolintao/10K_HGDP/example/shifted_rCRS.fasta", "w") as shifted_fasta:
    SeqIO.write([new_record], shifted_fasta, "fasta")
复制<br>接下来，我们需要对原始的 rCRS 和 shift 的 rCRS 进行建立索引以进行下一步研究：<br>bwa index shifted_rCRS.fasta
bwa index rCRS.fasta
复制<br>现在需要把 BAM 文件转到 fastq 文件，因为在使用 BWA 软件时必须要求如此：<br># 将BAM转换成FASTQ
samtools bam2fq chrM_unaligned.bam &gt; chrM_unaligned.fastq
# 使用BWA进行比对
bwa mem shifted_rCRS.fasta chrM_unaligned.fastq &gt; aligned.sam
复制<br>现在又需要把 SAM 文件转换成 BAM 文件，并排序、建立索引：<br># 将SAM转换为BAM
samtools view -bS aligned.sam &gt; aligned.bam

# 排序BAM文件
samtools sort aligned.bam -o shifted_aligned.bam

# 建立索引
samtools index shifted_aligned.bam

# 清理中间文件
rm aligned.sam
rm aligned.bam

复制<br>使用 GATK 合并 BAM 文件：<br>
在此之前，我们需要为 fasta 文件创建字典和索引：<br>gatk CreateSequenceDictionary -R rcrs.fasta -O rcrs.dict 

samtools faidx rcrs.fasta
复制<br>gatk MergeBamAlignment \
--REFERENCE_SEQUENCE rcrs.fasta \
--UNMAPPED_BAM chrM_unaligned.bam \
# --ALIGNED_BAM shifted_aligned.bam \
--OUTPUT merged.bam \
复制]]></description><link>软件\其它生信软件\使用心得\线粒体dna工作流程.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/线粒体DNA工作流程.md</guid><pubDate>Sun, 28 Jul 2024 08:36:41 GMT</pubDate></item><item><title><![CDATA[ClustalW]]></title><description><![CDATA[ 
 <br><br><br><a data-tooltip-position="top" aria-label="http://www.clustal.org/" rel="noopener nofollow" class="external-link" href="http://www.clustal.org/" target="_blank">这个软件</a>有多个版本，包括<br>
GUI (ClustalX), command line (ClustalW), web server。<br><br>这里是说明书：<br>&gt;&gt; HELP 9 &lt;&lt;             Help for command line parameters

                DATA (sequences)

-INFILE=file.ext                             :input sequences.
-PROFILE1=file.ext  and  -PROFILE2=file.ext  :profiles (old alignment).


                VERBS (do things)

-OPTIONS            :list the command line parameters
-HELP  or -CHECK    :outline the command line params.
-FULLHELP           :output full help content.
-ALIGN              :do full multiple alignment. # 这个可能有用
-TREE               :calculate NJ tree.
-PIM                :output percent identity matrix (while calculating the tree)
-BOOTSTRAP(=n)      :bootstrap a NJ tree (n= number of bootstraps; def. = 1000).
-CONVERT            :output the input sequences in a different file format.


                PARAMETERS (set things)

***General settings:****
-INTERACTIVE :read command line, then enter normal interactive menus
-QUICKTREE   :use FAST algorithm for the alignment guide tree
-TYPE=       :PROTEIN or DNA sequences
-NEGATIVE    :protein alignment with negative values in matrix
-OUTFILE=    :sequence alignment file name
-OUTPUT=     :CLUSTAL(default), GCG, GDE, PHYLIP, PIR, NEXUS and FASTA
-OUTORDER=   :INPUT or ALIGNED
-CASE        :LOWER or UPPER (for GDE output only)
-SEQNOS=     :OFF or ON (for Clustal output only)
-SEQNO_RANGE=:OFF or ON (NEW: for all output formats)
-RANGE=m,n   :sequence range to write starting m to m+n
-MAXSEQLEN=n :maximum allowed input sequence length
-QUIET       :Reduce console output to minimum
-STATS=      :Log some alignents statistics to file

***Fast Pairwise Alignments:***
-KTUPLE=n    :word size
-TOPDIAGS=n  :number of best diags.
-WINDOW=n    :window around best diags.
-PAIRGAP=n   :gap penalty
-SCORE       :PERCENT or ABSOLUTE


***Slow Pairwise Alignments:***
-PWMATRIX=    :Protein weight matrix=BLOSUM, PAM, GONNET, ID or filename
-PWDNAMATRIX= :DNA weight matrix=IUB, CLUSTALW or filename
-PWGAPOPEN=f  :gap opening penalty        
-PWGAPEXT=f   :gap opening penalty


***Multiple Alignments:***
-NEWTREE=      :file for new guide tree
-USETREE=      :file for old guide tree
-MATRIX=       :Protein weight matrix=BLOSUM, PAM, GONNET, ID or filename
-DNAMATRIX=    :DNA weight matrix=IUB, CLUSTALW or filename
-GAPOPEN=f     :gap opening penalty        
-GAPEXT=f      :gap extension penalty
-ENDGAPS       :no end gap separation pen. 
-GAPDIST=n     :gap separation pen. range
-NOPGAP        :residue-specific gaps off  
-NOHGAP        :hydrophilic gaps off
-HGAPRESIDUES= :list hydrophilic res.    
-MAXDIV=n      :% ident. for delay
-TYPE=         :PROTEIN or DNA
-TRANSWEIGHT=f :transitions weighting
-ITERATION=    :NONE or TREE or ALIGNMENT
-NUMITER=n     :maximum number of iterations to perform
-NOWEIGHTS     :disable sequence weighting


***Profile Alignments:***
-PROFILE      :Merge two alignments by profile alignment
-NEWTREE1=    :file for new guide tree for profile1
-NEWTREE2=    :file for new guide tree for profile2
-USETREE1=    :file for old guide tree for profile1
-USETREE2=    :file for old guide tree for profile2


***Sequence to Profile Alignments:***
-SEQUENCES   :Sequentially add profile2 sequences to profile1 alignment
-NEWTREE=    :file for new guide tree
-USETREE=    :file for old guide tree


***Structure Alignments:***
-NOSECSTR1     :do not use secondary structure-gap penalty mask for profile 1 
-NOSECSTR2     :do not use secondary structure-gap penalty mask for profile 2
-SECSTROUT=STRUCTURE or MASK or BOTH or NONE   :output in alignment file
-HELIXGAP=n    :gap penalty for helix core residues 
-STRANDGAP=n   :gap penalty for strand core residues
-LOOPGAP=n     :gap penalty for loop regions
-TERMINALGAP=n :gap penalty for structure termini
-HELIXENDIN=n  :number of residues inside helix to be treated as terminal
-HELIXENDOUT=n :number of residues outside helix to be treated as terminal
-STRANDENDIN=n :number of residues inside strand to be treated as terminal
-STRANDENDOUT=n:number of residues outside strand to be treated as terminal 


***Trees:***
-OUTPUTTREE=nj OR phylip OR dist OR nexus
-SEED=n        :seed number for bootstraps.
-KIMURA        :use Kimura's correction.   
-TOSSGAPS      :ignore positions with gaps.
-BOOTLABELS=node OR branch :position of bootstrap values in tree display
-CLUSTERING=   :NJ or UPGMA
复制<br>在Linux系统下，这个软件的操作命令如下：<br># 进入目录,进行Paire比对
clustalw2 -INFILE=/home/luolintao/mt_DNA_whole/DQ372883.fasta -OUTFILE=/home/luolintao/序列比对/output.aln

# 进行Multi比对
clustalw2 -INFILE=/home/luolintao/mt_DNA_whole/DQ372883.fasta -OUTFILE=/home/luolintao/序列比对/output.aln -OUTPUT=CLUSTAL -ALIGN
复制<br><br>上个软件的进化版<a data-tooltip-position="top" aria-label="http://www.clustal.org/omega/" rel="noopener nofollow" class="external-link" href="http://www.clustal.org/omega/" target="_blank">Clustal Omega - fast, accurate, scalable multiple sequence alignment for proteins</a>。新的 HMM 比对引擎而具有更高的准确性。数小时内比对数十万个序列。<br><br>只有CLI。可以使用Win、Mac、Linux。<br><br><a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a> 是一个用于类 UNIX 操作系统的多序列比对程序。它提供了一系列多重比对方法，L-INS-i（精确；用于&lt;∼200个序列的比对）、FFT-NS-2（快速；用于&lt;∼30,000个序列的比对）等。<br>
<a data-href="MAFFT" href="MAFFT" class="internal-link" target="_self" rel="noopener nofollow">MAFFT</a> - a multiple sequence alignment program (cbrc.jp)](<a rel="noopener nofollow" class="external-link" href="https://mafft.cbrc.jp/alignment/software/" target="_blank">https://mafft.cbrc.jp/alignment/software/</a>)<br><br>
<br><a data-tooltip-position="top" aria-label="https://mafft.cbrc.jp/alignment/software/macosx.html" rel="noopener nofollow" class="external-link" href="https://mafft.cbrc.jp/alignment/software/macosx.html" target="_blank">Mac OS X</a>&nbsp;&nbsp;Mac OS X
<br><a data-tooltip-position="top" aria-label="https://mafft.cbrc.jp/alignment/software/linux.html" rel="noopener nofollow" class="external-link" href="https://mafft.cbrc.jp/alignment/software/linux.html" target="_blank">Linux</a>&nbsp;&nbsp;Linux
<br><a data-tooltip-position="top" aria-label="https://mafft.cbrc.jp/alignment/software/windows.html" rel="noopener nofollow" class="external-link" href="https://mafft.cbrc.jp/alignment/software/windows.html" target="_blank">Windows</a>&nbsp;&nbsp;视窗
<br><br>% mafft [arguments] input &gt; output
% mafft-linsi input &gt; output
% mafft input &gt; output
% mafft --auto input &gt; output # 自动选择参数
复制<br><br><br>
<br>MUMmer是一种快速、准确的DNA序列比对工具。它可以用于比对多个长度有细微差异的fasta序列。MUMmer使用一种称为“最小唯一匹配”(MUM)的算法来进行比对。MUM算法可以快速找到两个序列之间的最大公共子序列(MCS)。

<br>MUMmer 3.0 can find all 20-basepair or longer exact matches between a pair of 5-megabase genomes in 13.7 seconds, using 78 MB of memory, on a 2.4 GHz Linux desktop computer.10条序列需要1分29秒。
<br>MUMmer 3.0 is described in our&nbsp;<a data-tooltip-position="top" aria-label="https://mummer.sourceforge.net/MUMmer3.pdf" rel="noopener nofollow" class="external-link" href="https://mummer.sourceforge.net/MUMmer3.pdf" target="_blank">2004&nbsp;<em></em>&nbsp;paper</a>Genome Biology. We also developed a GPU-accelerated version of MUMmer called&nbsp;<a data-tooltip-position="top" aria-label="http://mummergpu.sf.net/" rel="noopener nofollow" class="external-link" href="http://mummergpu.sf.net/" target="_blank">MUMmerGPU</a>.


<br><br>
<br>ClustalW是一种常用的多序列比对工具。它可以用于比对多个长度有细微差异的fasta序列。ClustalW使用一种称为“渐进式比对”的算法来进行比对。渐进式比对算法将多个序列逐个进行比对，最终得到一个多序列比对结果。- "Classic Clustal"&nbsp;“经典星团”- GUI (ClustalX), command line (ClustalW), web server versions available      GUI (ClustalX)、命令行 (ClustalW)、可用的 Web 服务器版本（web服务器限制Too large! datasize&gt;5,000,000 bytes）。10条序列需要1分49秒。
<br><br>
<br>Clustal Omega（ClustalΩ）: Clustal Omega是一个流行的序列比对工具，它支持多序列比对，适用于蛋白质和核酸序列。Clustal Omega特别适合大规模数据集的比对，能够快速准确地处理大量序列。# <a data-tooltip-position="top" aria-label="http://www.clustal.org/omega/" rel="noopener nofollow" class="external-link" href="http://www.clustal.org/omega/" target="_blank">Clustal Omega</a>&nbsp;水晶欧米茄- Latest version of Clustal - fast and scalable (can align hundreds of thousands of sequences in hours), greater accuracy due to new HMM alignment engine      最新版本的 Clustal - 快速且可扩展（可在数小时内比对数十万个序列），由于新的 HMM 比对引擎而具有更高的准确性- Command line/web server only (GUI public beta available soon)    仅命令行/Web 服务器（GUI 公共测试版即将推出）.速度：41s/10条。默认参数。
<br><br>Web，CLI，Web限制2MB<br><br>
<br><a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>: MAFFT是一种快速且高效的多序列比对工具，适用于大规模序列数据。它提供了多种比对策略，包括FFT-NS-2、FFT-NS-i等，用户可以根据需要选择最适合自己数据的比对策略。
<br>可以在Mac、Linux、Windows运行。支持CLI以及GUI，但是GUI比较慢且可选参数较少。L-INS-i（精确；用于&lt;∼200个序列的比对，4s10个序列）、FFT-NS-2（快速；用于&lt;∼30,000个序列的比对。1秒10个序列）等。
<br><br> MUSCLE: MUSCLE（Multiple Sequence Comparison by Log-Expectation）是一种用于蛋白质和核酸序列比对的软件，它以其高效和高准确性著称。MUSCLE适合用于大量序列的比对，尤其是当准确性非常重要时。它可以在低成本商用计算机（例如，具有 32 Gb RAM 的 8 核 Intel CPU）上高精度地对齐数万个序列。在大型数据集上，Muscle v5 的准确度比 MAFFT 和 Clustal-Omega 高 20-30%。Muscle 没有图形用户界面 (GUI)。支持win,linux。速度：1分52秒完成10条。super5模式下。19分21秒完成10条，PPP模式下。<br><br>T-Coffee: T-Coffee是一种多序列比对工具，它可以用于不同类型的序列比对任务，包括蛋白质序列、核酸序列和结构比对。T-Coffee试图在保持高准确度的同时最大化比对的速度和效率。它还能够将序列信息与蛋白质结构信息 (3D-Coffee/Expresso)、概况信息 (PSI-Coffee) 或 RNA 二级结构 (R-Coffee) 相结合。支持网页版服务，但是You can enter at most 150 sequences。只能在Linux和Mac中运行，CLI。渐进式比对算法将多个序列逐个进行比对，最终得到一个多序列比对结果。10条序列1分40秒。上传限制：2500 char length<br><br>MEGA: MEGA（Molecular Evolutionary Genetics Analysis）是一种集成工具，提供了系统发育分析、序列比对、分子钟分析等多种功能。它的用户界面友好，非常适合生物信息学和系统发育学的初学者使用。有GUI，有CLI，在Win、Linux和Mac均能运行。但是align方法用的是ClustalW和MUSCLE，所以不再进行比较。<br><br>HAlign是一个多平台Java软件工具包，旨在大规模多个相似的DNA/RNA/Protein序列比对。&nbsp;center star multiple sequence alignment strategy。支持网页端上传但是我登不上去。支持下载，可以在Win、Linux和Mac系统运行，需要Java环境。]]></description><link>软件\其它生信软件\使用心得\序列比对（align）软件评测.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/序列比对（Align）软件评测.md</guid><pubDate>Wed, 11 Sep 2024 07:30:09 GMT</pubDate></item><item><title><![CDATA[一、序列比对的两种策略]]></title><description><![CDATA[ 
 <br><a data-href="序列比对（Align）软件评测" href="软件\其它生信软件\使用心得\序列比对（align）软件评测.html" class="internal-link" target="_self" rel="noopener nofollow">序列比对（Align）软件评测</a><br>转载自如下：
<a data-tooltip-position="top" aria-label="https://www.yuque.com/wusheng/gw7a9p/otk3ex" rel="noopener nofollow" class="external-link" href="https://www.yuque.com/wusheng/gw7a9p/otk3ex" target="_blank">序列比对前须知(1) (yuque.com)</a>
<br><br>
<br>全局比对（global alignment）  :它是假设两条序列在整个长度上是相似的，然后从头到尾比较两条序列的最佳匹配，适合高度相关等长序列比对。不适用于发散的不同长度的序列，因为它不能识别两条序列中高度相似的局部序列。  
<br>局部比对（local alignment）  :不考虑两条序列全局相似，而是找两条序列中高度相似的局部区域而不考虑其他区域，适合包含相似模块分散的生物序列，可以找出domain（保守区）和motif。  
<br>两种策略的区别  ：假设如下两条序列Query和Subject，分别采用全局比对和局部比对比对策略，从下图红色方框里可以看出差异<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222910.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>有无Gap比对：两条序列，进行有空位比较和无空位比较的差异如下图：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603222925.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>不管采用哪种策略，比对算法基本上是相似的：  <br>
<br>点阵方法：构造一个二维矩阵。很容易识别序列中高度相似的区域，它在识别染色体重复（两条相同序列，即自己和自己比）和比较两个高度相关的基因组中的基因顺序的保守性非常有用。缺点：很难构造多序列比对。
<br>动态规划方法：匹配两条序列中所有可能字符，也是构造一个二维矩阵确定最优比对方法。里面引入“空位罚分”，即代表插入和删除的空位，因为在自然进化中插入和删除的发生频率比替换相对少。而且开始一个新的空位和拓展一个已经存在的空位付出的代价是不同的（原理是一旦插入和删除发生，那么临近的一些残基很容易插入和删除），两者会有不同的罚分。序列末尾不进行罚分，因为实际中很多同源序列是不等长的，如果末端进行了空位罚分反而是不实际的结果。  
<br>启发式算法：一个基于直观或经验构造的算法，在可接受的花费（指计算时间和空间）下给出待解决组合优化问题每一个实例 的 一个可行解，该可行解与最优解的偏离程度一般不能被预计。  
<br><br>
<br>全局比对的动态规划算法：经典的算法是Needleman-Wunsch算法。它必须对序列从头到尾进行计算得到最高比对得分。缺点，关注全长的最大比对的缺点是找不到局部的序列相似。对于发散序列和具有不同结构域的序列，这种方法达不到最理想的比对。全局两两比对的web程序是GAP.  
<br>局部比对的动态规划算法：正常比对序列中，两条被比对序列的分离水平是不知道的，两条序列的长度也可能不同，这种情况下，识别局部相似性比去对比包含残基的整个序列更加有意义。第一个运用动态规划进行局部序列比对的算法是Smith-Waterma算法。  
<br>&nbsp;<br>
这种算法下的局部比对分为两种：<br>
1. local alignment (Smith-Waterman) with affine gap costs (Gotoh)<br>
2. local alignment with generalized affine gap costs (Altschul)<br>
全局比对中，最终结果受到选用得分矩阵的影响，而局部比对的目标是找到局部最高分。这种方法适应于对分散序列和具有来自多个不同源的区域序列，目前大多数两两比对程序基本采用局部比对策略。<br><br>
<br>动态规划算法 &nbsp;可得到最优解，但是计算量非常大，实际中很难用于多序列比对。  
<br>启发式算法（heuristic algorithm)  

<br>渐进法（progressive methods）：Clustal, T-Coffee, MUSCLE  
<br>迭代法（iterative methods）：PRRP, DIALIGN  
<br>其它算法：Partial Order Algorithm、profile HMM、meta-methods (<a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>)…<br>
注：启发式算法的计算过程也有可能利用到动态规划算法，比如Clustal采用的渐进式算法：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603223405.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br><br>&nbsp;1. 核苷酸得分矩阵  ：核苷酸得分矩阵相对简单，对匹配位置赋予一个正值或者高分，对失配位置赋予一个负值或者低分。但是这种方法不符合实际，观察显示转换（嘌呤与嘌呤或者嘧啶与嘧啶之间的替换）发生频率比颠换（嘧啶与嘌呤之间的替换）高，因此，需要一个反应不同残基替换发生频率不同的更加复杂的统计模型。<br>
2. 氨基酸的替换矩阵 ：氨基酸的替换矩阵比较复杂，某种氨基酸很容易被具有相似理化性质的其他氨基酸替换，而很难被具有不同理化性质的氨基酸替换。而不同理化性质的氨基酸替换可能导致结构和功能的缺失，这种会导致分类的替换是很少被进化所选择的。  氨基酸替换矩阵是一个20×20矩阵，它用来反映氨基酸被替换的可能性。经验上的矩阵，包括PAM矩阵和BLOSUM矩阵。  <br><br>PAM的意思是可接受的点突变，观察到的突变不会改变蛋白质的一般功能，观察到的氨基酸突变被认为是自然选择接受了的。一个PAM单位被定义为有1%的氨基酸位点发生了变化或者100氨基酸有1个发生突变，但这并不意味100次PAM后，每个氨基酸都发生变化，因为其中一些位置可能会经过多次突变，甚至可能会变回到原来的氨基酸。一个特定残基对的PAM分数是通过一个多阶段过程得到的，这个过程包括：<br>
1. 计算相对突变率。一个特定氨基酸被同源氨基酸替换的总数除以在整个比对中这种氨基酸出现的总数；  &nbsp;<br>
2. 用随机替换率对预期的氨基酸替换率进行标准化，把标准化的突变率除以特定氨基酸出现频率，然后取以10为底的对数。把结果取整后填入替换矩阵，这个矩阵就可以反映氨基酸替换的可能性。<br>
3. 对于较分散序列的高阶PAM矩阵是通过对PAM1矩阵相乘推出来的，例如，PAM80就是对PAM1矩阵自乘80次得到的。一个PAM80矩阵只相当于观察到突变率的50%，PAN250表示一致性为20%。<br>
4. 缺点：PAM矩阵构建过程中，只是之间观察了PAM1中的基于一小堆极近相近序列中的残基替换（这个要从PAM矩阵的来源说起，当时构建这个矩阵用的序列是21组非常接近的蛋白质序列），PAM矩阵对于分散序列的比对是不可靠的。  PAM矩阵数字与序列相似度的对应关系：  <br><br>注意:PAM250矩阵&nbsp;→&nbsp;对应这些用于比对的序列相似度估计在14%&nbsp;–&nbsp;27%，一般PAM250矩阵对应序列相似度在20%。  <br><br>为了弥补PAM的缺点，BLOSUM矩阵应用而生，BLOSUM矩阵与PAM矩阵的不同之处在于：用于产生矩阵的蛋白质家族及多肽链数目，BLOSUM比PAM大约多20倍。BLOSUM矩阵不用推断，而是用实际上所选序列的残基一致性的比例来构建矩阵。与PAM矩阵的阶数相反，BLOSUM矩阵阶数越低代表序列越分散。比如BLOSUM60，这个矩阵首先寻找氨基酸模式，即有意义的一段氨基酸片断（如一个结构域及其相邻的两小段氨基酸序列），分别比较相同的氨基酸模式之间氨基酸的保守性（某种氨基酸对另一种氨基酸的取代数据），然后，以所有&nbsp;60％保守性的氨基酸模式之间的比较数据为根据，产生BLOSUM60；同理，以所有80％保守性的氨基酸模式之间的比较数据为根据，产生BLOSUM80。  <br><br>&nbsp; ①PAM1矩阵是通过一个进化模型得到的，而BLOSUM矩阵完全是由观测值构成，因此，BLOSUM矩阵可能没有PAM矩阵那么强的进化上的实际意义，这也是PAM矩阵被常用来重构系统发育树的原因。<br>
②PAM矩阵对发散序列可能不符合实际。<br>
③BLOSUM矩阵是完全通过保守序列的局部比对得到的，而PAM1是对包含保守和变化区域的整个序列全局比对得到的。这是BLOSUM矩阵更加适用于搜索数据库和寻找蛋白质中保守区域的原因。一些经验上测试BLOSUM矩阵在局部比对的正确性上胜过PAM矩阵。  <br>  对于一个给定的矩阵，一个正的分数说明在一个同源序列的数据集里观察到的氨基酸替换频率比随机替换频率高；零分表示在一个同源序列的数据集里观察到的氨基酸替换频率和随机替换频率相等；负数一个同源序列的数据集里观察到的氨基酸替换频率比随机替换频率低，这通常发生在不相似残基之间。  <br><br>序列比对本身就是一个随机问题，我们要对这种随机进行检验，比对完后，会有一个P值。<br>
P值的解读：  <br>
<br>如果P值小于10-100，表明两条序列是精确匹配的；  
<br>如果10-100&lt;P&lt;10-50，表明两条序列近似匹配；  
<br>如果10-50&lt;P&lt;10-5，表明两条序列有较近的同源关系；  
<br>如果10-5&lt;P&lt;10-1，表明两条序列可能存在较远的同源关系；  
<br>如果P&gt;10-1，那么这两条序列能匹配上可能是由于随机的关系；<br>
可以使用软件PRSS，用来评价序列两两比对在在统计学上显著性。  
<br><br>同一性和同源性这两个词经常被混淆。<br>
同源性只有高低之分，没有具体数值，属于"质”的属性；<br>
同一性才有具体数值，是“量”词。同一性数值越大，同源性越高。<br><br>多序列比对有3个东西是大家基本认同的：  <br>
<br>基因编码区的比对，需要采取基于密码子方式的比对；  
<br>目前多序列比对精确度：MAFFT&gt;Muscle&gt;Clustal；  
<br>多序列比对尽管是最基础的步骤，但是会影响后续的分析。  
<br>笔者最近发现了一个有趣的现象，不过也仅仅是这次处理的数据集才遇到，这让我重新审视了序列比对这个东西---多序列比对尽可能选用精确度高的工具，且一定要仔细检查比对后的Alignment文件。<br>
因为有时，对于某些数据，某种工具和某种方法比对的结果不一定是正常的(准确的)。<br>
笔者从NCBI下载了70条某个基因的基因编码区序列（已知是高度相似的），每条序列都从ATG起始密码子开始，如下：(截图仅显示了42条）<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224023.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
选用了MEGA X里的Muscle(Codons)，也就是Muscle的基于密码子的比对方法，比对结果部分截图如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224033.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
看上去好像有哪里不对劲，竟然在起始密码子(ATG)前引入了gap，尽管引入的gap数量是3个(3的倍数），但是gap引入位置不对，比对结果不合理，或者说前面9个碱基没比对正确。<br>
换用MEGA X里的ClustalW(Codons)进行多序列比对，比对结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224058.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
还是有很多序列在起始密码子(ATG)前引入了gap，比对结果不合理，和Muscle(Codons)的结果相差无几。  <br><br>换用MEGA X里的Muscle，这次采用不基于密码子的比对方法，比对结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224121.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
嗯，结果看上去，所有序列都是以ATG起始了，而且引入的gap的数量也是3的倍数。  <br>接下来看看，这种引入gap从生物学角度来看是否合理：  <br>
以EF203067和MF094681为例，其生物学意义是，如果考虑EF203067来自于MF094681的话，MF094681在其第二个密码子ACT在A和C之间插入了3个碱基(GTA),于是产生了EF203067的前9个碱基ATGAGTACT，相比于MF094681的前6个碱基“ATGACT”而言，这种插入碱基的方式，使得EF203067在该位置只是多了一个氨基酸而并没有发生移码突变，其他序列的插入碱基方式也没有造成移码突变，而这些序列已知是高度相似的，所以这种通过插入碱基而突变的方式是合理的。尽管有一定的生物学意义，但是基因编码区不基于密码方式子比对，结果可能不太可靠。
<br>换用MEGA X里的ClustalW，也采用不基于密码子的比对方法，结果如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224457.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
和ClustalW基于密码子的比对方法的结果差不多，在ATG前引入了gap，比对结果不合理。<br><br>主要是数据的引起的。  由于这个插入发生的位置比较特殊，非常靠前，而比对中引入gap本身就是一个带有概率的随机事件，所以Muscle和ClustalW在尽管采取了基于密码子的方式比对，结果还是不尽人意。<br>
通过上面的Muscle(Codons)和ClustalW(Codons)比对的结果，我们可以大致推测，自然界中真实的情况可能是，有几条序列在起始密码子ATG后面，从第4个碱基开始，直接插入了3个碱基，导致他们比其他序列在这个位置多了3个碱基(即多了1个氨基酸)。  <br>这个故事告诉我们：一定要仔细检查比对后的Alignment文件！！！  <br>既然问题已经发生了，重点在于怎么解决这个问题？<br>
很明显，Muscle和ClustalW对于笔者的数据已经败下阵来，那<a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>怎么样？<br>
可是，好像MAFFT没有基于密码子的方式比对呀？<br>
答案是有的，最新的两款生物信息学工具Phylosuite（张东博士）和BioAider（笔者），里面提供的MAFFT模块增加了MAFFT基于密码子的方式进行多序列比对。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240603224750.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注：笔者的数据分别用MAFFT里的E-INS-i策略(局部比对）和G-INS-i(全局比对）进行了测试，两种不同策略下，其结果是一模一样的，如上图所示。<br>使用MAFFT基于密码子的方式比对基因编码区，其结果基本正常了，而且证实了之前笔者的推测：自然界中真实的情况应该是，有几条序列相比其他序列在第1个密码子(ATG)的后面直接增加了3个碱基（比如EF203067），而那些少了3个碱基的序列（图中占大多数的那些序列）在比对时会在那个位置插入3个gap(-)。<br>多序列比对，还是推荐MAFFT，基因编码区的多序列比对，更加推荐使用MAFFT基于密码子的方式比对。]]></description><link>软件\其它生信软件\使用心得\序列比对的思考.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/序列比对的思考.md</guid><pubDate>Sun, 08 Sep 2024 13:18:10 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2754.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2754.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[软件准备]]></title><description><![CDATA[ 
 <br><br>我们需要：<br>
<br>GATK 软件
<br>JAVA 软件
<br>samtools 软件
<br>在这里，我们注意到 JAVA 软件和 samtools 软件都可以使用命令行安装，GATK 软件放在这里：<a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hf15pAiBYEKd65Vdfg?e=snFn1S" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hf15pAiBYEKd65Vdfg?e=snFn1S" target="_blank">gatk-4.4.0.0.zip</a><br>你可能需要更新一下 java 的版本：<br>sudo apt update
sudo apt install openjdk-17-jdk
复制<br>将全基因的 BAM 文件提取线粒体 DNA 并转为 FASTA 文件可以按照如下的操作：<br>#!/bin/bash
#!/bin/bash

# 指定搜索的根目录
root_dir="/mnt/d/MTDNA/古代DNA/ZhuMBG2024"

# 线粒体DNA的染色体名称（可以修改为实际的名称，例如 'MT', 'chrM' 等）
mt_chr="MT"

# 输出文件夹名称
output_dir="$root_dir/output_files"

# 错误日志文件路径
error_log="$output_dir/error.txt"

# 创建输出文件夹（如果不存在）
mkdir -p "$output_dir"

# 清空或者创建 error.txt 文件
&gt; "$error_log"

# 检查 samtools 是否已安装
if ! command -v samtools &amp;&gt; /dev/null
then
    echo "Error: samtools is not installed. Please install it before running this script."
    exit 1
fi

# 递归查找 .bam 文件
find "$root_dir" -name "*.bam" | while read bam_file; do
    # 检查文件是否存在且可读
    if [ ! -r "$bam_file" ]; then
        echo "Error: Cannot read BAM file: $bam_file"
        echo "$bam_file" &gt;&gt; "$error_log"
        continue
    fi
    
    # 为每个 .bam 文件生成索引（如果索引文件不存在）
    if [ ! -f "${bam_file}.bai" ]; then
        echo "Indexing BAM file: $bam_file"
        samtools index "$bam_file"
        if [ $? -ne 0 ]; then
            echo "Error: Failed to index BAM file: $bam_file"
            echo "$bam_file" &gt;&gt; "$error_log"
            continue
        fi
    else
        echo "BAM index already exists for: $bam_file"
    fi

    # 提取线粒体DNA (基于 mt_chr 变量)
    output_file="$output_dir/$(basename ${bam_file%.bam}_${mt_chr}.bam)"
    echo "Extracting mitochondrial DNA ($mt_chr) to: $output_file"
    samtools view -b "$bam_file" "$mt_chr" &gt; "$output_file"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to extract mitochondrial DNA from $bam_file"
        echo "$bam_file" &gt;&gt; "$error_log"
        continue
    fi

    # 为生成的线粒体 BAM 文件建立索引
    echo "Indexing mitochondrial BAM file: $output_file"
    samtools index "$output_file"
    if [ $? -ne 0 ]; then
        echo "Error: Failed to index mitochondrial BAM file: $output_file"
        echo "$bam_file" &gt;&gt; "$error_log"
    fi
done

# 打印完成信息
echo "Processing complete. All output files are in $output_dir. Any errors have been logged to $error_log."

复制<br><br>我们一般选择的是 RCRS17.2版本。你可以在 Haplogrep 软件中找到。路径如下：<br>/home/luolintao/Haplogrep/trees/phylotree-rcrs/17.2/rcrs.fasta
复制<br>将 FASTA 文件中 mtDNA 更改为 BAM 文件中的名字。因为有些线粒体 DNA 的名字是 "chrM" "MT" "M" "MtDNA" "ChrM" "chrMT" "26" "chr26" "Chr26" 等等。<br>
你可以通过以下代码查看 BAM 文件的染色体名称：<br>samtools idxstats HRR163260.bam|less -S
复制<br>可以看到下图是 MT。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240628220634.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
因此，你需要将线粒体的参考 fasta 名字更改为 MT.<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240628220757.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br># 建立字典
gatk CreateSequenceDictionary R=rcrs.fasta O=rcrs.dict
# 建立索引
samtools faidx rcrs.fasta
复制<br><br># 建立索引(假如已经存在索引，可以不再建立。)
samtools index 原始的文件.bam
# 提取
samtools view -b 原始的文件.bam MT &gt;线粒体.bam
samtools index 线粒体.bam
复制<br><br>/mnt/e/Scientifc_software/gatk-4.4.0.0/gatk --java-options "-Xmx8G" HaplotypeCaller \
-R $reference_genome \
-I "$bam_file" \
-O "$output_vcf" \
-L MT \
--sample-ploidy 1 \
复制<br><br>#!/bin/bash

# 指定根目录和输出文件夹
root_dir="/mnt/d/nDNA/ancient_DNA/LiSB2024/LiScibull2024/Extract_MT"
reference_genome="/mnt/e/Scientifc_software/Reference/MT.fasta"
output_dir="$root_dir/vcf_output"

# 创建输出文件夹
mkdir -p "$output_dir"

# 遍历 BAM 文件，使用 GATK HaplotypeCaller 生成 vcf 文件
for bam_file in "$root_dir"/*_MT.bam; do
    sample_name=$(basename "$bam_file" .bam)
    output_vcf="$output_dir/${sample_name}.vcf"
    echo "Running GATK HaplotypeCaller on $bam_file"

    /mnt/e/Scientifc_software/gatk-4.4.0.0/gatk --java-options "-Xmx8G" HaplotypeCaller \
    -R $reference_genome \
    -I "$bam_file" \
    -O "$output_vcf" \
    -L MT \
    --sample-ploidy 1 \
    #-ERC gvcf # 如果你希望生成g.vcf才使用这个参数。 
done

echo "HaplotypeCaller completed for all samples."


复制<br><br>在进行这一步之前，请一定要进入 Haplogrep 的安装文件夹。cd 你的安装路径。<br>./haplogrep3 classify \
--input /mnt/c/Users/victo/Desktop/获得的文件.vcf \
--tree phylotree-rcrs@17.2 \
--output /mnt/c/Users/victo/Desktop/分型结果.txt \
--write-fasta # 这个选项不是必须，如果你选了，那么就会生成一个全长的fasta文件。如果你是全测，那么可以勾选，否则会产生虚假的fasta序列。
复制<br>注意这个软件的--tree 选项只能在以下挑选！<br>phylotree-rcrs@17.2
phylotree-rcrs@17.1
phylotree-rcrs@17.0
phylotree-rcrs@16.0
phylotree-rcrs@15.0
phylotree-rsrs@17.1
phylotree-fu-rcrs@1.2
phylotree-fu-rcrs@1.1
phylotree-fu-rcrs@1.0
复制<br><br>#!/bin/bash

# 设置输入和输出目录
input_dir="输入文件的路径"
output_dir="输出文件的路径"

# 检查输出目录是否存在，如果不存在则创建
mkdir -p "$output_dir"
cd 进入你的软件安装路径
# 遍历目录中的所有 VCF 文件
for vcf_file in "$input_dir"/*.vcf; do
    # 获取不带路径的文件名
    filename=$(basename "$vcf_file")
    # 构建输出文件名
    output_file="${output_dir}/${filename%.vcf}_分型结果.txt"

    # 执行 classify 命令
    ./haplogrep3 classify \
        --input "$vcf_file" \
        --tree phylotree-rcrs@17.0 \
        --output "$output_file" \
        --write-fasta

    # 输出处理的文件名
    echo "Processed: $vcf_file"
done

echo "All files processed."

复制]]></description><link>软件\其它生信软件\使用心得\gatk：简易的mtdna分析流程.html</link><guid isPermaLink="false">软件/其它生信软件/使用心得/GATK：简易的mtDNA分析流程.md</guid><pubDate>Sun, 03 Nov 2024 08:20:06 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240628220634.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240628220634.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[安装]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="http://dambe.bio.uottawa.ca/DAMBE/dambe.aspx" rel="noopener nofollow" class="external-link" href="http://dambe.bio.uottawa.ca/DAMBE/dambe.aspx" target="_blank">XiaLab (uottawa.ca)</a><br>或者你也可以使用如下连接下载，在 Win10及以上电脑：<a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hflO2Qz-mZKR2fkGFQ?e=kJEmlp" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hflO2Qz-mZKR2fkGFQ?e=kJEmlp" target="_blank">DAMBE.msi</a><br>注意，这里有个 bug：
我们需要在属性中选择兼容模式运行，否则可能在 win11电脑闪退！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111713579.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>通常有两种方法：  <br>
<br>第一种是PAUP 软件验证替换饱和，简要操作如下：  在 PAUP 中分别计算 p 距离和 GTR+I+G 距离，然后在 Excel 中做散点图。（a）如果散点分别在 y=x 直线上，就说明没达到饱和；（b）如果 GTR+I+G 距离&gt;p 距离就说明饱和了。
<br>第二种用DAMBE 软件验证替换饱和，该法最方便实用，推荐使用。只要比较 ISS 和 ISS.c 值大小及显著与否，即可。当 ISS 小于 ISS.c 且 p=0.0000（极显著），就说明没序列替换未饱和，可以建树。<br>
我们在这里展示第二种方法的具体操作：
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111706837.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111706165.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111705181.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111533427.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我们发现了，Iss 小于 Iss.c 且 P&lt;0.05，因此替换没有饱和，适合建树。]]></description><link>软件\其它生信软件\a-j\dambe：核苷酸替代饱和度检测.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/DAMBE：核苷酸替代饱和度检测.md</guid><pubDate>Wed, 11 Sep 2024 09:13:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111713579.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111713579.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Discovery of Nuclear Mitochondrial Insertions (dinumt )]]></title><description><![CDATA[ 
 <br><br>用于查找 <a data-href="核-线片段（nuclear-mitochondrial DNA segments, NUMTs）" href="术语\核-线片段（nuclear-mitochondrial-dna-segments,-numts）.html" class="internal-link" target="_self" rel="noopener nofollow">核-线片段（nuclear-mitochondrial DNA segments, NUMTs）</a>。<br>该软件旨在从全基因组序列数据中识别线粒体来源的核插入并进行基因分型。它由两个程序组成：dinumt (di-nu-mite)，用于识别单个样本中的插入位点；gnomit (geno-mite)，用于对多个样本中的这些位点进行基因分型。还有一个名为 clusterNumtsVcf 的附加程序，它将多个样本中识别的位点合并到一个合并文件中以进行基因分型。<br>这些程序需要许多第三方软件包：<br>
<br>samtools:&nbsp;<a rel="noopener nofollow" class="external-link" href="http://samtools.sourceforge.net/" target="_blank">http://samtools.sourceforge.net/</a>&nbsp;samtools：<a rel="noopener nofollow" class="external-link" href="http://samtools.sourceforge.net/" target="_blank">http://samtools.sourceforge.net/</a>
<br>exonerate:&nbsp;<a rel="noopener nofollow" class="external-link" href="http://www.ebi.ac.uk/~guy/exonerate/" target="_blank">http://www.ebi.ac.uk/~guy/exonerate/</a>&nbsp;无罪释放：<a rel="noopener nofollow" class="external-link" href="http://www.ebi.ac.uk/~guy/exonerate/" target="_blank">http://www.ebi.ac.uk/~guy/exonerate/</a>
<br>vcftools:&nbsp;<a rel="noopener nofollow" class="external-link" href="http://vcftools.sourceforge.net/" target="_blank">http://vcftools.sourceforge.net/</a>&nbsp;vcftools：<a rel="noopener nofollow" class="external-link" href="http://vcftools.sourceforge.net/" target="_blank">http://vcftools.sourceforge.net/</a>

<br>you will need to make sure Vcf.pm is in your perl library path<br>
你需要确保 Vcf.pm 在你的 perl 库路径中


<br>此外，您将需要：<br>
<br>reference genome in fasta format (e.g. hs37d5.fasta)<br>
fasta 格式的参考基因组（例如 hs37d5.fasta）
<br>individual MT sequence (e.g. MT.fa or chrM.fa)&nbsp;单独的 MT 序列（例如 MT.fa 或 chrM.fa）
<br>bed file of annotated numts in reference (refNumts.bed for hg19 is included in package)<br>
参考中带注释的 numts 的 bed 文件（hg19 的 refNumts.bed 包含在包中）
<br>基因分型步骤需要使用包含各种样本级信息（平均插入片段大小、覆盖范围等）的样本索引文件。已经提供了模板，可以使用 GATK（DepthOfCoverage walker）和 Picard（CollectInsertSizeMetrics）或自定义脚本获取相关数据。如果您在参考基因组版本 GRCh38下的 cram 文件中运行 dinumt，请使用文件夹中相应的.pl。<br>Additional information about various parameters below:<br>
有关以下各种参数的附加信息：<br>
<br>--len_cluster_include&nbsp;: width of window to consider anchor reads as part of the same cluster, typically calculated as&nbsp;mean_insert_size&nbsp;+ 3 &nbsp;standard_deviation<br>
--len_cluster_include ：将锚读取视为同一簇一部分的窗口宽度，通常计算为mean_insert_size + 3  standard_deviation
<br>--len_cluster_link&nbsp;: width of window to link two clusters of anchor reads in proper orientation, typically calculated as 2 &nbsp;len_cluster_include<br>
--len_cluster_link ：以正确方向链接两个锚读取簇的窗口宽度，通常计算为 2  len_cluster_include
<br>--max_read_cov&nbsp;: maximum read depth at potential breakpoint location, used to filter out noisy regions of the genome, typically calculated as 5 &nbsp;mean_coverage<br>
--max_read_cov ：潜在断点位置的最大读取深度，用于过滤掉基因组的噪声区域，通常计算为 5 mean_coverage
<br>--output_support&nbsp;: output all sequence reads supporting an insertion event in SAM format to filename in&nbsp;--support_filename&nbsp;option<br>
--output_support ：将支持 SAM 格式插入事件的所有序列读取输出到 --support_filename 选项中的文件名
<br>--mask_filename&nbsp;: bed file of all numts annotated in reference sequence, one is provided for GRCh37 but additional versions can be obtained from UCSC Genome Browser<br>
--mask_filename ：以参考序列注释的所有 numt 的床文件，为 GRCh37 提供了一个，但可以从 UCSC 基因组浏览器获取其他版本
<br>--min_map_qual&nbsp;: mininum mapping quality required for anchor read to be considered for cluster support, default is 10 but can be adjusted as needed<br>
--min_map_qual ：考虑集群支持的锚点读取所需的最小映射质量，默认值为 10，但可以根据需要进行调整
<br><br>dinumt.pl \
--mask_filename=refNumts.bed \
--input_filename=sample1.bam \
--reference=hs37d5.fa \
--min_reads_cluster=1 \
--include_mask \
--output_filename=sample1.vcf \
--prefix=sample1 \
--len_cluster_include=577 \
--len_cluster_link=1154 \
--insert_size=334.844984 \
--max_read_cov=29 \
--output_support \
--support_filename=sample1_support.sam
复制<br>grep ^# sample1.vcf &gt; header.txt
cat *vcf | grep -v ^# | vcf-sort.pl | clusterNumtsVcf.pl --reference=hs37d5.fa &gt; data.txt
cat header.txt data.txt &gt; merged.vcf
复制<br>gnomit.pl \
--input_filename=merged.vcf \
--mask_filename=refNumts.bed \
--info_filename=sampleInfo \
--output_filename=merged_geno.vcf \
--samtools=samtools \
--reference=hs37d5.fa \
--breakpoint \
--min_map_qual=13 \
--dir_tmp=/tmp \
--exonerate=exonerate \
--mt_filename=MT.fa
复制]]></description><link>软件\其它生信软件\a-j\dinumt.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/dinumt.md</guid><pubDate>Sat, 02 Nov 2024 07:26:51 GMT</pubDate></item><item><title><![CDATA[DnaSP：基础使用说明]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://blog.sciencenet.cn/blog-3433349-1356293.html" rel="noopener nofollow" class="external-link" href="https://blog.sciencenet.cn/blog-3433349-1356293.html" target="_blank">科学网—[转载]DnaSP(基因多态性分析软件) - 吴潇的博文</a><br>DnaSP(基因多态性分析软件)是一款专业的DNA比对分析软件，多用于分析核苷酸多态性。属于生物、医学领域。DnaSP可通过合并估计，分析置信区间,输入比对后的DNA序列文件，进行基因多态性分析,其结果通过表格和图形的形式显示。下面我们就来介绍一下DnaSP分析序列的使用方法。<br>打开DnaSP<br>
<img src="https://pic4.zhimg.com/80/v2-4b74092fea46d2dc9cbf371f606b9bd7_720w.jpg" referrerpolicy="no-referrer"><br>打开一个序列<br>
<img src="https://pic4.zhimg.com/80/v2-c79fdb2a2fe735524cd0aa3fc832fabb_720w.jpg" referrerpolicy="no-referrer"><br>显示出了这段序列的一些特征<br>
<img src="https://pic4.zhimg.com/80/v2-2d42c8b6619436156a507033331cdedf_720w.jpg" referrerpolicy="no-referrer"><br>点击显示数据<br>
<img src="https://pic3.zhimg.com/80/v2-8457fe24549b4adea746ca1283f43736_720w.jpg" referrerpolicy="no-referrer"><br>数据显示出来了<br>
<img src="https://pic3.zhimg.com/80/v2-f4f0d0973498b74a8694cb113ff243c6_720w.jpg" referrerpolicy="no-referrer"><br>点击data，再点击format<br>
<img src="https://pic3.zhimg.com/80/v2-6f8f162c29b55cf23cd73b15ad946eaa_720w.jpg" referrerpolicy="no-referrer"><br>调整参数，选择是DNA还是RNA,还有在染色体的位置，以及倍型。<br>
<img src="https://pic4.zhimg.com/80/v2-8c7988b3a5541f37ecf634f9eb73dd0f_720w.jpg" referrerpolicy="no-referrer"><br>选择使用的密码子<br>
<img src="https://pic4.zhimg.com/80/v2-06da29e6d3b49e6cb2b0472c47cb32a7_720w.jpg" referrerpolicy="no-referrer"><br>选择一种密码子表<br>
<img src="https://pic4.zhimg.com/80/v2-baa0c947b7f63a12815399ac76fa0853_720w.jpg" referrerpolicy="no-referrer"><br>可以设定编码的区域<br>
<img src="https://pic3.zhimg.com/80/v2-31ca736a9294eef0ca30b1e82e6c604e_720w.jpg" referrerpolicy="no-referrer"><br>设定群体数据<br>
<img src="https://pic4.zhimg.com/80/v2-18f082bb3382088e919d9cb684612de3_720w.jpg" referrerpolicy="no-referrer"><img src="https://pic1.zhimg.com/80/v2-0985028406a906456cf0cf5c79dfe7c8_720w.jpg" referrerpolicy="no-referrer"><br>可以选择进行计算的序列<br>
<img src="https://pic1.zhimg.com/80/v2-6559a187e354ffca985c781bb54655fc_720w.jpg" referrerpolicy="no-referrer"><br>可以选择具体要计算的序列<br>
<img src="https://pic2.zhimg.com/80/v2-600938a165f52869b75bafb57887b1e9_720w.jpg" referrerpolicy="no-referrer"><br>点击分析多样性<br>
<img src="https://pic4.zhimg.com/80/v2-0247a18411d0500a9f992f4a605e08a3_720w.jpg" referrerpolicy="no-referrer"><br>设置参数，OK<br>
<img src="https://pic2.zhimg.com/80/v2-356423618504ef1a24b463bc0d01e399_720w.jpg" referrerpolicy="no-referrer"><br>可以看到核苷酸多样性（Pi）与单倍型多样性（Hd）<br>
<img src="https://pic4.zhimg.com/80/v2-9e35d3df4552df301e97f856d86a693f_720w.jpg" referrerpolicy="no-referrer"><br>单倍型数量<br>
<img src="https://pic1.zhimg.com/80/v2-f4ae3e6448de942dd0996d86a75cd9b0_720w.png" referrerpolicy="no-referrer"><br>核苷酸多样性（每个位点）（p Nucleotide diversity per site）<br>
<img src="https://pic4.zhimg.com/80/v2-3ce9054cb14f14c200472cd8f391d393_720w.png" referrerpolicy="no-referrer"><br>平均核苷酸差异数（k Average number of nucleotide differences）<br>
<img src="https://pic1.zhimg.com/80/v2-cd54f40a4faa329ee2215d867e8c8f04_720w.jpg" referrerpolicy="no-referrer"><br>点击分析多态位点<br>
<img src="https://pic1.zhimg.com/80/v2-9f1b366dd6cade8ee8a83fc2cea772f0_720w.jpg" referrerpolicy="no-referrer"><br>得到多态位点的信息<br>
<img src="https://pic2.zhimg.com/80/v2-a40f16f6547c443611ea84150c7393d9_720w.jpg" referrerpolicy="no-referrer"><br>不变的位点（单型位点）Invariable (monomorphic) sites:<br>
<img src="https://pic3.zhimg.com/80/v2-fe58a7fd5940196f1e2bdb813df813c6_720w.jpg" referrerpolicy="no-referrer"><br>变化的位点（多态性位点S Number of polymorphic sites）总突变位点<br>
<img src="https://pic3.zhimg.com/80/v2-156d43fcb73da363ac6cd83c888a28f6_720w.jpg" referrerpolicy="no-referrer"><br>变化位点与简约信息位点数（Npi Number of parsimony-informative characters）<br>
<img src="https://pic1.zhimg.com/80/v2-a7bef74a2fb13aaa3be89aed5b548b64_720w.jpg" referrerpolicy="no-referrer"><br>点击分析，taijima’s test,进行中性检验。<br>
<img src="https://pic1.zhimg.com/80/v2-41bae78c06ae3c1b6016e58618b3f37c_720w.jpg" referrerpolicy="no-referrer"><br>正值时说明序列进化方式为平衡选择，且有一些单倍型分化；负值时为负向选择或群体扩张。如果差异显著，认为目标序列的进化不遵循中性模型，如果差异不显著，则认为目标序列在进化上遵循中性模型。为0时为中性选择。如果不显著，则不能排除中性选择。<br>
<img src="https://pic4.zhimg.com/80/v2-671dfb39787f372aed8acf609ecd11ef_720w.jpg" referrerpolicy="no-referrer"><br>选择Synonymous and NonSynomymous Substitutions，点击这项进行Ka，Ks（即dN，dS）分析。<br>
<img src="https://pic4.zhimg.com/80/v2-f8612a03554523bf39f1332ed732818f_720w.jpg" referrerpolicy="no-referrer"><br>得到ka与ks<br>
<img src="https://pic4.zhimg.com/80/v2-b7fc7cb20502e2595c3944cb6b3c3c4f_720w.jpg" referrerpolicy="no-referrer"><br>还有一个文本的总结表<br>
<img src="https://pic3.zhimg.com/80/v2-bf4bcef750ad893bf4e1fb1fb7a54c5e_720w.jpg" referrerpolicy="no-referrer"><br>分析插入删除多态性<br>
<img src="https://pic4.zhimg.com/80/v2-8a99ce717830d97317bc774512c9bb0b_720w.jpg" referrerpolicy="no-referrer"><br>选择模型<br>
<img src="https://pic3.zhimg.com/80/v2-8c99adcf65eae8619ce0862c100e2c56_720w.jpg" referrerpolicy="no-referrer"><br>得到结果，看有没有插入删除事件<br>
<img src="https://pic3.zhimg.com/80/v2-4ceff1f441bd279b5abcffef79deb6ee_720w.jpg" referrerpolicy="no-referrer"><br>分析群体的分歧<br>
<img src="https://pic2.zhimg.com/80/v2-49428b342c75a1e7b04e96e765fbe141_720w.jpg" referrerpolicy="no-referrer"><br>得到结果<br>
<img src="https://pic4.zhimg.com/80/v2-875e8f4d18f0db9153199a58c6ddac47_720w.jpg" referrerpolicy="no-referrer"><br>分析保守区域<br>
<img src="https://pic2.zhimg.com/80/v2-db6de3b080ef9522b6e57e67893a67d1_720w.jpg" referrerpolicy="no-referrer"><br>得到保守区域的结果<br>
<img src="https://pic1.zhimg.com/80/v2-4b026edbc2dae243b2b7bc0d5d9746ec_720w.jpg" referrerpolicy="no-referrer"><br>分析密码选择偏好<br>
<img src="https://pic4.zhimg.com/80/v2-9dbc1a8f9887d401b9f7fa5faadafdeb_720w.jpg" referrerpolicy="no-referrer"><br>分析基因保守性<br>
<img src="https://pic3.zhimg.com/80/v2-dfa47601fd166b6d3aff1363259255a6_720w.jpg" referrerpolicy="no-referrer"><br>分析基因流与遗传分化<br>
<img src="https://pic3.zhimg.com/80/v2-f9de4336d8ece2d9cd88da856adaea1a_720w.jpg" referrerpolicy="no-referrer"><br>修改参数<br>
<img src="https://pic1.zhimg.com/80/v2-31d9c5224fd58dca4948588a207a229c_720w.jpg" referrerpolicy="no-referrer"><br>得到结果<br>
<img src="https://pic1.zhimg.com/80/v2-edbb1221a9fb1097e2b86ec0a79ed370_720w.jpg" referrerpolicy="no-referrer"><br>分析功能区域的多态性与分歧<br><img src="https://pic3.zhimg.com/80/v2-4a4e2aa4ea659b6a9ec2e0bf967e4cbe_720w.jpg" referrerpolicy="no-referrer"><br>分析连锁不平衡<br><img src="https://pic1.zhimg.com/80/v2-9390181c7107ec82b9b653ffda76c720_720w.jpg" referrerpolicy="no-referrer"><br>得到连锁不平衡结果<br>
<img src="https://pic1.zhimg.com/80/v2-ea44c3f5d5a332b25710f5e7b090f7d8_720w.jpg" referrerpolicy="no-referrer"><br>总体计算<br><img src="https://pic1.zhimg.com/80/v2-cd250d7576415bea0f3b7e3d40a690b8_720w.jpg" referrerpolicy="no-referrer"><br>分析群体分歧<br>
<img src="https://pic3.zhimg.com/80/v2-1847e2fc55c3fea9dc1e9e15699db202_720w.jpg" referrerpolicy="no-referrer"><br>选择要分析的群体<br>
<img src="https://pic1.zhimg.com/80/v2-9ad404c64e737a389a4c1d5c8de0c208_720w.jpg" referrerpolicy="no-referrer"><br>得到结果<br>
<img src="https://pic3.zhimg.com/80/v2-0c6f4d7f5c6eb5a0bba872840efdfda6_720w.jpg" referrerpolicy="no-referrer"><br>可以将各种分析具体结果保存<br>
<img src="https://pic1.zhimg.com/80/v2-337f7cf4ae1febf71c285156116e7198_720w.jpg" referrerpolicy="no-referrer">]]></description><link>软件\其它生信软件\a-j\dnasp：基础使用说明.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/DnaSP：基础使用说明.md</guid><pubDate>Tue, 29 Oct 2024 09:09:43 GMT</pubDate><enclosure url="https://pic4.zhimg.com/80/v2-4b74092fea46d2dc9cbf371f606b9bd7_720w.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://pic4.zhimg.com/80/v2-4b74092fea46d2dc9cbf371f606b9bd7_720w.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[AMOVA介绍]]></title><description><![CDATA[ 
 <br><br><a data-href="分子方差分析（AMOVA ，Analysis of Molecular Variance)" href="术语\分子方差分析（amova-，analysis-of-molecular-variance).html" class="internal-link" target="_self" rel="noopener nofollow">分子方差分析（AMOVA ，Analysis of Molecular Variance)</a>，是一种用于分析遗传数据中种群间和种群内遗传差异的统计方法。"分子方差分析"或"分子差异分析"。是一种用于分析遗传数据中种群间和种群内遗传差异的统计方法。<br><br>我们需要序列文件，文件格式可以使用fasta，其他格式可以参照说明书。<br><br><br>这是一个生物信息文件的格式转换器，可以在多种格式之间相互转换。我们需要将fasta文件转换成能够被Arlequin识别的形式。<br>
<br><a data-tooltip-position="top" aria-label="https://dnasp.software.informer.com/5.1/" rel="noopener nofollow" class="external-link" href="https://dnasp.software.informer.com/5.1/" target="_blank">DnaSP</a>下载
<br>打开DnaSP

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272244083.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272244260.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272245693.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>弹窗不管，点击close
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272246152.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>根据数据的类型选择，这里以<a data-href="线粒体DNA（mtDNA）" href="术语\线粒体dna（mtdna）.html" class="internal-link" target="_self" rel="noopener nofollow">线粒体DNA（mtDNA）</a>为代表<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272247099.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272247759.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272248321.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">这一步进行分组，分组的依据根据自己的情况而定。
<br>输入组名<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272249568.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>把所有序列进行分组，然后点击Update<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272250762.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272251623.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272251475.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>保存文件，注意有2个文件，请保存在一起。<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272252022.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br><br>打开Arlequin软件，[下载地址](<a data-tooltip-position="top" aria-label="http://cmpg.unibe.ch/software/arlequin35/" rel="noopener nofollow" class="external-link" href="http://cmpg.unibe.ch/software/arlequin35/" target="_blank">Arlequin 3.5 (unibe.ch)</a>)。<br>
1. <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272253988.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
2. <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272254163.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
3. <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272255968.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
- 排列次数（No. of permutations）：这个数字决定了统计显著性测试的准确性。1000是一个常用的默认值，但如果您的数据量很大或者您需要更精确的p值，您可以选择一个更高的值。<br>
- Compute Minimum Spanning Network (MSN) among haplotypes：如果您对不同单倍型间的关系感兴趣，可以选择这个选项来生成网络。<br>
- Compute distance matrix：这决定了是否计算距离矩阵。选择适合您数据类型的距离计算方法。例如，如果您的数据是序列数据，您可能会选择“Pairwise difference”。<br>
- Gamma a value：这是用于校正分子距离的形状参数，它依赖于数据。如果您不确定该如何设置，通常保留为0是一个安全的选择，这意味着不对距离进行校正。<br>
- Print distance matrix：如果您希望在输出中包含距离矩阵，可以勾选此选项。<br>
1. <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272255639.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
2. 最后<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272255877.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
3. 找到结果文件（通常在安装软件的文件夹里），用记事本打开XML文件<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272256317.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
4. 大功告成！！！！<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272257273.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在Arlequin软件中，“no of permutation”（排列数）是用来在进行种群遗传结构分析时估计统计显著性的参数。Arlequin是一个用于种群遗传学分析的软件包，它可以执行多种统计分析，包括AMOVA（分子方差分析）、F统计量、单倍型多样性分析等。排列测试（permutation test）是一种非参数统计方法，通过重新分配数据中的观察值来生成参考分布，从而计算出观察到的统计量的显著性。<br>在Arlequin中设置的“no of permutation”（排列次数）值越大，得到的统计结果越稳定，显著性测试的结果也越可靠。这是因为在排列测试中，通过随机重新排序样本标签来生成大量的数据集，然后对每个数据集进行分析，以计算出原始数据观察到的统计量的分布。这个过程重复进行指定的排列次数。“no of permutation”值决定了将生成多少个这样的随机数据集。例如，如果设置为1000，那么将生成1000个随机数据集来估计统计量的分布。<br>然而，需要注意的是，增加排列次数会显著增加计算时间。因此，在实际应用中需要在计算精度和计算时间之间做出平衡。通常情况下，排列次数设置为1000至10000之间可以提供一个相对稳定的显著性估计，但对于需要更高精度的分析，可能需要更多的排列次数。<br>总的来说，“no of permutation”参数在Arlequin软件中的作用是控制进行统计显著性测试时排列测试的重复次数，其数值大小直接影响到分析结果的稳定性和可靠性。在使用Arlequin进行分析时，适当选择“no of permutation”的值对于确保结果的准确性和科学性至关重要。]]></description><link>软件\其它生信软件\a-j\dnasp：amova及fst分析软件操作.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/DnaSP：AMOVA及Fst分析软件操作.md</guid><pubDate>Fri, 21 Jun 2024 07:31:27 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272244083.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401272244083.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[1. 群体演化历史分析的案例：]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://www.jianshu.com/p/a2ee87918b1c" rel="noopener nofollow" class="external-link" href="https://www.jianshu.com/p/a2ee87918b1c" target="_blank">fastsimcoal2推断群体演化历史(Demographic history) - 简书 (jianshu.com)</a><br><br>群体演化包括有效群体大小(Ne)变化、基因流，迁徙，分化等，会对等位基因频率产生显著影响，塑造了现有遗传多样性的模式和水平。群体演化、遗传漂变和自然选择共同决定了基因组遗传多样性的命运。<br>
Example: 苏格兰罕见的食肉褐鳟（rare piscivorous brown trout: ferox）与普通褐鳟（normal brown trout）在进化过程中是否有基因交流？<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021648695.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021648797.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><a data-href="等位基因频谱(allele frequency spectrum,SFS)" href="术语\等位基因频谱(allele-frequency-spectrum,sfs).html" class="internal-link" target="_self" rel="noopener nofollow">等位基因频谱(allele frequency spectrum,SFS)</a><br>
单个群体:<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021655249.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
两个群体:<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021656103.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
颜色越偏红，表示数量越多，越偏蓝表示数量越少。如果两个群体完全分开，那它们derived allele频率相同的交集就越少，表现在2D SFS上就是密度偏向各自的坐标轴，如果群体交流混合，它们derived allele频率相同的交集就越多，表现在2D SFS上就是密度偏向x=y的这条对角线。后面两个模型对SFS的影响很像，都是使两群体的SFS趋同，可能结合群体分化时间，核苷酸突变速率等推断具体是哪种模型。<br><br>  <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021657299.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">个人理解是，通过推断参数，给出模型，使expected SFS更合理，也就是更加类似于两个分离的群体。模型参数包括：群推分歧时间（也可以自己提供），基因流，以及有效群体大小。<br><br>fastsimcoal2是由伯尔尼大学的Laurent Excoffier小组2016年开发的一种非常灵活的人口统计(Demography)建模软件。 它通过执行合并模拟，使用位点频谱（SFS），推断最适合所观察数据的模型参数。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021658935.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>软件下载<br>
官网：<a data-tooltip-position="top" aria-label="https://links.jianshu.com/go?to=http%3A%2F%2Fcmpg.unibe.ch%2Fsoftware%2Ffastsimcoal2%2F" rel="noopener nofollow" class="external-link" href="https://links.jianshu.com/go?to=http%3A%2F%2Fcmpg.unibe.ch%2Fsoftware%2Ffastsimcoal2%2F" target="_blank">http://cmpg.unibe.ch/software/fastsimcoal2/</a><br>
使用fastsimcoal2和模拟数据在简单模型下推断参数<br>输入文件<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021658527.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>使用 fastsimcoal2和软件自带的测试数据在简单模型下推断参数:<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021658284.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>文件 2PopDiv20Mb_jointDAFpop1_0.obs 代表了一个<a data-href="联合衍生等位基因频率谱（joint Derived Allele Frequency spectrum， joint DAF）" href="术语\联合衍生等位基因频率谱（joint-derived-allele-frequency-spectrum，-joint-daf）.html" class="internal-link" target="_self" rel="noopener nofollow">联合衍生等位基因频率谱（joint Derived Allele Frequency spectrum， joint DAF）</a>的观察数据，用于描述两个种群之间的遗传分化。在这个上下文中，“joint”意味着数据矩阵同时考虑了两个种群的信息，而不是单独分析。这个文件特别用于分析和模拟两个种群的分化历史，其内容和结构反映了两个种群中衍生等位基因的分布情况。<br><br>文件2PopDiv20Mb_jointDAFpop1_0.obs是两个群体的观测2D-SFS.<br># set the working directory
setwd("/Software/fsc26_mac64/example files/2PopDiv20Mb")
# read the observed SFS
# 2D from two populations with different effective sizes that diverged some time ago
pop2d &lt;- read.table("2PopDiv20Mb_jointDAFpop1_0.obs", skip=1, stringsAsFactors = F, header=T, row.names = 1)
head(pop2d)
         d0_0 d0_1 d0_2 d0_3 d0_4 d0_5
d1_0 19985747 8350 1628  360   62    8
d1_1      966    0    0    0    0    0
d1_2      479    0    0    0    0    0
d1_3      328    0    0    0    0    0
d1_4      249    0    0    0    0    0
d1_5     1760   13   18   13   19    0
复制<br><br>每个模型都在 TPL 文件中定义，我们要推断的参数都有名称标签（例如 NPOP，TDIV）.<br>//Parameters for the coalescence simulation program : fsimcoal2.exe
2 samples to simulate :
//Population effective sizes (number of genes)
NPOP1
NPOP2
//Samples sizes and samples age 
5
5
//Growth rates  : negative growth implies population expansion
0
0
//Number of migration matrices : 0 implies no migration between demes
0
//historical event: time, source, sink, migrants, new deme size, new growth rate, migration matrix index
1  historical event
TDIV 1 0 1 RESIZE0 0 0
//Number of independent loci [chromosome] 
1 0
//Per chromosome: Number of contiguous linkage Block: a block is a set of contiguous loci
1
//per Block:data type, number of loci, per generation recombination and mutation rates and optional parameters
FREQ  1   0   2.5e-8 OUTEXP
复制<br>EST文件中定义了每个参数的搜索范围。 对于每个参数，我们使用相应的参数名称标签指定搜索范围。<br>// Search ranges and rules file
// ****************************

[PARAMETERS]
//#isInt? #name   #dist.#min  #max 
//all Ns are in number of haploid individuals
1  NPOP1       logunif  10   1e7   output
1  NPOP2       logunif  10   1e7   output
1  NANC        logunif  10   1e7   output 
1  TDIV        unif     10   1e5   output 

[RULES]

[COMPLEX PARAMETERS]

0  RESIZE0   = NANC/NPOP1      hide
复制<br>注意：TPL 和 EST 文件需要具有相同的文件名，但具有不同的扩展名：filename.est 和 filename.tpl。<br><br>#参数估计（推测群体演化历史）
fsc26 -t PopDiv_diff.tpl -e PopDiv_diff.est -d -0 -n 100000 -L 40 -s 0 -M -q -c 80
#还可以产生模拟群体数据（另一个功能）
#使用输入参数文件中定义的参数值来模拟进化模型下的数据
fsc26 -i test.par -n 100
#使用从先验随机抽取的参数值来模拟进化模型下的数据
fsc26 -t test.tpl -n 10 -e test.est -E 100
#使用在外部文件中定义的参数值来模拟进化模型下的数据
fsc26 -t test.tpl -n 100 -f test.def
复制<br>参数说明：<br>-n: 模拟次数，该值应大于100,000。建议使用200,000到1,000,000。
-L: 迭代次数，该值至少应为20，建议使用50和100之间。
-M: 使用似然优化推断参数。
-d: 对于derived SFS使用-d，对于MAF SFS使用-m。
-0: 说明观察到的SFS中没有单态位。
-q: 快速模式，不打印所有信息。
-C: 为具有至少1个SNP的所有输入计算似然。如果指定-Cx，则所有少于x个SNP的条目将汇集在一起。当观察到SFS中很多位点SNP很少时，此选项用以避免过度拟合。
-c: 指定多线程的选项。 -c1 -B1用于单核，-c4 -B4用于4核。
复制<br><br>对我们最重要的三个文件：<br>* .bestlhoods：具有最大似然参数估计值和相应似然性的文件。 这就是我们想要的-参数估计！
* _DAFpop0.txt：具有通过优化过程中使似然性最大化的参数获得的预期SFS的文件，对于检查SFS是否合适是必需的。
* .simparam：文件中包含运行模拟的设置示例，用于错误时检查。
复制<br>查看结果<br>#读取最大似然估计参数文件
maxlhoodEstimates &lt;- read.table(paste(settings$pathTo_InputFile, "/",
                                      settings$TPL_EST_file_tag, "/",
                                      settings$TPL_EST_file_tag, ".bestlhoods",
                                      sep=""), header=T)
#查看估计参数
maxlhoodEstimates
复制<br>其中，MaxObsLhood指如果期望值与观察到的SFS完全匹配，即期望的SFS是相对观察到的SFS，则值越大。MaxEstLhood是根据模型参数估计的最大似然，拟合度越高，MaxObsLhood和MaxEstLhood之间的差异就越小。<br>#获取观察到的和预期的SFS
# Fit of the model expected SFS to the observed SFS

# Tag for the end of the observed SFS file
obsfileend &lt;- "_jointDAFpop1_0"

# Read the observed SFS - SNP counts
obssfs &lt;- read.table(paste(settings$pathTo_InputFile,
                    settings$TPL_EST_file_tag, obsfileend, ".obs",
                    sep=""), skip=1, stringsAsFactors = F, header=T)

# Read the expected SFS - PROBABILITIES
expsfs &lt;- read.table(paste(settings$pathTo_InputFile,
                    settings$TPL_EST_file_tag, "/",
                    settings$TPL_EST_file_tag, obsfileend, ".txt",
                    sep=""), header=T)

# Plot the fit of the 2D SFS, including of the marginal 1D SFS
# the function plot2dSFS is defined in the utilfunctions.r
# you need to give as input the following arguments
#    obsSFS : matrix with observed SFS (counts)
#    expSFS : matrix with expected SFS (probabilities)
#    xtag : string with the label of x-axis
#    ytag : string with the label of y-axis
#    minentry : number with the minimum entry in the SFS (all entries with less than this are pooled together)
plot2dSFS(obsSFS=obssfs, expSFS=expsfs, xtag="Pop2", ytag="Pop1", minentry=1)
复制<br>  <img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021701041.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>#绘制模型
#maxL.par file 是一个最大似然估计参数文件
path_to_maxL_file &lt;- paste(settings$pathTo_InputFile, settings$TPL_EST_file_tag, "/",settings$TPL_EST_file_tag, "_maxL", sep="")
parFileInterpreter(args=path_to_maxL_file, pop.names=c("Pop1","Pop2"), gentime=1, printPDF=FALSE)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021702170.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\其它生信软件\a-j\fastsimcoal2.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/fastsimcoal2.md</guid><pubDate>Fri, 21 Jun 2024 07:31:06 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021648695.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404021648695.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[GUIDANCE2 Server ：多序列比对置信软件]]></title><description><![CDATA[ 
 <br><a data-href="GUIDANCE2：准确检测不可靠的对准区域，考虑多个参数的不确定性" href="文献及报道\文献\2023年阅读\guidance2：准确检测不可靠的对准区域，考虑多个参数的不确定性.html" class="internal-link" target="_self" rel="noopener nofollow">GUIDANCE2：准确检测不可靠的对准区域，考虑多个参数的不确定性</a><br><a data-tooltip-position="top" aria-label="https://taux.evolseq.net/guidance/source" rel="noopener nofollow" class="external-link" href="https://taux.evolseq.net/guidance/source" target="_blank">GUIDANCE Server - a web server for assessing alignment confidence score (evolseq.net)</a><br><a data-tooltip-position="top" aria-label="https://github.com/anzaika/guidance" rel="noopener nofollow" class="external-link" href="https://github.com/anzaika/guidance" target="_blank">anzaika/guidance: A stable place for guidance source code http://guidance.tau.ac.il/ver2/source.php (github.com)</a><br><br>这个软件的作用是调用多序列比对软件，例如<a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>,MUSCLE等进行比对，并对获得的位点按照系统发育置信度进行打分+可视化。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405091435936.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405091435963.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>下载，之后执行<br>tar -xzvf guidance.v2.02.tar.gz
cd guidance.v2.02  
make
复制<br><br>任选其一安装：<br>
<br>
MAFFT:&nbsp;Type "mafft" and check that you have version 6.712 or newer.

<br>
Else download and install MAFFT from:<br>
<a rel="noopener nofollow" class="external-link" href="http://mafft.cbrc.jp/alignment/software/" target="_blank">http://mafft.cbrc.jp/alignment/software/</a>

<br>
PRANK:&nbsp;Type "prank" and check that you have version v.100223 or newer.

<br>
Else download and install PRANK from:<br>
<a rel="noopener nofollow" class="external-link" href="http://www.ebi.ac.uk/goldman-srv/prank/prank/" target="_blank">http://www.ebi.ac.uk/goldman-srv/prank/prank/</a>

<br>
CLUSTALW:&nbsp;Type "clustalw" and check that you have it insalled  

<br>Else download and install CLUSTALW from:<br>
<a rel="noopener nofollow" class="external-link" href="http://www.ebi.ac.uk/Tools/clustalw2/index.html" target="_blank">http://www.ebi.ac.uk/Tools/clustalw2/index.html</a>


<br><br>GUIDANCE also uses Perl, BioPerl and Ruby:<br>
<br>
Type "perl -v" and check that you Perl installed.

<br>
Else download and install it from:<br>
<a rel="noopener nofollow" class="external-link" href="http://www.perl.org/" target="_blank">http://www.perl.org/</a>

<br>
Type "perl -e 'use Bio::SeqIO'" to check that you have BioPerl.

<br>
Else download and install it from:<br>
<a rel="noopener nofollow" class="external-link" href="http://www.bioperl.org/" target="_blank">http://www.bioperl.org/</a>

<br>
Type "ruby --version" to check that you have ruby.

<br>
Else download and install it from:<br>
<a rel="noopener nofollow" class="external-link" href="http://www.ruby-lang.org/en/" target="_blank">http://www.ruby-lang.org/en/</a>

<br><br>guidance/www/Guidance/guidance.pl
复制<br>用法：<br>perl &lt;guidance directory&gt;/www/Guidance/guidance.pl \ 
--seqFile SEQFILE \
--msaProgram [MAFFT|PRANK|CLUSTALW|MUSCLE] \
--seqType [aa|nuc|codon] \
--outDir FULL_PATH_OUTDIR
复制<br>必需的参数：<br>
dd<br>
<br>--seqFile：输入的序列文件，需为FASTA格式
<br>--msaProgram：使用的多序列比对（MSA）程序
<br>--seqType：对齐的序列类型（氨基酸、核苷酸或密码子）
<br>--outDir：将自动创建并包含所有输出文件的输出目录 [请提供完整路径而非相对路径]
<br>可选参数：<br>
<br>--program [GUIDANCE2|GUIDANCE|HoT] 默认=GUIDANCE2
<br>--bootstraps：引导迭代次数（仅限于 GUIDANCE）。默认值=100。越多越慢。
<br>--genCode：遗传密码标识符（仅限于密码子序列）。默认值=1
<br>1) Nuclear Standard
15) Nuclear Blepharisma
6) Nuclear Ciliate
10) Nuclear Euplotid
2) Mitochondria Vertebrate 线粒体脊椎动物
5) Mitochondria Invertebrate
3) Mitochondria Yeast
13) Mitochondria Ascidian
9) Mitochondria Echinoderm
14) Mitochondria Flatworm
4) Mitochondria Protozoan
复制<br>
<br>--outOrder  默认=aligned。也可以选择 as_input
<br>--msaFile：输入的比对文件 - 不推荐，参见概述部分
<br>--seqCutoff：置信度截止值，介于0到1之间。默认值=0.6
<br>--colCutoff：置信度截止值，介于0到1之间。默认值=0.93
<br>--mafft：mafft可执行文件的路径。默认值=mafft
<br>--prank：prank可执行文件的路径。默认值=prank
<br>--muscle：muscle可执行文件的路径。默认值=muscle
<br>--pagan：pagan可执行文件的路径。默认值=pagan
<br>--ruby：ruby可执行文件的路径。默认值=ruby
<br>--dataset：数据集的唯一名称 - 将用作输出的前缀（默认值=MSA）
<br>--MSA_Param：为比对程序传递参数，例如 PRANK 的-F。要传递包含‘-’的参数，每个‘-’前都要加上反斜杠，例如 \-F
<br>--proc_num：使用的处理器数量（默认值=1）<br>
示例：
<br>guidance \
--seqFile /mnt/e/Scientifc_software/guidance-master/guidance-master/Tem/Tem.fasta \
--msaProgram muscle \
--seqType nuc \
--genCode 2 \
--outDir /mnt/e/Scientifc_software/guidance-master/guidance-master/Tem/outcome \
--bootstraps 5 \
--proc_num 24 \
--dataset TEM \
# 将使用[[MAFFT]]比对“10K_1069_hap.fasta”中的序列，并将所有结果输出到目录“/somewhere/protein.guidance”  

perl &lt;guidance directory&gt;/www/Guidance/guidance.pl \
--seqFile codingSeq.fas \
--msaProgram PRANK \
--seqType codon \
--outDir /somewhere/codingSeq.guidance \
--genCode 2 \
--bootstraps 30 
# 将使用PRANK比对“codingSeq.fas”中的密码子序列，并使用脊椎动物线粒体遗传密码进行翻译，将所有结果输出到目录“/somewhere/codingSeq.guidance”。只进行30次引导迭代，而不是默认的100次（将运行时间缩短3倍）
复制]]></description><link>软件\其它生信软件\a-j\guidance2-server-：多序列比对置信软件.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/GUIDANCE2 Server ：多序列比对置信软件.md</guid><pubDate>Sun, 08 Sep 2024 13:18:10 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405091435936.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405091435936.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Haplocheck：检测污染]]></title><description><![CDATA[ 
 <br>Haplocheck是一款检测mtDNA污染的软件，通过与rCRS进行分单倍型。<br> # 创建文件架
 mkdir haplocheck
 # 下载压缩包
 wget https://github.com/genepi/haplocheck/releases/download/v1.3.3/haplocheck.zip
 # 解压压缩包
 unzip haplocheck.zip
 # 运行程序
 # 实例
 wget https://github.com/genepi/haplocheck/tree/master/test-data/contamination/1000G/all/1000g-nobaq.vcf.gz
 
 # 实例
./haplocheck --out /home/luolintao/haplocheck/example/example1000G /home/luolintao/haplocheck/example/1000g-nobaq.vcf.gz 

firefox 1000g-results/report/report.html
 # 命令  ./haplocheck --out 输出文件.txt 输入文件.vcf/bam
 ./haplocheck --out /home/luolintao/haplocheck/outcomes/Illumina_mtDNA.txt /home/luolintao/Illumina_data/Illumina_mtDNA.vcf
复制]]></description><link>软件\其它生信软件\a-j\haplocheck：检测污染.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/Haplocheck：检测污染.md</guid><pubDate>Fri, 21 Jun 2024 07:30:24 GMT</pubDate></item><item><title><![CDATA[在本地运行 Haplocheck]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://mitoverse.readthedocs.io/haplocheck/haplocheck/#bam-input" rel="noopener nofollow" class="external-link" href="https://mitoverse.readthedocs.io/haplocheck/haplocheck/#bam-input" target="_blank">Haplocheck - Mitoverse</a>
<br>Haplocheck 通过分析线粒体 DNA 来检测 mtDNA 和 WGS 测序研究中的污染模式。 Haplpcheck 还可以作为 nDNA 研究的代理工具，为用户提供图形报告以进一步调查污染情况。<br><br>在VCF文件上运行单倍体检查，同质基因型必须编码为“1/1”，异质基因型必须编码为“0/1”。您可以通过从 BAM 文件启动单倍体检查（请参阅 mutserve）或使用最先进的变体调用程序（例如 GATK MuTect2）来创建这样的 VCF。<br>mkdir haplocheck
wget https://github.com/genepi/haplocheck/releases/download/v1.3.2/haplocheck.zip
unzip haplocheck.zip
./haplocheck --out &lt;out-file&gt; &lt;input-vcf&gt;
复制]]></description><link>软件\其它生信软件\a-j\haplocheker：软件安装和使用.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/Haplocheker：软件安装和使用.md</guid><pubDate>Tue, 29 Oct 2024 09:52:02 GMT</pubDate></item><item><title><![CDATA[使用方法]]></title><description><![CDATA[ 
 <br>自从 <a data-href="Haplogrep3：软件使用手册" href="软件\其它生信软件\a-j\haplogrep3：软件使用手册.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🙂" aria-label="🙂" data-icon="🙂" aria-hidden="true" style="transform: translateY(0px);"></span>Haplogrep3：软件使用手册</a><img class="emoji" draggable="false" alt="🙂" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f642.svg" height="18px" style="max-width: 100%;"> 软件出来之后，这个软件已经被忽视了。但是最近我发现了一个很糟糕的事情，就是在第三代软件中不能够使用第二代可以使用的带位点信息的系统发育树。我发邮件问了作者，他让我用第二代。嗯，很简洁的回答。<br>
第二代网站链接 ： <a rel="noopener nofollow" class="external-link" href="https://github.com/seppinho/haplogrep-cmd" target="_blank">https://github.com/seppinho/haplogrep-cmd</a><br>
我不知道第二代会不会网站出问题，我存了一份，可以发邮件给我： <a data-tooltip-position="top" aria-label="mailto:gaintlinlinlin@gmail.com" rel="noopener nofollow" class="external-link" href="mailto:gaintlinlinlin@gmail.com" target="_blank">gaintlinlinlin@gmail.com</a><br><br><br>输入文件可以是 fasta 或者 vcf。<br>注意
fasta 文件不能够出现任何特殊字符，只能够包含 A,G,C,T。也就是说不能够使用对齐的 fasta 文件。用原始的即可。<br>
VCF 文件分为两种，一种是芯片的，另一种是 WGS 的。如果是芯片的，就要加上一个参数 --chip
<br><br><br>haplogrep-cmd-master/haplogrep classify \
--in input/你的数据.vcf \
--format vcf \
--phylotree 17 \
--extend-report \
--lineage 1 \
--out outcome/1 \
--chip
# --lineage 0代表不生成。1代表生成位点和拓扑结构。2代表生成拓扑结构。选1最好。
复制<br><br>haplogrep-cmd-master/haplogrep classify \
--in input/你的数据.vcf \
--format vcf \
--phylotree 17 \
--extend-report \
--lineage 1 \
--out outcome/1 \
复制<br><br>haplogrep-cmd-master/haplogrep classify \
--in input/你的数据.fasta \
--format fasta \
--phylotree 17 \
--extend-report \
--lineage 1 \
--out outcome/1 \
复制<br><br>会生成一个 dot 文件，使用 dot 软件即可。软件下载很简单，linux 系统直接 sudo get 即可。 这个文件被用于可视化：<br>dot 1.dot \
-Tpdf &gt; 1.pdf 
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407182231091.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这个软件只能使用 phylotree 17.0 。我不知道怎么使用更新的树。所以我直接把想要的树的内容（从 haplogrep3获得）将原本的文件进行替换。我不知道有没有用。<br>举例来说：<br>Haplogrep3/trees/phylotree-rcrs/17.2
Haplogrep3/trees/phylotree-rcrs/17.1
Haplogrep3/trees/phylotree-rcrs/17.0
复制<br>内部各自有 树.xml 和对应的权重 weghit.txt 文件：<br>Haplogrep3/trees/phylotree-rcrs/17.2/weights.txt
Haplogrep3/trees/phylotree-rcrs/17.2/tree.xml
复制<br>分别可以用来替换第二代软件的树文件：<br>haplogrep-cmd-master/data/phylotree/phylotree17_FU1a.xml
haplogrep-cmd-master/src/main/resources/phylotree17_FU1a.xml
haplogrep-cmd-master/src/main/resources/weights17_FU1a.txt
复制<br>请一定不要改文件名字，仅替换内容！]]></description><link>软件\其它生信软件\a-j\haplogrep2：生成突变位点信息的支系系统发育树.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/Haplogrep2：生成突变位点信息的支系系统发育树.md</guid><pubDate>Thu, 18 Jul 2024 14:42:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f601.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f601.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Haplogrep3：软件使用手册]]></title><description><![CDATA[ 
 <br>如果希望生成带有突变位点的系统发育树，可以使用第二代：<a data-href="Haplogrep2：生成突变位点信息的支系系统发育树" href="软件\其它生信软件\a-j\haplogrep2：生成突变位点信息的支系系统发育树.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="😁" aria-label="😁" data-icon="😁" aria-hidden="true" style="transform: translateY(0px);"></span>Haplogrep2：生成突变位点信息的支系系统发育树</a><img class="emoji" draggable="false" alt="😁" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f601.svg" height="18px" style="max-width: 100%;"><br>#### Haplogrep3 linux系统使用手册###
#### 软件的功能可以#################
#### 1.单倍群分型###################
#### 2.根据参考序列生成FASTA#######
#### 3.其他功能，我猜测你用不上#####
###################################  
## 先下载软件：
wget https://github.com/genepi/haplogrep3/releases/download/v3.2.1/haplogrep3-3.2.1-linux.zip
unzip haplogrep3-3.2.1-linux.zip
./haplogrep3
## 或者你可以使用最原始的办法，win访问，然后把安装包放进linux，解压
https://haplogrep.readthedocs.io/en/latest/installation/
https://github.com/genepi/haplogrep3/releases/tag/v3.2.1
# 现在开始安装参考树，请分步执行下列，我也不知道为什么，一起执行会报错
# 打开haplogrep目录
cd Haplogrep/
# 执行以下
## 安装PhyloTree (rCRS Human mtDNA)
./haplogrep3 install-tree phylotree-rcrs@17.2
./haplogrep3 install-tree phylotree-rcrs@17.1
./haplogrep3 install-tree phylotree-rcrs@17.0
./haplogrep3 install-tree phylotree-rcrs@16.0
./haplogrep3 install-tree phylotree-rcrs@15.0

## 安装PhyloTree (RSRS Human mtDNA)
./haplogrep3 install-tree phylotree-rsrs@17.1
./haplogrep3 install-tree phylotree-rsrs@17.0

## 安装phylotree-forensic-rcrs
./haplogrep3 install-tree phylotree-fu-rcrs@1.2
./haplogrep3 install-tree phylotree-fu-rcrs@1.1
./haplogrep3 install-tree phylotree-fu-rcrs@1.0

## 现在就可以运行了 

./haplogrep3 classify \
--input 你的文件.vcf \
--tree phylotree-rcrs@17.2 \
--output Haplogrep/outcome/1.txt
# 现在可以进行分类了

## 在haplogrep目录下执行以下，例如，注意不合理的空格

./haplogrep3 classify \
--input 你的文件.fasta \
--tree phylotree-rcrs@17.0 \
--output Haplogrep/outcome/Illumina_mtDNA.txt \
--write-fasta

./haplogrep3 classify \
--input 你的文件.vcf \
--tree phylotree-rcrs@17.0 \
--output 10K_HGDP/example2/new.txt \
--write-fasta

# 把上面的Haplogrep/data/examples/example-wgs.vcf替换成输入文件# vcf文件的格式可以参考# https://github.com/genepi/haplogrep3/blob/master/data/examples/example-wgs.vcf

# 把--tree phylotree-rcrs@17.2替换为安装的树
# 备选的树有以下
#—————————— phylotree-rcrs@17.2
#—————————— phylotree-rcrs@17.1
#—————————— phylotree-rcrs@17.0
#—————————— phylotree-rcrs@16.0
#—————————— phylotree-rcrs@15.0
#—————————— phylotree-rsrs@17.1
#—————————— phylotree-fu-rcrs@1.2
#—————————— phylotree-fu-rcrs@1.1
#—————————— phylotree-fu-rcrs@1.0
#把--out Haplogrep/outcome/XXX.txt替换为输出文件
# 非必须参数说明
--metric, distance &amp;nbsp;To change the classification metric to Hamming Distance (hamming) or Jaccard (jaccard) add this parameter (Default: Kulczynski Measure).
--extend-report For additional information on SNPs (e.g. found or remaining polymorphisms) please add the --extend-report flag (Default: off).
--tree &amp;nbsp;Specify one of the installed trees.
--chip &amp;nbsp;If you are using genotyping arrays, please add the --chip parameter to limit the range to array SNPs only (Default: off, VCF only). To get the same behaviour for hsd files, please add only the variants to the range, which are included on the array or in the range you have sequenced (e.g. control region). Range can be sepearted by a semicolon ;, both ranges and single positions are allowed (e.g. 16024-16569;1-576;8860).
--skip-alignment-rules &amp;nbsp;Add this option to skip our rules that fixes the mtDNA nomenclature for fasta import. Click here for further information. Applying the rules is the default option since v2.4.0
--hits &amp;nbsp;To export the best n hits for each sample add the --hits parameter. By default only the tophit is exported.
--write-fasta &amp;nbsp; Write results in fasta format.
--write-fasta-msa &amp;nbsp; Write multiple sequence alignment.
--hetLevel=&lt;value&gt; &amp;nbsp;Add heteroplasmies with a level &gt; from the VCF file to the profile (default: 0.9).
###
#完成！
复制]]></description><link>软件\其它生信软件\a-j\haplogrep3：软件使用手册.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/Haplogrep3：软件使用手册.md</guid><pubDate>Thu, 18 Jul 2024 14:46:41 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f642.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f642.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[一、iTol]]></title><description><![CDATA[ 
 <br><br>访问地址： <a rel="noopener nofollow" class="external-link" href="https://itol.embl.de" target="_blank">https://itol.embl.de</a><br>老牌的进化树美化网站，功能很多。但是网络速度较慢，偶尔需要魔法。<br><br>访问地址： <a rel="noopener nofollow" class="external-link" href="http://www.evolgenius.info/evolview/" target="_blank">http://www.evolgenius.info/evolview/</a><br>优点在于：每种画图方式都会给出示例文件，方便修改。给出的美化方式也非常多，选择性非常大~ 因此作为itol不能用的时候首推备选~  <br><br>访问地址： <a rel="noopener nofollow" class="external-link" href="https://www.chiplot.online/tvbot.html" target="_blank">https://www.chiplot.online/tvbot.html</a><br>优点：开发者制作了非常清楚的操作视频~中文的。<br>哔哩哔哩教程： <a data-tooltip-position="top" aria-label="https://space.bilibili.com/30493771/channel/collectiondetail?sid=192106" rel="noopener nofollow" class="external-link" href="https://space.bilibili.com/30493771/channel/collectiondetail?sid=192106" target="_blank">小驰Coding的个人空间-小驰Coding个人主页-哔哩哔哩视频 (bilibili.com)</a>]]></description><link>软件\其它生信软件\a-j\itol：进化树美化网站.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/iTol：进化树美化网站.md</guid><pubDate>Fri, 23 Aug 2024 08:30:18 GMT</pubDate></item><item><title><![CDATA[iTol：完全美化]]></title><description><![CDATA[ 
 <br>如果得到了 meg 文件，那么就可以构建进化树了。meg 文件的格式大致如下所示，可以使用 MEGA 软件打开。<br>#mega
!TITLE  Genetic distance data from 28 human populations;
!Format DataType=distance;
!Description
     Nei, M. and A. K. Roychoudhury. 1993. Mol. Biol. Evol. 10(5).
     Number of polymoprhic loci used = 18;

#001Han_Anhui
#002Han_Beijing
#003Han_Chongqing
#004Han_Fujian
#005Han_Gansu
#006Han_Guangdong
#007Han_Guangxi
#008Han_Guizhou
#009Han_Hebei
#010Han_Heilongjiang
#011Han_Henan
#012Han_Hubei
#013Han_Hunan
#014Han_InnerMongolia
#015Han_Jiangsu
#016Han_Jiangxi
#017Han_Jilin
#018Han_Liaoning
#019Han_Shaanxi
#020Han_Shandong
#021Han_Shanghai
#022Han_Shanxi
#023Han_Sichuan
#024Han_Tianjin
#025Han_Yunnan
#026Han_Zhejiang


                                                                                                                                                                                         
                                                                                                                        
0.00659                                                                                                                      
0.00006   0.00936                                                                                                                 
0.01018   0.02659   0.00771                                                                                                            
0.0137    0.00177   0.01292   0.02682                                                                                                       
0.0139    0.02777   0.00859   0.00468   0.03214                                                                                                  
0.01807   0.02553   0.01031   0.01301   0.03243   0.00167                                                                                             
0    0.0076    0    0.00334   0.01471   0.00298   0.00349                                                                                        
0.00833   0    0.00988   0.02116   0.00268   0.02359   0.0229    0.00761                                                                                   
0.00904   0.00625   0.01523   0.01808   0.01165   0.02036   0.02388   0.00803   0.00554                                                                              
0.00462   0    0.00647   0.01653   0.00158   0.02029   0.02157   0.00411   0    0.00451                                                                         
0.00186   0.00354   0.00264   0.00884   0.00863   0.01291   0.01533   0    0.00263   0.00387   0                                                                     
0.00635   0.0176    0    0.00874   0.02211   0.00786   0.00816   0    0.01625   0.02125   0.01343   0.00857                                                               
0.02384   0.00124   0.02137   0.04348   0    0.04671   0.03952   0.02248   0.00471   0.02058   0.00588   0.01349   0.03155                                                          
0    0.00795   0.00025   0.00991   0.01347   0.01512   0.01929   0.00016   0.0091    0.01223   0.0051    0.00231   0.00717   0.02379                                                     
0.00379   0.01898   0.00153   0.00771   0.02888   0.01031   0.0142    0    0.01857   0.0198    0.01532   0.00849   0.00307   0.03781   0.00588                                                
0.00569   0.00006   0.009     0.01722   0.00587   0.01996   0.01809   0.00496   0    0.00061   0    0    0.01461   0.0084    0.00569   0.01741                                           
0.00782   0    0.00928   0.0216    0.00215   0.02432   0.02286   0.00764   0    0.00406   0    0.00192   0.01582   0.00524   0.0075    0.01908   0                                       
0.0098    0.00181   0.01028   0.02722   0    0.03313   0.03135   0.01195   0.00326   0.0142    0.00215   0.00811   0.01764   0.00219   0.01062   0.02292   0.00526   0.00402                                 
0.00959   0.00134   0.0116    0.01979   0.00369   0.02397   0.02474   0.0096    0    0.00362   0.00036   0.0025    0.01886   0.00739   0.0094    0.02134   0    0    0.00663                            
0    0.00897   0    0.009     0.01691   0.01298   0.01734   0    0.01017   0.01473   0.0063    0.00319   0.00579   0.02828   0    0.00519   0.00738   0.0087    0.01285   0.01001                       
0.01337   0.00074   0.0144    0.0296    0.00015   0.0351    0.03368   0.01663   0.00178   0.01088   0.00092   0.00586   0.0257    0    0.01229   0.02949   0.00168   0.00153   0.00161   0.00185   0.01459                  
0.00308   0.00543   0    0.01062   0.00878   0.0102    0.00988   0    0.00521   0.01318   0.00364   0.00205   0.00252   0.01399   0.00345   0.00649   0.00464   0.00543   0.00767   0.00759   0.00294   0.0107              
0.01149   0    0.01084   0.02659   0.00201   0.03207   0.0312    0.01168   0.00134   0.01573   0.00249   0.00764   0.01939   0    0.01097   0.02015   0.00786   0.00247   0.00307   0.00342   0.01175   0.0023    0.00776        
0.00555   0.00875   0.00019   0.00348   0.00981   0.00685   0.00819   0    0.00399   0.00899   0.00367   0.00194   0.00259   0.0176    0.00577   0.00691   0.0043    0.00665   0.00892   0.00516   0.00489   0.01205   0.00108   0.00701   
0.00514   0.02349   0.00459   0.00656   0.03086   0.01392   0.02054   0.00309   0.0223    0.02534   0.01773   0.0108    0.00975   0.04352   0.00421   0.00435   0.01751   0.0206    0.02621   0.02228   0.00224   0.03013   0.01012   0.02225   0.00972

复制<br>使用 MEGA 打开之后，再选择导出为 new 或者 newick 文件。<br><br><br>这种风格可以把 ID 归类为不同的组别，如下所示。但是缺点是 不能生成任何颜色。<br>LABELS
#use this template to change the leaf labels, or define/change the internal node names (displayed in mouseover popups)

#lines starting with a hash are comments and ignored during parsing

#=================================================================#
#                    MANDATORY SETTINGS                           #
#=================================================================#
#select the separator which is used to delimit the data below (TAB,SPACE or COMMA).This separator must be used throughout this file (except in the SEPARATOR line, which uses space).

SEPARATOR TAB
#SEPARATOR SPACE
#SEPARATOR COMMA

#Internal tree nodes can be specified using IDs directly, or using the 'last common ancestor' method described in iTOL help pages
#=================================================================#
#       Actual data follows after the "DATA" keyword              #
#=================================================================#
DATA
#NODE_ID,LABEL

#Examples
#defined a name for an internal node


001Han_Anhui	Han_Anhui
002Han_Beijing	Han_Beijing
003Han_Chongqing	Han_Chongqing
004Han_Fujian	Han_Fujian
005Han_Gansu	Han_Gansu
006Han_Guangdong	Han_Guangdong
007Han_Guangxi	Han_Guangxi
008Han_Guizhou	Han_Guizhou
009Han_Hebei	Han_Hebei
010Han_Heilongjiang	Han_Heilongjiang
011Han_Henan	Han_Henan
012Han_Hubei	Han_Hubei
013Han_Hunan	Han_Hunan
014Han_InnerMongolia	Han_InnerMongolia
015Han_Jiangsu	Han_Jiangsu
016Han_Jiangxi	Han_Jiangxi
017Han_Jilin	Han_Jilin
018Han_Liaoning	Han_Liaoning
019Han_Shaanxi	Han_Shaanxi
020Han_Shandong	Han_Shandong
021Han_Shanghai	Han_Shanghai
022Han_Shanxi	Han_Shanxi
023Han_Sichuan	Han_Sichuan
024Han_Tianjin	Han_Tianjin
025Han_Yunnan	Han_Yunnan
026Han_Zhejiang	Han_Zhejiang

复制<br>修改前：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081445061.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
将上述文件保存为 txt，之后直接拖进已经制作好了的进化树。<br>
修改后：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081446213.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这种风格可以添加颜色，并且指定组别。但是我不知道为什么，组别没有显示出来。<br>注意这里有一个雷区，第一列的 ID 不能是群组的名称，必须是你放进进化树的原始 ID 名称，不然会无法识别。<br>TREE_COLORS
SEPARATOR TAB
DATA
Han_Anhui	range	#1b9e77	Southern China
Han_Beijing	range	#d95f02	Northern China
Han_Chongqing	range	#1b9e77	Southern China
Han_Fujian	range	#1b9e77	Southern China
Han_Gansu	range	#d95f02	Northern China
Han_Guangdong	range	#1b9e77	Southern China
Han_Guangxi	range	#1b9e77	Southern China
Han_Guizhou	range	#1b9e77	Southern China
Han_Hebei	range	#d95f02	Northern China
Han_Heilongjiang	range	#d95f02	Northern China
Han_Henan	range	#d95f02	Northern China
Han_Hubei	range	#1b9e77	Southern China
Han_Hunan	range	#1b9e77	Southern China
Han_InnerMongolia	range	#d95f02	Northern China
Han_Jiangsu	range	#1b9e77	Southern China
Han_Jiangxi	range	#1b9e77	Southern China
Han_Jilin	range	#d95f02	Northern China
Han_Liaoning	range	#d95f02	Northern China
Han_Shaanxi	range	#d95f02	Northern China
Han_Shandong	range	#d95f02	Northern China
Han_Shanghai	range	#1b9e77	Southern China
Han_Shanxi	range	#d95f02	Northern China
Han_Sichuan	range	#1b9e77	Southern China
Han_Tianjin	range	#d95f02	Northern China
Han_Yunnan	range	#1b9e77	Southern China
Han_Zhejiang	range	#1b9e77	Southern China
复制<br>修改后：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081449673.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>我没试过。<br>TREE_COLORS				
SEPARATOR TAB				
DATA				
Biaka_Africa017_B2a1a1a1a2	clade	#1b9e77	normal	1
Biaka_Africa018_B2b1a1b	clade	#1b9e77	normal	1
Mbuti_Africa084_B2b1b1b	clade	#1b9e77	normal	1
Ju_hoan_North_Africa059_B2b1b1a	clade	#1b9e77	normal	1
Ju_hoan_North_Africa061_B2b1b1a	clade	#1b9e77	normal	1
I3261_Y.bam__0-0-1-0	clade	#d95f02	dashed	2
I2085_Y.bam__1-0-9-0	clade	#d95f02	dashed	2
I10551_Y.bam__0-0-2-0	clade	#d95f02	dashed	2
I1055_Y.bam__0-0-2-0	clade	#d95f02	dashed	2
I12472_Y.bam__6-0-1-0	clade	#d95f02	dashed	2
X111.1761.9122_E1b1b1b2b2a1a1b	clade	#1b9e77	normal	1
X990.0002.0699_E1b1b1b2a1a1a1a1f1	clade	#1b9e77	normal	1
I3262_Y.bam__102-0-3-9	clade	#d95f02	dashed	2
I1985_Y.bam__121-0-2-8	clade	#d95f02	dashed	2
I6194_Y.bam__105-1-3-14	clade	#d95f02	dashed	2
I1799_Y.bam__104-0-2-12	clade	#d95f02	dashed	2
I6197_Y.bam__115-1-2-13	clade	#d95f02	dashed	2
I6899_Y.bam__119-0-1-8	clade	#d95f02	dashed	2
I6900_Y.bam__84-0-1-9	clade	#d95f02	dashed	2
X111.1731.1637_E1b1b1a1b2a4b1	clade	#1b9e77	normal	1
X111.1782.8401_E1b1b1a1b2a3	clade	#1b9e77	normal	1
X111.1738.4990_E1b1b1a1b2a3	clade	#1b9e77	normal	1
X111.1751.1785_E1b1b1a1b2a3	clade	#1b9e77	normal	1
X111.1825.2625_E1b1b1a1b2a3	clade	#1b9e77	normal	1
X111.1733.4278_E1b1b1a1b2a3	clade	#1b9e77	normal	1
BUR002.B0101_Y.bam__85-0-22-3	clade	#d95f02	dashed	2
HRR579055_Y.bam__7-0-2-2	clade	#d95f02	dashed	2
S13887_Y.bam__3-0-23-0	clade	#d95f02	dashed	2
S13886_Y.bam__5-0-32-0	clade	#d95f02	dashed	2
S13883_Y.bam__4-0-10-0	clade	#d95f02	dashed	2
HRR660200_Y.bam__2-0-5-0	clade	#d95f02	dashed	2
HRR660153_Y.bam__0-0-1-0	clade	#d95f02	dashed	2
HRR660135_Y.bam__1-0-1-0	clade	#d95f02	dashed	2
X111.1707.8820_D1a1b1a	clade	#1b9e77	normal	1
HRR660160_Y.bam__54-0-1-4	clade	#d95f02	dashed	2
X111.1809.7448_D1a1b1a2	clade	#1b9e77	normal	1
X990.0001.1246_D1a1b1a2	clade	#1b9e77	normal	1
HRR660189_Y.bam__92-0-2-0	clade	#d95f02	dashed	2
X111.1287.2485_D1a1b1a1a	clade	#1b9e77	normal	1
X111.1782.5226_D1a1b1a1	clade	#1b9e77	normal	1
X111.1832.1970_D1a1b1a1	clade	#1b9e77	normal	1
TAH002.A0101_Y.bam__81-0-1-0	clade	#d95f02	dashed	2
X111.1790.4105_D1a1b1a1	clade	#1b9e77	normal	1
X111.1783.3276_D1a1b1a1	clade	#1b9e77	normal	1
HRR660134_Y.bam__15-0-19-5	clade	#d95f02	dashed	2
HRR660121_Y.bam__32-0-14-12	clade	#d95f02	dashed	2
HRR660171_Y.bam__0-0-1-1	clade	#d95f02	dashed	2
HRR660138_Y.bam__48-0-18-20	clade	#d95f02	dashed	2
X111.1751.2850_D1a1a1b1a	clade	#1b9e77	normal	1
X111.1734.5045_D1a1a1b2	clade	#1b9e77	normal	1
Y5979_D1a1a1b2	clade	#1b9e77	normal	1
LJM25_Y.bam__2-0-1-0	clade	#d95f02	dashed	2
HRR660152_Y.bam__4-0-2-0	clade	#d95f02	dashed	2
X111.1795.3455_D1a1a1a1b2	clade	#1b9e77	normal	1
TSA004.B0101_Y.bam__79-0-9-6	clade	#d95f02	dashed	2
HRR660149_Y.bam__78-0-11-10	clade	#d95f02	dashed	2
HRR660183_Y.bam__77-0-7-9	clade	#d95f02	dashed	2
ZAM002.A0101_Y.bam__63-1-1-0	clade	#d95f02	dashed	2
X111.1837.3009_D1a1a1a1a1a	clade	#1b9e77	normal	1
X111.1713.9330_D1a1a1a1a1a	clade	#1b9e77	normal	1
X111.1770.0028_D1a1a1a1a1b	clade	#1b9e77	normal	1
X111.1726.8788_D1a1a1a1a1b1	clade	#1b9e77	normal	1
X111.1849.6197_D1a1a1a1a1b2b	clade	#1b9e77	normal	1
X111.1795.8535_D1a1a1a1a1b2a	clade	#1b9e77	normal	1
复制<br><br>全家桶风格，可以添加颜色、组别和标签。<br>
雷区同上。<br>
注意，<br>
<br>下列的 LEGEND_LABELS	Yangtze	Yellow River	Zhujiang请改成自己的标签。
<br>LEGEND_TITLE &nbsp; &nbsp;Classification 3请改成自己的标签。
<br>DATASET_COLORSTRIP
SEPARATOR TAB
BORDER_WIDTH	0.5
COLOR	#bebada
DATASET_LABEL	Classification 7
LEGEND_COLORS	#1b9e77	#d95f02	#7570b3
LEGEND_LABELS	Yangtze	Yellow River	Zhujiang
LEGEND_SHAPES	1	1	1 
LEGEND_TITLE	Classification 7 #什么屌用没有
MARGIN	5
STRIP_WIDTH	25
DATA
Han_Shandong	#d95f02	Yellow River
Han_Henan	#d95f02	Yellow River
Han_Shanxi	#d95f02	Yellow River
Han_Shaanxi	#d95f02	Yellow River
Han_Hebei	#d95f02	Yellow River
Han_Tianjin	#d95f02	Yellow River
Han_Beijing	#d95f02	Yellow River
Han_InnerMongolia	#d95f02	Yellow River
Han_Liaoning	#d95f02	Yellow River
Han_Jilin	#d95f02	Yellow River
Han_Heilongjiang	#d95f02	Yellow River
Han_Jiangsu	#1b9e77	Yangtze
Han_Anhui	#1b9e77	Yangtze
Han_Hubei	#1b9e77	Yangtze
Han_Chongqing	#1b9e77	Yangtze
Han_Sichuan	#1b9e77	Yangtze
Han_Yunnan	#1b9e77	Yangtze
Han_Guizhou	#7570b3	Zhujiang
Han_Hunan	#1b9e77	Yangtze
Han_Jiangxi	#1b9e77	Yangtze
Han_Guangxi	#7570b3	Zhujiang
Han_Guangdong	#7570b3	Zhujiang
Han_Fujian	#7570b3	Zhujiang
Han_Zhejiang	#1b9e77	Yangtze
Han_Shanghai	#1b9e77	Yangtze
Han_Gansu	#d95f02	Yellow River

复制<br>实例效果：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081456091.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>TREE_COLORS
SEPARATOR COMMA
DATA
9031|9606,clade,#0000ff,normal,2
601|340,clade,#ff0000,dashed,2
915|777,branch,#00ff00,dashed,10
9606|5664,branch,#00ff00,dashed,5
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403261339992.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>TREE_COLORS
SEPARATOR TAB
DATA
155892	clade	#d59c9d
I54	clade	#399ca8
214688	clade	#d823bc
83331	clade	#11247c
I112	clade	#aa6e31
1488	clade	#ac96a5
I140	clade	#41c639
193567	clade	#d5588b
44689	clade	#80edad
562	clade	#18a9e1
86665	clade	#2de38e
I182	clade	#6e369d
I172	clade	#a1f50a
I123	clade	#f8f38e
197	clade	#886cb9
I148	clade	#7d6137
I156	clade	#d58066
I7	clade	#60e000
2371	clade	#c19ab1
33903	clade	#6b9e02
I139	clade	#b9a845
I79	clade	#b5cb36
I92	clade	#fc39d2
730	clade	#68ec9c
666	clade	#05ad5f
I88	clade	#9546f6
217992	clade	#bf7ff9
I162	clade	#e25406
I25	clade	#2f1934
I163	clade	#8e7310
I75	clade	#2d5ba5
28227	clade	#4f662a
193567	clade	#763aa2
519	clade	#e3cba9
I128	clade	#59a4a4
35554	clade	#28a7db
83560	clade	#6da85a
I67	clade	#90cd4c
100379	clade	#2d356c
2107	clade	#a07574
5664	clade	#7a1980
I65	clade	#785f89
I103	clade	#d1485e
59374	clade	#7701c3
I77	clade	#09b583
I171	clade	#056379
I11	clade	#cdb3d3
2234	clade	#562f61
I33	clade	#6b2785
217992	clade	#c6d3ec
I185	clade	#dcede9
28227	clade	#ddaaeb
I40	clade	#725a76
I178	clade	#af52de
I161	clade	#23dba3
1309	clade	#913402
I8	clade	#557f42
I20	clade	#e7152c
I184	clade	#9bdb3f
I13	clade	#85601c
I137	clade	#22a736
182710	clade	#3d0602
I8	clade	#567e78
I35	clade	#1b6aa6
I57	clade	#291e86
I52	clade	#5908ca
323	clade	#90daee
I185	clade	#3c815f
I16	clade	#5bd784
I43	clade	#1aa6ab
I21	clade	#0ca5aa
I149	clade	#34c9ad
I165	clade	#c6a67b
267748	clade	#c2070a
83557	clade	#27a72f
134821	clade	#c9c229
9598	clade	#e258ad
I46	clade	#0f1c25
I114	clade	#320254
9031	clade	#2a3942
I84	clade	#244220
I12	clade	#bd4b51
I167	clade	#0b7619
53953	clade	#fe7548
118099	clade	#a314b5
I78	clade	#ef720b
I76	clade	#ca2cc4
I15	clade	#efbf46
4896	clade	#2b9cd8
1309	clade	#ce90cc
I138	clade	#68f531
13773	clade	#02790b
I97	clade	#c0d60b
1148	clade	#2e7ec5
1423	clade	#963d0d
I118	clade	#7fe1c6
I100	clade	#7128e2
I76	clade	#add262
I118	clade	#cbc83f
243164	clade	#78e8f6
155864	label	#4ec8d8		1
83334	label	#1e7756		2
217992	label	#1b0a3d	bold-italic	3
562	label	#894b2e	italic	1.6
198215	label	#155549		2.9
623	label	#139183	italic	0.6
209261	label	#9b5939	italic	2.2
601	label	#142ad6		0.3
602	label	#002e63	italic	0.3
229193	label	#a2f54f		0.3
187410	label	#3c902e	bold	0.7
632	label	#ca5280	italic	1.4
141679	label	#12ce81		1.6
203907	label	#e3be5e	bold-italic	2.5
36870	label	#44d02c	italic	1.8
135842	label	#61a868	italic	0.6
118099	label	#610095	bold	1.6
98794	label	#a79539	italic	0.1
747	label	#548c6d	italic	2.6
727	label	#197192		2.8
730	label	#d5f00f	italic	1
196600	label	#f99ad1		3
672	label	#c83f8a	bold	2.1
670	label	#191eef	bold	2.2
666	label	#96542a		2.2
74109	label	#318d17		2.6
70863	label	#b04dd4		1.6
160488	label	#ffa1fd		2.5
323	label	#19ac81	bold-italic	1.1
287	label	#55d392		0.2
183190	label	#6e001d		0.3
2371	label	#1a1967	italic	1.1
92829	label	#579f7f	bold-italic	2
340	label	#e58d2c		2.1
777	label	#73571d		0.5
65699	label	#f99021	bold-italic	1.8
491	label	#3bfef1		2.8
536	label	#a41642	bold-italic	1.9
520	label	#55fd91		1.9
519	label	#05f1cd		2.7
518	label	#55bfe8		3
305	label	#63c414	bold-italic	0.5
915	label	#9c4626		2.9
181661	label	#8b6ab3	bold	0.5
180835	label	#2b25a6	bold	1.9
382	label	#41550e	bold-italic	1.1
29461	label	#33249f	bold	0.1
29459	label	#4c152b	bold-italic	1.5
381	label	#fa8e16	bold	1.1
1076	label	#2e66d2	bold	1.2
375	label	#86404a	bold-italic	2.8
155892	label	#5894a8	italic	2.2
66077	label	#cb25cd	italic	0.3
782	label	#3c5086	italic	2.7
781	label	#6e8296	bold-italic	1.1
85963	label	#a233d9	italic	0.1
210	label	#ff7900	bold-italic	2.2
32025	label	#f817fe	bold-italic	2.5
844	label	#6f58f4	bold-italic	0.4
197	label	#5c2323		1
882	label	#fcc564		1.6
35554	label	#0f9c26		0.1
959	label	#79b55b	bold	1.6
240015	label	#f7b168	bold	1.7
234267	label	#127b3f	bold	1.5
76856	label	#f17a58	italic	0.3
63363	label	#90a355	bold	0.9
2336	label	#4e8b57	italic	1.6
262724	label	#3bfcd1	bold-italic	2.3
1299	label	#054bc2		0.8
243164	label	#66e411		2.7
103690	label	#265049	bold	1.5
1148	label	#89a4c7	bold-italic	1.6
32046	label	#5ead9e	bold-italic	2.1
84588	label	#4f63c9	bold-italic	2.5
74547	label	#aa9864	bold-italic	1
1219	label	#c38fa3	italic	0.1
59919	label	#fa6d94	bold-italic	1.8
33072	label	#37e7a9	bold-italic	1.2
214688	label	#5b05b3	italic	2
117	label	#b81976	italic	1
44275	label	#bb6429		0.7
173	label	#83ebc0	bold-italic	2.9
160	label	#492f07	italic	2.3
158	label	#61c3b7		2
139	label	#b4c54b		2.2
218496	label	#04c94a	italic	0.2
203267	label	#2e3529	bold	1.3
216816	label	#a1e65c		3
196627	label	#61f4a6	italic	0
1718	label	#848d42		3
152794	label	#0f0160	italic	1.3
1717	label	#986b24	bold	1
1765	label	#e35c6c		1.4
83331	label	#216059	bold	2.4
83332	label	#22b0b2	italic	2.4
1769	label	#94335f	italic	1
1770	label	#2a9e2b	bold-italic	1.7
33903	label	#461ab8	italic	1
1902	label	#ac7912	bold-italic	2.4
59374	label	#903a8a	bold-italic	1.3
1097	label	#41323f	bold	1.9
837	label	#4b0e39	bold-italic	2.4
818	label	#8de36b	italic	0.4
182082	label	#dea709	bold-italic	1.4
138677	label	#2aa65a	bold-italic	1.6
115713	label	#cd94db	bold	2.9
115711	label	#aea241	bold	1.3
83557	label	#5b09cf	italic	2.7
83560	label	#93eb7f	bold-italic	1.2
813	label	#d1c9e8	italic	2.3
119072	label	#dfe385	bold	0.8
1513	label	#450786	bold-italic	1.3
1502	label	#eb36c8	italic	1.2
1488	label	#f805a0	italic	1.6
267748	label	#f49ff0		1.8
2107	label	#767abd	bold-italic	1.2
2104	label	#501b6a	bold-italic	1.8
2097	label	#736ef0		1.4
2096	label	#9a15dc		2.6
28227	label	#c35d6b	bold	1.3
134821	label	#f7774a		0
44101	label	#f30088	bold-italic	1.4
100379	label	#a8d375	bold	1.3
265669	label	#67924c		0.4
1639	label	#6d5f8f		0.6
1642	label	#f79d5f	bold	0
182710	label	#9ef4b3	bold	1.7
86665	label	#1bd38d	bold-italic	1.8
226900	label	#bc3cda	bold	0
222523	label	#b0b252		1
198094	label	#28fc0a	italic	1.2
1423	label	#dd6b41	italic	1.6
196620	label	#ec3a76		1.5
158879	label	#20f9ff		1.8
158878	label	#05a4cb		0.8
1282	label	#48bd64	italic	0.3
216495	label	#d12b6f	italic	1.9
216466	label	#44ce76	italic	0.8
1314	label	#070ed6	bold	0.9
186103	label	#05ec2c	italic	1.7
198466	label	#702bd8		2.9
193567	label	#7179ec	italic	1
1309	label	#2fc5dc		0.6
171101	label	#e2c03d	italic	3
1313	label	#10d04e	italic	0.3
1360	label	#661d87	italic	0.6
1351	label	#c6420a	italic	2.8
33959	label	#cc0f0c	bold	2.9
1590	label	#917c5e	italic	1
296543	label	#80a0e9	bold	1
237895	label	#5906ff		2.4
36329	label	#2f323f		2.2
4530	label	#1158ec	bold-italic	0.2
3702	label	#d4acb0	italic	2.1
45157	label	#924a16	italic	2.2
44689	label	#513e91	italic	0
33169	label	#064b7c	italic	2.5
4932	label	#655e0b	bold	0.4
4896	label	#535ae5	bold-italic	1.9
180454	label	#4fb8b2	bold	1.8
7227	label	#b1504c	bold	2.7
31033	label	#8e1223	bold	2.6
7955	label	#ce9aaa		1.6
10116	label	#66a8dc	bold	2
10090	label	#8a0606	italic	0.2
9606	label	#37a632		2.4
9598	label	#834ccb		2.5
9031	label	#74a0fd	bold-italic	3
6239	label	#632a27	bold	1.6
6238	label	#18f555	bold	2.4
5664	label	#e4c05b	bold-italic	0.1
184922	label	#611001	bold	0
160232	label	#3453bb	italic	2.6
111955	label	#46ba7e		1.8
2287	label	#dcdf91	italic	1.9
56636	label	#7c249a		2.6
13773	label	#1357c8	bold-italic	1.8
50339	label	#4a23a7	bold-italic	2.9
2303	label	#13fab7		0.2
187420	label	#7a6d6b	bold-italic	2.5
2320	label	#6cc975	italic	2.8
39152	label	#a65f47	bold-italic	0.5
2190	label	#222336	bold-italic	1.3
53953	label	#57bcb2	italic	0.1
29292	label	#175c27	bold-italic	1.6
2261	label	#ac0260	bold-italic	2.9
2234	label	#3e0fe8	italic	1.1
64091	label	#3a38ce		0.4
2214	label	#2ab1b0		1.4
2209	label	#7e30a1	italic	2.2

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403261340341.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>TREE_COLORS
SEPARATOR TAB
DATA
777	clade	#c04104	normal	1
I132	clade	#0c4e99	normal	1
187410	clade	#8c6d63	normal	1
I65	clade	#e41ea4	normal	1
I141	clade	#c23551	normal	1
I12	clade	#a16088	normal	1
I57	clade	#02165d	dashed	1
I49	clade	#8a3861	dashed	1
I74	clade	#cabd33	dashed	1
I14	clade	#c4f71f	dashed	1
I142	clade	#d3ddfc	normal	1
36870	clade	#f4980b	dashed	1
85963	clade	#15daa6	normal	1
9031	clade	#d89de9	dashed	1
33072	clade	#bdb6fb	dashed	1
I82	clade	#17785a	normal	1
1717	clade	#d5b328	normal	1
I174	clade	#3ec8c8	normal	1
I65	clade	#48d4ba	dashed	1
562	clade	#661b92	normal	1
155864	clade	#7cd344	dashed	1
216495	clade	#562ca8	dashed	1
173	clade	#e4c19a	dashed	1
4932	clade	#604f50	dashed	1
I29	clade	#f11e8c	normal	1
I39	clade	#f36f0f	normal	1
632	clade	#944341	dashed	1
234267	clade	#90a533	dashed	1
I25	clade	#6fd557	normal	1
I120	clade	#d98334	dashed	1
I78	clade	#4c59d5	dashed	1
1148	clade	#9da900	dashed	1
2107	clade	#26ce7b	dashed	1
I27	clade	#0890ad	normal	1
2190	clade	#fb4f66	dashed	1
56636	clade	#7d70ac	dashed	1
I36	clade	#86dee1	dashed	1
I109	clade	#f11f36	normal	1
I149	clade	#c96de4	dashed	1
I144	clade	#6e89a7	dashed	1
I91	clade	#e84d0e	dashed	1
I159	clade	#68b17d	dashed	1
I85	clade	#89bcf9	normal	1
1513	clade	#254961	dashed	1
I140	clade	#af18a3	normal	1
83331	clade	#6dab05	normal	1
I108	clade	#1f57e7	dashed	1
36329	clade	#b0b398	normal	1
I25	clade	#c4ea85	normal	1
I182	clade	#f52ddf	dashed	1
I104	clade	#5ac3f7	normal	1
882	clade	#2c5f78	normal	1
I80	clade	#fcc1de	normal	1
I166	clade	#f44c45	dashed	1
2104	clade	#16b2ea	dashed	1
134821	clade	#b93601	dashed	1
I105	clade	#72422a	normal	1
182082	clade	#549df3	normal	1
818	clade	#e2f9be	dashed	1
226900	clade	#41db62	normal	1
1360	clade	#f20d36	dashed	1
I181	clade	#bd46f7	normal	1
44275	clade	#d88453	normal	1
I12	clade	#658a38	dashed	1
1502	clade	#852d64	normal	1
64091	clade	#9c64e8	dashed	1
I158	clade	#2f5644	normal	1
118099	clade	#bbe4d7	normal	1
I42	clade	#6279db	normal	1
I70	clade	#236f6e	dashed	1
I84	clade	#b74a7c	normal	1
74109	clade	#400af3	normal	1
I63	clade	#769611	normal	1
139	clade	#6f6544	dashed	1
85963	clade	#a1583a	dashed	1
1639	clade	#e369b2	normal	1
782	clade	#f34457	normal	1
I113	clade	#40cc1f	dashed	1
I162	clade	#a49035	dashed	1
I22	clade	#c90fb1	normal	1
53953	clade	#c53775	normal	1
36870	clade	#1a214a	normal	1
I90	clade	#ec29b0	dashed	1
193567	clade	#fdf50a	dashed	1
2214	clade	#9d9b67	dashed	1
I81	clade	#3d3057	normal	1
I158	clade	#8a265e	dashed	1
382	clade	#d9f4f6	dashed	1
1717	clade	#b86820	normal	1
I188	clade	#681e1f	dashed	1
I53	clade	#806e84	dashed	1
518	clade	#3adb91	normal	1
187420	clade	#9cfabd	normal	1
I50	clade	#21b8f4	dashed	1
1299	clade	#39d38f	normal	1
I69	clade	#9f78c2	dashed	1
31033	clade	#9d87f8	dashed	1
171101	clade	#5b6832	normal	1
I44	clade	#bd63cd	normal	1
3702	clade	#2a279a	normal	1
155864	label	#8f81c6	italic	1.8
83334	label	#07cf6b	bold-italic	1.6
217992	label	#68421f		0.7
562	label	#607f86	bold	0.4
198215	label	#7eb8a3	bold	1.2
623	label	#843dd4	bold	2.2
209261	label	#888a5c		1.2
601	label	#612a06	bold	1.1
602	label	#17b582	bold	1.2
229193	label	#f7c7d8	italic	2
187410	label	#f8973a	bold	2.4
632	label	#d2353c	bold	0.1
141679	label	#4f5962		1.2
203907	label	#1cf3e2		1.6
36870	label	#513c25	bold	0
135842	label	#269606	bold	0.1
118099	label	#b03c37	bold-italic	1.6
98794	label	#59e12c	bold	0
747	label	#5d3c4e	italic	0
727	label	#364cea	italic	1.5
730	label	#b26c8f		2.6
196600	label	#29fb9a	bold	0.4
672	label	#b76706	bold-italic	2
670	label	#a132bf	bold	1.7
666	label	#da48e1	bold	1.5
74109	label	#8d17ef	bold-italic	2.1
70863	label	#979024		0.1
160488	label	#10194e	bold-italic	2.7
323	label	#4e6020		2.4
287	label	#df7ae9	bold	2.3
183190	label	#12853b		0.6
2371	label	#8aa569	bold-italic	2.4
92829	label	#6af6a1	bold	0.9
340	label	#3d5c8d	bold	2.5
777	label	#fcea84	bold-italic	2.6
65699	label	#9c38e7		0.3
491	label	#564642	italic	1.7
536	label	#91332f		0
520	label	#ea09e4	bold	2.7
519	label	#f27403	bold	3
518	label	#2e955d		0.9
305	label	#d35bf7	bold-italic	1.2
915	label	#2b4df3	bold	2.8
181661	label	#709cb8	bold-italic	3
180835	label	#839a4d	italic	0.9
382	label	#b95702	italic	0.5
29461	label	#ce4146	italic	2.4
29459	label	#151f4e	bold	0.8
381	label	#c1e4cd		2.2
1076	label	#4da7ac	bold-italic	2.8
375	label	#8b23d4		2.3
155892	label	#436d61	bold-italic	0.1
66077	label	#e5f2a1		0.4
782	label	#2f7308	bold	0.2
781	label	#9fd164		1.4
85963	label	#0e6671	italic	2.1
210	label	#3c65f8	bold-italic	0.2
32025	label	#8e0e51		1.3
844	label	#5736d3	bold	0.3
197	label	#b73582	italic	2.1
882	label	#d0c7e3	bold	2.7
35554	label	#1c8c16		2.2
959	label	#9b5bbd	bold	1.7
240015	label	#2b7c20	italic	2.7
234267	label	#1a31fe		0
76856	label	#5f8fff	bold-italic	0.3
63363	label	#374658	bold	0.4
2336	label	#e3f2f2	bold	0.4
262724	label	#f58e6b	italic	2.2
1299	label	#ea5e0b	bold-italic	0.3
243164	label	#01f2c0	italic	2.2
103690	label	#1b8da7		1.1
1148	label	#168acd	bold	2.1
32046	label	#546fe0	italic	2.4
84588	label	#810415	bold-italic	1.8
74547	label	#30f542	bold	0.1
1219	label	#d899b8	bold	0.5
59919	label	#614c88	bold-italic	1.5
33072	label	#c314b2	bold-italic	0.2
214688	label	#5d5de9	bold	1.9
117	label	#09918c	bold-italic	1.5
44275	label	#f8ad17	bold-italic	3
173	label	#0aae2d	bold-italic	1.8
160	label	#a024e7	italic	2
158	label	#fa2f41	italic	0.6
139	label	#82dc72	bold-italic	2.2
218496	label	#37828d	bold	2.8
203267	label	#50f47f	bold-italic	0.2
216816	label	#c9a68c	bold-italic	1.6
196627	label	#9578cf	bold-italic	0.4
1718	label	#8fdfeb	bold-italic	2.7
152794	label	#50b139		2.3
1717	label	#744a3d	italic	1.4
1765	label	#c8e216	bold-italic	1.9
83331	label	#7d169b	bold-italic	1.3
83332	label	#b7c524	italic	3
1769	label	#d3c276	bold	0.4
1770	label	#e4228b	bold-italic	0.5
33903	label	#028126	italic	1
1902	label	#8c06d3	bold	2.9
59374	label	#fca05a	bold	1.6
1097	label	#992184	bold	2.9
837	label	#a1c98d	italic	1.5
818	label	#a79c0d	bold	2.8
182082	label	#a160a4	italic	2.9
138677	label	#f33263		0
115713	label	#6e3e14	bold	0.1
115711	label	#ee0c12		1
83557	label	#b43080	italic	0.4
83560	label	#d2740c		2.3
813	label	#fb5c19		0
119072	label	#8180c2	bold-italic	0.3
1513	label	#7aaa4f	italic	1.8
1502	label	#320b90	bold	0.6
1488	label	#889fc3	bold	1.8
267748	label	#7f653d	bold-italic	1.6
2107	label	#9789e6	italic	1.8
2104	label	#64981d	italic	0
2097	label	#0e1276	bold	0.5
2096	label	#bac07e	bold	0.2
28227	label	#6b2074		3
134821	label	#9fb308	bold-italic	0.4
44101	label	#697716	bold	3
100379	label	#fa086f		2.7
265669	label	#624435	italic	1.5
1639	label	#c637ce	bold	1.5
1642	label	#9ab26a	italic	1
182710	label	#d8356a		1.2
86665	label	#6ba1d4		1.8
226900	label	#01240c		1.5
222523	label	#9996b5	italic	1.8
198094	label	#ee0214	bold	2.3
1423	label	#9832e3	bold-italic	0.8
196620	label	#678306		2.5
158879	label	#f036b1	italic	1.8
158878	label	#7d5dcd	bold	2.5
1282	label	#a29120		2.7
216495	label	#2ab191	bold-italic	3
216466	label	#26b7db	bold	2.7
1314	label	#ec8d2e	bold	0.4
186103	label	#2b3cf1		0.3
198466	label	#faf78f		0.2
193567	label	#7e2a36	bold-italic	3
1309	label	#e20760	bold-italic	0.1
171101	label	#fb6250	italic	2.6
1313	label	#b4512e	bold-italic	2.8
1360	label	#656200	bold	0.8
1351	label	#644725		0.4
33959	label	#128244	italic	1.7
1590	label	#fcd761	bold-italic	0.7
296543	label	#4d8f95	bold-italic	0.2
237895	label	#3c0ce4		2.4
36329	label	#ba55bd	italic	1.1
4530	label	#089732	bold	0.6
3702	label	#8ac523		0.9
45157	label	#470ef0		2.5
44689	label	#ae5dad	italic	0.3
33169	label	#f7ba2a	italic	0.4
4932	label	#1a8274	italic	1.4
4896	label	#466a4d		2.5
180454	label	#f56a88	bold-italic	2.9
7227	label	#3e2741	bold-italic	0.2
31033	label	#734322	italic	0.5
7955	label	#1878f9	bold	1.2
10116	label	#5cc50a	bold	3
10090	label	#8ba555	bold	1.8
9606	label	#8ea041		2.1
9598	label	#d36d34	italic	2.5
9031	label	#3a5345	italic	1.7
6239	label	#47bc34	bold-italic	0.6
6238	label	#87eb38		0.1
5664	label	#5139ad	italic	2.2
184922	label	#22ed35	bold	1.3
160232	label	#347f0e	bold-italic	2
111955	label	#85c03a		1.3
2287	label	#39f7a2	italic	1.6
56636	label	#803b64	bold-italic	0.9
13773	label	#bb8827	bold	0.5
50339	label	#cdc425	bold	0.9
2303	label	#3e42b6	bold	2.1
187420	label	#f7f2ca	bold-italic	0.4
2320	label	#ee405f	italic	1.7
39152	label	#e37e20		1.1
2190	label	#0f5f17		0.9
53953	label	#b1ac8a	italic	0.9
29292	label	#982409	bold	1.5
2261	label	#50da3d	bold	0.5
2234	label	#4f8b30	bold-italic	1.6
64091	label	#7b7e99	italic	2.6
2214	label	#c5796e	bold	0.3
2209	label	#2d0252	bold-italic	2.1

复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403261341895.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>TREE_COLORS
SEPARATOR TAB
DATA
I148	range	#eeffee	test1
I110	range	#ddddff	test2

复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403261344429.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\其它生信软件\a-j\itol：完全美化.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/iTol：完全美化.md</guid><pubDate>Fri, 21 Jun 2024 07:31:06 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081445061.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403081445061.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[iTol：注释编辑表格]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://itoleditor.letunic.com/help.html#strip" rel="noopener nofollow" class="external-link" href="https://itoleditor.letunic.com/help.html#strip" target="_blank">原文链接</a><br>After selecting a tree from your account, click the&nbsp;'Create or edit annotation'&nbsp;button to display the dialog shown on the right. It consists of 5 tabs:<br>
从您的帐户中选择一棵树后，单击“创建或编辑注释”按钮以显示右侧所示的对话框。它由 5 个选项卡组成：<br>
<br>
New dataset: create a new dataset on the selected tree<br>
新数据集：在选定的树上创建新数据集

<br>
Existing datasets: if the selected tree has some existing datasets, you can load and edit them through this tab<br>
现有数据集：如果所选树有一些现有数据集，您可以通过此选项卡加载和编辑它们

<br>
Colored ranges: edit or create new&nbsp;<a data-tooltip-position="top" aria-label="https://itol.embl.de/help.cgi#ranges" rel="noopener nofollow" class="external-link" href="https://itol.embl.de/help.cgi#ranges" target="_blank">tree colored ranges</a><br>
颜色范围：编辑或创建新的树颜色范围

<br>
Labels: load and edit the labels for tree leaves and internal nodes<br>
标签：加载和编辑树叶和内部节点的标签

<br>
Metadata: load and edit the tree node metadata, including bootstrap values (if present)<br>
元数据：加载和编辑树节点元数据，包括引导值（如果存在）<br>
有两个可用的复选框：

<br>
Include all tree leaves:&nbsp;If checked, tree leaves will be pre-loaded into the sheet editing area, so you don't have to type them manually<br>
包括所有树叶：如果选中，树叶将被预先加载到工作表编辑区域中，因此您不必手动输入它们

<br>
Sort nodes:&nbsp;When nodes are pre-loaded into the sheet (for example, when editing an existing dataset or tree labels), these will appear in the same order as they are shown in iTOL when the default sorting is applied<br>
对节点进行排序：当节点预先加载到工作表中时（例如，编辑现有数据集或树标签时），这些节点将按照应用默认排序时在 iTOL 中显示的顺序显示。

<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051618169.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
有时候我们生成的树的标签是数字，我们希望将其改变为其他文字，就会使用到该功能。<br><br>请注意，这些旧的颜色范围已被弃用。<a data-tooltip-position="top" aria-label="https://itoleditor.letunic.com/help.html#dsrange" rel="noopener nofollow" class="external-link" href="https://itoleditor.letunic.com/help.html#dsrange" target="_blank">新的彩色/标记范围</a>数据集提供相同的功能，并具有许多附加功能。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051639963.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>使用此数据集类型通过使用彩色框或括号突出显示各种进化枝或叶范围。 范围通过开始和结束 ID（E 列和 G 列）以及 H 列及以后的相应样式来定义。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051622754.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Fill color: 填充颜​​色：这是唯一必填字段。它将用作范围的主要填充颜色或括号的颜色。<br>
Gradient color: 渐变颜色：如果提供，范围填充（或括号颜色）将是从填充颜色到渐变颜色的渐变。<br>
Line style: 线条样式：可以是“实线”、“虚线”或“点线”。<br>
Line width: 线宽：边框或括号线宽，以像素为单位。<br>
Line color: 线条颜色：范围边框的颜色。如果指定，它也将用作括号颜色。<br>
Label text: 标签文本：显示在范围框中或括号旁边的可选文本。<br>注意，这里的线条指的是上图下方大框的描边。<br>Label color: 标签颜色：文本标签的颜色。<br>
Label size factor: 标签大小系数：计算出的标签字体大小将乘以该数字。<br>
Label style: 标签样式：标签字体样式：'正常'、'粗体'、'斜体' 或 '粗斜体'。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051626569.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
此数据集类型允许自定义树枝和叶子标签的颜色和显示样式。必须将树 ID 输入到 E 列，并将其相应的数据输入到 G 列及以后的列中。要将样式分配给整个分支，如果您的树没有内部节点 ID，请使用 iTOL 的最后一个<a data-tooltip-position="top" aria-label="https://itol.embl.de/help.cgi#ids" rel="noopener nofollow" class="external-link" href="https://itol.embl.de/help.cgi#ids" target="_blank">公共祖先方法</a>。或者，您可以单击 iTOL Web 树显示中的任何节点，然后单击弹出窗口底部的节点 ID 将其复制到剪贴板。将其粘贴到树节点 ID 列中，并根据需要指定样式。<br>
Type: 类型：可以是“分支”或“标签”。 'branch' 会将自定义应用于树枝，而 'label' 应用于叶子文本标签。<br>
What: 可以是“节点”或“分支”，并且仅与内部树节点相关。 “Node”将仅将自定义应用于单个节点，而“clade”也将应用于所有子节点。<br>
Color: 颜色：分支或标签的颜色。<br>
Factor: 对于“分支”类型，指定与全局分支宽度设置相比的相对分支宽度。对于“标签”类型，指定与全局字体大小相比的相对字体大小。<br>
Style: 样式：对于“分支”类型，可以是“正常”或“虚线” 。对于“标签”类型，可以是“正常”、“粗体”、“斜体”或“粗斜体”之一。<br>
Background color: 背景颜色：可选，仅与“标签”类型相关。指定标签背景的颜色。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051631864.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
1:filled shape&nbsp;填充形状<br>
0:empty shape&nbsp;空的形状<br>
-1:shape completely omitted&nbsp;形状完全省略<br><br>在简单的条形图中，每个节点都与一个数值相关联，该数值在树外部显示为条形。 条形将使用与数据集图例相同的颜色（在单元格 B16 中设置）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051632876.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在多值条形图中，每个节点都与多个数值相关联，这些数值在树外部显示为堆叠或对齐的条形图。首先，您必须在标题中定义从 G 列开始的字段。对于每个字段，在第 1 行中提供标签，在第 2 行中提供其颜色。定义字段后，开始在第 4 行及以下行中填充数据。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051632946.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在饼图数据集中，每个节点都与多个数值相关联，这些数值直接在节点分支上或树外部显示为饼图。首先，您必须在标题中从 I 列开始定义饼图字段。对于每个字段，在第 1 行中提供标签并在第 2 行中提供其颜色。定义字段后，开始填充第 4 行及以下行中的字段数据。除了饼图值之外，您还必须为每个节点填充两列：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051633681.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Position: 7. 位置：如果该值为-1，饼图将是外部的（即显示在树的外部）。否则，它应该是 0 到 1 之间的数字，饼图将沿着节点分支定位在指定值处（位置 0 正好在节点分支的开始处，位置 0.5 位于中间，位置 1 位于节点分支的中间）。在最后）。<br>
Radius: 半径：与所有其他饼图相比，饼图的相对半径。实际大小将取决于显示数据集时 iTOL 控件中的最大半径设置。对于外部饼图，半径将取决于叶之间的可用空间，并且可以通过在 iTOL 中的“高级”控制选项卡上设置垂直缩放系数来更改。<br><br>在热图中，每个节点都与多个数值相关联，这些数值显示为一组彩色框。当数据集在 iTOL 中可见时，值将映射到数据集选项中定义的颜色渐变。使用字母“X”指定缺失值或空值。首先，您必须在标题中定义从 G 列开始的字段。对于每个字段，在第 1 行中提供标签。定义字段后，开始填写第 3 行及以下行中的数据。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051634806.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在文本标签数据集中，每个节点都与单个文本标签相关联，该文本标签可以直接显示在节点分支上或树外部。 对于每个标签，可以指定确切位置、颜色、字体样式、大小系数和旋转程度。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051635179.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Position: 位置：如果该值为-1，标签将是外部的（即显示在树的外部）。否则，它应该是0到1之间的数字，并且标签将沿着节点分支定位在指定值处（位置0正好在节点分支的开始处，位置0.5在中间，位置1在结束）<br>
Color: 颜色：标签的颜色<br>
Style: 样式：标签的字体样式，可以是“正常”、“粗体”、“斜体”或“粗斜体”之一<br>
Factor: 指定与全局字体大小相比的相对字体大小。<br>
Text label: 文本标签：标签的实际文本。<br><br>在颜色渐变数据集中，每个节点都与一个数值相关联，该数值映射到树上的彩色框。显示数据集时，在 iTOL 控制框中定义与最小值和最大值（加上可选的中点值）相对应的颜色。在右侧的示例中，最小值显示为红色，最大值显示为蓝色。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051638866.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\其它生信软件\a-j\itol：注释编辑表格.html</link><guid isPermaLink="false">软件/其它生信软件/A-J/iTol：注释编辑表格.md</guid><pubDate>Fri, 05 Jul 2024 08:39:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051618169.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051618169.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[主要特点]]></title><description><![CDATA[ 
 <br>MAFFT（Multiple Alignment using Fast Fourier Transform）是一款广泛使用的多序列比对软件，用于进行氨基酸或核苷酸序列的比对。该工具以其高效率和准确性而受到生物信息学领域的青睐，尤其适用于大规模序列数据的处理。<br>
<a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a><br><br>
<br>快速和高效：MAFFT利用快速傅里叶变换（FFT）优化比对算法，提高了处理大规模数据集的速度。
<br>多种比对策略：软件提供了多种比对策略，包括：

<br>进化方法（如FFT-NS-1和FFT-NS-2），适用于序列数量较大时；
<br>迭代细化方法（如L-INS-i和E-INS-i），在处理具有保守区域和长间隙的序列时更加精确。


<br>灵活的输出格式：支持多种输出格式，如FASTA、Clustal、PHYLIP等，方便与其他生物信息学工具兼容。
<br>自动和手动配置：用户可以选择自动设置以快速开始比对，或根据特定需求调整详细参数，以优化比对结果。
<br>适用性广：适用于基因组、转录组和蛋白质序列的比对，能够处理从几个到几千个序列的数据集。
<br><br>MAFFT广泛应用于生物信息学研究，包括系统发育分析、功能基因的识别、进化生物学研究等领域。它的高效处理能力使其成为处理大规模序列数据时的首选工具。]]></description><link>软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html</link><guid isPermaLink="false">软件/其它生信软件/A-J/MAFFT：(Multiple alignment program for amino acid or nucleotide sequences).md</guid><pubDate>Sun, 08 Sep 2024 13:18:49 GMT</pubDate></item><item><title><![CDATA[下载]]></title><description><![CDATA[ 
 <br><a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a> 是常见的多序列比对工具之一，通常来说，它的比对速度弱于 Muscle，但是准确性更强。在 MAFFT 主页可以下载到可执行文件和代码。<br><br>官网： <a rel="noopener nofollow" class="external-link" href="https://link.zhihu.com/?target=https%3A//mafft.cbrc.jp/alignment/software/" target="_blank">https://link.zhihu.com/?target=https%3A//mafft.cbrc.jp/alignment/software/</a><br>MAFFT 官网提供了 rpm 和 deb 两种安装方式。<br>
对应使用rpm工具进行安装，命令如下。<br>rpm -Uvh mafft-x.xx-xxx.xxx.rpm
复制<br>
<br>同样地，使用deb安装时，同样下载对应的安装包，如下所示。
<br>对应使用deb工具进行安装，命令如下。
<br>dpkg -i mafft_x.xxx-x_amd64.deb
复制<br><br><br>打开解压安装包的文件mafft.bat，或者，添加环境变量后，直接在终端输入命令mafft，即可打开MAFFT主界面。<br>主界面会依次引导你做如下几件事：<br>（1）输入文件的路径，需要标准的fasta格式<br><img src="https://pic1.zhimg.com/80/v2-228921aacce254b038dc86d20d5499b0_1440w.webp" referrerpolicy="no-referrer"><br>输出文件路径<br>（2）输出文件的路径<br><img src="https://pic1.zhimg.com/80/v2-a0a1c61398f552a81bf1555710554664_1440w.webp" referrerpolicy="no-referrer"><br>输出文件路径<br>（3）选择输出文件的格式，输入对应序号按回车即可，支持Clustal、Fasta和Phylip三种格式。<br><img src="https://pic2.zhimg.com/80/v2-13528a5f6791ef69dc131808822341d5_1440w.webp" referrerpolicy="no-referrer"><br>输出文件格式<br>（4）选择比对策略，输入对应序号按回车即可，通常选择1<br><img src="https://pic4.zhimg.com/80/v2-76338ecdc12aaf3c58af073ff91576d3_1440w.webp" referrerpolicy="no-referrer"><br>比对策略<br>（5）附加参数，如果没有，直接按回车即可<br><img src="https://pic4.zhimg.com/80/v2-97f4e30c141b3ebf18707b7d12927ad3_1440w.webp" referrerpolicy="no-referrer"><br>其他参数<br>做完以上5个步骤，MAFFT工具通过你的各个选项，自动生成了运行命令，此时输入"Y"即可开始进行比对，如下所示。<br><img src="https://pic3.zhimg.com/80/v2-68c460e182c290b037327cfa7bcc323a_1440w.webp" referrerpolicy="no-referrer"><br>得到命令<br>同时我们也发现，MAFFT界面只是引导你生成命令，如果记住命令的话，可以直接在终端输入命令，开始比对。<br><br>
<br>默认设置：默认大法好。
<br> mafft --auto input.fasta &gt; output.fasta
 mafft input.fasta &gt; output.fasta
复制<br>
<br>增加线程：
<br>mafft --thread 4 input.fasta &gt; output.fasta # 使用4个线程快一点
复制<br>
<br>调整比对精度：一般你不会用到，你可以在官网找到更多的内容。
以下内容并不完整！

<br>mafft --retree 1 input.fasta &gt; output.fasta # 快速模式（FFT-NS-1）
mafft --retree 2 input.fasta &gt; output.fasta # 中等精度模式（FFT-NS-2）
mafft --genafpair --maxiterate 1000 input.fasta &gt; output.fasta  # 全局配对和迭代（G-INS-i）
mafft --localpair --maxiterate 1000 input.fasta &gt; output.fasta # 局部配对和迭代（L-INS-i）
mafft --globalpair --maxiterate 1000 input.fasta &gt; output.fasta  # 全局配对信息提高精度（G-INS-i）
mafft --genafpair --maxiterate 1000 --op 1.53 input.fasta &gt; output.fasta # 改变Gap开销，影响插入或删除空位的频率。
mafft --add new_sequences.fasta existing_alignment.fasta &gt; new_alignment.fasta # 将新序列添加到已有的比对中，而不重新计算整个比对。
复制<br>其中的 --add 还可以替换为如下：<br>
<br>--addprofile：

<br>参数：mafft --addprofile new_sequences.fasta existing_alignment.aln
<br>描述：将新序列作为一个独立的多序列比对文件，与现有比对文件进行全局比对。这种方法通常是最精确的，因为它考虑了所有新序列的上下文。
<br>使用场景：适用于有较多新序列的情况，需要高精度的对齐。


<br>--addfull：

<br>参数：mafft --addfull new_sequences.fasta existing_alignment.aln
<br>描述：将新序列添加到现有比对中，进行全局比对，考虑整个序列的上下文。
<br>使用场景：适用于需要较高精度对齐但新序列不多的情况。


<br>--add：

<br>参数：mafft --add new_sequences.fasta existing_alignment.aln
<br>描述：将新序列逐个添加到现有比对中，进行局部比对。这种方法比 --addfull 快，但精度较低。
<br>使用场景：适用于需要快速对齐且新序列较少的情况。


<br>--keeplength：

<br>参数：mafft --add new_sequences.fasta --keeplength existing_alignment.aln &gt; new.fasta
<br>描述：类似 --add，但确保现有比对的长度保持不变。精度最低，但对已有比对影响最小。
<br>使用场景：适用于需要确保现有比对完全不变的情况，即使新序列的对齐精度较低。


<br><br>该软件的基本用法如下：<br>
mafft  input &gt; output<br>
input 为 fasta 格式的输入序列文件，output 为 fasta 格式的输出结果文件。mafft 支持核酸和蛋白序列的多序列比对，内置了多种序列比对算法，可以分为以下3大类别<br>
<br>consistency based methods
<br>iterative refinment methods
<br>progressive methods
<br>这三种类别的算法在准确度和速度上各有优势，对于运行速度而言，3&gt;2&gt;1;对于准确度而言，1&gt;2&gt;3。<br><br>此类算法包含了 L-INS-i, E-INS-i, G-INS-i 3种算法。<br><br>mafft --localpair --maxiterate 1000 input_file &gt; output_file
复制<br><br>mafft --genafpair --maxiterate 1000 input_file &gt; output_file
复制<br>G-INS-i 用法如下<br>mafft --globalpair --maxiterate 1000 input_file &gt; output_file
复制<br><br>此类算法包含了 FFT-NS-i, NW-NS-i 两种算法。<br><br>mafft --maxiterate 1000 input_file &gt; output_file
复制<br><br>mafft --maxiterate 1000 input_file &gt; output_file
复制<br><br>此类算法包含了 FFT-NS-1, FFT-NS-2 2种算法。<br><br>mafft --retree 1 input_file &gt; output_file
复制<br><br>mafft --retree 2 input_file &gt; output_file
复制<br>如果在比对时，不知道如何选取合适的算法，可以使用以下设置<br>mafft --auto input &gt; output
复制]]></description><link>软件\其它生信软件\k-s\mafft：使用教程.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/MAFFT：使用教程.md</guid><pubDate>Thu, 31 Oct 2024 01:54:58 GMT</pubDate><enclosure url="https://pic1.zhimg.com/80/v2-228921aacce254b038dc86d20d5499b0_1440w.webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src="https://pic1.zhimg.com/80/v2-228921aacce254b038dc86d20d5499b0_1440w.webp"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[软件]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://github.com/Theory-Lun/McAN" rel="noopener nofollow" class="external-link" href="https://github.com/Theory-Lun/McAN" target="_blank">Theory-Lun/McAN (github.com)</a><br><br>git clone https://github.com/Theory-Lun/McAN.git
cd McAN
mkdir build
cd build
cmake ..
make
复制<br><br><br>MCAN 可以从变体调用格式 (VCF) 或突变格式中读取变异。这里突变格式定义如下：提供了一个 perl 程序 (convert/variant_mark.pl)，用于从 fasta 格式的多基因组序列比对文件生成 突变格式 的文件。<br>&lt;SampleName&gt;\t&lt;AccessionID&gt;\t[&lt;POS&gt;(&lt;VariantType&gt;:&lt;REF&gt;-&gt;&lt;ALT&gt;)[;&lt;POS&gt;(&lt;VariantType&gt;:&lt;REF&gt;-&gt;&lt;ALT&gt;)]...]
复制<br>hCoV-19/England/ALDP-143B269/2021	EPI_ISL_1454756	1(Deletion:AATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGT-&gt;-);241(SNP:C-&gt;T);913(SNP:C-&gt;T);2062(SNP:C-&gt;T);3037(SNP:C-&gt;T);3267(SNP:C-&gt;T);5388(SNP:C-&gt;A);5986(SNP:C-&gt;T);6954(SNP:T-&gt;C);9746(SNP:C-&gt;T);11287(Deletion:GTCTGGTTTT-&gt;G);14408(SNP:C-&gt;T);14590(SNP:T-&gt;G);14676(SNP:C-&gt;T);15279(SNP:C-&gt;T);16176(SNP:T-&gt;C);16391(SNP:C-&gt;T);17615(SNP:A-&gt;G);19656(SNP:G-&gt;A);21764(Deletion:ATACATG-&gt;A);21990(Deletion:TTTA-&gt;T);23063(SNP:A-&gt;T);23271(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23709(SNP:C-&gt;T);24506(SNP:T-&gt;G);24914(SNP:G-&gt;C);26461(SNP:C-&gt;T);26730(SNP:G-&gt;C);27972(SNP:C-&gt;T);28048(SNP:G-&gt;T);28111(SNP:A-&gt;G);28270(Deletion:TA-&gt;T);28280(SNP:G-&gt;C);28281(SNP:A-&gt;T);28282(SNP:T-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);28973(SNP:A-&gt;G);28977(SNP:C-&gt;T);29109(SNP:C-&gt;A);29541(SNP:C-&gt;T);29640(SNP:C-&gt;T);29773(SNP:G-&gt;T);29836(Deletion:CCCATGTGATTTTAATAGCTTCTTAGGAGAATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;C)
hCoV-19/Denmark/DCGC-48559/2021	EPI_ISL_1124652	1(Deletion:AATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGT-&gt;-);158(SNP:A-&gt;G);241(SNP:C-&gt;T);913(SNP:C-&gt;T);3037(SNP:C-&gt;T);3267(SNP:C-&gt;T);5388(SNP:C-&gt;A);5986(SNP:C-&gt;T);6954(SNP:T-&gt;C);11287(Deletion:GTCTGGTTTT-&gt;G);12388(SNP:T-&gt;C);14408(SNP:C-&gt;T);14676(SNP:C-&gt;T);15279(SNP:C-&gt;T);16176(SNP:T-&gt;C);21764(Deletion:ATACATG-&gt;A);21990(Deletion:TTTA-&gt;T);23063(SNP:A-&gt;T);23271(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23709(SNP:C-&gt;T);24506(SNP:T-&gt;G);24914(SNP:G-&gt;C);25448(SNP:A-&gt;G);26681(SNP:C-&gt;T);27972(SNP:C-&gt;T);28048(SNP:G-&gt;T);28111(SNP:A-&gt;G);28270(Deletion:TA-&gt;T);28280(SNP:G-&gt;C);28281(SNP:A-&gt;T);28282(SNP:T-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);28977(SNP:C-&gt;T);29421(SNP:C-&gt;T);29422(SNP:G-&gt;T);29627(SNP:C-&gt;T);29836(Deletion:CCCATGTGATTTTAATAGCTTCTTAGGAGAATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;C)
hCoV-19/Germany/NW-RKI-I-063476/2021	EPI_ISL_1568686	1(Deletion:AAT-&gt;-);241(SNP:C-&gt;T);913(SNP:C-&gt;T);1937(SNP:T-&gt;C);1979(SNP:G-&gt;A);2110(SNP:C-&gt;T);3037(SNP:C-&gt;T);3267(SNP:C-&gt;T);4582(SNP:C-&gt;T);5388(SNP:C-&gt;A);5986(SNP:C-&gt;T);6954(SNP:T-&gt;C);7267(SNP:C-&gt;T);9856(SNP:G-&gt;T);11083(SNP:G-&gt;T);11287(Deletion:GTCTGGTTTT-&gt;G);14120(SNP:C-&gt;T);14408(SNP:C-&gt;T);14676(SNP:C-&gt;T);15279(SNP:C-&gt;T);16176(SNP:T-&gt;C);21766(Deletion:ACATGTC-&gt;A);21990(Deletion:TTTA-&gt;T);23063(SNP:A-&gt;T);23271(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23690(SNP:A-&gt;G);23709(SNP:C-&gt;T);24506(SNP:T-&gt;G);24914(SNP:G-&gt;C);27972(SNP:C-&gt;T);28048(SNP:G-&gt;T);28095(SNP:A-&gt;T);28111(SNP:A-&gt;G);28270(Deletion:TA-&gt;T);28280(SNP:G-&gt;C);28281(SNP:A-&gt;T);28282(SNP:T-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);28977(SNP:C-&gt;T);29732(Deletion:CCGAGGCCACGCGGAGTACGATCGAGTGTACAGTGAACAATGCTAGGGAGAGCTGCCTATATGGAAGAGCCCTAATGTGTAAAATTAATTTTAGTAGTGCTATCCCCATGTGATTTTAATAGCTTCTTAGGAGAATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;C)
hCoV-19/USA/AK-PHL8275/2021	EPI_ISL_1821911	1(Deletion:AATTAAAGGTTTATACCT-&gt;-);201(SNP:T-&gt;C);203(SNP:C-&gt;T);222(SNP:C-&gt;T);241(SNP:C-&gt;T);507(Deletion:ATGGTCATGTTATGGT-&gt;A);1738(SNP:G-&gt;T);3037(SNP:C-&gt;T);3140(SNP:C-&gt;T);9979(SNP:C-&gt;T);10029(SNP:C-&gt;T);10954(SNP:C-&gt;T);11117(SNP:A-&gt;G);12789(SNP:C-&gt;T);14408(SNP:C-&gt;T);17079(SNP:G-&gt;A);18255(SNP:G-&gt;T);19839(SNP:T-&gt;C);19974(SNP:A-&gt;G);21306(SNP:C-&gt;T);22995(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23756(SNP:A-&gt;G);25553(SNP:C-&gt;T);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);29197(SNP:C-&gt;T);29866(Deletion:ATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;A)
hCoV-19/USA/KS-KHEL-0753/2020	EPI_ISL_1412493	1(Deletion:AATTAAAGGTTTATACCTTCCCAG-&gt;-);219(SNP:G-&gt;T);241(SNP:C-&gt;T);3037(SNP:C-&gt;T);3122(SNP:G-&gt;T);4950(SNP:T-&gt;C);7108(SNP:C-&gt;T);7687(SNP:A-&gt;G);9409(SNP:A-&gt;G);11595(SNP:A-&gt;G);12970(SNP:C-&gt;T);14408(SNP:C-&gt;T);14712(SNP:G-&gt;T);16610(SNP:C-&gt;A);16733(SNP:C-&gt;T);18079(SNP:G-&gt;T);18555(SNP:C-&gt;T);19839(SNP:T-&gt;C);20069(SNP:G-&gt;T);23403(SNP:A-&gt;G);23426(SNP:G-&gt;T);23587(SNP:G-&gt;T);23756(SNP:A-&gt;G);24138(SNP:C-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);29640(SNP:C-&gt;T);29870(Deletion:CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;C)
hCoV-19/USA/TX-HMH-MCoV-18622/2020	EPI_ISL_784970	1(Deletion:AATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACC-&gt;-);241(SNP:C-&gt;T);1059(SNP:C-&gt;T);2388(SNP:C-&gt;T);2448(SNP:G-&gt;T);3037(SNP:C-&gt;T);4331(SNP:C-&gt;T);6040(SNP:C-&gt;T);6422(SNP:G-&gt;A);9112(SNP:C-&gt;M);10319(SNP:C-&gt;T);14408(SNP:C-&gt;T);15380(SNP:G-&gt;T);16926(SNP:T-&gt;C);18424(SNP:A-&gt;G);19974(SNP:A-&gt;G);21304(SNP:C-&gt;T);22225(SNP:G-&gt;T);23403(SNP:A-&gt;G);25563(SNP:G-&gt;T);25907(SNP:G-&gt;T);27964(SNP:C-&gt;T);28472(SNP:C-&gt;T);28869(SNP:C-&gt;T);29053(SNP:A-&gt;C);29095(SNP:C-&gt;T);29752(SNP:A-&gt;T);29854(Deletion:CTTCTTAGGAGAATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;C)
hCoV-19/USA/CA-CHLA-PLM57647613/2020	EPI_ISL_753540	241(SNP:C-&gt;T);1059(SNP:C-&gt;T);1927(SNP:T-&gt;C);3037(SNP:C-&gt;T);6196(SNP:C-&gt;T);10319(SNP:C-&gt;T);14408(SNP:C-&gt;T);15766(SNP:G-&gt;T);18424(SNP:A-&gt;G);18538(SNP:G-&gt;T);21304(SNP:C-&gt;T);22255(SNP:A-&gt;T);23120(SNP:G-&gt;T);23403(SNP:A-&gt;G);24146(SNP:C-&gt;G);25563(SNP:G-&gt;T);25907(SNP:G-&gt;T);25930(SNP:T-&gt;C);26951(SNP:G-&gt;T);27964(SNP:C-&gt;T);28472(SNP:C-&gt;T);28869(SNP:C-&gt;T);29439(SNP:A-&gt;T);29903(Insertion:A-&gt;AHMSHCAMTCSCYGWHMSHCVGSADBATCHCMSRSARSCVARAGTBACSSTSATCMR)
hCoV-19/Germany/SN-RKI-I-038205/2021	EPI_ISL_1284566	1(Deletion:AATTAAAGGTTTATACCTTCCCAGGTAACAA-&gt;-);241(SNP:C-&gt;T);815(SNP:C-&gt;T);913(SNP:C-&gt;T);1245(SNP:G-&gt;A);3037(SNP:C-&gt;T);3267(SNP:C-&gt;T);5388(SNP:C-&gt;A);5986(SNP:C-&gt;T);6954(SNP:T-&gt;C);11287(Deletion:GTCTGGTTTT-&gt;G);13446(SNP:C-&gt;T);14408(SNP:C-&gt;T);14676(SNP:C-&gt;T);15096(SNP:T-&gt;C);15279(SNP:C-&gt;T);16176(SNP:T-&gt;C);21764(Deletion:ATACATG-&gt;A);21990(Deletion:TTTA-&gt;T);23063(SNP:A-&gt;T);23271(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23709(SNP:C-&gt;T);24506(SNP:T-&gt;G);24914(SNP:G-&gt;C);25785(SNP:G-&gt;T);27972(SNP:C-&gt;T);28048(SNP:G-&gt;T);28111(SNP:A-&gt;G);28270(Deletion:TA-&gt;T);28280(SNP:G-&gt;C);28281(SNP:A-&gt;T);28282(SNP:T-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);28977(SNP:C-&gt;T);29764(SNP:G-&gt;A)
hCoV-19/Germany/un-RKI-I-145500/2021	EPI_ISL_2129747	1(Deletion:AATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCA-&gt;-);241(SNP:C-&gt;T);913(SNP:C-&gt;T);3037(SNP:C-&gt;T);3267(SNP:C-&gt;T);3302(SNP:G-&gt;A);3817(SNP:C-&gt;T);4002(SNP:C-&gt;T);5388(SNP:C-&gt;A);5944(SNP:C-&gt;T);5986(SNP:C-&gt;T);6954(SNP:T-&gt;C);11287(Deletion:GTCTGGTTTT-&gt;G);14408(SNP:C-&gt;T);14676(SNP:C-&gt;T);15096(SNP:T-&gt;C);15279(SNP:C-&gt;T);16176(SNP:T-&gt;C);21764(Deletion:ATACATG-&gt;A);21990(Deletion:TTTA-&gt;T);23063(SNP:A-&gt;T);23271(SNP:C-&gt;A);23403(SNP:A-&gt;G);23604(SNP:C-&gt;A);23709(SNP:C-&gt;T);24506(SNP:T-&gt;G);24914(SNP:G-&gt;C);25424(SNP:G-&gt;T);27389(SNP:C-&gt;T);27972(SNP:C-&gt;T);28048(SNP:G-&gt;T);28095(SNP:A-&gt;T);28111(SNP:A-&gt;G);28114(SNP:T-&gt;C);28270(Deletion:TA-&gt;T);28280(SNP:G-&gt;C);28281(SNP:A-&gt;T);28282(SNP:T-&gt;A);28881(SNP:G-&gt;A);28882(SNP:G-&gt;A);28883(SNP:G-&gt;C);28884(SNP:G-&gt;C);28977(SNP:C-&gt;T);29853(Deletion:GCTTCTTAGGAGAATGACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA-&gt;G)
hCoV-19/Wuhan/HB-WH1-122/2020	EPI_ISL_454904
复制<br><br>其中每个记录包含 6 个字段：SampleName、AccessionID、SamplingDate（yyyy-mm-dd 或 yyyy-mm）、Country、State 和 City。所有数据行均以制表符分隔。缺失值用“*”指定。<br>*	EPI_ISL_1454756	2021-03-13	United Kingdom	*	*
*	EPI_ISL_1124652	2021-02-15	Denmark	*	*
*	EPI_ISL_1568686	2021-03-22	Germany	*	*
*	EPI_ISL_1821911	2021-04-19	United States	*	*
*	EPI_ISL_1412493	2020-12-10	United States	*	*
*	EPI_ISL_784970	2020-11-20	United States	*	*
*	EPI_ISL_753540	2020-11-30	United States	*	*
*	EPI_ISL_1284566	2021-03-02	Germany	*	*
*	EPI_ISL_2129747	2021-04-30	Germany	*	*
*	EPI_ISL_454904	2020-03-02	China	*	*
复制<br><br>Usage: McAN [option]...

Options:
  -f, --vcf &lt;file&gt;        input vcf file
  -u, --mutation &lt;file&gt;   input mutation file
  -m, --meta &lt;file&gt;       input meta file
  -s, --sitemask &lt;file&gt;   input sitemask file [optional]
  -x, --maxnsample &lt;int&gt;  maximum number of samples [default: Inf]
  -t, --nthread &lt;int&gt;     number of worker threads [default: 1]
  -o, --outDir &lt;dir&gt;      directory for output
  -J, --oJSON             output network with JSON format
  -T, --oTSV              output network with TSV format
  -G, --oGraphML          output network with GraphML format
  -M, --oMutation         convert vcf into mutation format and output

  -h, --help              display this help and exit
  -v, --version           output version information and exit


Usage: McAN siteMask [option]...

Options:
  -f, --vcf &lt;file&gt;       input vcf file
  -u, --mutation &lt;file&gt;  input mutation file
  -m, --meta &lt;file&gt;      input meta file
  -q, --minfreq &lt;int&gt;    filtered out variants with frequency &lt; minfreq
  -o, --out &lt;file&gt;       output sitemask file
  -M, --oMutation        convert vcf into mutation format and output

  -h, --help             display this help and exit
  -v, --version          output version information and exit
复制<br><br>cd .../McAN/example 
mkdir out # 创建输出目录
# 过滤位点并生成位点掩码文件
../build/McAN siteMask \
--mutation data/mut \
--meta data/meta \
--out out/siteMask \ 
--minfreq 0.01
# 构建网络
../build/McAN \
--mutation data/mut \
--meta data/meta \
--sitemask out/siteMask \
--outDir out \ # 输出目录为out
--oJSON \ # 输出JSON格式
--oGraphML \ # 输出GraphML格式
--oTSV \ # 输出TSV格式
--nthread 3
复制]]></description><link>软件\其它生信软件\k-s\mcan：软件操作.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/McAN：软件操作.md</guid><pubDate>Sun, 22 Sep 2024 10:13:23 GMT</pubDate></item><item><title><![CDATA[什么是N-J树]]></title><description><![CDATA[ 
 <br><br>Neighbor-Joining (NJ)树推理方法最初是由 Saitou 和 Nei 于 1987 年编写的。<br>它属于一类基于距离的方法用于构建进化树。 NJ 方法采用给定序列之间的成对进化距离矩阵来构建进化树。Neighbor-Joining是一种bottom-up&nbsp;的聚类方法，常被用于系统发育树 (phylogenetic tree) 的构建当中。<br>成对距离通常从序列比对算法中获得，例如&nbsp;Smith-Waterman&nbsp;和&nbsp;BLAST&nbsp;，它们将每个基因序列与每个其他基因序列进行比对。比对得分可用作序列之间进化距离的估计。<br><br>绘制N-J树需要准备<a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a>计算所得的频率矩阵。如下所示：<br><br>不知道如何得到Fst矩阵的请查看<a data-href="R：Fst绘制频率热图矩阵" href="软件\r语言语法\r：fst绘制频率热图矩阵.html" class="internal-link" target="_self" rel="noopener nofollow">R：Fst绘制频率热图矩阵</a>。<br>得到频率矩阵之后，需要对数据进行预处理。步骤如下：<br>
<br>将矩阵变为半矩阵：
<br><br>
<br>将其中的负数转换为0。因为利用N-J树要求最低的关系为0。这一步可以在Excel表格中进行，选中区域后进行替换，-*替换为0。
<br><br>
<br>准备好矩阵第一列的名字，并以#开始。
<br>#Chongqing
#Fujian
#Gansu
#Guangdong
#Guangxi
#Guizhou
#Hainan
#Henan
#Hunan
#InnerMongulia
#Shaanxi
#Shandong
#Shanxi
#Sichuan
#Tibet
#Yunnan
复制<br>
<br>构建.meg文件

<br>新建一个.txt文件，将下列文本粘贴进去。
<br>注意替换列表和频率矩阵。
<br>将文本后缀改为meg。


<br>#mega
!TITLE  Genetic distance data from 16 human populations;
!Format DataType=distance;
!Description
     Nei, M. and A. K. Roychoudhury. 1993. Mol. Biol. Evol. 10(5).
     Number of polymoprhic loci used = 18;

#Chongqing
#Fujian
#Gansu
#Guangdong
#Guangxi
#Guizhou
#Hainan
#Henan
#Hunan
#InnerMongulia
#Shaanxi
#Shandong
#Shanxi
#Sichuan
#Tibet
#Yunnan

0.00302														
0.00345	0.00862													
0.0054	0.00458	0.01901												
0.00891	0.00543	0.02545	0.00063											
0.00443	0.00428	0.01369	0.00691	0.00537										
0.01291	0.00799	0.02997	0.00201	0.00167	0.01078									
0.00103	0.00457	0	0.01473	0.01855	0.00916	0.0227								
0.01076	0.01213	0.02381	0.00374	0.00097	0.01232	0.00752	0.01975							
0.00795	0.01892	0.00482	0.01624	0.02669	0.02148	0.03418	0.00896	0.0162						
0.00591	0.00912	0	0.01818	0.02709	0.01394	0.03073	0.00021	0.02946	0.00577					
0.00642	0.00842	0.00367	0.02032	0.0278	0.01514	0.03152	0.00143	0.03403	0.01934	0.00085				
0.00111	0.00484	0.00159	0.01344	0.02094	0.01065	0.02545	0	0.02561	0.01231	0.00072	0.00005			
0	0.00406	0.00577	0.0055	0.00679	0.0007	0.01155	0.00363	0.00996	0.01102	0.00727	0.0096	0.00553		
0.0124	0.02207	0.00415	0.03464	0.0449	0.02944	0.05712	0.00757	0.04396	0.01337	0.01214	0.01472	0.01	0.01753	
0.00455	0.00952	0.0059	0.01073	0.01459	0.00942	0.02233	0.00578	0.01236	0	0.00689	0.01517	0.00876	0.00386	0.01924  

复制<br><br>
<br>
打开MEGA软件，将刚才的文件拖进去。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021601157.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
点击Phylogeny→Construct/Test Neighbor-Joining (NJ) tree→OK。

<br>
得到树。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021603251.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
但是不太美观，所以进一步编辑。点击File→Export Current Tree。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021603847.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
得到一个newick或者nwk后缀文件。

<br>
打开网站：<a data-tooltip-position="top" aria-label="https://itol.embl.de/" rel="noopener nofollow" class="external-link" href="https://itol.embl.de/" target="_blank">进化树美化</a><a data-href="iTol：进化树美化网站" href="软件\其它生信软件\a-j\itol：进化树美化网站.html" class="internal-link" target="_self" rel="noopener nofollow">iTol：进化树美化网站</a>

<br>
注册一个账号，然后点击上传newick或者nwk文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021607253.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
进行修改后，导出文件，完成！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021607081.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

]]></description><link>软件\其它生信软件\k-s\mega：基于fst绘制neighbor-joining-(nj)树.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/MEGA：基于Fst绘制Neighbor-Joining (NJ)树.md</guid><pubDate>Sun, 29 Sep 2024 09:00:37 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021601157.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402021601157.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[mitofinder 软件的作用]]></title><description><![CDATA[ 
 <br>本文提供了 Mitofinder 软件的安装和使用说明，该软件用于分析线粒体 DNA 数据，识别线粒体单倍群和估计进化距离。<br><br>mitofinder 是一款用于分析线粒体 DNA 序列的软件。它可以帮助研究人员识别线粒体 DNA 中的突变和变异，并了解这些变化与疾病之间的关系。<br><br>
<br>
安装依赖项：

<br>sudo apt-get update 
<br>sudo apt-get install -y git wget


<br>
克隆源码：

<br>git clone https://github.com/RemiAllio/MitoFinder.git
<br>cd MitoFinder


<br>
运行安装脚本：

<br>./install.sh


<br>
设置环境变量：

<br>在 .bashrc 中添加：

<br>echo 'export PATH=$PATH:/home/luolintao/MitoFinder-master' &gt;&gt; ~/.bashrc source ~/.bashrc




<br>验证安装<br>
<br>mitofinder --version
<br><br>MitoFinder 运行需要指定参考基因组文件。你需要提供一个参考的线粒体基因组文件，并使用 -r 选项来指定这个文件。<br>你需要一个参考的线粒体基因组文件，通常是 GenBank 格式（.gb）的文件。你可以从 NCBI 或其他数据库下载合适的参考基因组。<br>请运行上述命令，MitoFinder 应该会开始处理并进行线粒体基因组的注释。如果你在运行过程中遇到任何问题，请提供详细的错误信息以便进一步帮助。<br>mitofinder -j 基因注释 -a /home/luolintao/MitoFinder-master/input_file/没有去除33个位点.fasta -r /home/luolintao/MitoFinder-master/input_file/GRCH38线粒体DNA.gb -o 2 -m mitochondrion
复制<br><br>
<br>The Standard Code: 常规的标准遗传密码表。
<br>The Vertebrate Mitochondrial Code: 脊椎动物线粒体密码表。
<br>The Yeast Mitochondrial Code: 酵母线粒体密码表。
<br>The Mold, Protozoan, and Coelenterate Mitochondrial Code and the Mycoplasma/Spiroplasma Code: 霉菌、原生动物、刺胞动物线粒体密码表以及支原体/螺旋体密码表。
<br>The Invertebrate Mitochondrial Code: 无脊椎动物线粒体密码表。
<br>The Ciliate, Dasycladacean and Hexamita Nuclear Code: 纤毛虫、Dascycladacean 和 Hexamita 核密码表。
<br>The Echinoderm and Flatworm Mitochondrial Code: 棘皮动物和扁虫线粒体密码表。
<br>The Euplotid Nuclear Code: Euplotid 核密码表。
<br>The Bacterial, Archaeal and Plant Plastid Code: 细菌、古菌和植物质体密码表。
<br>The Alternative Yeast Nuclear Code: 替代的酵母核密码表。
<br>The Ascidian Mitochondrial Code: 被囊动物线粒体密码表。
<br>The Alternative Flatworm Mitochondrial Code: 替代的扁虫线粒体密码表。
<br>Chlorophycean Mitochondrial Code: 绿藻线粒体密码表。
<br>Trematode Mitochondrial Code: 吸虫线粒体密码表。
<br>Scenedesmus obliquus Mitochondrial Code: Scenedesmus obliquus 线粒体密码表。
<br>Thraustochytrium Mitochondrial Code: Thraustochytrium 线粒体密码表。
<br>Pterobranchia Mitochondrial Code: Pterobranchia 线粒体密码表。
<br>Candidate Division SR1 and Gracilibacteria Code: SR1 候选门类和 Gracilibacteria 密码表。
]]></description><link>软件\其它生信软件\k-s\mitofinder：软件安装和使用.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/mitofinder：软件安装和使用.md</guid><pubDate>Mon, 08 Jul 2024 02:36:50 GMT</pubDate></item><item><title><![CDATA[1. 安装]]></title><description><![CDATA[ 
 <br><br>使用以下命令将 MToolBox 的路径添加到系统的 PATH 中：<br>export PATH="/path/to/MToolBox/:$PATH"
复制<br>在 MToolBox 文件夹中，运行 install.sh 脚本（适用于 Linux 发行版）：<br>./install.sh
复制<br>以完成所有 MToolBox 依赖项的完整安装。这可能需要一些时间，请耐心等待，直到整个过程成功完成。我们强烈建议使用安装脚本提供的默认版本：<br>
<br>GSNAP 2015-12-31.v7
<br>Anaconda distribution 2-2.5.0
<br>Zlib 版本 1.2.8
<br>MUSCLE 版本 3.8.31
<br>samtools 版本 1.3
<br>GSNAP 数据库生成的默认 Kmer 值是 15。不过，如果您想在完整安装过程中更改上述版本或参数，请指定以下选项：<br>./install.sh -g &lt;gsnap_version&gt; -a &lt;anaconda_version&gt; -z &lt;zlib_version&gt; -m &lt;muscle_file&gt; -s &lt;samtools_version&gt; -k &lt;kmer_to_build_gsnap_db&gt;
复制<br>要重新安装/更新某个 MToolBox 软件依赖项，请执行：<br>./install.sh -i &lt;software_name&gt;
复制<br>其中可能的软件名称包括：<br>
<br>gsnap
<br>anaconda
<br>muscle
<br>zlib
<br>samtools
<br>gsnap_db
<br>./install.sh -i gsnap -g 2016-05-25
复制<br><br>
<br>基于 UNIX 的操作系统
<br>MacOS
<br>运行 install.sh 脚本（参见安装部分）时，将默认安装以下所有依赖项：<br>
<br>Python 2.7 (<a data-tooltip-position="top" aria-label="http://www.python.org" rel="noopener nofollow" class="external-link" href="http://www.python.org" target="_blank">www.python.org</a>, 包含在 Anaconda 发行版中, <a rel="noopener nofollow" class="external-link" href="https://www.continuum.io/downloads" target="_blank">https://www.continuum.io/downloads</a>)<br>

<br>GSNAP (<a rel="noopener nofollow" class="external-link" href="http://research-pub.gene.com/gmap/" target="_blank">http://research-pub.gene.com/gmap/</a>)<br>

<br>Samtools (<a rel="noopener nofollow" class="external-link" href="https://sourceforge.net/projects/samtools/files/samtools/" target="_blank">https://sourceforge.net/projects/samtools/files/samtools/</a>)<br>

<br>MUSCLE (<a rel="noopener nofollow" class="external-link" href="http://www.drive5.com/muscle/downloads.htm" target="_blank">http://www.drive5.com/muscle/downloads.htm</a>)<br>

<br>ZLIB (<a rel="noopener nofollow" class="external-link" href="http://zlib.net/" target="_blank">http://zlib.net/</a>)<br>

<br>GATK（可选，仅当用户希望运行 GATK IndelRealigner 时。<a data-tooltip-position="top" aria-label="https://software.broadinstitute.org/gatk/download/%EF%BC%89%E3%80%82%E7%94%A8%E6%88%B7%E9%9C%80%E8%A6%81%E5%B0%86" rel="noopener nofollow" class="external-link" href="https://software.broadinstitute.org/gatk/download/%EF%BC%89%E3%80%82%E7%94%A8%E6%88%B7%E9%9C%80%E8%A6%81%E5%B0%86" target="_blank">https://software.broadinstitute.org/gatk/download/）。用户需要将</a> GATK 包放入 MToolBox/ext_tools 文件夹中：
  mv GenomeAnalysisTK.jar /path/to/MToolBox/MToolBox/ext_tools/
<br><br>默认情况下，MToolBox 采用 RSRS（重建智人参考序列，PMID: 22482806）作为线粒体参考基因组，hg19 作为核参考基因组。或者，用户也可以选择使用 rCRS（修订的剑桥参考序列）。MToolBox 使用的 hg19 核基因组和线粒体 DNA 的 Fasta 文件可在 <a rel="noopener nofollow" class="external-link" href="https://sourceforge.net/projects/mtoolbox/" target="_blank">https://sourceforge.net/projects/mtoolbox/</a> 上获取，以方便用户使用。然而，在流水线的 v.1.0 版本中，这些文件现在通过 install.sh MToolBox 脚本默认下载和安装，连同所需的 GSNAP 数据库。<br>MToolBox 文件夹包含 MITOMAP_HMTDB_known_indels.vcf 文件，其中包含 127 个在 MITOMAP 和 HmtDB 中注释的已知插入缺失变异，以及 GATK 的 GenomeAnalysisTK.jar 模块使用的相关 intervals_file.list。MToolBox 文件夹还包括两个以制表符分隔的文件，patho_table.txt 和 sitevar_modified.txt，分别包含在注释步骤中使用的变异特异性和位点特异性信息。<br><br>对于每个样本，输出文件夹和文件的基本名称将从输入文件名中解析。<br>
<br>BAM/SAM 文件：BAM 或 SAM 文件必须重命名为 &lt;sample_name&gt;.ext，例如：
<br>mv HG00096.chrom20.ILLUMINA.bwa.GBR.low_coverage.20101123.bam HG00096.bam
复制<br>这样“HG00096”将成为输出的基本名称。<br>
<br>FASTQ 文件：对于配对末端数据，FASTQ 文件必须重命名为 &lt;sample_name&gt;.R1.fastq 和 &lt;sample_name&gt;.R2.fastq；对于单端数据，必须重命名为 &lt;sample_name&gt;.fastq。FASTQ 压缩输入文件可以接受 *.fastq.gz 扩展名。
<br>重要：请注意，MToolBox 每个样本只能识别一对配对末端的 fastq 文件（R1+R2）和一个单端 fastq 文件。<br><br><br>
<br>-i 配置文件 [强制]
<br>-m mapExome 脚本的选项 [参见 mapExome.py -h 了解详情]
<br>-a assembleMTgenome 脚本的选项 [参见 assembleMTgenome.py -h 了解详情]
<br>-c mt-classifier 脚本的选项 [参见 mt-classifier.py -h 了解详情]
<br><br>
<br>-h 显示此帮助信息
<br>-v 显示版本信息
<br>用法：<br>#此命令将在 test_config.sh 文件中指定的样本集上运行 MToolBox，将 GSNAP 线程数设置为 20 (-m "-t 20..")，即从读取端允许保留 indel 的最小核苷酸距离为 10 （-a“-t 10”）和在 FASTA 共有序列中保留核苷酸变异的异质性阈值 0.6（-a“-z 0.6”）。
MToolBox.sh -i test_config.sh -m "-t 20" -a "-t 10 -z 0.6"
复制<br><br>MToolBox 的默认输出包括：<br>
<br>VCF_file.vcf：包含所有与 RSRS/rCRS 比较的线粒体变异位点和其他元信息。
<br>mt_classification_best_results.csv：报告每个序列的最佳单倍群预测结果。如果排序结果中有多个具有相同概率的最佳单倍群预测，输出将包含所有这些预测。
<br>prioritized_variants.txt：仅包含每个样本分析的优先变异的注释，这些变异是由三个参考序列（rCRS、RSRS 和 MHCS）识别的，并按核苷酸变异性递增排序。
<br>summary_&lt;date_time&gt;.txt：报告所选选项的简要摘要、预测的单倍群、每个样本的总变异和优先变异数量，以及仅针对 NGS 数据的重建基因组覆盖率、同质性和异质性变异数量。
<br>OUT_&lt;sample_name&gt; 文件夹包含：

<br>outmt.sam：映射到人类线粒体 DNA 的读取；
<br>logmt.txt：线粒体 DNA 映射的 GSNAP 日志文件；
<br>outmt.fastq：从 outmt.sam 文件中提取的单端读取的 fastq 文件；
<br>outmt1.fastq：从 outmt.sam 文件中提取的配对读取的 fastq 文件；
<br>outmt2.fastq：从 outmt.sam 文件中提取的配对读取的 fastq 文件；
<br>outhumanS.sam：映射到整个基因组的人类基因组的单端读取；
<br>loghumanS.txt：人类基因组映射（单端读取）的 GSNAP 日志文件；
<br>outhumanP.sam：映射到整个基因组的人类基因组的配对读取；
<br>loghumanP.txt：人类基因组映射（配对读取）的 GSNAP 日志文件；
<br>OUT.sam：唯一映射到线粒体基因组的读取对齐；
<br>OUT2.sam：在经过 IndelRealigner 和/或 MarkDuplicates 处理后唯一映射到线粒体基因组的读取对齐。即使禁用这两个过程，也会生成此文件；
<br>mtDNAassembly-table.txt：描述组装位置的主要表格；
<br>mtDNAassembly-Contigs.fasta：包含所有重建 contig 的 fasta 文件；
<br>mtDNAassembly-coverage.txt：包含每个 contig 和已知线粒体注释的覆盖率的文本文件；
<br>logassemble.txt：assembleMTgenome.py 脚本的日志文件；
<br>sorted.csv：包含预测概率超过 90% 的每个单倍群的表格。包括以下字段：

<br>N = 片段序列相对于 RSRS 的 SNP 数量；
<br>Nph = 在 Phylotree 中映射的 SNP 数量（在 N 中）；
<br>Nph_tot = 整个基因组中定义单倍群的 SNP 数量；
<br>Nph_exp = 片段区域中定义单倍群的 SNP 数量；
<br>P_Hg = 单倍群的预测百分比值（Nph/Nph_exp*100）；
<br>Missing sites = 在查询基因组中不存在但预期在其相应路径上的变异。这些变异也可能指向测序错误；


<br>merged_diff.csv：报告查询基因组与三个序列（RSRS、rCRS 和 hg_MHCS（Macro-Haplogroup Consensus Sequence））之间的 SNP；
<br>&lt;sample_name&gt;.csv：包含表格，其中报告了 Phylotree Build 15 中所有单倍群的相同数据，如 &lt;sequence_name&gt;.sorted.csv 文件，但不包括 Missing Sites 字段；
<br>annotation.csv：对 merged_diff.csv 文件的进一步解释，为每个 mt 变异等位基因提供多个注释，查询基因组和三个序列（RSRS、rCRS 和 hg_MHCS）之间：

<br>Sample = 样本名称；
<br>Variant Allele = 线粒体基因组中的核苷酸位置及变异等位基因；
<br>HF = VCF 文件中报告的异质性比例；
<br>CI_lower;CI_upper = 异质性比例置信区间的上下限；
<br>RSRS = 如果为“是”，则变异被 RSRS 识别；
<br>MHCS = 如果为“是”，则变异被 Macro-Haplogroup Consensus Sequence 识别；
<br>rCRS = 如果为“是”，则变异被 rCRS 识别；
<br>Haplogroup = 最佳预测单倍群；
<br>Other Haplogroups = 如果为“+”，则该变异定义了其他单倍群；
<br>Locus = 线粒体基因座；
<br>Nt Variability = SiteVar 变异性值；
<br>Codon Position = 密码子内的核苷酸位置；
<br>Aa Change = 氨基酸变化；
<br>Aa variability = MitVarProt 氨基酸变异性值；
<br>tRNA annotation = 线粒体 tRNA 基因的特定信息（在 tRNA 中的位置；tRNA 类型；三叶草二级结构区域；成熟核苷酸；特定位置在 tRNA 折叠中的作用）；
<br>Disease score = 综合疾病评分，作为非同义变异致病性预测评分的加权平均。详情参见相关出版物（PMID: 26621530）；
<br>RNA predictions = rRNA 基因中的 49 个变异和 tRNA 基因中的 207 个变异的评分（PMID: 24092330；PMID: 21882289；PMID: 23696415）。评分在 0 到 1 的范围内。rRNA 的阈值为 0.51，tRNA 的阈值为 0.31。低致病性低于固定阈值；
<br>MutPred pred = MutPred 预测（高致病性，低致病性）；
<br>MutPred Score = MutPred 致病性评分（0.000-1.000）；
<br>PolyPhen-2 HumDiv Pred = Polyphen-2 HumDiv 预测（良性，可能有害，可能有害，未知）；
<br>PolyPhen-2 HumDiv Prob = Polyphen-2 HumDiv 概率（0.000-1.000）；
<br>PolyPhen-2 HumVar Pred = Polyphen-2 HumVar 预测（良性，可能有害，可能有害，未知）；
<br>PolyPhen-2 HumVar Prob = Polyphen-2 HumVar 概率（0.000-1.000）；
<br>PANTHER Pred = 通过 SNPs&amp;GO 软件进行的 PANTHER 预测（中性，疾病，未分类）；
<br>PANTHER Prob = 通过 SNPs&amp;GO 软件进行的 PANTHER 概率（0.000-1.000）；
<br>PhD-SNP Pred = 通过 SNPs&amp;GO 软件进行的 PhD-SNP 预测（中性，疾病，未分类）；
<br>PhD-SNP Prob = 通过 SNPs&amp;GO 软件进行的 PhD-SNP 概率（0.000-1.000）；
<br>SNPs&amp;GO Pred = 通过 SNPs&amp;GO 软件进行的 SNPs&amp;GO 预测（中性，疾病，未分类）；
<br>SNPs&amp;GO Prob = 通过 SNPs&amp;GO 软件进行的 SNPs&amp;GO 概率（0.000-1.000）；
<br>Mitomap Associated Disease(s) = MITOMAP 疾病相关突变注释；
<br>Mitomap Homoplasmy = MITOMAP 同质性状态注释；
<br>Mitomap Heteroplasmy = MITOMAP 异质性状态注释；
<br>Somatic Mutations = MITOMAP 体细胞突变的细胞或组织类型注释；
<br>SM Homoplasmy = 体细胞突变的同质性状态注释；
<br>SM Heteroplasmy = 体细胞突变的异质性状态注释；
<br>ClinVar = ClinVar 疾病相关突变注释；
<br>OMIM Link = OMIM 条目链接；
<br>dbSNP ID = dbSNP 数据库中的 rs ID；
<br>Mamit-tRNA link = Mamit-tRNA 站点注释链接；
<br>PhastCons20Way = 使用 hg38+rCRS 作为参考序列计算的 20 个脊椎动物的 PhastCons 保守性评分；
<br>PhyloP20Way = 使用 hg38+rCRS 作为参考序列计算的 20 个脊椎动物的 PhyloP 保守性评分；
<br>AC/AN 1000 Genomes = 在 1000 Genomes 中找到的可能致病变异的等位基因计数与等位基因数量的比率；
<br>1000 Genomes Homoplasmy = 1000 Genomes 变异中的同质性状态注释；
<br>1000 Genomes Heteroplasmy = 1000 Genomes 变异中的异质性状态注释。




<br>警告：请注意，异质性比例和相关的置信区间仅会报告针对选择的参考序列进行读取映射的变异。<br><br>这些脚本位于 MToolBox 的 aux 文件夹中。<br>
<br>get_prioritized_var_and_summary.sh 是一个辅助脚本，用于从 MToolBox 的 annotation.csv 文件中生成 summary.txt 和 prioritized_variant.txt 文件。<br>
运行以下命令以获取脚本使用帮助：<br>
./get_prioritized_var_and_summary.sh
<br>filter_HF.py 是一个辅助脚本，用于过滤 MToolBox VCF 文件（版本 &lt; 1.2 生成的文件）的 HF 和 DP，并将多等位基因 SNP 分割为每行一个。<br>
python filter_HF.py 
<br>filter_HF_v2.py 是一个辅助脚本，用于过滤 MToolBox VCF 文件（版本 &gt;= 1.2 生成的文件）的 HF 和 DP，并将多等位基因 SNP 分割为每行一个。<br>
python filter_HF_v2.py
<br><br>python filter_HF_v2.py HG00119 HG00119.vcf 0.8 100 txt HG00119.txt No
复制<br><br>]]></description><link>软件\其它生信软件\k-s\mtoolbox.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/MToolBox.md</guid><pubDate>Sun, 28 Jul 2024 08:36:26 GMT</pubDate></item><item><title><![CDATA[mtPhyl 软件]]></title><description><![CDATA[ 
 <br><br>这是一个专注于线粒体 DNA 的系统发育的软件，具备很多功能，多篇文献提到。<br>
这个软件最初被放到谷歌云端协作平台，但是后来谷歌更新之后就找不到了。这里我放一个离线版本，可以正常使用。<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/mtphyl5.003.zip?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1726414714&amp;Signature=W950pO420G5lWcBHZhtMzg4hffM%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/mtphyl5.003.zip?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1726414714&amp;Signature=W950pO420G5lWcBHZhtMzg4hffM%3D" target="_blank">下载地址</a><br><br>这个困扰了我很久，软件的输入的文件格式是 fasta，但是对格式有一定的要求，这里给一个<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/mtPhyl%E6%A8%A1%E6%9D%BF.fasta?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1725514850&amp;Signature=v6%2BVF%2FEYQarPV3X6y%2F1rvpiJw0U%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/mtPhyl%E6%A8%A1%E6%9D%BF.fasta?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1725514850&amp;Signature=v6%2BVF%2FEYQarPV3X6y%2F1rvpiJw0U%3D" target="_blank">模板</a>。<br><br>输入的 ID 必须以 &gt;gi| 开头，然后后面再加上自己的 ID。例如：<br>&gt;gi|K1910
GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTTCGTCTGGGGGGTGTGCACGCGATAGCATTGCGAGACGCTGGAGCCGGAGCACCCTATGTCGCAGTATCTGTCTTTGATTCCTGCCTCATCTCATTATTTATCGCACCTACGTTCAATATTACAGGCGAACATACTTACTAAAGTGTGTTAATTAAT
复制<br><br>
<br>点击 import
<br>extract 到一个文件夹
<br>选 add files from folder <br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040956422.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040959183.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
]]></description><link>软件\其它生信软件\k-s\mtphyl：数据导入.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/mtPhyl：数据导入.md</guid><pubDate>Wed, 04 Sep 2024 01:59:56 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040956422.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040956422.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备工作]]></title><description><![CDATA[ 
 <br><br>
<br>参考基因组：可以选用 GRCH37 或者 GRCH38 。

<br>参考基因组可以在 NCBI 下载。我在这里已经建立好了两个版本的索引，足以应对大多数情况。
<br>建立索引：利用 BWA 的 index。
<br>获得索引文件。


<br>下载 NUMTs-detection-1.0

<br>这个包可以在 Github 下载。但是正如作者所说，这些脚本已经不再维护。里面有很多语法已经被淘汰。地址： <a data-tooltip-position="top" aria-label="https://github.com/WeiWei060512/NUMTs-detection" rel="noopener nofollow" class="external-link" href="https://github.com/WeiWei060512/NUMTs-detection" target="_blank">WeiWei060512/NUMTs-detection: Detecting NUMTs from WGS (github.com)</a>
<br>解压至 Linux 系统。
<br>对里面的一些文件进行修改。


<br><br><br>修改目的：<br>
<br>无法使用模组，因为缺乏高性能计算环境，因此在这里注释掉。
<br>修改了输出的 split.bam。源代码最后一句删掉了相关文件，并且在该 shell 脚本的 Python 中也没有出现过这个文件。但是在另外一个 sh 中有用的，所以我建议最后还是不要删除。这个文件被指向了一个 wgs 的目录，这应该是一个错误。
<br>#! /bin/bash

################################################################################
## This script detects NUMTs from whole genome sequencing BAM files
## samtools, samblaster and blat need to be installed to run the pipeline
## samtools can be downloaded at http://www.htslib.org/download/
## samblaster can be downloaded at https://github.com/GregoryFaust/samblaster
## blat can be downloaded at http://hgdownload.soe.ucsc.edu/admin/exe/
################################################################################

## load modules on HPC
# module load samblaster/0.1.24
# module load samtools 
# module load Sambamba/0.6.6


INPUT_BAM=/home/luolintao/Reference_GRCH/INPUT_TEST/HG006_5X.bam # input WGS bam file
OUTPUT_DIR=/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT # output folder path
REF_GRCh38=/home/luolintao/Reference_GRCH/GRCh38_latest_genomic.fna # human reference genome

CLUSTER_SCRIPT='/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/scripts/searchNumtCluster_fromDiscordantReads.py'
BREAKPOINT_SCRIPT='/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/scripts/searchBreakpoint_fromblatoutputs.py'

SAMPLE_ID1="${INPUT_BAM##*/}"
SAMPLE_ID2=${SAMPLE_ID1%.bam}
OUTPUT="${OUTPUT_DIR}/${SAMPLE_ID2}"

INPUT_DISC="${OUTPUT}.mt.disc.sam"
# INPUT_SPLIT="${OUTPUT_wgs}.mt.split.sam" 我至今不能明白为什么这个前缀包含了一个wgs。可能只有上帝能明白。
INPUT_SPLIT="${OUTPUT}.mt.split.sam"

# 文件预处理
echo "正在使用samtools预处理文件!"
samtools view -@ 16 -m 10G -h -F 2 $INPUT_BAM | grep -e @ -e MT -e chrM | samtools sort -@ 16 -m 10G -n  | samtools view -h | samblaster --ignoreUnmated -e -d $INPUT_DISC -s $INPUT_SPLIT -o /dev/null
# 聚类查找
python3 $CLUSTER_SCRIPT ${SAMPLE_ID2} ${INPUT_BAM} ${INPUT_DISC}
echo "查找聚类已经完成，现在开始寻找断点!"


# 定义文件名
filelines=$(cat ${INPUT_DISC}.breakpointINPUT.tsv)


# 使用制表符作为分隔符读取文件的每一行，并将其字段分配给相应的变量
while IFS=$'\t' read -r sampleID cluster_no disFile splitFile wgsBAM chr start end; do
    # 去除 chr 字段中的多余字符（如括号和引号）
    chr=$(echo $chr | tr -d "(),'")    
    # 输出当前行的各个字段，方便调试
    echo "$sampleID $cluster_no $disFile $splitFile $wgsBAM $chr $start $end"    
    # 构建区域字符串，用于 samtools 提取指定区域
    REGION="${chr}:${start}-${end}"    
    # 构建输出文件的前缀路径
    OUTPUT="${OUTPUT_DIR}/${sampleID}_${chr}.${start}.${end}"    
    #使用 samtools 从 BAM 文件中提取指定区域的数据，并保存为 SAM 格式
    samtools view "${wgsBAM}" "${REGION}" &gt; "${OUTPUT}.sam"    
    # 使用 awk 过滤掉特定 CIGAR 值的行，并提取第1和第10列，将结果保存为 FASTA 格式
    awk '$6 !~ /150M|149M|148M|149S|148S/' "${OUTPUT}.sam" | cut -f1,10 &gt; "${OUTPUT}.fasta"
    # 使用 perl 脚本在每行前添加 '&gt;' 符号，符合 FASTA 格式要求
    perl -pi -e 's/^/&gt;/g' "${OUTPUT}.fasta"    
    # 使用 perl 脚本将制表符替换为换行符，符合 FASTA 格式要求
    perl -pi -e 's/\t/\n/g' "${OUTPUT}.fasta"    
    # 使用 BLAT 工具将 FASTA 文件比对到参考基因组，结果保存为 PSL 格式
    /home/luolintao/blat "${REF_GRCh38}" "${OUTPUT}.fasta" "${OUTPUT}.psl"    
    # 运行 Python 脚本处理 BLAT 生成的 PSL 文件，提取断点信息
    python3 "${BREAKPOINT_SCRIPT}" "${OUTPUT}.psl" "${sampleID}" "${chr}" "${start}" "${end}" "${OUTPUT}"
    # 删除临时生成的 FASTA 文件
    rm "${OUTPUT}.fasta"    
    # 删除临时生成的 SAM 文件
    rm "${OUTPUT}.sam"
done &lt;&lt;&lt; "$filelines"
复制<br><br>这个文件是上述 shell 需要调用的。但是里面存在一些问题：<br>
<br>不规范的缩进。
<br>不再被维护的 pandas 语法。<br>
为了不改变原有功能，我对部分语句进行了维护。使用的版本是 Python 3.9.15。<br>
修改之后的代码如下：
<br>#!/usr/bin/env python

################################################################################
## This script takes outputs from NUMT_detection.sh to generates NUMT clusters
################################################################################

import fileinput
import sys, os
import pandas as pd
import glob
import scipy.stats as stats
import numpy as np

#################### Extract cluster from mtDNA discordant sam files  #############################

def cluster(data, maxgap):
    data.sort()
    groups = [[data[0]]]
    for x in data[1:]:
        if abs(x - groups[-1][-1]) &lt;= maxgap:
            groups[-1].append(x)
        else:
            groups.append([x])
    return groups

sampleID, wgsBAM, input1 = sys.argv[1:]

sampleID = sampleID.replace(".mt.disc", "")

df0 = pd.read_csv(input1, names=['QNAME', 'FLAG', 'RNAME', 'POS', 'MAPQ', 'CIGAR', 'RNEXT', 'PNEXT', 'TLEN', 'SEQ', 'QUAL', 'SM', 'RG', 'NM', 'BC', 'OC', 'ZX', 'ZY', 'SA'], sep="\t", engine='python', comment="@")

df = df0[~df0.RNAME.str.contains("Un_|random|\.")]
df1 = df[(df["MAPQ"].astype(int) &gt; 0) &amp; ((df['RNAME'] == "MT") | (df['RNAME'] == "chrM") | (df['RNEXT'] == "MT") | (df['RNEXT'] == "chrM"))]

##### remove reads map to mtDNA #####
df3 = df1[~df1.RNAME.isin(['chrM', 'MT'])]

##### order by chromosome and pos #####
df3 = df3.sort_values(['RNAME', 'POS'])

##### look for the clutser by mapgap on pos #####
##### extract the clusters with number of elements no less than 5 #####

output1 = pd.DataFrame([])

df_chr = df3.groupby(['RNAME'])
for clusterID, myclusters in df_chr:
    myclusters['POS'] = myclusters['POS'].astype(int)
    sub_cluster = cluster(myclusters['POS'].tolist(), maxgap=500)
    for x in sub_cluster:
        if len(x) &gt;= 2:
            mycluster = x
            df_cluster = df3[df3.POS.isin(mycluster)]
            df_cluster_pairMT = df[df.QNAME.isin(df_cluster['QNAME'])]
            mt_cluster = cluster(df_cluster['PNEXT'].tolist(), maxgap=500)
            for y in mt_cluster:
                df_cluster_pairMT_out = df_cluster_pairMT[df_cluster_pairMT.PNEXT.isin(y)]
                df_cluster_pairMT_out = df_cluster_pairMT_out.assign(
                    subCluster_No=len(y),
                    Cluster_No=len(x),
                    Cluster_ID=f"{clusterID}_{min(df_cluster['POS'])}_{max(df_cluster['POS'])}_MTboth_{min(df_cluster_pairMT_out['PNEXT'])}_{max(df_cluster_pairMT_out['PNEXT'])}"
                )
                output = pd.concat([pd.DataFrame(), df_cluster_pairMT_out], ignore_index=True)

            output1 = pd.concat([output1, output])
            output1["IndividualID"] = sampleID
            if len(output1) != 0:
                output1['clusterID'] = output1['RNAME'].astype(str) + "_" + output1['Cluster_No'].astype(str)
                output2 = output1.groupby(['IndividualID', 'Cluster_ID', 'Cluster_No', 'subCluster_No']).size().to_frame('size').reset_index()
            else:
                continue

output1.to_csv(input1 + '.cluster.tsv', sep='\t', header=True, index=False)
output2.to_csv(input1 + '.cluster.summary.tsv', sep='\t', header=False, index=True)

##### generate cluster range for defining the breakpoints ######

output2['disFile'] = input1
output2['splitFile'] = input1.replace('disc', 'split')
output2['wgsBAM'] = wgsBAM
df_pos = pd.DataFrame(output2['Cluster_ID'].str.split('_').tolist(), columns=['chr', 'start', 'end', 'chrM', 'mtstart', 'mtend'])
del output2['Cluster_ID']
del output2['subCluster_No']
del output2['size']
output3 = pd.concat([output2, df_pos[['chr', 'start', 'end']]], axis=1)
output3 = output3.drop_duplicates(['chr', 'start', 'end'])
output3['start'] = output3['start'].astype(int) - 500
output3['end'] = output3['end'].astype(int) + 500 + 150
output3['Cluster_No'] = output3['Cluster_No'].astype(int)

##### output to file #####
output3.to_csv(input1 + '.breakpointINPUT.tsv', sep='\t', header=False, index=False)

复制<br><br>也是上述 shell 需要调用的。但是里面存在很大的问题：<br>
<br>使用了旧版本的 pandas 语法。令人遗憾的，新版本的语法似乎不能很好读取 blat 生成的 psl 文件，标题行存在很大的问题。为了不让原本的功能丧失，我只能够用最愚蠢的办法将数值转为字符串。尽管这会报错，但是并不影响最终的结果。
<br>在 GRCH38 版本中，染色体的名字是以 NCXXXXXX 命名的，不再使用了 chr。因此，需要重新对其进行映射。
<br>可以根据自己的需求就筛选过滤措施，这对于低深度的测序也许有一定的作用。<br>
修改之后的脚本：
<br>#!/usr/bin/env python
################################################################################
## This script takes outputs from searchNumtCluster_fromDiscordantReads.py and NUMT_detection.sh 
## to look for NUMT breakpoints
################################################################################

import sys, os
import pandas as pd

def classify_breakpoint(row, type='nu'):
    mismatchLEN = 3 # 默认是3 请在这里选择过滤措施，选择更大的数量会得到更多的结果，但是可能会并不会特别精确。
    readLEN = 150 # 默认是150 请在这里选择过滤措施，选择更长的长度会得到更多的结果，但是可能会并不会特别精确。
    if type == 'nu':
        if row['strand'] == "+" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "nu_Tstart_Bright"
        elif row['strand'] == "+" and row['Qstart'] &lt;= mismatchLEN:
            return "nu_Tend_Bleft"
        elif row['strand'] == "-":
            return "nu_NegStrand"
        else:
            return "nu_useLess"
    else:
        if row['strand'] == "+" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "mt_Tstart"
        elif row['strand'] == "+" and row['Qstart'] &lt;= mismatchLEN:
            return "mt_Tend"
        elif row['strand'] == "-" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "mt_Tend"
        elif row['strand'] == "-" and row['Qstart'] &lt;= mismatchLEN:
            return "mt_Tstart"
        else:
            return "mt_useLess"

# 从命令行读取输入参数
INPUT_PSL, SAMPLEID, CHR, START, END, OUTPUT = sys.argv[1:]
START, END = int(START), int(END)

# 读取数据并预处理
psl_columns = [
    "match", "misMatch", "repMatch", "Ns", "QgapCount", "QgapBases", "TgapCount", "TgapBases", "strand",
    "Qname", "Qsize", "Qstart", "Qend", "Tname", "Tsize", "Tstart", "Tend", "blockCount", "blockSizes",
    "qStarts", "tStarts"
]
df = pd.read_csv(INPUT_PSL, skiprows=5, sep="\t", names=psl_columns)
df['matchLEN'] = df['Tend'] - df['Tstart']

# 过滤数据，过滤措施
filtered_df = df[(df['matchLEN'] &lt; 170) &amp; (df['misMatch'] &lt;= 7)] # 默认是df[(df['matchLEN'] &lt; 140) &amp; (df['misMatch'] &lt;= 3)]请在这里选择过滤措施，选择更大的数量会得到更多的结果，但是可能会并不会特别精确。
filtered_df = filtered_df[(filtered_df['Tend'] &gt;= 140) | (filtered_df['Tend'] &lt;= 10)] # 默认是[(filtered_df['Tend'] &gt;= 147) | (filtered_df['Tend'] &lt;= 3)]请在这里选择过滤措施，选择更长的长度会得到更多的结果，但是可能会并不会特别精确。

# Tname映射
tname_mapping = {
    'NC_012920.1': 'MT',
    'NC_000001.11': 'chr1',
    'NC_000002.12': 'chr2',
    'NC_000003.12': 'chr3',
    'NC_000004.12': 'chr4',
    'NC_000005.10': 'chr5',
    'NC_000006.12': 'chr6',
    'NC_000007.14': 'chr7',
    'NC_000008.11': 'chr8',
    'NC_000009.12': 'chr9',
    'NC_000010.11': 'chr10',
    'NC_000011.10': 'chr11',
    'NC_000012.12': 'chr12',
    'NC_000013.11': 'chr13',
    'NC_000014.9': 'chr14',
    'NC_000015.10': 'chr15',
    'NC_000016.10': 'chr16',
    'NC_000017.11': 'chr17',
    'NC_000018.10': 'chr18',
    'NC_000019.10': 'chr19',
    'NC_000020.11': 'chr20',
    'NC_000021.9': 'chr21',
    'NC_000022.11': 'chr22',
    'NC_000023.11': 'chrX',
    'NC_000024.10': 'chrY'
}

filtered_df['Tname_mapped'] = filtered_df['Tname'].map(tname_mapping).fillna(filtered_df['Tname'])

# 分离线粒体和核序列
mt_df = filtered_df.loc[filtered_df['Tname_mapped'] == 'MT'].copy()
nu_df = filtered_df.loc[(filtered_df['Tname_mapped'] == CHR) &amp; (filtered_df['Tstart'] &gt; START) &amp; (filtered_df['Tend'] &lt; END)].copy()

if not nu_df.empty:
    # 应用分类函数
    nu_df.loc[:, 'pointGroup'] = nu_df.apply(classify_breakpoint, axis=1, type='nu')
    if not mt_df.empty:
        mt_df.loc[:, 'pointGroup'] = mt_df.apply(classify_breakpoint, axis=1, type='mt')

    nu_df.loc[:, 'Group'] = nu_df['pointGroup'].str.replace(r'_T.*B', '', regex=True)
    nu_df.loc[:, 'chr'] = CHR
    if not mt_df.empty:
        mt_df.loc[:, 'chr'] = 'chrM'

    def group_and_count(df, by_cols):
        return df.groupby(by_cols).size().reset_index(name="readsCount")

    # 核DNA断点
    nu_left = group_and_count(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft'], ['pointGroup', 'Group', 'chr', 'Tend', 'strand'])
    nu_right = group_and_count(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright'], ['pointGroup', 'Group', 'chr', 'Tstart', 'strand'])
    nu_both = pd.concat([nu_left, nu_right])

    # 同时映射到线粒体的核DNA断点
    nu_mt = nu_df.loc[nu_df['Qname'].isin(mt_df['Qname'])]
    nu_mt_left = group_and_count(nu_mt.loc[nu_mt['pointGroup'] == 'nu_Tend_Bleft'], ['pointGroup', 'Group', 'chr', 'Tend', 'strand'])
    nu_mt_right = group_and_count(nu_mt.loc[nu_mt['pointGroup'] == 'nu_Tstart_Bright'], ['pointGroup', 'Group', 'chr', 'Tstart', 'strand'])
    nu_mt_both = pd.concat([nu_mt_left, nu_mt_right])

    # 线粒体断点
    if not mt_df.empty:
        mt_tend = group_and_count(mt_df.loc[mt_df['pointGroup'] == 'mt_Tend'], ['pointGroup', 'chr', 'Tend', 'strand'])
        mt_tstart = group_and_count(mt_df.loc[mt_df['pointGroup'] == 'mt_Tstart'], ['pointGroup', 'chr', 'Tstart', 'strand'])
        mt_both = pd.concat([mt_tend, mt_tstart])
        mt_both['Group'] = 'UKn'

        # 同时映射到核DNA的线粒体断点
        mt_conf = pd.concat([
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tstart')], ['pointGroup', 'chr', 'Tstart', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tend')], ['pointGroup', 'chr', 'Tend', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tstart')], ['pointGroup', 'chr', 'Tstart', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tend')], ['pointGroup', 'chr', 'Tend', 'strand'])
        ])

        # 捕获并忽略异常
        try:
            mt_conf['Group'] = mt_conf.apply(lambda row: 'mtLeft' if 'left' in row['pointGroup'] else 'mtRight', axis=1)
        except ValueError as e:
            print(f"尽管这里出现了一个问题: {e}, 但是我们并不打算解决它，因为它并不影响最终的结果。")

        all_breakpoints = pd.concat([nu_both, mt_both])
        mt_conf['sampleID'] = SAMPLEID
        mt_conf['Tstart'] = mt_conf['Tstart'].fillna(-1).astype(int)
        mt_conf['Tend'] = mt_conf['Tend'].fillna(-1).astype(int)
    else:
        all_breakpoints = nu_both
        mt_conf = pd.DataFrame()

    # 输出结果
    all_breakpoints['sampleID'] = SAMPLEID
    all_breakpoints['Tstart'] = all_breakpoints['Tstart'].fillna(-1).astype(int)
    all_breakpoints['Tend'] = all_breakpoints['Tend'].fillna(-1).astype(int)

    confident_breakpoints = pd.concat([nu_mt_both, mt_conf])
    confident_breakpoints['sampleID'] = SAMPLEID
    confident_breakpoints['Tstart'] = confident_breakpoints['Tstart'].fillna(-1).astype(int)
    confident_breakpoints['Tend'] = confident_breakpoints['Tend'].fillna(-1).astype(int)

    # 写入文件
    confident_breakpoints.to_csv(OUTPUT + '.Breakpoints.tsv', sep='\t', header=False, index=False)
else:
    print("过滤后没有发现核序列。")

复制<br><br><br>该文件包含断点信息。每行表示一个断点，通常包括以下列：<br>
<br>pointGroup: 断点组
<br>Group: 断点类别
<br>chr: 染色体
<br>Tend: 结束位置
<br>strand: 链方向
<br>readsCount: 读取计数
<br>Tstart: 起始位置
<br>sampleID: 样本ID
<br>示例数据：<br>nu_Tend_Bleft	nuleft	chr1	634078	+	1	-1	HG001
nu_Tstart_Bright	nuright	chr1	-1	+	1	633739	HG001
nu_Tstart_Bright	nuright	chr1	-1	+	1	633939	HG001
复制<br><br>该文件包含每个找到的断点的信息。每行表示一个断点区域，通常包括以下列：<br>
<br>sampleID: 样本ID
<br>cluster_no: 聚类编号
<br>disFile: 不一致的文件路径
<br>splitFile: 分裂的文件路径
<br>wgsBAM: WGS BAM文件路径
<br>chr: 染色体编号
<br>start: 起始位置
<br>end: 结束位置
<br>HG001	3	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr1',)	633396	634563
HG001	2	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr11',)	49861375	49863007
HG001	3	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr5',)	32337638	32339245

复制<br><br>该文件包含聚类的摘要信息。每行表示一个聚类，通常包括以下列：<br>
<br>IndividualID: 样本ID
<br>Cluster_ID: 聚类ID
<br>Cluster_No: 聚类编号
<br>subCluster_No: 子聚类编号
<br>size: 聚类大小
<br>0	HG001	('chr1',)_633896_633913_MTboth_9735_9872	3	3	3
1	HG001	('chr11',)_49861875_49862357_MTboth_16089_16340	2	2	2
2	HG001	('chr5',)_32338138_32338595_MTboth_14835_14835	3	1	1
复制<br>在这里，需要具体解释一下：<br>
前三列很好理解。但是针对后面：<br>
<br>
第四列*(Cluster_No)：这是主聚类中读段的总数。这些读段是基于它们在染色体上的位置（POS）进行聚类的，聚类的条件是相邻读段之间的距离不超过 500 个碱基对。检测出的满足第三列条件的 NUMT 的数量。

<br>
第五列和第六列表示在这个聚类中的子聚类数目，它们是对这些读段进一步聚类后满足特定条件的读段数目。根据代码的逻辑，第五列（subCluster_No）和第六列（subCluster_No）的值在通常情况下会相同。(subCluster_No)：这是子聚类中读数的总数。这些读数是基于它们在配对读数的位置（PNEXT）进行进一步聚类的，聚类的条件同样是相邻读数之间的距离不超过 500 个碱基对。<br>
NUMTs 是线粒体 DNA 插入到核基因组中的片段，通常我们预期插入到核基因组中的 NUMT 片段会比它们的原始线粒体序列短或相等。然而，你的示例中出现了相反的情况，这确实需要解释。在你的例子中，核基因组中的序列长度（444 bp）大于线粒体中的序列长度（190 bp）。有几种可能的解释：

<br>
插入事件和重排：

<br>NUMTs 插入到核基因组中时，可能会发生重排或扩展，从而导致插入片段在核基因组中的长度增加。
<br>核基因组中的插入片段可能包含了其他序列，这些序列可能是插入事件导致的。


<br>
重复序列：

<br>核基因组中可能包含重复序列，这些重复序列与线粒体 DNA 部分相同，但总体长度较长。
<br>这些重复序列可能是由于插入事件或其他基因组重排造成的。


<br>
比对错误或读数拼接：

<br>在比对过程中，可能会出现错误，导致比对到核基因组的读数长度增加。
<br>读数拼接也可能会引入额外的序列，从而增加比对长度。


<br><br>该文件包含聚类的详细信息。每行表示一个聚类的具体信息，通常包括以下列：<br>
<br>QNAME: 序列名称
<br>FLAG: 标志
<br>RNAME: 参考序列名称
<br>POS: 位置
<br>MAPQ: 比对质量
<br>CIGAR: 比对描述
<br>RNEXT: 下一个参考名称
<br>PNEXT: 下一个位置
<br>TLEN: 模板长度
<br>SEQ: 序列
<br>QUAL: 质量
<br>SM: 样本
<br>RG: 读取组
<br>NM: 不匹配数
<br>BC: 条形码
<br>OC: 原始 CIGAR
<br>ZX: 自定义标记1
<br>ZY: 自定义标记2
<br>SA: 辅助比对
<br>subCluster_No: 子聚类编号
<br>Cluster_No: 聚类编号
<br>Cluster_ID: 聚类ID
<br>IndividualID: 样本ID
<br>clusterID: 聚类ID
<br>QNAME	FLAG	RNAME	POS	MAPQ	CIGAR	RNEXT	PNEXT	TLEN	SEQ	QUAL	SM	RG	NM	BC	OC	ZX	ZY	SA	subCluster_No	Cluster_No	Cluster_ID	IndividualID	clusterID
HISEQ1:11:H8GV6ADXX:1:2212:16190:45635	97	chr1	633900	32	148M	chrM	9735	0	TCTCTTATACTAGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTC	C									3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HISEQ1:17:H947YADXX:2:2206:7902:6711	97	chr1	633913	32	148M	chrM	9872	0	TATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTCGCTCTAAGATTAA										3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HWI-D00360:5:H814YADXX:2:2107:18825:98678	161	chr1	633896	20	148M	chrM	9818	0	CTGATCTCTTATACTAGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGCGCGCAGTGATTATAGGC	&lt;									3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HWI-D00360:5:H814YADXX:2:1106:18751:35433	113	chr11	49862357	70	148M	chrM	16089	0	GTATTTAATGTAATTTATAGAGAACCGTTTGAATGAAACTAAGTTTTTACTGGAAATACAGCAATTTTTTTTTTTTCAGAATATGCTTATAGGTGTGGAATTGCAGAGGCTGTTGGTCTTCCAAGTATTCCTGTTCATCCAATTGGAT	CEEDDDEECDDDDDDEDEDDCDB									2	2	('chr11',)_49861875_49862357_MTboth_16089_16340	HG001	chr11_2
HWI-D00360:6:H81VLADXX:1:2211:17678:67136	65	chr11	49861875	70	143M5S	chrM	16340	0	GGAAAGGAAAAGAAAAGAAAAGTAATGATTAGATTTATTTTTAAAGTCTTGCTTATTACAATGATGGAAAATTTGAGATTTTCCATCATATTGTATGTATTAGTTTTTTTCTTTTGTTGATTGAGCAGCATTCCATTGTGGGAAAATA	CCCFFFFFHHHHHJIIJJJJJJHHJIJJJJJJJJJJIIJJJJJJJJHIJJJJJJJJJJJJJJJJJJJJJJJJJJJJJHHHHHHHFFFFFFFEEEEEEEFFFDEDDEDDDDDDDDDDDDDDDDDDDDDDDDDDDEEEEDEDDDDDDDDD	PG:Z:novoalign	AS:i:90	UQ:i:90	NM:i:1	MD:Z:22A120				2	2	('chr11',)_49861875_49862357_MTboth_16089_16340	HG001	chr11_2
HISEQ1:17:H947YADXX:2:1209:10750:98109	145	chr5	32338595	70	148M	chrM	14835	0	TTTTGTATTTTTAGTAGAGATGGGGTTTCTCCACATTGGTCAGGCTGGTCTCGAACTCCCAGCCTCAGGTGATCTGCCTGTCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATTGCACCCAGCTGAGTAAATCAGGTTTT	BDDCC									1	3	('chr5',)_32338138_32338595_MTboth_14835_14835	HG001	chr5_3

复制<br><br><br>我真的是服了 circos 这个软件了。他很强大，的确，也很弱智。一点也不智能。真不敢相信在2024年我还能见到如同 WindowsXP 一般的软件。转念一想，科研嘛，大概都是这样的。不然人人都是科学家。<br><br><a data-tooltip-position="top" aria-label="https://circos.ca/software/download/" rel="noopener nofollow" class="external-link" href="https://circos.ca/software/download/" target="_blank">Download // CIRCOS Circular Genome Data Visualization</a><br>
将压缩包解压到 Linux 环境。其实这个软件 win 也能运行，但是不太方便。<br>运行 circos 需要一些环境配置，但是这些都不太难。官网查一查就行了，问问呢 ChatGPT 也不会出错。<br><br>修改代码如下：<br>################################################################################
## 本脚本生成circos图
## 需要安装Circos
## Circos可以从 http://circos.ca/software/download/circos/ 下载
################################################################################

# 引入Circos的housekeeping配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/housekeeping.conf&gt;&gt;

# Circos的一般设置
anti_aliasing* = no  # 禁用抗锯齿
max_points_per_track* = 30000  # 每个轨道的最大点数

# 引入颜色、字体和模式的配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors_fonts_patterns.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.brewer.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.ucsc.conf&gt;&gt;

# 引入NUMTs的ideogram配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ideogram_NUMTs.conf&gt;&gt; 
# 引入NUMTs的刻度配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ticks_NUMTs.conf&gt;&gt;

# karyotype设置，使用人类karyotype文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/karyotype_NUMTs.conf&gt;&gt;
karyotype* = /mnt/e/Scientifc_software/circos-current/circos-0.69-9/data/karyotype/karyotype.human.hg38.txt


# 染色体显示和缩放设置
# chromosomes_display_default = no
# chromosomes_units   = 0.1  # 染色体单位
# chromosomes     = /hs[1-9XYM]/  # 要显示的染色体
# chromosomes_reverse = hsM  # 反转M染色体
# chromosomes_radius = hsM:1.0r  # M染色体的半径
# chromosomes_scale   = hsM=0.5r  # M染色体的缩放

# 图片设置
&lt;image&gt;
angle_offset* = -90  # 角度偏移
file*  = CircosPlot_allBreakpoint.germline.png # 输出文件名
svg*   = no  # 使用SVG格式
png* = yes # 使用PNG格式
radius* = 1500p  # 减少图像半径以提高速度

# 引入图片配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/image.conf&gt;&gt;
&lt;/image&gt;

# NUMTs链接设置
&lt;links&gt;
&lt;link&gt;
ribbon           = no  # 不使用带状显示
bezier_radius    = 0r  # 贝塞尔曲线半径
crest                = 0.50  # 贝塞尔曲线的峰值
bezier_radius_purity = 1  # 贝塞尔曲线半径纯度
file		= /mnt/c/Users/victo/Desktop/circos_format_data_no_header.txt # mtDNA和核基因组NUMTs之间的链接文件
radius		= 0.999r  # 链接的半径
thickness	= 10  # 链接的厚度

# 引入链接规则配置文件
&lt;rules&gt;
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/links_allBreakpoints.soma.conf&gt;&gt;
&lt;/rules&gt;
&lt;/link&gt;
&lt;/links&gt;

# # 绘图设置
# &lt;plots&gt;
# # 引入中心的MitoGenes配置文件
# &lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/MitoGenes.centre.germline.conf&gt;&gt; weiwei的文件夹怎么可能找到？

# &lt;plot&gt;
# type            = tile  # 绘图类型为瓷砖
# file            = mtFragments_germline.txt  # mtFragments文件
# r1                      = 1.15r  # 外半径
# r0                      = 0.5r  # 内半径
# orientation             = out  # 方向为向外
# layers                  = 1  # 图层数
# margin                  = 0.1u  # 边距
# thickness               = 5  # 厚度
# padding                 = 0.1  # 内边距

# color                   = vdred  # 颜色为深红色
# layers_overflow         = grow  # 图层溢出时扩展

# stroke_thickness        = 1  # 描边厚度
# stroke_color            = white  # 描边颜色为白色

# &lt;/plot&gt;

# &lt;/plots&gt;

复制<br>注意
这里面的所有的路径一定要修改成自己的路径。推荐使用绝对路径。
<br><br>&lt;ideogram&gt;

&lt;spacing&gt;
# 默认间距设置为染色体半径的0.002倍
default = 0.002r

# 在边缘处是否显示轴断裂
axis_break_at_edge = yes

# 是否显示轴断裂
axis_break = yes

# 设置轴断裂的样式为2
axis_break_style = 2

# 断裂样式1的定义
&lt;break_style 1&gt;
stroke_color = black      # 描边颜色为黑色
fill_color = blue         # 填充颜色为蓝色
thickness = 0.25r         # 厚度为0.25倍染色体半径
stroke_thickness = 2p     # 描边厚度为2像素
&lt;/break_style&gt;

# 断裂样式2的定义
&lt;break_style 2&gt;
stroke_color = black      # 描边颜色为黑色
stroke_thickness = 5p     # 描边厚度为5像素
thickness = 2r            # 厚度为2倍染色体半径
&lt;/break_style&gt;

# 设置染色体hs1和hsM之间的间距为4倍染色体半径
&lt;pairwise hs1 hsM&gt;
spacing = 4r
&lt;/pairwise&gt;

# 设置染色体hsY和hsM之间的间距为4倍染色体半径
&lt;pairwise hsY hsM&gt;
spacing = 4r
&lt;/pairwise&gt;

&lt;/spacing&gt;

# Ideogram位置，填充和轮廓
radius = 0.85r               # 半径设置为0.85倍染色体半径
thickness = 80p              # 厚度为20像素
fill = yes                   # 是否填充
stroke_color = dgrey         # 描边颜色为深灰色
stroke_thickness = 0p        # 描边厚度为2像素

#label.conf
show_label = yes             # 是否显示标签
label_font = default         # 标签字体为默认字体
label_radius = 1.075r        # 标签半径为1.075倍染色体半径
label_size = 30              # 标签大小为30
label_parallel = yes         # 标签是否平行于染色体

##bands.conf
show_bands = yes             # 是否显示染色体带
fill_bands = yes             # 是否填充染色体带
band_transparency = 4        # 染色体带透明度

radius* = 0.85r              # 半径设置为0.85倍染色体半径

&lt;/ideogram&gt;

复制<br><br>karyotype = /mnt/e/Scientifc_software/circos-current/circos-0.69-9/data/karyotype/karyotype.human.hg38.txt # 指定karyotype文件的路径，该文件定义了染色体的基本信息（如名称、长度、带）。
chromosomes_display_default = no # 禁用默认显示所有染色体，只显示在chromosomes参数中明确指定的染色体。
chromosomes_units   = 0.1 # 设置染色体的单位长度，以百万碱基对（Mb）为单位（这里是0.1 Mb）。
chromosomes     = /hs[1-9XYM]/ # 通过正则表达式指定要显示的染色体，这里是人类的染色体1-9，X，Y和线粒体M。
chromosomes_reverse = hsM # 反转显示指定的染色体，这里是线粒体M。
# 在这里修改染色体的颜色
chromosomes_color   = hs1=black,hs2=orange,hs3=yellow,hs4=green,hs5=blue,hs6=dpurple,hs7=red,hs8=orange,hs9=yellow,hs10=green,hs11=blue,hs12=dpurple,hs13=red,hs14=orange,hs15=yellow,hs16=green,hs17=blue,hs18=dpurple,hs19=red,hs20=orange,hs21=yellow,hs22=green,hsX=blue,hsY=vdblue,hsM=vlblue
#chromosomes_color  = hsM=vlblue
chromosomes_radius = hsM:1r # 为线粒体染色体指定显示半径，这里是1.05倍的默认半径。
chromosomes_scale   = hsM=0.5r # 为线粒体染色体指定显示比例尺，这里是0.5倍的默认比例尺。

复制<br><br>&lt;rule&gt;
# 条件：在染色体hs1和线粒体染色体hsM之间
condition = between(hs1,hsM)
# 颜色：chr1（应在颜色定义文件中定义）
color = black
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs2和线粒体染色体hsM之间
condition = between(hs2,hsM)
# 颜色：chr2（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs3和线粒体染色体hsM之间
condition = between(hs3,hsM)
# 颜色：chr3（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs4和线粒体染色体hsM之间
condition = between(hs4,hsM)
# 颜色：chr4（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs5和线粒体染色体hsM之间
condition = between(hs5,hsM)
# 颜色：chr5（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs6和线粒体染色体hsM之间
condition = between(hs6,hsM)
# 颜色：chr6（应在颜色定义文件中定义）
color = cdpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs7和线粒体染色体hsM之间
condition = between(hs7,hsM)
# 颜色：chr7（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs8和线粒体染色体hsM之间
condition = between(hs8,hsM)
# 颜色：chr8（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs9和线粒体染色体hsM之间
condition = between(hs9,hsM)
# 颜色：chr9（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs10和线粒体染色体hsM之间
condition = between(hs10,hsM)
# 颜色：chr10（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs11和线粒体染色体hsM之间
condition = between(hs11,hsM)
# 颜色：chr11（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs12和线粒体染色体hsM之间
condition = between(hs12,hsM)
# 颜色：chr12（应在颜色定义文件中定义）
color = dpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs13和线粒体染色体hsM之间
condition = between(hs13,hsM)
# 颜色：chr13（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs14和线粒体染色体hsM之间
condition = between(hs14,hsM)
# 颜色：chr14（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs15和线粒体染色体hsM之间
condition = between(hs15,hsM)
# 颜色：chr15（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs16和线粒体染色体hsM之间
condition = between(hs16,hsM)
# 颜色：chr16（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs17和线粒体染色体hsM之间
condition = between(hs17,hsM)
# 颜色：chr17（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs18和线粒体染色体hsM之间
condition = between(hs18,hsM)
# 颜色：chr18（应在颜色定义文件中定义）
color = dpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs19和线粒体染色体hsM之间
condition = between(hs19,hsM)
# 颜色：chr19（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs20和线粒体染色体hsM之间
condition = between(hs20,hsM)
# 颜色：chr20（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs21和线粒体染色体hsM之间
condition = between(hs21,hsM)
# 颜色：chr21（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs22和线粒体染色体hsM之间
condition = between(hs22,hsM)
# 颜色：chr22（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hsX和线粒体染色体hsM之间
condition = between(hsX,hsM)
# 颜色：chrx（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hsY和线粒体染色体hsM之间
condition = between(hsY,hsM)
# 颜色：chry（应在颜色定义文件中定义）
color = vdblue
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

复制<br><br># 是否显示刻度
show_ticks = yes
# 是否显示刻度标签
show_tick_labels = yes
&lt;ticks&gt;
# 刻度的半径，设置在染色体外侧半径+10像素的位置
radius = dims(ideogram,radius_outer) + 10p
# 刻度线的厚度
thickness = 3p
# 刻度线的方向，设置为向外
orientation = out
# 刻度线的颜色
color = black
# 刻度标签的大小
size = 12p
# 标签的偏移量
label_offset = 5p
# 标签的格式，这里设置为显示整数
format = %d
&lt;tick&gt;
# 默认情况下不显示刻度
chromosomes_display_default = no
# 仅对线粒体染色体显示刻度
chromosomes = hsM
# 刻度的间隔，设置为2000
spacing = 2000
# 是否显示刻度标签
show_label = yes
# 标签的半径，设置在染色体外侧半径+50像素的位置
label_radius = dims(ideogram,radius_outer) + 50p
# 标签是否平行于刻度线，这里设置为否
label_parallel = no
# 标签的大小
label_size = 30p
&lt;/tick&gt;
&lt;/ticks&gt;

复制<br>修改的地方有几点：<br>
<br>排除了一些指代不明的路径：最显然的就是 MitoGenes.centre.germline.conf 这个配置文件中的路径。里面指向了一个非常有趣的路径：/home/wwei1/Scripts/Circos/MitoGenes_codingGenes.txt。嗯，我很确定这一定是作者的私人电脑。
<br>添加了大量的注释，因为我不是很了解 Pl 语法，所以我只能这样才能看得懂。我觉得 pl 有点像 Latex，都是分片段式的写作。
<br>使用方法：<br>/mnt/e/Scientifc_software/circos-current/circos-0.69-9/bin/circos -conf /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/circos_allNUMTs.conf
复制<br>记得改路径！<br><br><br>这个脚本可以把生成出来的 mt.disc.sam.cluster.tsv 文件转成 circos 能够识别的格式。<br><br>QNAME	FLAG	RNAME	POS	MAPQ	CIGAR	RNEXT	PNEXT	TLEN	SEQ	QUAL	SM	RG	NM	BC	OC	ZX	ZY	SA	subCluster_No	Cluster_No	Cluster_ID	IndividualID	clusterID
HISEQ1:20:H9V1RADXX:2:2103:12132:43712	81	chr1	633911	28	148M	chrM	10226	0	AGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTCGCTCTAAGATT	CDDDDDDDEEDDDDDDDDDDDDDDDCDC									1	3	('chr1',)_633902_633918_MTboth_10226_10226	HG002_5X	chr1_3
HISEQ1:18:H8VC6ADXX:1:1215:7921:64214	177	chr11	49862122	70	148M	chrM	16373	0	CTTGTGGGCATGTGCTCATCTTTCTAGAGTAAATACCCAGTAATGGAATTGCTGTGCCATAGTGCACATTTCTGCTTGACATTGCTTTTTAAAAGAGTTACCTTAAGTGATTGTATAATTTTAGCCTAAATTATCACAAGCATTGTAT	DD									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:18:H8VC6ADXX:1:2103:3036:6488	177	chr11	49862289	70	148M	chrM	16095	0	CATTCTCGCTACATTTGAATTGTTAATCTGTTTTTCTTTAGCAATTCTAGCAAATGTGAAATTAGAATGTATTTAATGTAATTTATAGAGAACCGTTTGAATGAAACTAAGTTTTTACTGGAAATACAGCAATTTTTTTTTTTTCAGA	C&gt;500&lt;5(C									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:19:H8VDAADXX:2:2201:6106:14884	177	chr11	49862020	70	3S145M	chrM	16338	0	GAAATACTATGGATTGTTTATTCACTCTTCTGTTAGAAACCTGGACTTTGTCTAATATTTGGCTAATATAAACATGGCTGTTTTGAACAATATTGTACACGTCTTCTTGTGGGCATGTGCTCATCTTTCTAGAGTAAATACCCAGTAA	EEEDC									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:20:H9V1RADXX:1:1101:17382:26035	113	chr11	49862183	70	148M	chrM	16391	0	GTGCACATTTCTGCTTGACATTGCTTTTTAAAAGAGTTACCTTAAGTGATTGTATAATTTTAGCCTAAATTATCACAAGCATTGTATGAGGATTTCAGTTGCTCCACATTCTCGCTACATTTGAATTGTTAATCTGTTTTACTTTAGC										4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:18:H8VC6ADXX:1:1109:1327:84770	81	chr21	9676786	32	148M	chrM	12811	0	TATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGTGAAGTGTTTTCATGTGTGTGACTATTTCCGTGTTATAGCAGTGTGTCCGTATGCTGTGTGAAGTGTTTTCGTGTATATGACTATTTGCGTGTTACAGCAGTGT	CDDDDCACDDDDDDDDDDEDDDDDDDDDDDDDDDDDDDDDBCCCCDCDDDDDDDDDDDDDDDDEDDDDDDDDDDEEEEEEFFFFFFHHHGHIJIIHJIIIJJJJJJJJJJJJJJJJJJJJJJJIIJJJJJJJJJJHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:0	UQ:i:0	NM:i:0	MD:Z:148				5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:18:H8VC6ADXX:2:1104:13981:10006	97	chr21	9676333	28	148M	chrM	12706	0	GTGTTATAGCAGTGTGTCAGTGTGCTGCGTGACGTGTTTTCATGTGTATATGAGTATTTGTGTGTTATAGCCATGTGTCAGTGTGCTGTGTGTTTTGTTTATGTGACTATTTGCGTGTTATAGCAGTGTGTCAGTGTGCTGTGTGAAG	B									5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:19:H8VDAADXX:2:2203:3229:99021	97	chr21	9676197	30	148M	chrM	12389	0	TCATGTGTATATGACTATTTGCATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATGTATATGACTATTTGCGTGTTATGGCAG	CCCFFFFDHHHHHJJJJJJJJJJJJJJJJJJJJJIIIIJJJJHIIIJJJHIHIJJJJJJJGGGJIIHIIJJJJJJJJJJJIJJJJJJHHHHEHFFFFDEDCCCECEDDBDDDD&lt;ADDDDDCCDEEEEEDDDDEEEDDDDDDCDDCDDB	PG:Z:novoalign	AS:i:60	UQ:i:60	NM:i:2	MD:Z:106T36A4				5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:2:2104:5110:91237	97	chr21	9676218	32	148M	chrM	12746	0	CATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATGTATATGACTATTTGCGTGTTATAGCAGTGTGTCAGTGTGCTGCGTGAC										5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:2:2106:12761:29990	161	chr21	9676198	17	148M	chrM	12452	0	CATGTGTATATGACTATTTGCATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATTTATATGACTATTTGCGTGTTATAGCAGT	CCCFFFFDHHGBHGIGIJJIIIGHEJJGCEHJJJJIIGIJIAHHIJFI&gt;GCFHHBGGGCE=8BFHGICGHICHIIB									5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:1:2110:10981:37943	81	chr5	32338499	70	148M	chrM	14831	0	CTCACTACAACCTCCGCCTCCCAGGTTCAAGAGATTCTTCTGCCTCAGCCTCCCGAGTAGCTGGGATTACAGGCATGTGCCACTGTGCCCAGCTAATTTTGTATTTTTAGTAGAGATGGGGTTTCTCCACATTGGTCAGGCTGGTCTC	AECCDDDDB?DDDDDDDDDDDDDDDDDDDEDDDDDDDDDDDDDDBBBDDDBDDDDDDDDDDDDDEEEEEEEEFFFFFFHHHEJJJJJJIJJJJJJJJJJJJJIIJJJJJJIJJJJJJJJJJJJJJJJJIJJJIJJHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:0	UQ:i:0	NM:i:0	MD:Z:148				1	3	('chr5',)_32338282_32338638_MTboth_14831_14831	HG002_5X	chr5_3
HISEQ1:18:H8VC6ADXX:1:2114:15204:54396	113	chr9	129996	70	148M	chrM	6262	0	GCATATATGTTACTGTACTGAACAGTGTATGGAAATAATAACGCAATGGTAAGTATTTGCGTGACAGTAATTTTTCAGCTCCATCGTAATCTTATGCAGTCTGACTAAAACGCTGTTACGCGGTGCATGACTATACTTATTTTTAAAT	DEEEEDDDDDDDEDDEDDEDDCDDDDDDEEEEEEEEDDDDDDDDDDCDDEDDDDDDDDDDDDDEDDDDDDDDDDEEEEDFFFFHHHHJJJJIJJJJJJJJJIHGJJJJJHJJJJJIHGJJJJJJJJJJJIJJIHFHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:30	UQ:i:30	NM:i:1	MD:Z:118T29				2	2	('chr9',)_129555_129996_MTboth_6262_6296	HG002_5X	chr9_2
HISEQ1:18:H8VC6ADXX:2:2203:12359:8789	65	chr9	129555	70	148M	chrM	6296	0	AGCCAGTCACAGAAGGACAAATATTGCATGAATCCACTAATATCAGGTGTCTAAAACAGTGAAACTCATCAAATCAGAAAGTAAAACGGTGGTTACCAGGGGTGAGAGGGAGACGAAAATTGGGGGGGGCTGTTCAATGGGGATAATT	CCCFFFDFHHFGHJJJBHIJJJIJJJIJJJIJJJJGIIGIIJIJJJJBFGIIIIJJGJG?FHIJJJJJIJJIJGIJGJHGHAEHHFFFDACD									2	2	('chr9',)_129555_129996_MTboth_6262_6296	HG002_5X	chr9_2

复制<br><br>import pandas as pd

# Load the uploaded file
file_path = '/mnt/c/Users/victo/Desktop/HG002_5X.mt.disc.sam.cluster.tsv'
data = pd.read_csv(file_path, sep='\t')

# Extract the relevant column
cluster_id_col = data['Cluster_ID']

# Define a function to convert the Cluster_ID column into the desired format
def convert_to_circos_format(cluster_id):
    parts = cluster_id.split('_')
    hs1 = parts[0].replace("('", "").replace("',)", "")
    start1 = parts[1]
    end1 = parts[2]
    hsM = 'hsM'
    startM = parts[4]
    endM = parts[5]
    return f"{hs1} {start1} {end1} {hsM} {startM} {endM}"

# Apply the function to the Cluster_ID column
circos_format_data = cluster_id_col.apply(convert_to_circos_format)

# Create a DataFrame from the results
circos_format_df = circos_format_data.str.split(expand=True)
circos_format_df.columns = ['hs1', 'start1', 'end1', 'hsM', 'startM', 'endM']

# Remove the header and replace 'chr' with 'hs'
circos_format_df_no_header = circos_format_df.replace({'chr': 'hs'}, regex=True)

# Save the result to a file without the header
output_file_path = '/mnt/c/Users/victo/Desktop/circos_format_data_no_header.txt'
circos_format_df_no_header.to_csv(output_file_path, sep=' ', index=False, header=False)

print(f"File saved to {output_file_path}")

复制<br><br>hs1 633902 633918 hsM 10226 10226
hs11 49862020 49862289 hsM 16095 16391
hs14 49862020 49862289 hsM 16095 16391
hs13 49862020 49862289 hsM 16095 16391
hs11 49862020 49862289 hsM 16095 16391
hs12 49862020 49862289 hsM 16095 16391
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs3 9676197 9676786 hsM 12389 12811
hs21 9676197 9676786 hsM 12389 12811
hs20 9676197 9676786 hsM 12389 12811
hs21 9676197 9676786 hsM 12389 12811
hs5 32338282 32338638 hsM 14831 14831
hs9 129555 129996 hsM 6262 6296
hsy 129555 129996 hsM 6262 6296
复制]]></description><link>软件\其它生信软件\k-s\numt-detector.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/NUMT-detector.md</guid><pubDate>Sun, 28 Jul 2024 08:36:36 GMT</pubDate></item><item><title><![CDATA[NUMT-detector错误排查]]></title><description><![CDATA[ 
 <br>7个 Python 脚本<br>4个 shell 脚本<br>
<br>mtVariantCalling.sh
<br>nanopolish_methylationDetection.sh
<br>NUMTs_detection.sh
<br>VarDetection_fromDiscSplitReads.sh
<br>
<br>searchNumtCluster_fromDiscordantReads.py
<br>searchBreakpoint_fromblatoutputs.py
<br>
<br>enrichment_creatingRefgenome.py
<br>enrichment_simulation.py
<br>generateVariantTable.Human.py
<br>generateVariantTable.HumanChimp.py
<br>groupNumtCluster_fromMultipleSamples.py
<br>Turn_cluster_to_circos.py<br>6个 circos 脚本<br>
<br>circos_allNUMTs.conf
<br>ticks_NUMTs.conf
<br>MitoGenes.centre.germline.conf
<br>links_allBreakpoints.soma.conf
<br>karyotype_NUMTs.conf
<br>ideogram_NUMTs.conf
<br>NUMTs-detection-1.0压缩包<br>根据分析结果的 NUMTs 断点进行可视化<br>CIRCOS软件的使用<br>################################################################################
## 本脚本生成circos图
## 需要安装Circos
## Circos可以从 http://circos.ca/software/download/circos/ 下载
################################################################################

# 引入Circos的housekeeping配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/housekeeping.conf&gt;&gt;

# Circos的一般设置
anti_aliasing* = no  # 禁用抗锯齿
max_points_per_track* = 30000  # 每个轨道的最大点数

# 引入颜色、字体和模式的配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors_fonts_patterns.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.brewer.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.ucsc.conf&gt;&gt;

# 引入NUMTs的ideogram配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ideogram_NUMTs.conf&gt;&gt; 
# 引入NUMTs的刻度配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ticks_NUMTs.conf&gt;&gt;

# karyotype设置，使用人类karyotype文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/karyotype_NUMTs.conf&gt;&gt;
karyotype* = /mnt/e/Scientifc_software/circos-current/circos-0.69-9/data/karyotype/karyotype.human.hg38.txt


# 染色体显示和缩放设置
# chromosomes_display_default = no
# chromosomes_units   = 0.1  # 染色体单位
# chromosomes     = /hs[1-9XYM]/  # 要显示的染色体
# chromosomes_reverse = hsM  # 反转M染色体
# chromosomes_radius = hsM:1.0r  # M染色体的半径
# chromosomes_scale   = hsM=0.5r  # M染色体的缩放

# 图片设置
&lt;image&gt;
angle_offset* = -90  # 角度偏移
file*  = CircosPlot_allBreakpoint.germline.png # 输出文件名
svg*   = no  # 使用SVG格式
png* = yes # 使用PNG格式
radius* = 1500p  # 减少图像半径以提高速度

# 引入图片配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/image.conf&gt;&gt;
&lt;/image&gt;

# NUMTs链接设置
&lt;links&gt;
&lt;link&gt;
ribbon           = no  # 不使用带状显示
bezier_radius    = 0r  # 贝塞尔曲线半径
crest                = 0.50  # 贝塞尔曲线的峰值
bezier_radius_purity = 1  # 贝塞尔曲线半径纯度
file		= /mnt/c/Users/victo/Desktop/circos_format_data_no_header.txt # mtDNA和核基因组NUMTs之间的链接文件
radius		= 0.999r  # 链接的半径
thickness	= 10  # 链接的厚度

# 引入链接规则配置文件
&lt;rules&gt;
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/links_allBreakpoints.soma.conf&gt;&gt;
&lt;/rules&gt; 
&lt;/link&gt;
&lt;/links&gt;

# # 绘图设置
# &lt;plots&gt;
# # 引入中心的MitoGenes配置文件
# &lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/MitoGenes.centre.germline.conf&gt;&gt; weiwei的文件夹怎么可能找到？

# &lt;plot&gt;
# type            = tile  # 绘图类型为瓷砖
# file            = mtFragments_germline.txt  # mtFragments文件
# r1                      = 1.15r  # 外半径
# r0                      = 0.5r  # 内半径
# orientation             = out  # 方向为向外
# layers                  = 1  # 图层数
# margin                  = 0.1u  # 边距
# thickness               = 5  # 厚度
# padding                 = 0.1  # 内边距

# color                   = vdred  # 颜色为深红色
# layers_overflow         = grow  # 图层溢出时扩展

# stroke_thickness        = 1  # 描边厚度
# stroke_color            = white  # 描边颜色为白色

# &lt;/plot&gt;

# &lt;/plots&gt;

复制<br>运行 /home/luolintao/NUMTs-detection-1.0/scripts/NUMTs_detection.sh<br>Python 的语法太旧了
缩进问题<br>
路径引用不正确。
<br>Python 格式问题
字符串和数值的错误引用。
<br>shell 语法问题
cut 语法切出来的名字不对，加了很多莫名其妙的_。
<br>/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/scripts/searchBreakpoint_fromblatoutputs.py<br>未被使用的 split 文件
虽然定义了了该文件的路径，但是整个脚本中并没有传递这个参数。
<br>路径不明
有些 conf 指向软件默认安装目录，但有些不是<br>
有些 conf 指向作者的私人电脑，但是我没有找到这些文件
<br>多重定义
有些 conf 对于同一个变量进行了多次定义，导致软件报错。<br>
软件操作很繁琐，每次报错只会报一个错，排查起来过于浪费时间。
<br># chrM Genes
&lt;plot&gt;
type            = heatmap
file            = /home/wwei1/Scripts/Circos/MitoGenes_Dloop.txt
r1                      = 0.4r
r0                      = 0.25r
color   = pastel1-4-qual-1
&lt;/plot&gt;

&lt;plot&gt;
type            = heatmap
file            = /home/wwei1/Scripts/Circos/MitoGenes_codingGenes.txt
r1                      = 0.4r
r0                      = 0.25r
color   = pastel1-4-qual-2
&lt;/plot&gt;

复制<br><br>
<br>参考基因组：可以选用 GRCH37 或者 GRCH38 。

<br>参考基因组可以在 NCBI 下载。我在这里已经建立好了两个版本的索引，足以应对大多数情况。
<br>建立索引：利用 BWA 的 index。
<br>获得索引文件。


<br>下载 NUMTs-detection-1.0

<br>这个包可以在 Github 下载。但是正如作者所说，这些脚本已经不再维护。里面有很多语法已经被淘汰。地址： <a data-tooltip-position="top" aria-label="https://github.com/WeiWei060512/NUMTs-detection" rel="noopener nofollow" class="external-link" href="https://github.com/WeiWei060512/NUMTs-detection" target="_blank">WeiWei060512/NUMTs-detection: Detecting NUMTs from WGS (github.com)</a>
<br>解压至 Linux 系统。
<br>对里面的一些文件进行修改。


<br><br><br>修改目的：<br>
<br>无法使用模组，因为缺乏高性能计算环境，因此在这里注释掉。
<br>修改了输出的 split.bam。源代码最后一句删掉了相关文件，并且在该 shell 脚本的 Python 中也没有出现过这个文件。但是在另外一个 sh 中有用的，所以我建议最后还是不要删除。这个文件被指向了一个 wgs 的目录，这应该是一个错误。
<br>#! /bin/bash

################################################################################
## This script detects NUMTs from whole genome sequencing BAM files
## samtools, samblaster and blat need to be installed to run the pipeline
## samtools can be downloaded at http://www.htslib.org/download/
## samblaster can be downloaded at https://github.com/GregoryFaust/samblaster
## blat can be downloaded at http://hgdownload.soe.ucsc.edu/admin/exe/
################################################################################

## load modules on HPC
# module load samblaster/0.1.24
# module load samtools 
# module load Sambamba/0.6.6


INPUT_BAM=/home/luolintao/Reference_GRCH/INPUT_TEST/HG006_5X.bam # input WGS bam file
OUTPUT_DIR=/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT # output folder path
REF_GRCh38=/home/luolintao/Reference_GRCH/GRCh38_latest_genomic.fna # human reference genome

CLUSTER_SCRIPT='/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/scripts/searchNumtCluster_fromDiscordantReads.py'
BREAKPOINT_SCRIPT='/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/scripts/searchBreakpoint_fromblatoutputs.py'

SAMPLE_ID1="${INPUT_BAM##*/}"
SAMPLE_ID2=${SAMPLE_ID1%.bam}
OUTPUT="${OUTPUT_DIR}/${SAMPLE_ID2}"

INPUT_DISC="${OUTPUT}.mt.disc.sam"
# INPUT_SPLIT="${OUTPUT_wgs}.mt.split.sam" 我至今不能明白为什么这个前缀包含了一个wgs。可能只有上帝能明白。
INPUT_SPLIT="${OUTPUT}.mt.split.sam"

# 文件预处理
echo "正在使用samtools预处理文件!"
samtools view -@ 16 -m 10G -h -F 2 $INPUT_BAM | grep -e @ -e MT -e chrM | samtools sort -@ 16 -m 10G -n  | samtools view -h | samblaster --ignoreUnmated -e -d $INPUT_DISC -s $INPUT_SPLIT -o /dev/null
# 聚类查找
python3 $CLUSTER_SCRIPT ${SAMPLE_ID2} ${INPUT_BAM} ${INPUT_DISC}
echo "查找聚类已经完成，现在开始寻找断点!"


# 定义文件名
filelines=$(cat ${INPUT_DISC}.breakpointINPUT.tsv)


# 使用制表符作为分隔符读取文件的每一行，并将其字段分配给相应的变量
while IFS=$'\t' read -r sampleID cluster_no disFile splitFile wgsBAM chr start end; do
    # 去除 chr 字段中的多余字符（如括号和引号）
    chr=$(echo $chr | tr -d "(),'")    
    # 输出当前行的各个字段，方便调试
    echo "$sampleID $cluster_no $disFile $splitFile $wgsBAM $chr $start $end"    
    # 构建区域字符串，用于 samtools 提取指定区域
    REGION="${chr}:${start}-${end}"    
    # 构建输出文件的前缀路径
    OUTPUT="${OUTPUT_DIR}/${sampleID}_${chr}.${start}.${end}"    
    #使用 samtools 从 BAM 文件中提取指定区域的数据，并保存为 SAM 格式
    samtools view "${wgsBAM}" "${REGION}" &gt; "${OUTPUT}.sam"    
    # 使用 awk 过滤掉特定 CIGAR 值的行，并提取第1和第10列，将结果保存为 FASTA 格式
    awk '$6 !~ /150M|149M|148M|149S|148S/' "${OUTPUT}.sam" | cut -f1,10 &gt; "${OUTPUT}.fasta"
    # 使用 perl 脚本在每行前添加 '&gt;' 符号，符合 FASTA 格式要求
    perl -pi -e 's/^/&gt;/g' "${OUTPUT}.fasta"    
    # 使用 perl 脚本将制表符替换为换行符，符合 FASTA 格式要求
    perl -pi -e 's/\t/\n/g' "${OUTPUT}.fasta"    
    # 使用 BLAT 工具将 FASTA 文件比对到参考基因组，结果保存为 PSL 格式
    /home/luolintao/blat "${REF_GRCh38}" "${OUTPUT}.fasta" "${OUTPUT}.psl"    
    # 运行 Python 脚本处理 BLAT 生成的 PSL 文件，提取断点信息
    python3 "${BREAKPOINT_SCRIPT}" "${OUTPUT}.psl" "${sampleID}" "${chr}" "${start}" "${end}" "${OUTPUT}"
    # 删除临时生成的 FASTA 文件
    rm "${OUTPUT}.fasta"    
    # 删除临时生成的 SAM 文件
    rm "${OUTPUT}.sam"
done &lt;&lt;&lt; "$filelines"
复制<br><br>这个文件是上述 shell 需要调用的。但是里面存在一些问题：<br>
<br>不规范的缩进。
<br>不再被维护的 pandas 语法。<br>
为了不改变原有功能，我对部分语句进行了维护。使用的版本是 Python 3.9.15。<br>
修改之后的代码如下：
<br>#!/usr/bin/env python

################################################################################
## This script takes outputs from NUMT_detection.sh to generates NUMT clusters
################################################################################

import fileinput
import sys, os
import pandas as pd
import glob
import scipy.stats as stats
import numpy as np

#################### Extract cluster from mtDNA discordant sam files  #############################

def cluster(data, maxgap):
    data.sort()
    groups = [[data[0]]]
    for x in data[1:]:
        if abs(x - groups[-1][-1]) &lt;= maxgap:
            groups[-1].append(x)
        else:
            groups.append([x])
    return groups

sampleID, wgsBAM, input1 = sys.argv[1:]

sampleID = sampleID.replace(".mt.disc", "")

df0 = pd.read_csv(input1, names=['QNAME', 'FLAG', 'RNAME', 'POS', 'MAPQ', 'CIGAR', 'RNEXT', 'PNEXT', 'TLEN', 'SEQ', 'QUAL', 'SM', 'RG', 'NM', 'BC', 'OC', 'ZX', 'ZY', 'SA'], sep="\t", engine='python', comment="@")

df = df0[~df0.RNAME.str.contains("Un_|random|\.")]
df1 = df[(df["MAPQ"].astype(int) &gt; 0) &amp; ((df['RNAME'] == "MT") | (df['RNAME'] == "chrM") | (df['RNEXT'] == "MT") | (df['RNEXT'] == "chrM"))]

##### remove reads map to mtDNA #####
df3 = df1[~df1.RNAME.isin(['chrM', 'MT'])]

##### order by chromosome and pos #####
df3 = df3.sort_values(['RNAME', 'POS'])

##### look for the clutser by mapgap on pos #####
##### extract the clusters with number of elements no less than 5 #####

output1 = pd.DataFrame([])

df_chr = df3.groupby(['RNAME'])
for clusterID, myclusters in df_chr:
    myclusters['POS'] = myclusters['POS'].astype(int)
    sub_cluster = cluster(myclusters['POS'].tolist(), maxgap=500)
    for x in sub_cluster:
        if len(x) &gt;= 2:
            mycluster = x
            df_cluster = df3[df3.POS.isin(mycluster)]
            df_cluster_pairMT = df[df.QNAME.isin(df_cluster['QNAME'])]
            mt_cluster = cluster(df_cluster['PNEXT'].tolist(), maxgap=500)
            for y in mt_cluster:
                df_cluster_pairMT_out = df_cluster_pairMT[df_cluster_pairMT.PNEXT.isin(y)]
                df_cluster_pairMT_out = df_cluster_pairMT_out.assign(
                    subCluster_No=len(y),
                    Cluster_No=len(x),
                    Cluster_ID=f"{clusterID}_{min(df_cluster['POS'])}_{max(df_cluster['POS'])}_MTboth_{min(df_cluster_pairMT_out['PNEXT'])}_{max(df_cluster_pairMT_out['PNEXT'])}"
                )
                output = pd.concat([pd.DataFrame(), df_cluster_pairMT_out], ignore_index=True)

            output1 = pd.concat([output1, output])
            output1["IndividualID"] = sampleID
            if len(output1) != 0:
                output1['clusterID'] = output1['RNAME'].astype(str) + "_" + output1['Cluster_No'].astype(str)
                output2 = output1.groupby(['IndividualID', 'Cluster_ID', 'Cluster_No', 'subCluster_No']).size().to_frame('size').reset_index()
            else:
                continue

output1.to_csv(input1 + '.cluster.tsv', sep='\t', header=True, index=False)
output2.to_csv(input1 + '.cluster.summary.tsv', sep='\t', header=False, index=True)

##### generate cluster range for defining the breakpoints ######

output2['disFile'] = input1
output2['splitFile'] = input1.replace('disc', 'split')
output2['wgsBAM'] = wgsBAM
df_pos = pd.DataFrame(output2['Cluster_ID'].str.split('_').tolist(), columns=['chr', 'start', 'end', 'chrM', 'mtstart', 'mtend'])
del output2['Cluster_ID']
del output2['subCluster_No']
del output2['size']
output3 = pd.concat([output2, df_pos[['chr', 'start', 'end']]], axis=1)
output3 = output3.drop_duplicates(['chr', 'start', 'end'])
output3['start'] = output3['start'].astype(int) - 500
output3['end'] = output3['end'].astype(int) + 500 + 150
output3['Cluster_No'] = output3['Cluster_No'].astype(int)

##### output to file #####
output3.to_csv(input1 + '.breakpointINPUT.tsv', sep='\t', header=False, index=False)

复制<br><br>也是上述 shell 需要调用的。但是里面存在很大的问题：<br>
<br>使用了旧版本的 pandas 语法。令人遗憾的，新版本的语法似乎不能很好读取 blat 生成的 psl 文件，标题行存在很大的问题。为了不让原本的功能丧失，我只能够用最愚蠢的办法将数值转为字符串。尽管这会报错，但是并不影响最终的结果。
<br>在 GRCH38 版本中，染色体的名字是以 NCXXXXXX 命名的，不再使用了 chr。因此，需要重新对其进行映射。
<br>可以根据自己的需求就筛选过滤措施，这对于低深度的测序也许有一定的作用。<br>
修改之后的脚本：
<br>#!/usr/bin/env python
################################################################################
## This script takes outputs from searchNumtCluster_fromDiscordantReads.py and NUMT_detection.sh 
## to look for NUMT breakpoints
################################################################################

import sys, os
import pandas as pd

def classify_breakpoint(row, type='nu'):
    mismatchLEN = 3 # 默认是3 请在这里选择过滤措施，选择更大的数量会得到更多的结果，但是可能会并不会特别精确。
    readLEN = 150 # 默认是150 请在这里选择过滤措施，选择更长的长度会得到更多的结果，但是可能会并不会特别精确。
    if type == 'nu':
        if row['strand'] == "+" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "nu_Tstart_Bright"
        elif row['strand'] == "+" and row['Qstart'] &lt;= mismatchLEN:
            return "nu_Tend_Bleft"
        elif row['strand'] == "-":
            return "nu_NegStrand"
        else:
            return "nu_useLess"
    else:
        if row['strand'] == "+" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "mt_Tstart"
        elif row['strand'] == "+" and row['Qstart'] &lt;= mismatchLEN:
            return "mt_Tend"
        elif row['strand'] == "-" and row['Qend'] &gt;= readLEN - mismatchLEN:
            return "mt_Tend"
        elif row['strand'] == "-" and row['Qstart'] &lt;= mismatchLEN:
            return "mt_Tstart"
        else:
            return "mt_useLess"

# 从命令行读取输入参数
INPUT_PSL, SAMPLEID, CHR, START, END, OUTPUT = sys.argv[1:]
START, END = int(START), int(END)

# 读取数据并预处理
psl_columns = [
    "match", "misMatch", "repMatch", "Ns", "QgapCount", "QgapBases", "TgapCount", "TgapBases", "strand",
    "Qname", "Qsize", "Qstart", "Qend", "Tname", "Tsize", "Tstart", "Tend", "blockCount", "blockSizes",
    "qStarts", "tStarts"
]
df = pd.read_csv(INPUT_PSL, skiprows=5, sep="\t", names=psl_columns)
df['matchLEN'] = df['Tend'] - df['Tstart']

# 过滤数据，过滤措施
filtered_df = df[(df['matchLEN'] &lt; 170) &amp; (df['misMatch'] &lt;= 7)] # 默认是df[(df['matchLEN'] &lt; 140) &amp; (df['misMatch'] &lt;= 3)]请在这里选择过滤措施，选择更大的数量会得到更多的结果，但是可能会并不会特别精确。
filtered_df = filtered_df[(filtered_df['Tend'] &gt;= 140) | (filtered_df['Tend'] &lt;= 10)] # 默认是[(filtered_df['Tend'] &gt;= 147) | (filtered_df['Tend'] &lt;= 3)]请在这里选择过滤措施，选择更长的长度会得到更多的结果，但是可能会并不会特别精确。

# Tname映射
tname_mapping = {
    'NC_012920.1': 'MT',
    'NC_000001.11': 'chr1',
    'NC_000002.12': 'chr2',
    'NC_000003.12': 'chr3',
    'NC_000004.12': 'chr4',
    'NC_000005.10': 'chr5',
    'NC_000006.12': 'chr6',
    'NC_000007.14': 'chr7',
    'NC_000008.11': 'chr8',
    'NC_000009.12': 'chr9',
    'NC_000010.11': 'chr10',
    'NC_000011.10': 'chr11',
    'NC_000012.12': 'chr12',
    'NC_000013.11': 'chr13',
    'NC_000014.9': 'chr14',
    'NC_000015.10': 'chr15',
    'NC_000016.10': 'chr16',
    'NC_000017.11': 'chr17',
    'NC_000018.10': 'chr18',
    'NC_000019.10': 'chr19',
    'NC_000020.11': 'chr20',
    'NC_000021.9': 'chr21',
    'NC_000022.11': 'chr22',
    'NC_000023.11': 'chrX',
    'NC_000024.10': 'chrY'
}

filtered_df['Tname_mapped'] = filtered_df['Tname'].map(tname_mapping).fillna(filtered_df['Tname'])

# 分离线粒体和核序列
mt_df = filtered_df.loc[filtered_df['Tname_mapped'] == 'MT'].copy()
nu_df = filtered_df.loc[(filtered_df['Tname_mapped'] == CHR) &amp; (filtered_df['Tstart'] &gt; START) &amp; (filtered_df['Tend'] &lt; END)].copy()

if not nu_df.empty:
    # 应用分类函数
    nu_df.loc[:, 'pointGroup'] = nu_df.apply(classify_breakpoint, axis=1, type='nu')
    if not mt_df.empty:
        mt_df.loc[:, 'pointGroup'] = mt_df.apply(classify_breakpoint, axis=1, type='mt')

    nu_df.loc[:, 'Group'] = nu_df['pointGroup'].str.replace(r'_T.*B', '', regex=True)
    nu_df.loc[:, 'chr'] = CHR
    if not mt_df.empty:
        mt_df.loc[:, 'chr'] = 'chrM'

    def group_and_count(df, by_cols):
        return df.groupby(by_cols).size().reset_index(name="readsCount")

    # 核DNA断点
    nu_left = group_and_count(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft'], ['pointGroup', 'Group', 'chr', 'Tend', 'strand'])
    nu_right = group_and_count(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright'], ['pointGroup', 'Group', 'chr', 'Tstart', 'strand'])
    nu_both = pd.concat([nu_left, nu_right])

    # 同时映射到线粒体的核DNA断点
    nu_mt = nu_df.loc[nu_df['Qname'].isin(mt_df['Qname'])]
    nu_mt_left = group_and_count(nu_mt.loc[nu_mt['pointGroup'] == 'nu_Tend_Bleft'], ['pointGroup', 'Group', 'chr', 'Tend', 'strand'])
    nu_mt_right = group_and_count(nu_mt.loc[nu_mt['pointGroup'] == 'nu_Tstart_Bright'], ['pointGroup', 'Group', 'chr', 'Tstart', 'strand'])
    nu_mt_both = pd.concat([nu_mt_left, nu_mt_right])

    # 线粒体断点
    if not mt_df.empty:
        mt_tend = group_and_count(mt_df.loc[mt_df['pointGroup'] == 'mt_Tend'], ['pointGroup', 'chr', 'Tend', 'strand'])
        mt_tstart = group_and_count(mt_df.loc[mt_df['pointGroup'] == 'mt_Tstart'], ['pointGroup', 'chr', 'Tstart', 'strand'])
        mt_both = pd.concat([mt_tend, mt_tstart])
        mt_both['Group'] = 'UKn'

        # 同时映射到核DNA的线粒体断点
        mt_conf = pd.concat([
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tstart')], ['pointGroup', 'chr', 'Tstart', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tend_Bleft', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tend')], ['pointGroup', 'chr', 'Tend', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tstart')], ['pointGroup', 'chr', 'Tstart', 'strand']),
            group_and_count(mt_df.loc[(mt_df['Qname'].isin(nu_df.loc[nu_df['pointGroup'] == 'nu_Tstart_Bright', 'Qname'])) &amp; (mt_df['pointGroup'] == 'mt_Tend')], ['pointGroup', 'chr', 'Tend', 'strand'])
        ])

        # 捕获并忽略异常
        try:
            mt_conf['Group'] = mt_conf.apply(lambda row: 'mtLeft' if 'left' in row['pointGroup'] else 'mtRight', axis=1)
        except ValueError as e:
            print(f"尽管这里出现了一个问题: {e}, 但是我们并不打算解决它，因为它并不影响最终的结果。")

        all_breakpoints = pd.concat([nu_both, mt_both])
        mt_conf['sampleID'] = SAMPLEID
        mt_conf['Tstart'] = mt_conf['Tstart'].fillna(-1).astype(int)
        mt_conf['Tend'] = mt_conf['Tend'].fillna(-1).astype(int)
    else:
        all_breakpoints = nu_both
        mt_conf = pd.DataFrame()

    # 输出结果
    all_breakpoints['sampleID'] = SAMPLEID
    all_breakpoints['Tstart'] = all_breakpoints['Tstart'].fillna(-1).astype(int)
    all_breakpoints['Tend'] = all_breakpoints['Tend'].fillna(-1).astype(int)

    confident_breakpoints = pd.concat([nu_mt_both, mt_conf])
    confident_breakpoints['sampleID'] = SAMPLEID
    confident_breakpoints['Tstart'] = confident_breakpoints['Tstart'].fillna(-1).astype(int)
    confident_breakpoints['Tend'] = confident_breakpoints['Tend'].fillna(-1).astype(int)

    # 写入文件
    confident_breakpoints.to_csv(OUTPUT + '.Breakpoints.tsv', sep='\t', header=False, index=False)
else:
    print("过滤后没有发现核序列。")

复制<br><br><br>该文件包含断点信息。每行表示一个断点，通常包括以下列：<br>
<br>pointGroup: 断点组
<br>Group: 断点类别
<br>chr: 染色体
<br>Tend: 结束位置
<br>strand: 链方向
<br>readsCount: 读取计数
<br>Tstart: 起始位置
<br>sampleID: 样本ID
<br>示例数据：<br>nu_Tend_Bleft	nuleft	chr1	634078	+	1	-1	HG001
nu_Tstart_Bright	nuright	chr1	-1	+	1	633739	HG001
nu_Tstart_Bright	nuright	chr1	-1	+	1	633939	HG001
复制<br><br>该文件包含每个找到的断点的信息。每行表示一个断点区域，通常包括以下列：<br>
<br>sampleID: 样本ID
<br>cluster_no: 聚类编号
<br>disFile: 不一致的文件路径
<br>splitFile: 分裂的文件路径
<br>wgsBAM: WGS BAM文件路径
<br>chr: 染色体编号
<br>start: 起始位置
<br>end: 结束位置
<br>HG001	3	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr1',)	633396	634563
HG001	2	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr11',)	49861375	49863007
HG001	3	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.disc.sam	/home/luolintao/NUMTs-detection-1.0/NUMTs-detection-1.0/RESULT/HG001.mt.split.sam	/home/luolintao/Reference_GRCH/INPUT_TEST/HG001.bam	('chr5',)	32337638	32339245

复制<br><br>该文件包含聚类的摘要信息。每行表示一个聚类，通常包括以下列：<br>
<br>IndividualID: 样本ID
<br>Cluster_ID: 聚类ID
<br>Cluster_No: 聚类编号
<br>subCluster_No: 子聚类编号
<br>size: 聚类大小
<br>0	HG001	('chr1',)_633896_633913_MTboth_9735_9872	3	3	3
1	HG001	('chr11',)_49861875_49862357_MTboth_16089_16340	2	2	2
2	HG001	('chr5',)_32338138_32338595_MTboth_14835_14835	3	1	1
复制<br>在这里，需要具体解释一下：<br>
前三列很好理解。但是针对后面：<br>
<br>
第四列*(Cluster_No)：这是主聚类中读段的总数。这些读段是基于它们在染色体上的位置（POS）进行聚类的，聚类的条件是相邻读段之间的距离不超过 500 个碱基对。检测出的满足第三列条件的 NUMT 的数量。

<br>
第五列和第六列表示在这个聚类中的子聚类数目，它们是对这些读段进一步聚类后满足特定条件的读段数目。根据代码的逻辑，第五列（subCluster_No）和第六列（subCluster_No）的值在通常情况下会相同。(subCluster_No)：这是子聚类中读数的总数。这些读数是基于它们在配对读数的位置（PNEXT）进行进一步聚类的，聚类的条件同样是相邻读数之间的距离不超过 500 个碱基对。<br>
NUMTs 是线粒体 DNA 插入到核基因组中的片段，通常我们预期插入到核基因组中的 NUMT 片段会比它们的原始线粒体序列短或相等。然而，你的示例中出现了相反的情况，这确实需要解释。在你的例子中，核基因组中的序列长度（444 bp）大于线粒体中的序列长度（190 bp）。有几种可能的解释：

<br>
插入事件和重排：

<br>NUMTs 插入到核基因组中时，可能会发生重排或扩展，从而导致插入片段在核基因组中的长度增加。
<br>核基因组中的插入片段可能包含了其他序列，这些序列可能是插入事件导致的。


<br>
重复序列：

<br>核基因组中可能包含重复序列，这些重复序列与线粒体 DNA 部分相同，但总体长度较长。
<br>这些重复序列可能是由于插入事件或其他基因组重排造成的。


<br>
比对错误或读数拼接：

<br>在比对过程中，可能会出现错误，导致比对到核基因组的读数长度增加。
<br>读数拼接也可能会引入额外的序列，从而增加比对长度。


<br><br>该文件包含聚类的详细信息。每行表示一个聚类的具体信息，通常包括以下列：<br>
<br>QNAME: 序列名称
<br>FLAG: 标志
<br>RNAME: 参考序列名称
<br>POS: 位置
<br>MAPQ: 比对质量
<br>CIGAR: 比对描述
<br>RNEXT: 下一个参考名称
<br>PNEXT: 下一个位置
<br>TLEN: 模板长度
<br>SEQ: 序列
<br>QUAL: 质量
<br>SM: 样本
<br>RG: 读取组
<br>NM: 不匹配数
<br>BC: 条形码
<br>OC: 原始 CIGAR
<br>ZX: 自定义标记1
<br>ZY: 自定义标记2
<br>SA: 辅助比对
<br>subCluster_No: 子聚类编号
<br>Cluster_No: 聚类编号
<br>Cluster_ID: 聚类ID
<br>IndividualID: 样本ID
<br>clusterID: 聚类ID
<br>QNAME	FLAG	RNAME	POS	MAPQ	CIGAR	RNEXT	PNEXT	TLEN	SEQ	QUAL	SM	RG	NM	BC	OC	ZX	ZY	SA	subCluster_No	Cluster_No	Cluster_ID	IndividualID	clusterID
HISEQ1:11:H8GV6ADXX:1:2212:16190:45635	97	chr1	633900	32	148M	chrM	9735	0	TCTCTTATACTAGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTC	C									3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HISEQ1:17:H947YADXX:2:2206:7902:6711	97	chr1	633913	32	148M	chrM	9872	0	TATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTCGCTCTAAGATTAA										3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HWI-D00360:5:H814YADXX:2:2107:18825:98678	161	chr1	633896	20	148M	chrM	9818	0	CTGATCTCTTATACTAGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGCGCGCAGTGATTATAGGC	&lt;									3	3	('chr1',)_633896_633913_MTboth_9735_9872	HG001	chr1_3
HWI-D00360:5:H814YADXX:2:1106:18751:35433	113	chr11	49862357	70	148M	chrM	16089	0	GTATTTAATGTAATTTATAGAGAACCGTTTGAATGAAACTAAGTTTTTACTGGAAATACAGCAATTTTTTTTTTTTCAGAATATGCTTATAGGTGTGGAATTGCAGAGGCTGTTGGTCTTCCAAGTATTCCTGTTCATCCAATTGGAT	CEEDDDEECDDDDDDEDEDDCDB									2	2	('chr11',)_49861875_49862357_MTboth_16089_16340	HG001	chr11_2
HWI-D00360:6:H81VLADXX:1:2211:17678:67136	65	chr11	49861875	70	143M5S	chrM	16340	0	GGAAAGGAAAAGAAAAGAAAAGTAATGATTAGATTTATTTTTAAAGTCTTGCTTATTACAATGATGGAAAATTTGAGATTTTCCATCATATTGTATGTATTAGTTTTTTTCTTTTGTTGATTGAGCAGCATTCCATTGTGGGAAAATA	CCCFFFFFHHHHHJIIJJJJJJHHJIJJJJJJJJJJIIJJJJJJJJHIJJJJJJJJJJJJJJJJJJJJJJJJJJJJJHHHHHHHFFFFFFFEEEEEEEFFFDEDDEDDDDDDDDDDDDDDDDDDDDDDDDDDDEEEEDEDDDDDDDDD	PG:Z:novoalign	AS:i:90	UQ:i:90	NM:i:1	MD:Z:22A120				2	2	('chr11',)_49861875_49862357_MTboth_16089_16340	HG001	chr11_2
HISEQ1:17:H947YADXX:2:1209:10750:98109	145	chr5	32338595	70	148M	chrM	14835	0	TTTTGTATTTTTAGTAGAGATGGGGTTTCTCCACATTGGTCAGGCTGGTCTCGAACTCCCAGCCTCAGGTGATCTGCCTGTCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATTGCACCCAGCTGAGTAAATCAGGTTTT	BDDCC									1	3	('chr5',)_32338138_32338595_MTboth_14835_14835	HG001	chr5_3

复制<br><br><br>我真的是服了 circos 这个软件了。他很强大，的确，也很弱智。一点也不智能。真不敢相信在2024年我还能见到如同 WindowsXP 一般的软件。转念一想，科研嘛，大概都是这样的。不然人人都是科学家。<br><br><a data-tooltip-position="top" aria-label="https://circos.ca/software/download/" rel="noopener nofollow" class="external-link" href="https://circos.ca/software/download/" target="_blank">Download // CIRCOS Circular Genome Data Visualization</a><br>
将压缩包解压到 Linux 环境。其实这个软件 win 也能运行，但是不太方便。<br>运行 circos 需要一些环境配置，但是这些都不太难。官网查一查就行了，问问呢 ChatGPT 也不会出错。<br><br>修改代码如下：<br>################################################################################
## 本脚本生成circos图
## 需要安装Circos
## Circos可以从 http://circos.ca/software/download/circos/ 下载
################################################################################

# 引入Circos的housekeeping配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/housekeeping.conf&gt;&gt;

# Circos的一般设置
anti_aliasing* = no  # 禁用抗锯齿
max_points_per_track* = 30000  # 每个轨道的最大点数

# 引入颜色、字体和模式的配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors_fonts_patterns.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.brewer.conf&gt;&gt;
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/colors.ucsc.conf&gt;&gt;

# 引入NUMTs的ideogram配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ideogram_NUMTs.conf&gt;&gt; 
# 引入NUMTs的刻度配置文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/ticks_NUMTs.conf&gt;&gt;

# karyotype设置，使用人类karyotype文件
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/karyotype_NUMTs.conf&gt;&gt;
karyotype* = /mnt/e/Scientifc_software/circos-current/circos-0.69-9/data/karyotype/karyotype.human.hg38.txt


# 染色体显示和缩放设置
# chromosomes_display_default = no
# chromosomes_units   = 0.1  # 染色体单位
# chromosomes     = /hs[1-9XYM]/  # 要显示的染色体
# chromosomes_reverse = hsM  # 反转M染色体
# chromosomes_radius = hsM:1.0r  # M染色体的半径
# chromosomes_scale   = hsM=0.5r  # M染色体的缩放

# 图片设置
&lt;image&gt;
angle_offset* = -90  # 角度偏移
file*  = CircosPlot_allBreakpoint.germline.png # 输出文件名
svg*   = no  # 使用SVG格式
png* = yes # 使用PNG格式
radius* = 1500p  # 减少图像半径以提高速度

# 引入图片配置文件
&lt;&lt;include /mnt/e/Scientifc_software/circos-current/circos-0.69-9/etc/image.conf&gt;&gt;
&lt;/image&gt;

# NUMTs链接设置
&lt;links&gt;
&lt;link&gt;
ribbon           = no  # 不使用带状显示
bezier_radius    = 0r  # 贝塞尔曲线半径
crest                = 0.50  # 贝塞尔曲线的峰值
bezier_radius_purity = 1  # 贝塞尔曲线半径纯度
file		= /mnt/c/Users/victo/Desktop/circos_format_data_no_header.txt # mtDNA和核基因组NUMTs之间的链接文件
radius		= 0.999r  # 链接的半径
thickness	= 10  # 链接的厚度

# 引入链接规则配置文件
&lt;rules&gt;
&lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/links_allBreakpoints.soma.conf&gt;&gt;
&lt;/rules&gt;
&lt;/link&gt;
&lt;/links&gt;

# # 绘图设置
# &lt;plots&gt;
# # 引入中心的MitoGenes配置文件
# &lt;&lt;include /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/confs/MitoGenes.centre.germline.conf&gt;&gt; weiwei的文件夹怎么可能找到？

# &lt;plot&gt;
# type            = tile  # 绘图类型为瓷砖
# file            = mtFragments_germline.txt  # mtFragments文件
# r1                      = 1.15r  # 外半径
# r0                      = 0.5r  # 内半径
# orientation             = out  # 方向为向外
# layers                  = 1  # 图层数
# margin                  = 0.1u  # 边距
# thickness               = 5  # 厚度
# padding                 = 0.1  # 内边距

# color                   = vdred  # 颜色为深红色
# layers_overflow         = grow  # 图层溢出时扩展

# stroke_thickness        = 1  # 描边厚度
# stroke_color            = white  # 描边颜色为白色

# &lt;/plot&gt;

# &lt;/plots&gt;

复制<br>注意
这里面的所有的路径一定要修改成自己的路径。推荐使用绝对路径。
<br><br>&lt;ideogram&gt;

&lt;spacing&gt;
# 默认间距设置为染色体半径的0.002倍
default = 0.002r

# 在边缘处是否显示轴断裂
axis_break_at_edge = yes

# 是否显示轴断裂
axis_break = yes

# 设置轴断裂的样式为2
axis_break_style = 2

# 断裂样式1的定义
&lt;break_style 1&gt;
stroke_color = black      # 描边颜色为黑色
fill_color = blue         # 填充颜色为蓝色
thickness = 0.25r         # 厚度为0.25倍染色体半径
stroke_thickness = 2p     # 描边厚度为2像素
&lt;/break_style&gt;

# 断裂样式2的定义
&lt;break_style 2&gt;
stroke_color = black      # 描边颜色为黑色
stroke_thickness = 5p     # 描边厚度为5像素
thickness = 2r            # 厚度为2倍染色体半径
&lt;/break_style&gt;

# 设置染色体hs1和hsM之间的间距为4倍染色体半径
&lt;pairwise hs1 hsM&gt;
spacing = 4r
&lt;/pairwise&gt;

# 设置染色体hsY和hsM之间的间距为4倍染色体半径
&lt;pairwise hsY hsM&gt;
spacing = 4r
&lt;/pairwise&gt;

&lt;/spacing&gt;

# Ideogram位置，填充和轮廓
radius = 0.85r               # 半径设置为0.85倍染色体半径
thickness = 80p              # 厚度为20像素
fill = yes                   # 是否填充
stroke_color = dgrey         # 描边颜色为深灰色
stroke_thickness = 0p        # 描边厚度为2像素

#label.conf
show_label = yes             # 是否显示标签
label_font = default         # 标签字体为默认字体
label_radius = 1.075r        # 标签半径为1.075倍染色体半径
label_size = 30              # 标签大小为30
label_parallel = yes         # 标签是否平行于染色体

##bands.conf
show_bands = yes             # 是否显示染色体带
fill_bands = yes             # 是否填充染色体带
band_transparency = 4        # 染色体带透明度

radius* = 0.85r              # 半径设置为0.85倍染色体半径

&lt;/ideogram&gt;

复制<br><br>karyotype = /mnt/e/Scientifc_software/circos-current/circos-0.69-9/data/karyotype/karyotype.human.hg38.txt # 指定karyotype文件的路径，该文件定义了染色体的基本信息（如名称、长度、带）。
chromosomes_display_default = no # 禁用默认显示所有染色体，只显示在chromosomes参数中明确指定的染色体。
chromosomes_units   = 0.1 # 设置染色体的单位长度，以百万碱基对（Mb）为单位（这里是0.1 Mb）。
chromosomes     = /hs[1-9XYM]/ # 通过正则表达式指定要显示的染色体，这里是人类的染色体1-9，X，Y和线粒体M。
chromosomes_reverse = hsM # 反转显示指定的染色体，这里是线粒体M。
# 在这里修改染色体的颜色
chromosomes_color   = hs1=black,hs2=orange,hs3=yellow,hs4=green,hs5=blue,hs6=dpurple,hs7=red,hs8=orange,hs9=yellow,hs10=green,hs11=blue,hs12=dpurple,hs13=red,hs14=orange,hs15=yellow,hs16=green,hs17=blue,hs18=dpurple,hs19=red,hs20=orange,hs21=yellow,hs22=green,hsX=blue,hsY=vdblue,hsM=vlblue
#chromosomes_color  = hsM=vlblue
chromosomes_radius = hsM:1r # 为线粒体染色体指定显示半径，这里是1.05倍的默认半径。
chromosomes_scale   = hsM=0.5r # 为线粒体染色体指定显示比例尺，这里是0.5倍的默认比例尺。

复制<br><br>&lt;rule&gt;
# 条件：在染色体hs1和线粒体染色体hsM之间
condition = between(hs1,hsM)
# 颜色：chr1（应在颜色定义文件中定义）
color = black
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs2和线粒体染色体hsM之间
condition = between(hs2,hsM)
# 颜色：chr2（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs3和线粒体染色体hsM之间
condition = between(hs3,hsM)
# 颜色：chr3（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs4和线粒体染色体hsM之间
condition = between(hs4,hsM)
# 颜色：chr4（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs5和线粒体染色体hsM之间
condition = between(hs5,hsM)
# 颜色：chr5（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs6和线粒体染色体hsM之间
condition = between(hs6,hsM)
# 颜色：chr6（应在颜色定义文件中定义）
color = cdpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs7和线粒体染色体hsM之间
condition = between(hs7,hsM)
# 颜色：chr7（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs8和线粒体染色体hsM之间
condition = between(hs8,hsM)
# 颜色：chr8（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs9和线粒体染色体hsM之间
condition = between(hs9,hsM)
# 颜色：chr9（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs10和线粒体染色体hsM之间
condition = between(hs10,hsM)
# 颜色：chr10（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs11和线粒体染色体hsM之间
condition = between(hs11,hsM)
# 颜色：chr11（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs12和线粒体染色体hsM之间
condition = between(hs12,hsM)
# 颜色：chr12（应在颜色定义文件中定义）
color = dpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs13和线粒体染色体hsM之间
condition = between(hs13,hsM)
# 颜色：chr13（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs14和线粒体染色体hsM之间
condition = between(hs14,hsM)
# 颜色：chr14（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs15和线粒体染色体hsM之间
condition = between(hs15,hsM)
# 颜色：chr15（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs16和线粒体染色体hsM之间
condition = between(hs16,hsM)
# 颜色：chr16（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs17和线粒体染色体hsM之间
condition = between(hs17,hsM)
# 颜色：chr17（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs18和线粒体染色体hsM之间
condition = between(hs18,hsM)
# 颜色：chr18（应在颜色定义文件中定义）
color = dpurple
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs19和线粒体染色体hsM之间
condition = between(hs19,hsM)
# 颜色：chr19（应在颜色定义文件中定义）
color = red
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs20和线粒体染色体hsM之间
condition = between(hs20,hsM)
# 颜色：chr20（应在颜色定义文件中定义）
color = orange
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs21和线粒体染色体hsM之间
condition = between(hs21,hsM)
# 颜色：chr21（应在颜色定义文件中定义）
color = yellow
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hs22和线粒体染色体hsM之间
condition = between(hs22,hsM)
# 颜色：chr22（应在颜色定义文件中定义）
color = green
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hsX和线粒体染色体hsM之间
condition = between(hsX,hsM)
# 颜色：chrx（应在颜色定义文件中定义）
color = blue
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

&lt;rule&gt;
# 条件：在染色体hsY和线粒体染色体hsM之间
condition = between(hsY,hsM)
# 颜色：chry（应在颜色定义文件中定义）
color = vdblue
# 厚度：1像素
# thickness = 1
# z轴坐标：0
z = 0
# 半径：0.998倍的默认半径
radius1 = 0.998r
&lt;/rule&gt;

复制<br><br># 是否显示刻度
show_ticks = yes
# 是否显示刻度标签
show_tick_labels = yes
&lt;ticks&gt;
# 刻度的半径，设置在染色体外侧半径+10像素的位置
radius = dims(ideogram,radius_outer) + 10p
# 刻度线的厚度
thickness = 3p
# 刻度线的方向，设置为向外
orientation = out
# 刻度线的颜色
color = black
# 刻度标签的大小
size = 12p
# 标签的偏移量
label_offset = 5p
# 标签的格式，这里设置为显示整数
format = %d
&lt;tick&gt;
# 默认情况下不显示刻度
chromosomes_display_default = no
# 仅对线粒体染色体显示刻度
chromosomes = hsM
# 刻度的间隔，设置为2000
spacing = 2000
# 是否显示刻度标签
show_label = yes
# 标签的半径，设置在染色体外侧半径+50像素的位置
label_radius = dims(ideogram,radius_outer) + 50p
# 标签是否平行于刻度线，这里设置为否
label_parallel = no
# 标签的大小
label_size = 30p
&lt;/tick&gt;
&lt;/ticks&gt;

复制<br>修改的地方有几点：<br>
<br>排除了一些指代不明的路径：最显然的就是 MitoGenes.centre.germline.conf 这个配置文件中的路径。里面指向了一个非常有趣的路径：/home/wwei1/Scripts/Circos/MitoGenes_codingGenes.txt。嗯，我很确定这一定是作者的私人电脑。
<br>添加了大量的注释，因为我不是很了解 Pl 语法，所以我只能这样才能看得懂。我觉得 pl 有点像 Latex，都是分片段式的写作。
<br>使用方法：<br>/mnt/e/Scientifc_software/circos-current/circos-0.69-9/bin/circos -conf /mnt/e/Scientifc_software/NUMTs-detection-1.0/scripts/circos_allNUMTs.conf
复制<br>记得改路径！<br><br><br>这个脚本可以把生成出来的 mt.disc.sam.cluster.tsv 文件转成 circos 能够识别的格式。<br><br>QNAME	FLAG	RNAME	POS	MAPQ	CIGAR	RNEXT	PNEXT	TLEN	SEQ	QUAL	SM	RG	NM	BC	OC	ZX	ZY	SA	subCluster_No	Cluster_No	Cluster_ID	IndividualID	clusterID
HISEQ1:20:H9V1RADXX:2:2103:12132:43712	81	chr1	633911	28	148M	chrM	10226	0	AGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCGCAGTGATTATAGGCTTTCGCTCTAAGATT	CDDDDDDDEEDDDDDDDDDDDDDDDCDC									1	3	('chr1',)_633902_633918_MTboth_10226_10226	HG002_5X	chr1_3
HISEQ1:18:H8VC6ADXX:1:1215:7921:64214	177	chr11	49862122	70	148M	chrM	16373	0	CTTGTGGGCATGTGCTCATCTTTCTAGAGTAAATACCCAGTAATGGAATTGCTGTGCCATAGTGCACATTTCTGCTTGACATTGCTTTTTAAAAGAGTTACCTTAAGTGATTGTATAATTTTAGCCTAAATTATCACAAGCATTGTAT	DD									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:18:H8VC6ADXX:1:2103:3036:6488	177	chr11	49862289	70	148M	chrM	16095	0	CATTCTCGCTACATTTGAATTGTTAATCTGTTTTTCTTTAGCAATTCTAGCAAATGTGAAATTAGAATGTATTTAATGTAATTTATAGAGAACCGTTTGAATGAAACTAAGTTTTTACTGGAAATACAGCAATTTTTTTTTTTTCAGA	C&gt;500&lt;5(C									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:19:H8VDAADXX:2:2201:6106:14884	177	chr11	49862020	70	3S145M	chrM	16338	0	GAAATACTATGGATTGTTTATTCACTCTTCTGTTAGAAACCTGGACTTTGTCTAATATTTGGCTAATATAAACATGGCTGTTTTGAACAATATTGTACACGTCTTCTTGTGGGCATGTGCTCATCTTTCTAGAGTAAATACCCAGTAA	EEEDC									4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:20:H9V1RADXX:1:1101:17382:26035	113	chr11	49862183	70	148M	chrM	16391	0	GTGCACATTTCTGCTTGACATTGCTTTTTAAAAGAGTTACCTTAAGTGATTGTATAATTTTAGCCTAAATTATCACAAGCATTGTATGAGGATTTCAGTTGCTCCACATTCTCGCTACATTTGAATTGTTAATCTGTTTTACTTTAGC										4	4	('chr11',)_49862020_49862289_MTboth_16095_16391	HG002_5X	chr11_4
HISEQ1:18:H8VC6ADXX:1:1109:1327:84770	81	chr21	9676786	32	148M	chrM	12811	0	TATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGTGAAGTGTTTTCATGTGTGTGACTATTTCCGTGTTATAGCAGTGTGTCCGTATGCTGTGTGAAGTGTTTTCGTGTATATGACTATTTGCGTGTTACAGCAGTGT	CDDDDCACDDDDDDDDDDEDDDDDDDDDDDDDDDDDDDDDBCCCCDCDDDDDDDDDDDDDDDDEDDDDDDDDDDEEEEEEFFFFFFHHHGHIJIIHJIIIJJJJJJJJJJJJJJJJJJJJJJJIIJJJJJJJJJJHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:0	UQ:i:0	NM:i:0	MD:Z:148				5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:18:H8VC6ADXX:2:1104:13981:10006	97	chr21	9676333	28	148M	chrM	12706	0	GTGTTATAGCAGTGTGTCAGTGTGCTGCGTGACGTGTTTTCATGTGTATATGAGTATTTGTGTGTTATAGCCATGTGTCAGTGTGCTGTGTGTTTTGTTTATGTGACTATTTGCGTGTTATAGCAGTGTGTCAGTGTGCTGTGTGAAG	B									5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:19:H8VDAADXX:2:2203:3229:99021	97	chr21	9676197	30	148M	chrM	12389	0	TCATGTGTATATGACTATTTGCATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATGTATATGACTATTTGCGTGTTATGGCAG	CCCFFFFDHHHHHJJJJJJJJJJJJJJJJJJJJJIIIIJJJJHIIIJJJHIHIJJJJJJJGGGJIIHIIJJJJJJJJJJJIJJJJJJHHHHEHFFFFDEDCCCECEDDBDDDD&lt;ADDDDDCCDEEEEEDDDDEEEDDDDDDCDDCDDB	PG:Z:novoalign	AS:i:60	UQ:i:60	NM:i:2	MD:Z:106T36A4				5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:2:2104:5110:91237	97	chr21	9676218	32	148M	chrM	12746	0	CATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATGTATATGACTATTTGCGTGTTATAGCAGTGTGTCAGTGTGCTGCGTGAC										5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:2:2106:12761:29990	161	chr21	9676198	17	148M	chrM	12452	0	CATGTGTATATGACTATTTGCATGTTATAGCAGTGTGTCAGTGTGCTGTGTGACTTTGGTGTATGTGACTATTTGCATGTTACAGCAGTGTGTCAGTGTGCTGTGGGAAGGGTTTTCATTTATATGACTATTTGCGTGTTATAGCAGT	CCCFFFFDHHGBHGIGIJJIIIGHEJJGCEHJJJJIIGIJIAHHIJFI&gt;GCFHHBGGGCE=8BFHGICGHICHIIB									5	5	('chr21',)_9676197_9676786_MTboth_12389_12811	HG002_5X	chr21_5
HISEQ1:20:H9V1RADXX:1:2110:10981:37943	81	chr5	32338499	70	148M	chrM	14831	0	CTCACTACAACCTCCGCCTCCCAGGTTCAAGAGATTCTTCTGCCTCAGCCTCCCGAGTAGCTGGGATTACAGGCATGTGCCACTGTGCCCAGCTAATTTTGTATTTTTAGTAGAGATGGGGTTTCTCCACATTGGTCAGGCTGGTCTC	AECCDDDDB?DDDDDDDDDDDDDDDDDDDEDDDDDDDDDDDDDDBBBDDDBDDDDDDDDDDDDDEEEEEEEEFFFFFFHHHEJJJJJJIJJJJJJJJJJJJJIIJJJJJJIJJJJJJJJJJJJJJJJJIJJJIJJHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:0	UQ:i:0	NM:i:0	MD:Z:148				1	3	('chr5',)_32338282_32338638_MTboth_14831_14831	HG002_5X	chr5_3
HISEQ1:18:H8VC6ADXX:1:2114:15204:54396	113	chr9	129996	70	148M	chrM	6262	0	GCATATATGTTACTGTACTGAACAGTGTATGGAAATAATAACGCAATGGTAAGTATTTGCGTGACAGTAATTTTTCAGCTCCATCGTAATCTTATGCAGTCTGACTAAAACGCTGTTACGCGGTGCATGACTATACTTATTTTTAAAT	DEEEEDDDDDDDEDDEDDEDDCDDDDDDEEEEEEEEDDDDDDDDDDCDDEDDDDDDDDDDDDDEDDDDDDDDDDEEEEDFFFFHHHHJJJJIJJJJJJJJJIHGJJJJJHJJJJJIHGJJJJJJJJJJJIJJIHFHHHHHFFFFFCCC	PG:Z:novoalign	AS:i:30	UQ:i:30	NM:i:1	MD:Z:118T29				2	2	('chr9',)_129555_129996_MTboth_6262_6296	HG002_5X	chr9_2
HISEQ1:18:H8VC6ADXX:2:2203:12359:8789	65	chr9	129555	70	148M	chrM	6296	0	AGCCAGTCACAGAAGGACAAATATTGCATGAATCCACTAATATCAGGTGTCTAAAACAGTGAAACTCATCAAATCAGAAAGTAAAACGGTGGTTACCAGGGGTGAGAGGGAGACGAAAATTGGGGGGGGCTGTTCAATGGGGATAATT	CCCFFFDFHHFGHJJJBHIJJJIJJJIJJJIJJJJGIIGIIJIJJJJBFGIIIIJJGJG?FHIJJJJJIJJIJGIJGJHGHAEHHFFFDACD									2	2	('chr9',)_129555_129996_MTboth_6262_6296	HG002_5X	chr9_2

复制<br><br>import pandas as pd

# Load the uploaded file
file_path = '/mnt/c/Users/victo/Desktop/HG002_5X.mt.disc.sam.cluster.tsv'
data = pd.read_csv(file_path, sep='\t')

# Extract the relevant column
cluster_id_col = data['Cluster_ID']

# Define a function to convert the Cluster_ID column into the desired format
def convert_to_circos_format(cluster_id):
    parts = cluster_id.split('_')
    hs1 = parts[0].replace("('", "").replace("',)", "")
    start1 = parts[1]
    end1 = parts[2]
    hsM = 'hsM'
    startM = parts[4]
    endM = parts[5]
    return f"{hs1} {start1} {end1} {hsM} {startM} {endM}"

# Apply the function to the Cluster_ID column
circos_format_data = cluster_id_col.apply(convert_to_circos_format)

# Create a DataFrame from the results
circos_format_df = circos_format_data.str.split(expand=True)
circos_format_df.columns = ['hs1', 'start1', 'end1', 'hsM', 'startM', 'endM']

# Remove the header and replace 'chr' with 'hs'
circos_format_df_no_header = circos_format_df.replace({'chr': 'hs'}, regex=True)

# Save the result to a file without the header
output_file_path = '/mnt/c/Users/victo/Desktop/circos_format_data_no_header.txt'
circos_format_df_no_header.to_csv(output_file_path, sep=' ', index=False, header=False)

print(f"File saved to {output_file_path}")

复制<br><br>hs1 633902 633918 hsM 10226 10226
hs11 49862020 49862289 hsM 16095 16391
hs14 49862020 49862289 hsM 16095 16391
hs13 49862020 49862289 hsM 16095 16391
hs11 49862020 49862289 hsM 16095 16391
hs12 49862020 49862289 hsM 16095 16391
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs4 9676197 9676786 hsM 12389 12811
hs3 9676197 9676786 hsM 12389 12811
hs21 9676197 9676786 hsM 12389 12811
hs20 9676197 9676786 hsM 12389 12811
hs21 9676197 9676786 hsM 12389 12811
hs5 32338282 32338638 hsM 14831 14831
hs9 129555 129996 hsM 6262 6296
hsy 129555 129996 hsM 6262 6296
复制NUMT-detector补充生成NUMTs分布位置及断点错误排查问题排查]]></description><link>软件\其它生信软件\k-s\numt-detector错误排查.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/NUMT-detector错误排查.canvas</guid><pubDate>Tue, 29 Oct 2024 09:53:55 GMT</pubDate></item><item><title><![CDATA[软件介绍]]></title><description><![CDATA[ 
 <br><br>PAML（Phylogenetic Analysis by Maximum Likelihood）是一款广泛使用的生物信息学软件，主要用于基于最大似然方法的分子序列数据的进化分析。这个工具主要用来估计核苷酸或氨基酸序列之间的进化关系，如分支长度的估计。PAML 特别适用于复杂的统计模型，如可变替换率和不同的进化模型。<br>请注意，这个软件不适用于大规模数据的最大似然树建立!!!
相反的，你应该需要使用其它建树软件建立最大似然树，并连同 树文件 和 序列文件 一同输入到 PAMLX 软件中进行计算。<br>
建树软件 <a data-href="iqtree：线粒体最大似然树" href="软件\iqtree\iqtree：线粒体最大似然树.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🗣" aria-label="🗣" data-icon="🗣" aria-hidden="true" style="transform: translateY(0px);"></span>iqtree：线粒体最大似然树</a><img class="emoji" draggable="false" alt="🗣" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5e3.svg" height="18px" style="max-width: 100%;"> 等
<br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409031655470.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
<br>seqfile：指定序列数据文件的路径。
<br>treefile：指定进化树文件的路径。
<br><br>
<br>outfile name：输出文件的名称。
<br>location：输出文件的保存位置。
<br><br>
<br>fix k：固定转换/颠换比率（κ）的值。如果选中，用户必须提供κ的值。请阅读<a data-href="图解核苷酸替换模型（Nucleotide substitution models）" href="术语\图解核苷酸替换模型（nucleotide-substitution-models）.html" class="internal-link" target="_self" rel="noopener nofollow">图解核苷酸替换模型（Nucleotide substitution models）</a>

<br>fix_kappa 用于指定是否固定 K80、F84 或 HKY85 模型中的转换与颠换比率参数 κ。
<br>如果 fix_kappa = 1，则 kappa 的值被固定；
<br>如果 fix_kappa = 0，则 kappa 的值用作迭代的初始估计值。
<br>对于 JC69 或 F81（没有 κ 参数）以及 TN93 和 REV（分别有两个和五个速率参数，且都从数据中估计）模型，fix_kappa 和 kappa 无效。
<br>fix_kappa = 2 与 nhomo = 5 一起用于 baseml 中指定非平稳模型，其中整个速率矩阵 Q（包括交换性参数和碱基频率参数）在不同分支上变化。


<br>fix α：固定 gamma 分布形状参数（α）。如果选中，α的值为0，表示不考虑位点间的变异。

<br>fix_alpha 和 alpha 选项控制跨位点的可变替换速率的 gamma 分布的形状参数 α。当选中了 fix_alpha 且alpha = 0（0 表示无穷大）时，所有位点的替换率相同。正值的 alpha 表示使用离散 gamma 模型，ncatG 是离散 gamma 模型的类别数。


<br>different alpha's for genes：为不同基因设置不同的 alpha 值。
<br>ncatG：离散 gamma 模型的类别数。
<br>fix ρ：固定相关性参数ρ。选中时，ρ的值为0，表示位点间的替换率独立。
<br>TipDate：如果进行时间相关的序列分析，此处可以输入样本的采集时间。
<br>time unit：时间单位，用于时间树分析。
<br>nparK：选择非参数速率模型的类型。
<br>Small_Diff：算法中使用的数值微分阈值。
<br>ndata：数据集的数量，用于多数据集分析。

<br>ndata 设置ndata 变量用于指定序列文件中数据集的数量，有助于处理模拟数据集或多基因对齐数据。具体设置如下：
<br>ndata = 100：所有数据集/对齐使用相同的树结构（一个树块）。
<br>ndata = 100 separate_trees：每个对齐有其自己的树块。
<br>ndata = 100 maintree：使用主树文件生成每个基因对齐的子树，并进行最大似然（ML）分析。
<br>ndata = 100 maintree 1：同上，进行ML分析。
<br>ndata = 100 maintree 0：生成基因树文件，但不运行ML分析。
<br>我们一般选择默认。


<br><br>
<br>runmode：选择树的使用方式，0 表示使用用户提供的树。

<br>runmode = 0：评估树结构文件中指定的树拓扑。
<br>runmode = 1 和 2：通过启发式搜索算法构建进化树，其中1从多分枝树开始，2从星形树开始。
<br>runmode = 3：逐步添加法。
<br>runmode = 4 和 5：从给定或计算的树开始进行NNI（最近邻互换）扰动。
<br>不建议对大规模数据集执行搜树，因为该软件的速度不能满足需求！


<br>model：选择核苷酸替换模型，7 表示使用 REV（GTR）模型。

<br>模型编号：0 至 8 对应不同的替换模型，如 JC69、K80、F81、HKY85 等。
<br>model = 9 和 10：分别对应特殊情况的 REV/GTR 和 UNREST 模型。
<br>自定义速率模型：如 UNRESTu 和 REVu，允许用户指定具有相同速率的核苷酸对。
<br>具体参见<a data-href="图解核苷酸替换模型（Nucleotide substitution models）" href="术语\图解核苷酸替换模型（nucleotide-substitution-models）.html" class="internal-link" target="_self" rel="noopener nofollow">图解核苷酸替换模型（Nucleotide substitution models）</a>


<br>clock：选择分子钟模型，0 表示不使用分子钟。

<br>clock 变量定义了分子钟假设的不同模型：
<br>clock = 0：无分子钟模型，分支速率完全自由变化，使用无根树。
<br>clock = 1：全局分子钟模型，所有分支速率相同，适用于有根树。
<br>clock = 2：局部分子钟模型，大多数分支符合钟模型，默认速率为1，但某些特定分支可有不同速率。
<br>clock = 3：用于多基因或多区段数据的分析，允许不同数据区段的分支速率以不同方式变化。
<br>对于人类的 Y 染色体和 mtDNA 来说，我们一般选择严格分子钟，在这里，也就是说我们选择1.另外，我们需要注意，如果我们输入的 DNA 中明显分区，那么我们就应该选择3.


<br>Mgene：选择是否对基因或数据分区进行不同处理，0 表示速率相同。

<br>多基因或多位点分析（Mgene）Mgene = 0：未使用 G 选项，假设完全同质。
<br>Mgene = 1：分区模型，对每个基因或位点使用相同的模型。
<br>Mgene = 2、3、4：不同的设置允许基因之间具有不同的频率和速率参数。
<br>这点比较适合一段 DNA 序列包含多个分区的情况！


<br>nhomo：选择是否使用非同质性模型，0 表示同质模型。

<br>nhomo = 0：基本替换模型，碱基频率参数通过观察频率平均值估计。
<br>nhomo = 1：同质模型，通过最大似然迭代估计频率参数 πT, πC 和 πA。
<br>nhomo = 2：每个树枝使用一个转换/颠换比率（κ），适用于 K80、F84 和 HKY85 模型。
<br>nhomo = 3, 4, 5：非同质模型，碱基组成在树上发生漂移，不同树枝使用不同的频率参数。
<br>nhomo = 3：每个末端分支、所有内部分支和根使用各自的一套频率参数。
<br>nhomo = 4：为根和每个分支设定不同的初始和末端频率参数。
<br>nhomo = 5：用户指定多套频率参数，并定义每个分支使用哪套参数。


<br>fix blength：选择是否固定分支长度。
<br>optimization method：选择参数优化方法。
<br>icode：选择遗传密码表，用于编码序列。
<br><br>
<br>getSE：计算参数的标准误差。

<br>建议勾选


<br>RateAncestor：计算祖先节点的速率。

<br>可以勾选。即使勾选，也可以在结果中忽视。


<br>clean data：在分析前清洗数据。

<br>可以勾选。


<br><br>让我们来看看别人如何使用。PAML 软件经常被用来计算<a data-href="最近共同祖先时间(Time to most recent common ancestor，MRCA)" href="术语\最近共同祖先时间(time-to-most-recent-common-ancestor，mrca).html" class="internal-link" target="_self" rel="noopener nofollow">最近共同祖先时间(Time to most recent common ancestor，MRCA)</a> 。怎么计算呢？<br>
首先，利用建树软件获得最大似然树（例如 RaxML-NG、<a data-href="iqtree：基础操作" href="软件\iqtree\iqtree：基础操作.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：基础操作</a>等），然后将序列与树文件丢给 PAML 计算最大似然值。获得的最大似然值可以被当作是遗传距离的一种指标。<br>当然，你也可以直接把序列之间的平均核苷酸差异数目当成是遗传距离的指标。并不一定说要把最大似然值作为遗传距离的指标。<a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a> <a data-href="σ方法" href="术语\σ方法.html" class="internal-link" target="_self" rel="noopener nofollow">σ方法</a>。<br>知道了遗传距离之后，就可以通过以下公式求得<a data-href="最近共同祖先时间(Time to most recent common ancestor，MRCA)" href="术语\最近共同祖先时间(time-to-most-recent-common-ancestor，mrca).html" class="internal-link" target="_self" rel="noopener nofollow">最近共同祖先时间(Time to most recent common ancestor，MRCA)</a>：<br><br>突变率往往在之前的研究中提到，单位一般是  或者 。具体查看 <a data-href="2009 AJHG 净化选择校正：改进的人类线粒体分子钟" href="文献及报道\文献\2024年阅读\7-12月\2009-ajhg-净化选择校正：改进的人类线粒体分子钟.html" class="internal-link" target="_self" rel="noopener nofollow">2009 AJHG 净化选择校正：改进的人类线粒体分子钟</a>。<br><br>
In details, we used PAMLX v1.3.1 (Yang 1997) to calculate maximum-likelihood (ML)  estimates, assuming the HKY85 mutation model with gamma-distributed (32 categories) rates (plus  invariant sites) and two partitions: coding region (positions 00577–16023) and control region  (positions 16024–00576), as described elsewhere (Achilli et al. 2013). We converted ML mutational  distances into years by employing both the corrected molecular clock embedded in the calculator of  (Soares et al. 2009) and the linear mutation rate recently published of 2.7±0.2 x 10-8 base substitution  per nucleotide per year (Posth et al. 2016)<a data-footref="1" href="about:blank#fn-1-5c8c56d3c4b9c60f" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>.
<br>由上我们可知，该研究者使用该软件。选定了 HKY85 模型，并设定了32种突变速率。<br>不明白模型的区别？看看这个：<a data-href="图解核苷酸替换模型（Nucleotide substitution models）" href="术语\图解核苷酸替换模型（nucleotide-substitution-models）.html" class="internal-link" target="_self" rel="noopener nofollow">图解核苷酸替换模型（Nucleotide substitution models）</a>。<br>然后，作者将 mtDNA 分隔成2个区域，分别是控制区和编码区。计算得到了最大似然值的距离，然后根据分子钟转化为时间。<br><br>加入我想选择最熟悉的 GTR+G 模型，应该如何设置呢？<br>
<br>首先，GTR 模型通常设置为：model = 7；
<br>设置分子钟模型为严格分子钟：clock = 1；
<br>启用伽马分布 (G) 和不变位点 (I)：如下，我们需要注意伽马分布的类别数需要自行设定，一个比较保险的方式是设定为 5. 当然，你也可以设定的更多。具体的优缺点请自行 chatgpt。
<br>fix_alpha = 0
alpha = 0.5
ncatG = 5
复制<br><br><br>
<br>
<br>Mitogenome Diversity in Sardinians: A Genetic Window onto an Island's Past 附件<a href="about:blank#fnref-1-5c8c56d3c4b9c60f" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\其它生信软件\k-s\pamlx：最大似然计算.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/PAMLX：最大似然计算.md</guid><pubDate>Tue, 03 Sep 2024 09:41:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5e3.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5e3.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://www.robertlanfear.com/partitionfinder/tutorial/" rel="noopener nofollow" class="external-link" href="https://www.robertlanfear.com/partitionfinder/tutorial/" target="_blank">PartitionFinder tutorial (robertlanfear.com)</a><br>
Cognato AI, Vogler AP. Exploring data interaction and nucleotide alignment in a multiple gene analysis of Ips (Coleoptera: Scolytinae). Systematic Biology. 2001 Nov 1;50(6):758-80.
<br>简单来说，该软件能够以相当快的速度对 phy 文件进行分析，获得构建系统发育树的最佳参数。<br><br>假如你只有一个 fasta 文件，不要紧，可以通过多种方法将其转为 phy 文件格式。作者提到了一个软件：Geneious。你可以从这里下载（免费版本）： <a rel="noopener nofollow" class="external-link" href="http://www.geneious.com/" target="_blank">http://www.geneious.com/</a> 针对任何对齐序列（如果你对於对齐有什么问题，可以查看 <a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a> 和 <a data-href="Trimal：自动修剪序列不可靠区域" href="软件\其它生信软件\t-z\trimal：自动修剪序列不可靠区域.html" class="internal-link" target="_self" rel="noopener nofollow">Trimal：自动修剪序列不可靠区域</a>），只需要在 导出 时选择 phy 格式即可。<br>现在你获得了 phy 文件的对齐文件，你需要再准备一个参数的配置文件 cfg。你可以在该软件的软件包中找到模板，并进行修改成适合你自己的参数。<br><br>地址： <a rel="noopener nofollow" class="external-link" href="https://github.com/brettc/partitionfinder/releases/latest" target="_blank">https://github.com/brettc/partitionfinder/releases/latest</a><br>
下载之后，解压到电脑。可以在 win/Linux 运行。以下以 linux 为范例。<br><br>软件使用 python2.7.X 版本。版本比较老，可以创建虚拟环境：<br># 创建环境
conda create -n partitionfinder_env python=2.7
# 激活环境
conda activate partitionfinder_env
复制<br>另外，你可能需要一些安装包：<br>conda install numpy
pip install numexpr==2.6.9
# conda install numexpr==2.6.9
# conda install numexpr
pip install tables==3.4.4
# conda install tables==3.4.4
# conda install tables
pip install scikit-learn==0.20.4
# conda install scikit-learn==0.20.4
# conda install scikit-learn
复制<br>准备完毕之后，就可以使用了。<br><br>举个例子，在 partitionfinder-2.1.1/examples 中，以 partitionfinder-2.1.1/examples/nucleotide/partition_finder.cfg 为例，打开该文件，查看其中的参数：<br>## ALIGNMENT FILE ##
alignment = test.phy;

## BRANCHLENGTHS: linked | unlinked ##
branchlengths = linked;

## MODELS OF EVOLUTION: all | allx | mrbayes | beast | gamma | gammai | &lt;list&gt; ##
models = GTR, GTR+G, GTR+I+G;

# MODEL SELECCTION: AIC | AICc | BIC #
model_selection = aicc;

## DATA BLOCKS: see manual for how to define ##
[data_blocks]
Gene1_pos1 = 1-789\3;
Gene1_pos2 = 2-789\3;
Gene1_pos3 = 3-789\3;
Gene2_pos1 = 790-1449\3;
Gene2_pos2 = 791-1449\3;
Gene2_pos3 = 792-1449\3;
Gene3_pos1 = 1450-2208\3;
Gene3_pos2 = 1451-2208\3;
Gene3_pos3 = 1452-2208\3;

## SCHEMES, search: all | user | greedy | rcluster | rclusterf | kmeans ##
[schemes]
search = greedy;

复制<br>具体的参数你可以在软件手册中查到，这里简略说说：<br>
<br>ALIGNMENT FILE：输入文件名字
<br>BRANCHLENGTHS：告诉 PartitionFinder2 是否估计所有分区上的一组分支长度，或者每个分区是否应该有自己的一组独立分支长度。事先可能很难知道这些选项中哪一个是最好的，但并非所有系统发育程序都允许不连锁的分支长度。因此，我们将将此选项保留为“branchlenghts = linked;”&nbsp;PartitionFinder2 手册对此选项有更深入的解释。
<br>models： 为每个分区分析哪些替代模型。我们将选择一组小但合理的模型。保持较小的值在这里很有用，因为它将使我们的分析运行得更快。如果你想更为全面，设置 all 吧。
<br>model_selection：将保留为“AICc”。这意味着模型选择和划分方案比较将使用修正后的艾凯克信息准则进行。您应该始终选择此选项而不是 AIC，但您选择 AICc 还是 BIC 仍然是个人选择的问题。有时它确实会影响结果，尽管通常影响不大，而且通常不会影响树拓扑。
<br>data_blocks：最重要的。这是我们定义整个分析所基于的站点集的地方。 PartitionFinder2 的工作原理是基于这些数据块尝试不同的分区方案，因此其想法是使用数据块来定义您认为可能以类似方式演变的站点集。 PartitionFinder2 永远不会尝试细分任何数据块，因此最好定义大量小数据块，而不是几个大数据块（在合理范围内）。对于蛋白质编码基因来说，明智的做法是为每个基因中的每个密码子位置定义一个数据块。如果您不知道密码子位置在哪里，那么弄清楚这一点并向数据块部分中的 PartitionFinder2 提供信息非常重要。未能定义蛋白质编码基因中的密码子位置可能会导致系统发育树的估计非常差。
<br>schemes：告诉 PartitionFinder2 如何比较分区方案。在本教程中，我们将设置“search=greedy”。这告诉 PartitionFinder2 使用 2012 年 MBE 中的 PartitionFinder 论文中描述的启发式搜索算法来搜索良好的分区方案。由于我们定义了 7 个数据块，因此可以分析所有可能的分区方案（'search=all'），但这可能需要相当长的时间，因为 7 个数据块有很多可能的分区方案。如果您的计算机有很多处理器，或者有时间等待，您可能想尝试一下。但对于大多数分析来说，贪婪搜索会得到与 search=all 完全相同的答案。<br>
现在 .cfg 文件已设置完毕，我们只需保存它即可运行 PartitionFinder2。
<br><br># 确保你激活了虚拟环境
cd partitionfinder-2.1.1/examples/nucleotide
# 运行软件
(partitionfinder_env) root@Lin:partitionfinder-2.1.1/examples/nucleotide$ 
python \
'partitionfinder-2.1.1/PartitionFinder.py'\
'partitionfinder-2.1.1/examples/nucleotide/partition_finder.cfg'
复制<br>哗啦啦，运行完成了！<br><br>在 partitionfinder-2.1.1/examples/nucleotide/analysis/best_scheme.txt 中，看到 Best partitioning scheme 中的信息：<br>Scheme Name       : step_4
Scheme lnL        : -5413.049224853516
Scheme AICc       : 10934.7558313
Number of params  : 53
Number of sites   : 2208
Number of subsets : 5

Subset | Best Model | # sites    | subset id                        | Partition names       
1      | GTR+G      | 736        | 15599ff31918c11c18f1186d6302f726 | Gene2_pos1, Gene1_pos1, Gene3_pos1
2      | GTR+G      | 516        | 9b7ee22854477034e1596f067a4f37f3 | Gene3_pos2, Gene1_pos2
3      | GTR+G      | 263        | fb291d83f956259cc94c6ba41b0fdf14 | Gene1_pos3            
4      | GTR        | 220        | 9801c30aa4490587ec45223422b9aed8 | Gene2_pos2            
5      | GTR+G      | 473        | 0f87f6848266d4913a7204898e9de700 | Gene3_pos3, Gene2_pos3
复制<br>排第一的时 GTR+G 因此这个模型最佳！<br><br>接下来你需要去建树。每个软件都有格式要求，Partitionfinder 想到了这一点，因此针对不同的软件给出了不同的命令，都在 partitionfinder-2.1.1/examples/nucleotide/analysis/best_scheme.txt 中：<br>Nexus formatted character sets for IQtree
Warning: the models written in the charpartition are just the best model found in this analysis. Not all models are available in IQtree, so you may need to set up specific model lists for your analysis
# 给IQTREE软件的
#nexus
begin sets;
charset Subset1 = 790-1449\3 1-789\3 1450-2208\3;
charset Subset2 = 1451-2208\3 2-789\3;
charset Subset3 = 3-789\3;
charset Subset4 = 791-1449\3;
charset Subset5 = 1452-2208\3 792-1449\3;
charpartition PartitionFinder = GTR+G:Subset1, GTR+G:Subset2, GTR+G:Subset3, GTR:Subset4, GTR+G:Subset5;
end;

# 给RaxML软件的
RaxML-style partition definitions
Warning: RAxML allows for only a single model of rate heterogeneity in partitioned analyses. I.e. all partitions must be assigned one of three types of model: No heterogeneity (e.g. GTR); +G (e.g. GTR+G); or +I+G (e.g. GTR+I+G). If the best models for your datasetcontain different types of model for different subsets you will need to decide on the best rate heterogeneity model before you run RAxML. If you prefer to do things more rigorously, you can run separate PartitionFinder analyses for each type of rate heterogenetity Then choose the scheme with the lowest AIC/AICc/BIC score. Note that these re-runs will be quick!

DNA, Subset1 = 790-1449\3, 1-789\3, 1450-2208\3
DNA, Subset2 = 1451-2208\3, 2-789\3
DNA, Subset3 = 3-789\3
DNA, Subset4 = 791-1449\3
DNA, Subset5 = 1452-2208\3, 792-1449\3


MrBayes block for partition definitions
Warning: MrBayes only allows a relatively small collection of models. If any model in your analysis is not one that is included in MrBayes (e.g. by setting nst = 1, 2, or 6 for DNA sequences; or is not in the available list of protein models for MrBayes)then this MrBayes block will just set that model to nst = 6 for DNA, or 'wag' for Protein. Similarly, the only additional parameters that this MrBayes block will include are +I and +G. Other  parameters, such as +F and +X, are ignored. If you want to use this MrBayes block for your analysis, please make sure to check it carefully before you use it we've done our best to make it accurate, but there may be errors that remain!
# 给MrBayes软件的
begin mrbayes;

charset Subset1 = 790-1449\3 1-789\3 1450-2208\3;
charset Subset2 = 1451-2208\3 2-789\3;
charset Subset3 = 3-789\3;
charset Subset4 = 791-1449\3;
charset Subset5 = 1452-2208\3 792-1449\3;

partition PartitionFinder = 5:Subset1, Subset2, Subset3, Subset4, Subset5;
set partition=PartitionFinder;

lset applyto=(1) nst=6 rates=gamma;
lset applyto=(2) nst=6 rates=gamma;
lset applyto=(3) nst=6 rates=gamma;
lset applyto=(4) nst=6;
lset applyto=(5) nst=6 rates=gamma;

prset applyto=(all) ratepr=variable;
unlink statefreq=(all) revmat=(all) shape=(all) pinvar=(all) tratio=(all);

end;

复制]]></description><link>软件\其它生信软件\k-s\partitionfinder：一站式解决系统发育树参数问题.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/Partitionfinder：一站式解决系统发育树参数问题.md</guid><pubDate>Sun, 08 Sep 2024 13:16:04 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f600.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f600.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[工作流程]]></title><description><![CDATA[ 
 <br><br><br>从 vcf 文件生成系统发育树（例如，使用 RAxML 或 MEGA，运行多次迭代）。树的质量对分支的 SNP 分配以及所有下游分析有重大影响。<br><br># 将生成一个 branches.snp 文件，包含每个 SNP 映射到树的哪个分支的信息。
phynder -B -o branches.snp tree.nwk tree.vcf.gz
复制<br><br>运行 pathPhynder 来调用给定的古代样本数据集中的 SNP，并找到可以将它们映射到树中的最佳路径和分支。<br># 准备数据 - 这将输出一个用于调用变异的 bed 文件和用于系统发育定位的表格
pathPhynder -s prepare -i tree.nwk -p &lt;prefix_output&gt; -f branches.snp

# 运行 pathphynder
pathPhynder -s all -i tree.nwk -p tree_data/&lt;prefix_output&gt; -b &lt;sample.bam&gt;

# 或者，如果分析多个 bam 文件
pathPhynder -s all -i tree.nwk -p tree_data/&lt;prefix_output&gt; -l &lt;bam_list&gt;
复制<br><br><br>对于每个古代样本，在信息丰富的分支定义位点运行 pileup，并进行脱氨和错配过滤。<br>pathPhynder -s &lt;1 或 pileup_and_filter&gt; -i &lt;tree&gt;.nwk -p tree_data/&lt;prefix_output&gt; -l &lt;sample.list&gt;
复制<br><br>对于每个古代样本，在树的每个分支添加衍生和祖先状态信息。遍历树，评估每个分支上的祖先和衍生标记的数量，并识别最佳路径。<br>pathPhynder -s &lt;2 或 chooseBestPath&gt; -i &lt;tree&gt;.nwk -p tree_data/&lt;prefix_output&gt; -l &lt;bam_list&gt;
复制<br><br>pathPhynder -s &lt;3 或 addAncToTree&gt; -i &lt;tree&gt;.nwk -p tree_data/&lt;prefix_output&gt; -l &lt;bam_list&gt;
复制<br><br>选项:
-s STEP, --step=STEP
指定运行的步骤。选项:
- prepare - 准备运行 pathPhynder 所需的文件。
- all - 运行所有步骤以将古代 SNP 映射到分支 (1,2,3)。
- 1 或 pileup_and_filter - 在古代 bam 文件中运行 pileup 并过滤碱基。
- 2 或 chooseBestPath - 为每个样本找到树的最佳分支/节点。
- 3 或 addAncToTree - 将古代样本添加到树中。
[默认 all]

-i INPUT_TREE, --input_tree=INPUT_TREE
Newick 格式的输入树。[必需]

-f BRANCHES_FILE, --branches_file=BRANCHES_FILE
branches.snp 文件 - 使用 phynder 创建的 SNP 放置文件。

-p PREFIX, --prefix=PREFIX
与树关联的数据文件的前缀。
这些文件在分支分配步骤中之前生成。[必需]

-b BAM_FILE, --bam_file=BAM_FILE
输入 bam 文件。[必需]

-l LIST_OF_BAM_FILES, --list_of_bam_files=LIST_OF_BAM_FILES
bam 文件路径列表。[必需]

-r REFERENCE, --reference=REFERENCE
参考基因组（fasta 格式）。[默认 "/home/luolintao/miniconda3/envs/pathPhynder/lib/R/library/pathphynder/R/../data/reference_sequences/hs37d5_Y.fa.gz"]

-m FILTERING_MODE, --filtering_mode=FILTERING_MODE
pileup 过滤模式。选项: default, no-filter 或 transversions。[默认 "default"]

-t MAXIMUMTOLERANCE, --maximumTolerance=MAXIMUMTOLERANCE
遍历树时容许的最大 ALT 等位基因数量。
如果超过，算法将停止并切换到下一个路径。[默认 3]

-q BASEQUALITY, --baseQuality=BASEQUALITY
samtools mpileup 的最小碱基质量。[默认 20]

-c PILEUP_READ_MISMATCH_THRESHOLD, --pileup_read_mismatch_threshold=PILEUP_READ_MISMATCH_THRESHOLD
接受变异的错配阈值（用于 pileup 中同时存在两种等位基因的情况）。
为了使变异通过过滤，包含最频繁等位基因的读数必须至少占总读数的 x 比例。1 是最严格的，0.5 是最宽松的。[默认 0.7]

-o OUTPUT_PREFIX, --output_prefix=OUTPUT_PREFIX
样本名称。这仅在单个 bam 文件用作输入时有效。[默认 bamFileName]

-G HAPLOGROUPS, --haplogroups=HAPLOGROUPS
已知单倍群定义的 SNP 列表

-h, --help
显示此帮助信息并退出

复制]]></description><link>软件\其它生信软件\k-s\pathphynder：将古代dna上到最大似然树.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/pathPhynder：将古代DNA上到最大似然树.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br><a data-href="pathPhynder：将古代DNA上到最大似然树" href="软件\其它生信软件\k-s\pathphynder：将古代dna上到最大似然树.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🥵" aria-label="🥵" data-icon="🥵" aria-hidden="true" style="transform: translateY(0px);"></span>pathPhynder：将古代DNA上到最大似然树</a><img class="emoji" draggable="false" alt="🥵" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" height="18px" style="max-width: 100%;"><br><br><br>你需要一个最大似然树的文件。<br>注意
你的最大似然树的文件内出现的所有 ID 必须在下面提到的 VCF 文件内存在！
<br>你如何才能得到一个最大似然树的文件呢？很多软件都可以建树：<a data-href="iqtree：基础操作" href="软件\iqtree\iqtree：基础操作.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：基础操作</a>，<a data-href="GUIDANCE2 Server ：多序列比对置信软件" href="软件\其它生信软件\a-j\guidance2-server-：多序列比对置信软件.html" class="internal-link" target="_self" rel="noopener nofollow">GUIDANCE2 Server ：多序列比对置信软件</a>，还有 <a data-tooltip-position="top" aria-label="https://github.com/amkozlov/raxml-ng" rel="noopener nofollow" class="external-link" href="https://github.com/amkozlov/raxml-ng" target="_blank">RaxML-ng</a> 等。<br>举例来说：F_ML.bestTree<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408301015385.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>如上所述，你需要一个 VCF 文件。这个 VCF 文件中存在的 ID 必须和最大似然树文件内的 ID 对应。<br>
例如：F.vcf<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408301017894.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>记住，ID 必须一一对应，不能一个 ID 连个单倍群后缀，而另外则没有！<br><br>既然是古 DNA 上树，那么古代 DNA 文件不可或缺。你需要将这些 BAM 文件存放在一个位置，并打印出所有的文件的绝对路径。<br>例如，all_bam_list：<br>/home/zhiyongwang/9_f/925+F/ancient_DNA/0LS10.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/11KBM1_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/84001.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/91KLH18_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/91KLM2_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/ARG001.B0101_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/ARG003.A0101_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BAY001.A0101_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BGD004.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY001.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY003.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY005.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY007.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY008.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY009.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BIY012.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BOO002.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BOO004.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BOT15.Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BRG004.A0101_Y.bam
/home/zhiyongwang/9_f/925+F/ancient_DNA/BRG005.B0101_Y.bam
复制<br><br>这个在软件安装包内存在，不用管。<br><br># 定义常用路径变量，减少重复路径定义
PHYNDER_PATH="/home/zhiyongwang/phynder/phynder" # 不用修改
pathPhynder_script="Rscript /home/zhiyongwang/pathPhynder/pathPhynder.R" # 不用修改
TREE_FILE="你的最大似然树文件"
VCF_FILE="你的VCF文件"
ISOGG_FILE="/home/zhiyongwang/pathPhynder/data/210513.snps_isogg_curated.txt" # 不用修改
BAM_LIST="bam文件绝对路径的txt文件" # 上述的all_bam_list
PREFIX="tree_data/result" # 输出文件的前缀名字，随便写

# 生成包含每个树枝上SNP信息的 branches.snp 文件
$PHYNDER_PATH -B -o branches.snp $TREE_FILE $VCF_FILE

# 输出用于变异调用的 BED 文件和系统发育定位的表格
$pathPhynder_script -s prepare -i $TREE_FILE -p 2M -f branches.snp -G $ISOGG_FILE

# 并行处理 BAM 文件列表，输出日志
nohup bash -c '
cat $BAM_LIST | xargs -P 30 -I {} bash -c '"'"'
    pathPhynder -i $TREE_FILE -p $PREFIX -b {} -s all -t 100 -G $ISOGG_FILE
'"'"'
' &gt; place_new_aDNA.log 2&gt;&amp;1 &amp;
复制<br><br>当所有的程序都运行完成之后，就可以在 PREFIX="tree_data/result" 内找到：<br>
<br>final_tree.nwk
<br>final_tree.pdf <br>
其中 nwk 是树文件，另一个是 pdf。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408301523500.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
]]></description><link>软件\其它生信软件\k-s\pathphynder：y染色体古代dna上系统发育树.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/pathPhynder：Y染色体古代DNA上系统发育树.md</guid><pubDate>Fri, 30 Aug 2024 07:23:43 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[PhyloSuite：不简单的工作让系统发育分析变得简单]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://sci-hub.se/10.1002/imt2.87" rel="noopener nofollow" class="external-link" href="https://sci-hub.se/10.1002/imt2.87" target="_blank">论文链接</a><br>
<a data-tooltip-position="top" aria-label="https://www.youtube.com/watch?v=qtAL8X3314g" rel="noopener nofollow" class="external-link" href="https://www.youtube.com/watch?v=qtAL8X3314g" target="_blank">视频链接</a><br>
<a data-tooltip-position="top" aria-label="https://github.com/dongzhang0725/PhyloSuite" rel="noopener nofollow" class="external-link" href="https://github.com/dongzhang0725/PhyloSuite" target="_blank">Github链接</a>
<br>这是兰州大学张东团队2023年2月16日在iMeta期刊上发表的题为“Using PhyloSuite for molecular phylogeny and tree-based analyses&nbsp;”的文章，该论文主要是对他们研发出的新软件PhyloSuite进行一个介绍。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271721355.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>新一代测序技术的进步让公共数据库获得的基因数据量得到大幅增加，尽管这为数据分析提供了更多的原材料，但同时也对我们检索和管理能力提出了更高的要求，而对于生信初学者和不太懂计算机的研究人员而言是耗时且困难的。系统发育分析的标准程序是选择和下载GenBank文件，提取目标基因，序列比对再优化，然后比对连接（针对多基因串联），测试得到最佳拟合划分方案和进化模型，再进行系统发育建树，最后可对构建的树进行注释和美化。但往往这些程序的操作可能会非常耗时，尤其是在批量处理数据时，对于生信刚入门的同学来说，无疑是难上加难，PhyloSuite便是为此而生，它是一个简单高效的可视化系统发育分析平台，整合了以上所述的所有分析步骤所需的软件，适用于单基因及多基因联合系统发育分析，只要简单地点击几下鼠标，它就会帮你自动解决一系列流程方面的疑难杂症，让你能把更多的时间和精力放在对科学问题的思考上。<br>今天便以多基因串联建树为例，对PhyloSuite的功能进行一个基本的介绍。<br>PhyloSuite提供了灵活的GenBank获取选项卡，首先在打开PhyloSuite后，点击菜单栏中的“File”选项中的“Import file(s) or ID(s)”后会出现Input窗口，可输入从NCBI获得的ID号。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271703918.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>在输入ID号并单击“start”后，PhyloSuite会从NCBI中提取到GenBank文件并批量输出到工作区内（如下图红色选中区）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271703273.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
再点击"File"中的“Extract GenBank file”,在弹出的Extracter界面，可在“Lineage&nbsp; color”一栏中根据需求来对纲、目、科等进行颜色的选择，点击“start”后，Phylosuite会将序列以CDS、外显子、内含子等模块进行拆解分类，代替手动剥取基因这一步骤，从而大大节约时间。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271703776.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271703030.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
值得注意的是，在files一栏中文件的由上往下是对你操作步骤顺序的反映，每一个文件对应一个操作流程所产生的结果。接着是通过“Alignment”中的“<a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a>”进行一个序列比对，在Input一栏中输入你想选择的基因FASTA文件，然后选择“Alignment Mode”开始运行，再将比对好的结果用Gblock来切除。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271704636.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271704636.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271704668.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>将Gblock切好后的基因序列比对文件进行一个串联，串联后再通过ModelFinder，找到适用于MrBayes（以MrBayes为例）的模型，然后最后通过Mrbayes来建树，建树的结果会生成一个文件，可打开Figtree来查看，并且可以通过iTOL进化树美化网站进行修饰和注释。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271704753.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271705148.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271705691.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271705543.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271705518.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>以上便是本期的内容，总的来说，这是一款非常方便实用的软件，不管是做进化方向研究的科学家还是生信刚入门的同学，都可以试试它，说不定会起到事半功倍的效果。]]></description><link>软件\其它生信软件\k-s\phylosuite：不简单的工作让系统发育分析变得简单.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/PhyloSuite：不简单的工作让系统发育分析变得简单.md</guid><pubDate>Sun, 08 Sep 2024 13:18:10 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271721355.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312271721355.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[PLINK：转换 VCF 到 BED, BIM, FAM]]></title><description><![CDATA[ 
 <br>
<br>
.BED 文件：

<br>这是一个二进制文件，用于存储基因型数据。
<br>.bed 文件通常与 .bim 和 .fam 文件一起使用，这三个文件共同描述了基因型矩阵。
<br>文件中的数据是压缩的，每个位点的基因型用两位二进制数表示（00, 01, 10, 11），使得文件既紧凑又易于快速处理。


<br>
.BIM 文件：

<br>这是一个文本文件，包含了 SNP（单核苷酸多态性）标记的元数据。
<br>每一行代表一个位点，包含六个字段：染色体号、SNP 标识符、遗传距离（摩尔距离）、物理位置以及两个等位基因（通常是 A 和 B）。
<br>.bim 文件帮助在 .bed 文件中定位各个基因型数据点。


<br>
.FAM 文件：

<br>这也是一个文本文件，用于描述样本信息。
<br>每一行代表一个个体，包含六个字段：家族ID、个体ID、父亲ID、母亲ID、性别（1表示男性，2表示女性，0表示未知）和表型（通常是1和2表示不同的表型，-9或0表示未知或未测量的表型）。
<br>.fam 文件在关联分析和群体遗传学研究中尤为重要，因为它提供了分析需要的家族结构和表型信息。


<br>
BED 文件：这是 PLINK 的二进制基因型文件，包含个体的基因型数据。这种格式允许快速的读取和高效的存储。

<br>
BIM 文件：相当于基因型信息的标记文件，列出了每个位点的染色体位置、基因位点标识符、摩尔体位置、以及等位基因类型。

<br>
FAM 文件：描述样本的文件，通常包含家族信息、个体ID、父母信息、性别、表型等。

<br>打开命令行界面（例如终端或命令提示符），使用以下命令将 VCF 文件转换为 PLINK 的 BED, BIM, FAM 格式：<br>plink --vcf 您的文件.vcf --recode compound-genotypes --double-id --make-bed --out 您的文件（无后缀）
复制<br>这里：<br>
<br>--vcf input.vcf 指定了输入的 VCF 文件。
<br>--make-bed 命令告诉 PLINK 创建 BED, BIM, FAM 文件。
<br>--out output_prefix 指定输出文件的前缀。例如，如果您指定 output_prefix 为 mydata，PLINK 将生成 mydata.bed、mydata.bim 和 mydata.fam 文件。
<br>运行这个命令后，您应该会在指定的输出目录中看到三个新文件：.bed、.bim 和 .fam。]]></description><link>软件\其它生信软件\k-s\plink：转换-vcf-到-bed,-bim,-fam.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/PLINK：转换 VCF 到 BED, BIM, FAM.md</guid><pubDate>Fri, 21 Jun 2024 07:30:21 GMT</pubDate></item><item><title><![CDATA[软件下载]]></title><description><![CDATA[ 
 <br>Haplotype Network（单倍型网络）是一种用于研究和可视化遗传序列变异之间关系的工具，尤其在生物信息学、遗传学和考古学等领域中具有重要应用。单倍型（Haplotype）是指一组在个体或物种特定染色体区段上紧密连锁的遗传标记，它们通常作为一个整体一起遗传。这些遗传标记可以是单个核苷酸多态性（SNPs）、插入/缺失（indels）或其他类型的遗传变异。通过分析这些遗传变异的模式，研究人员可以揭示个体或物种之间的遗传关系、进化历史和群体结构。<br>进行绘图之前，应该已经使用DnaSP和Arlequin计算出了Fst。如果没有，请看<a data-href="DnaSP：AMOVA及Fst分析软件操作" href="软件\其它生信软件\a-j\dnasp：amova及fst分析软件操作.html" class="internal-link" target="_self" rel="noopener nofollow">DnaSP：AMOVA及Fst分析软件操作</a>。<br><br><a data-tooltip-position="top" aria-label="https://popart.maths.otago.ac.nz/" rel="noopener nofollow" class="external-link" href="https://popart.maths.otago.ac.nz/" target="_blank">下载地址</a>。<br><br>这个软件的设计十分糟糕，所以数据准备至关重要。<br>
建议认真阅读下列每一个要点，以防止出现格式错误！！<br><br>数据的格式如下所示:<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041250489.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>PopART 数据格式，主要有两个data和Traits模块，红色部分为主要模板参数，黑色部分需要通过实际数据修改<br>举个例子来说，应该如下所示：<br>#NEXUS

Begin Data;

    Dimensions ntax=12 nchar=13;

    Format datatype=DNA missing=N gap=-;

    Matrix

Hap_1   TACATCAGGGTAG

Hap_2   TACATCAGGGTAC

Hap_3   TACATTAGGGTAC

Hap_4   TCCACCAGGGAAC

Hap_5   TCCATCAGGGTAC

Hap_6   TACATCAGGGTCC

Hap_7   TACAACAGCTTAC

Hap_8   TACAACAGGGTAC

Hap_9   CACCACCACGTAC

Hap_10  TATAACAAGGTAC

Hap_11  TACTTCAGGGAAC

Hap_12  TACAACAGGGAAC

;

END;

 

Begin Traits;

Dimensions NTraits=4;

format labels=yes missing=? separator=Comma;

TraitLabels Fujian Hebei Heilongjiang Jiangsu;

Matrix

 

Hap_1 10,0,0,0

Hap_2 19,9,12,10

Hap_3 1,0,0,0

Hap_4 1,0,0,0

Hap_5 1,0,0,0

Hap_6 0,0,1,0

Hap_7 0,0,1,0

Hap_8 0,0,2,0

Hap_9 0,0,1,0

Hap_10 0,0,0,7

Hap_11 0,0,0,2

Hap_12 0,0,0,1

;

End;
复制<br><br>#NEXUS

Begin Data;

    Dimensions ntax=280 nchar=111;

    Format datatype=DNA missing=N gap=-;

    Matrix

Hap_1 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_2 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_3 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATATTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_4 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_5 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCCTTGATATGTTGTATAAATCCTGAGCTCGTTCCACTCCACCTGTCCAGTTCATCGTATCA
Hap_6 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTACCA
Hap_7 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCCTGGGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_8 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_9 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_10 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAGTCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_11 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_12 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_13 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_14 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATTGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_15 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_16 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_17 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_18 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_19 CATTGTCGTTTATTGATTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_20 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_21 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCAGTTCATCGTATCA
Hap_22 CATCGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_23 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTCCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_24 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCATATCA
Hap_25 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_26 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGGACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_27 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_28 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTCTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_29 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTCTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_30 CATTGTCGTTTATTGACTGCGTTAGCTAGAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_31 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGCCCAGTTCATCGTATCA
Hap_32 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTCCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_33 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_34 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_35 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_36 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_37 CATTGTCGTTTATTGACTGTGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_38 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_39 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_40 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_41 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_42 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGCATCA
Hap_43 CATTGTCATTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_44 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTCCATCGTATCA
Hap_45 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGCCCAGTTCATCGTATCA
Hap_46 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTATCCAGTTCATCGTATCA
Hap_47 CATTATCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_48 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCCCGTTCTACTCCACCTGTCCAGTTCATCGTATTA
Hap_49 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAATCCATCGTATCA
Hap_50 CACTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_51 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTTATCATATCA
Hap_52 CATTGTCGTTTATTGGCTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTGTAAATCTTGAGCTCGTTCTATTCCACCTGTCCAGTTCATCGTATCA
Hap_53 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCGTCGTATCA
Hap_54 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTCGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_55 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCGGTTCATCGTATCA
Hap_56 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCAGTTCATCGTATCA
Hap_57 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGTGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_58 CATTGTCGTTTACTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTCGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_59 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_60 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTACCA
Hap_61 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATATTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_62 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_63 CATTGTCGTTTATTGACTGCGTTAGCTAAGGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGTGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_64 CATTGTCGTTTATTGACTGCGTTAGCTAAAGATAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_65 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_66 CATTGTCGCTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_67 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCGGTTCATCGTATCA
Hap_68 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCAACTGTCCAGTTCATCGTATCA
Hap_69 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGCATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_70 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTCCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_71 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGTGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATTA
Hap_72 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATTA
Hap_73 CATTGTCGTTTATTGACTGCGTTAGATAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_74 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGGTATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_75 CGTTGTCGTTTATTGACTGCGTTAACTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_76 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTCCACCGTATCA
Hap_77 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTTGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_78 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCGTCGTATCA
Hap_79 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_80 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCCTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_81 CATTGTCGTTTATTGACCGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_82 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAGTCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_83 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCTAGTTCATCGTATCA
Hap_84 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_85 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTTTACTCCACCTGTCCAGTTCATCGTATCA
Hap_86 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTATTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_87 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTCGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_88 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_89 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCATATCA
Hap_90 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCCCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_91 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTCGATATGTTGTATGAATCTCGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_92 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGTTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_93 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGCTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_94 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCCACTCCACCTGTCCAGTTCATCGTATCA
Hap_95 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTCCATCGTATCA
Hap_96 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTGGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_97 CATTGTCGTTTATTAACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_98 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATATTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_99 CATTGTCGTTTATTGACTGCGTTAACTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGGATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_100 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGGGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_101 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_102 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACTTGTCCAGTTCATCGTACCA
Hap_103 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGCTCATCGTATCA
Hap_104 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATTA
Hap_105 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATTGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_106 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGACATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_107 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_108 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTCCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_109 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCCTGGGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_110 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATACGTTGTATGAATTTCGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_111 CATTGTCGTTTATTGATTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTGTCA
Hap_112 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAGTCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATTA
Hap_113 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGGTATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_114 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_115 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTATATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_116 CATTGTCGTTTATTGACTGCGTTAGCTAAAGGCAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_117 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_118 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_119 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCAGTTCATCGTATTA
Hap_120 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCACTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_121 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCCTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_122 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_123 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_124 CATTGTCGTTTATTGACTGCGTCAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_125 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAACCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_126 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTATATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_127 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_128 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGCATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_129 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_130 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATTA
Hap_131 CATTGTCGCTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_132 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTAATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_133 CATTGTCGTTTATTGACTGTGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_134 CATTGTCGTTTATTGACTGCGTTAGCTAGAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_135 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCCACTCCACCTGTCCAGTTCATCGTATCA
Hap_136 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTTATCATATCA
Hap_137 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATATTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_138 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAGTCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATTA
Hap_139 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_140 CATTGTCGTTTATTGACTGCGTTAGCTAAGGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGTGAATCTTGAGCCCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_141 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGCATCA
Hap_142 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_143 CATTGTCGTTTATTGACTGCGTTGGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_144 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_145 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCGCTTGTCCAGTTCATCGTATCA
Hap_146 CATTGTCGTTTATCGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_147 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCACTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_148 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTTCACCTGTCCAGTTCATCGTATCA
Hap_149 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTCCATCGTATCA
Hap_150 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTATCCAGTTTATCGTATCA
Hap_151 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_152 CATTGTCGTCTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_153 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_154 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTCGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_155 TATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGCGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_156 CATTATCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_157 CATTGCCGTTTATTGACTGCGTTAGCTAAAGGCAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_158 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_159 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTAATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_160 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_161 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTATATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_162 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAGTCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_163 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATACGTTGTATGAATTTCGGACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_164 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGCCCAGTTCATCGTATCA
Hap_165 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGACATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_166 CGTTGTCGCTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_167 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAGTCGCTAGAGGCTTTCTTTGATATGCTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_168 CATTGTCGTTTATTGACTGCGTCAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_169 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCCCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_170 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTCGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_171 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAAAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_172 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTAATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGCCCAGTTCATCGTATCA
Hap_173 CATTGTCGTTTATTGGCTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCCTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_174 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCCTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_175 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATGTGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_176 CATTGTCGTTTATTGACTGCGTTAACTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGGATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_177 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGACATGTTATATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_178 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGGACTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_179 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTATCCAGTTCATCGTATCA
Hap_180 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTGGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_181 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAGATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCATATCA
Hap_182 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCCACTCCACCTGTCCAGTTCATCATATCA
Hap_183 CATTGTCATTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCCCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_184 CATTGTCGTTTATTGACTGCGTTAGCTAGAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTTCACCTGTCCAGTTCATCGTATCA
Hap_185 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATTA
Hap_186 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGACTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_187 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGACTTTCTTTGATATGTTATATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_188 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCG
Hap_189 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTTCAGTTCATCGTATCA
Hap_190 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_191 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTGTCGTATCA
Hap_192 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTACCA
Hap_193 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_194 CATTGTCGTTCATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_195 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTACGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_196 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCGTCGTATCA
Hap_197 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_198 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_199 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTAAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_200 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTAAGCTCGTTCTACTCCACTTGTCCAGTTCGTCGTATCA
Hap_201 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTAAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_202 CATTGTCGTTTATTGACTGTGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCAGTTCATCGTATCA
Hap_203 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_204 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCG
Hap_205 CATTGTCGTTTATTGACTGCATTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_206 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACCCCACCTGTCCAGTTCACCGTATCA
Hap_207 CATTATCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_208 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTACCA
Hap_209 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTGTCGTATCA
Hap_210 CATTATCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCCACTCCACCTGTCCAGTTCATCGTATCG
Hap_211 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTAAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_212 CATTGTCGTTTGTTGACTGCGCTAGCTAAAGACAAATCGCTAGAGGTTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_213 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCGTCGTATCA
Hap_214 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_215 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCGGTTCATCGTATCA
Hap_216 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGCCCAGTTCATCGTATCA
Hap_217 CATTGTCGTTTGTTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_218 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCCTGAGCTCGTTCTACTCCACCTGTCTAGTTCATCGTATCA
Hap_219 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTTGTCGTATCA
Hap_220 CATTGTCGTTTATTGACTGCGTTAGCTGAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_221 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTCCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_222 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATTTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_223 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_224 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAGTCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_225 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTCGATATGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_226 CGTTGTCGTTTATTGACTGCGTTAACTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_227 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCG
Hap_228 CATTGTCGTTTATTGATTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTACCA
Hap_229 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCGTCGTATCA
Hap_230 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTGCTCCACCTGTCCAGTTCATCGTATCA
Hap_231 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCCACTCCACCTGTCCAGTTCGTCGTATCA
Hap_232 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCTACCTGTCCAGTTCATCGTATCA
Hap_233 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGGTATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_234 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTTACCATATCA
Hap_235 CATTGCCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_236 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_237 CGTTGTCGTTTATTGACCGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATTGTATCA
Hap_238 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTACGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_239 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCCTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_240 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCCGTCCAGTTCATCGTATCA
Hap_241 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTGTCA
Hap_242 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGGGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_243 CATTGTCGTTTATTGACTGCGTTAGCCAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCCTGAGCTCATTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_244 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGTTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_245 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_246 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATACGTTGTATGAATCTTGAACTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_247 CATTGTCGTTTATTGACTGCGTTAACTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_248 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTCCATCGTATCA
Hap_249 CATTGTCGTTTATTGACTGCGTTAGCTAAAAACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_250 CATTGTTGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTATCCAGTTCATCGTATCA
Hap_251 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAAGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTACCA
Hap_252 CATTGTCGTCTATTGACTGCGTTAGCTAAAGACGAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_253 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTTTACTCCACCTGTCCAGTTCATCGTATCA
Hap_254 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_255 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTCCTACTCCACCTGTCCAGTTCATCATATCA
Hap_256 CATTGTCGTTTGTTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_257 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGTTCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_258 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTAAGCTCGTTCTACTCCACCTGTCCAGTTCACCGTATCA
Hap_259 CATTGTCGTTTATTGATTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_260 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAGTCTCGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_261 CATTGTCGTCTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGCTCATCGTATCA
Hap_262 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTCGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_263 CATTGTCGTTTACTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_264 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAATTCATCGTATCA
Hap_265 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAACCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAATTCATCGTATCA
Hap_266 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATTGCCAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_267 CATTGCCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_268 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTGTGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTGTCA
Hap_269 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_270 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTAAGCTCGTTCTACTCCACTTGTCCAGTTCATCGTATCA
Hap_271 CATTGTCGTTTATTGACTACGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGCCCAGTTCATCGTATCA
Hap_272 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGCTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_273 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAGTCTTGAGCTCGTTCTACTCCGCCTGTCCAGTTCATCGTATCA
Hap_274 CATTGTCGTTTATTGACTACGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_275 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCCTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_276 CATTGTCATTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATAAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_277 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCCCATTCTACTCCACCTGTCCAGTTTATCGTATCA
Hap_278 CGTTGTCGTTTATTGACTGCGTTAGCTAGAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_279 CATTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCCTTCTTTGATACGTTGTATGAATCTTAAGCTCGTTCTACTCCACCTGTCCAGTTCATCGTATCA
Hap_280 CGTTGTCGTTTATTGACTGCGTTAGCTAAAGACAAATCGCTAGAGGCTTTCTTTGATATGTTGTATGAATCTTGAGCTCGTTCTACTCCACCTGTCCAGTTCATTGTATCA
;

END;

 

Begin Traits;

Dimensions NTraits=9;

format labels=yes missing=? separator=Comma;

TraitLabels Austroasiatic HmongMien Mongolic SiniticN SiniticS TaiKadai TibetoBurman Tungusic Turkic;

Matrix

 

Hap_1 17,238,65,361,702,493,204,27,10
Hap_2 5,57,33,234,150,48,45,7,8
Hap_3 1,0,0,1,0,0,0,0,0
Hap_4 3,13,19,85,68,21,34,1,4
Hap_5 3,0,0,0,1,1,0,0,0
Hap_6 1,0,0,0,0,0,0,0,0
Hap_7 0,0,0,6,5,2,0,0,0
Hap_8 0,7,1,21,23,15,6,1,0
Hap_9 0,1,1,14,16,14,1,0,1
Hap_10 0,4,4,42,32,5,14,1,0
Hap_11 0,17,0,2,8,4,9,1,0
Hap_12 0,6,6,19,41,31,21,1,0
Hap_13 0,1,0,1,3,2,0,0,0
Hap_14 0,2,1,8,12,0,2,0,1
Hap_15 0,4,0,4,10,11,0,0,0
Hap_16 0,3,0,34,30,10,9,1,3
Hap_17 0,4,2,5,5,9,5,0,1
Hap_18 0,5,6,35,38,34,3,0,0
Hap_19 0,5,0,12,23,10,1,1,0
Hap_20 0,1,0,0,0,0,0,0,0
Hap_21 0,1,0,1,1,0,0,0,0
Hap_22 0,2,0,0,0,0,0,0,0
Hap_23 0,1,0,0,3,1,0,0,0
Hap_24 0,1,0,2,8,2,1,0,0
Hap_25 0,2,2,21,9,5,1,3,0
Hap_26 0,1,0,2,7,7,0,0,0
Hap_27 0,3,0,32,43,18,1,0,0
Hap_28 0,1,0,0,0,0,0,0,0
Hap_29 0,2,0,0,0,0,0,0,0
Hap_30 0,1,0,0,0,0,0,0,0
Hap_31 0,3,11,22,9,1,10,0,3
Hap_32 0,1,1,0,1,0,0,2,0
Hap_33 0,1,0,0,4,1,0,1,0
Hap_34 0,2,0,14,26,27,7,0,1
Hap_35 0,1,0,0,0,0,0,0,0
Hap_36 0,1,0,2,0,0,1,0,0
Hap_37 0,1,0,0,2,2,1,0,0
Hap_38 0,1,0,1,15,12,0,0,0
Hap_39 0,1,0,1,5,9,0,0,0
Hap_40 0,3,0,23,19,13,4,1,0
Hap_41 0,7,0,0,0,0,6,0,0
Hap_42 0,1,0,0,3,1,0,0,0
Hap_43 0,1,0,1,0,0,0,0,0
Hap_44 0,2,2,0,5,0,0,1,0
Hap_45 0,1,0,0,0,6,2,0,0
Hap_46 0,1,0,3,7,2,0,0,0
Hap_47 0,1,0,0,0,0,0,0,0
Hap_48 0,1,0,1,0,0,0,0,0
Hap_49 0,1,0,2,2,1,0,1,0
Hap_50 0,1,0,1,2,0,0,0,0
Hap_51 0,0,1,17,8,0,5,0,0
Hap_52 0,0,1,0,0,0,0,0,0
Hap_53 0,0,1,2,2,2,0,0,0
Hap_54 0,0,1,1,3,2,0,0,0
Hap_55 0,0,1,0,0,0,0,0,0
Hap_56 0,0,2,14,5,3,1,1,0
Hap_57 0,0,1,0,0,0,0,0,1
Hap_58 0,0,1,0,0,0,0,0,0
Hap_59 0,0,1,2,3,9,0,0,0
Hap_60 0,0,1,0,0,0,0,0,0
Hap_61 0,0,2,0,0,0,0,0,0
Hap_62 0,0,1,0,0,0,0,0,0
Hap_63 0,0,1,2,0,0,0,0,0
Hap_64 0,0,2,0,0,0,0,0,0
Hap_65 0,0,1,0,0,0,0,0,0
Hap_66 0,0,1,1,0,0,0,0,0
Hap_67 0,0,1,3,3,0,0,0,0
Hap_68 0,0,1,0,0,0,0,0,0
Hap_69 0,0,1,0,4,1,0,0,0
Hap_70 0,0,1,0,0,0,0,0,0
Hap_71 0,0,1,0,1,0,0,0,0
Hap_72 0,0,1,0,0,0,0,0,0
Hap_73 0,0,1,0,0,0,0,0,0
Hap_74 0,0,1,1,0,0,0,0,0
Hap_75 0,0,0,4,0,0,0,0,0
Hap_76 0,0,0,1,0,0,0,0,0
Hap_77 0,0,0,1,0,0,0,0,0
Hap_78 0,0,0,2,0,0,0,0,0
Hap_79 0,0,0,1,0,0,0,0,0
Hap_80 0,0,0,1,0,0,0,0,0
Hap_81 0,0,0,3,0,2,0,0,0
Hap_82 0,0,0,3,7,0,1,0,0
Hap_83 0,0,0,1,0,0,1,0,0
Hap_84 0,0,0,2,0,0,0,0,0
Hap_85 0,0,0,1,0,0,1,0,0
Hap_86 0,0,0,1,1,0,0,0,0
Hap_87 0,0,0,1,0,0,0,0,0
Hap_88 0,0,0,2,3,1,0,0,0
Hap_89 0,0,0,1,0,0,0,0,0
Hap_90 0,0,0,7,8,2,1,0,0
Hap_91 0,0,0,1,0,0,0,0,0
Hap_92 0,0,0,1,0,0,0,0,0
Hap_93 0,0,0,1,0,0,0,0,0
Hap_94 0,0,0,1,5,3,1,0,0
Hap_95 0,0,0,1,0,0,0,0,0
Hap_96 0,0,0,1,0,0,0,0,0
Hap_97 0,0,0,2,0,0,0,0,0
Hap_98 0,0,0,1,0,0,0,0,0
Hap_99 0,0,0,1,0,0,0,0,0
Hap_100 0,0,0,4,0,0,0,0,0
Hap_101 0,0,0,1,0,0,0,0,0
Hap_102 0,0,0,1,0,0,0,0,0
Hap_103 0,0,0,3,1,1,0,0,0
Hap_104 0,0,0,1,3,5,0,0,0
Hap_105 0,0,0,2,1,0,0,0,0
Hap_106 0,0,0,2,0,0,2,0,0
Hap_107 0,0,0,1,0,0,0,0,0
Hap_108 0,0,0,1,1,0,0,0,0
Hap_109 0,0,0,2,1,0,0,0,0
Hap_110 0,0,0,4,4,1,0,0,0
Hap_111 0,0,0,1,2,0,0,0,0
Hap_112 0,0,0,1,0,0,0,0,0
Hap_113 0,0,0,1,0,0,0,0,0
Hap_114 0,0,0,1,0,0,0,0,0
Hap_115 0,0,0,2,0,0,1,0,0
Hap_116 0,0,0,1,0,0,0,0,0
Hap_117 0,0,0,1,0,0,0,0,0
Hap_118 0,0,0,1,0,0,0,0,0
Hap_119 0,0,0,1,0,0,0,0,0
Hap_120 0,0,0,1,1,0,0,0,0
Hap_121 0,0,0,1,4,2,0,0,0
Hap_122 0,0,0,1,1,0,0,0,0
Hap_123 0,0,0,1,0,1,0,0,0
Hap_124 0,0,0,2,0,0,0,0,0
Hap_125 0,0,0,3,3,3,1,0,0
Hap_126 0,0,0,1,0,0,0,0,0
Hap_127 0,0,0,1,0,0,0,0,0
Hap_128 0,0,0,2,0,0,0,0,1
Hap_129 0,0,0,1,0,0,0,0,0
Hap_130 0,0,0,1,6,0,0,0,0
Hap_131 0,0,0,1,1,0,0,0,0
Hap_132 0,0,0,1,0,0,0,0,0
Hap_133 0,0,0,1,0,1,0,0,0
Hap_134 0,0,0,1,1,0,0,0,0
Hap_135 0,0,0,1,0,1,1,0,0
Hap_136 0,0,0,1,0,0,1,0,0
Hap_137 0,0,0,1,0,0,0,0,0
Hap_138 0,0,0,1,0,0,0,0,0
Hap_139 0,0,0,1,1,1,0,0,0
Hap_140 0,0,0,1,0,0,0,0,0
Hap_141 0,0,0,1,3,1,0,0,0
Hap_142 0,0,0,1,0,0,0,0,0
Hap_143 0,0,0,1,0,0,0,0,0
Hap_144 0,0,0,2,0,0,0,0,0
Hap_145 0,0,0,1,0,0,0,0,0
Hap_146 0,0,0,1,1,0,0,0,0
Hap_147 0,0,0,1,0,0,0,0,0
Hap_148 0,0,0,1,3,2,1,0,0
Hap_149 0,0,0,1,0,0,0,0,0
Hap_150 0,0,0,1,0,0,0,0,0
Hap_151 0,0,0,2,2,0,0,0,0
Hap_152 0,0,0,2,0,3,0,0,0
Hap_153 0,0,0,1,0,0,0,0,0
Hap_154 0,0,0,1,0,0,0,0,0
Hap_155 0,0,0,1,0,0,0,0,0
Hap_156 0,0,0,1,0,0,0,0,0
Hap_157 0,0,0,1,0,0,0,0,0
Hap_158 0,0,0,1,0,0,0,0,0
Hap_159 0,0,0,1,0,0,0,0,0
Hap_160 0,0,0,1,1,0,0,0,0
Hap_161 0,0,0,1,0,0,0,0,0
Hap_162 0,0,0,2,0,0,0,0,0
Hap_163 0,0,0,1,0,0,0,0,0
Hap_164 0,0,0,1,0,0,0,0,0
Hap_165 0,0,0,1,0,0,0,0,0
Hap_166 0,0,0,1,0,0,0,0,0
Hap_167 0,0,0,1,0,0,0,0,0
Hap_168 0,0,0,1,1,0,0,0,0
Hap_169 0,0,0,1,1,0,0,0,0
Hap_170 0,0,0,2,3,1,1,0,0
Hap_171 0,0,0,1,0,0,0,0,0
Hap_172 0,0,0,1,0,0,0,0,0
Hap_173 0,0,0,1,1,0,0,0,0
Hap_174 0,0,0,1,0,0,0,0,0
Hap_175 0,0,0,1,1,0,0,0,0
Hap_176 0,0,0,1,0,0,0,0,0
Hap_177 0,0,0,1,2,1,3,0,0
Hap_178 0,0,0,1,0,0,0,0,0
Hap_179 0,0,0,1,0,0,0,0,0
Hap_180 0,0,0,0,2,0,0,0,0
Hap_181 0,0,0,0,1,0,0,0,0
Hap_182 0,0,0,0,1,0,0,0,0
Hap_183 0,0,0,0,2,0,0,0,0
Hap_184 0,0,0,0,1,0,0,0,0
Hap_185 0,0,0,0,1,0,0,0,0
Hap_186 0,0,0,0,1,0,0,0,0
Hap_187 0,0,0,0,1,0,0,0,0
Hap_188 0,0,0,0,1,0,0,0,0
Hap_189 0,0,0,0,10,2,0,0,0
Hap_190 0,0,0,0,1,0,0,0,0
Hap_191 0,0,0,0,7,0,0,0,0
Hap_192 0,0,0,0,5,1,0,0,0
Hap_193 0,0,0,0,1,0,0,0,0
Hap_194 0,0,0,0,1,0,0,0,0
Hap_195 0,0,0,0,1,1,0,0,0
Hap_196 0,0,0,0,1,0,0,0,0
Hap_197 0,0,0,0,5,3,1,0,0
Hap_198 0,0,0,0,1,0,0,2,0
Hap_199 0,0,0,0,1,0,0,0,0
Hap_200 0,0,0,0,2,0,0,0,0
Hap_201 0,0,0,0,1,0,0,0,0
Hap_202 0,0,0,0,2,0,0,0,0
Hap_203 0,0,0,0,1,0,0,0,0
Hap_204 0,0,0,0,1,1,0,0,0
Hap_205 0,0,0,0,1,0,0,0,0
Hap_206 0,0,0,0,1,0,0,0,0
Hap_207 0,0,0,0,2,1,0,0,0
Hap_208 0,0,0,0,1,0,0,0,0
Hap_209 0,0,0,0,1,0,0,0,0
Hap_210 0,0,0,0,1,0,0,0,0
Hap_211 0,0,0,0,3,0,0,0,0
Hap_212 0,0,0,0,2,0,0,0,0
Hap_213 0,0,0,0,1,0,0,0,0
Hap_214 0,0,0,0,1,0,0,0,0
Hap_215 0,0,0,0,1,0,0,0,0
Hap_216 0,0,0,0,1,0,0,0,0
Hap_217 0,0,0,0,1,0,0,0,0
Hap_218 0,0,0,0,1,0,0,0,0
Hap_219 0,0,0,0,1,0,0,0,0
Hap_220 0,0,0,0,1,0,0,0,0
Hap_221 0,0,0,0,1,0,0,0,0
Hap_222 0,0,0,0,3,0,0,0,0
Hap_223 0,0,0,0,1,1,0,0,0
Hap_224 0,0,0,0,1,1,0,0,0
Hap_225 0,0,0,0,1,0,0,0,0
Hap_226 0,0,0,0,1,0,0,0,0
Hap_227 0,0,0,0,1,0,0,0,0
Hap_228 0,0,0,0,1,0,0,0,0
Hap_229 0,0,0,0,1,1,0,0,0
Hap_230 0,0,0,0,1,0,0,0,0
Hap_231 0,0,0,0,1,0,0,0,0
Hap_232 0,0,0,0,1,1,0,0,0
Hap_233 0,0,0,0,1,1,0,2,0
Hap_234 0,0,0,0,1,0,0,0,0
Hap_235 0,0,0,0,1,0,0,0,0
Hap_236 0,0,0,0,1,0,0,0,0
Hap_237 0,0,0,0,1,0,0,0,0
Hap_238 0,0,0,0,1,0,0,0,0
Hap_239 0,0,0,0,1,0,0,0,0
Hap_240 0,0,0,0,1,0,0,0,0
Hap_241 0,0,0,0,1,0,0,0,0
Hap_242 0,0,0,0,1,0,0,0,0
Hap_243 0,0,0,0,1,0,0,0,0
Hap_244 0,0,0,0,0,2,0,0,0
Hap_245 0,0,0,0,0,1,0,0,0
Hap_246 0,0,0,0,0,1,0,0,0
Hap_247 0,0,0,0,0,1,0,0,0
Hap_248 0,0,0,0,0,1,0,0,0
Hap_249 0,0,0,0,0,1,0,0,0
Hap_250 0,0,0,0,0,1,0,0,0
Hap_251 0,0,0,0,0,1,0,0,0
Hap_252 0,0,0,0,0,1,0,0,0
Hap_253 0,0,0,0,0,1,0,0,0
Hap_254 0,0,0,0,0,1,0,0,0
Hap_255 0,0,0,0,0,1,0,0,0
Hap_256 0,0,0,0,0,1,0,0,0
Hap_257 0,0,0,0,0,1,0,0,0
Hap_258 0,0,0,0,0,1,1,0,0
Hap_259 0,0,0,0,0,1,0,0,0
Hap_260 0,0,0,0,0,1,0,0,0
Hap_261 0,0,0,0,0,1,0,0,0
Hap_262 0,0,0,0,0,1,0,0,0
Hap_263 0,0,0,0,0,1,0,0,0
Hap_264 0,0,0,0,0,3,0,0,0
Hap_265 0,0,0,0,0,1,0,0,0
Hap_266 0,0,0,0,0,0,1,0,0
Hap_267 0,0,0,0,0,0,2,0,0
Hap_268 0,0,0,0,0,0,1,0,0
Hap_269 0,0,0,0,0,0,3,0,0
Hap_270 0,0,0,0,0,0,1,0,0
Hap_271 0,0,0,0,0,0,1,0,0
Hap_272 0,0,0,0,0,0,2,0,0
Hap_273 0,0,0,0,0,0,1,0,0
Hap_274 0,0,0,0,0,0,3,0,0
Hap_275 0,0,0,0,0,0,1,0,0
Hap_276 0,0,0,0,0,0,1,0,0
Hap_277 0,0,0,0,0,0,1,0,0
Hap_278 0,0,0,0,0,0,1,0,0
Hap_279 0,0,0,0,0,0,0,0,1
Hap_280 0,0,0,0,0,0,0,0,1
;

End;
复制<br>所以，需要准备频率和序列的数据。<br><br>确保之前使用Arlequin已经对数据进行了分析，如果没有，请看<a data-href="DnaSP：AMOVA及Fst分析软件操作" href="软件\其它生信软件\a-j\dnasp：amova及fst分析软件操作.html" class="internal-link" target="_self" rel="noopener nofollow">DnaSP：AMOVA及Fst分析软件操作</a>。<br>注意勾选分析方法应该保证如下（可以多不能少！）：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041253151.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041253986.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>分析完成之后，会得到结果文件，找到那个最大的xml文件，用文本编辑器打开，然后找到Haplotype frequencies in populations，将其转化成如下格式。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041255430.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041255182.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>Hap_1 10,0,0,0
Hap_2 19,9,12,10
Hap_3 1,0,0,0
Hap_4 1,0,0,0
Hap_5 1,0,0,0
Hap_6 0,0,1,0
Hap_7 0,0,1,0
Hap_8 0,0,2,0
Hap_9 0,0,1,0
Hap_10 0,0,0,7
Hap_11 0,0,0,2
Hap_12 0,0,0,1
复制<br>雷区
注意，第一列与第二列之间需要以空格分隔，其余列需要以逗号分隔。并且中间不能出现任何空行或者多余的空格。不允许使用逗号和制表符。
<br>我在这里写了一个python脚本，可以把频率的excel表格转化成第一列与第二列之间需要以空格分隔，其余列需要以逗号分隔。但是我不确定是否用得上。<br>import pandas as pd
# 读取Excel文件
file_path = 'C:/Users/a/Desktop/工作簿1.xlsx' # Excel文件路径
df = pd.read_excel(file_path)
# 将DataFrame转换为所需格式的字符串
formatted_lines = []
for index, row in df.iterrows():
# 使用iloc通过位置索引访问，将第一列与其余列分开处理
# 第一列后空格，其余列逗号分隔
  formatted_line = f"{row.iloc[0]} " + ",".join([str(row.iloc[i]) for i in range(1, len(row))])
  formatted_lines.append(formatted_line)
 # 将处理过的数据保存到文本文件
output_file_path = 'C:/Users/a/Desktop/Fuckyou.txt' # 输出文件的路径
with open(output_file_path, 'w', encoding='utf-8') as output_file:
  output_file.write("\n".join(formatted_lines))
print(f"文件已成功保存到 {output_file_path}")
复制<br><br>还是在刚才的最大的xml文件中，找到序列信息，并转化成如下格式：<br>Hap_1   TACATCAGGGTAG
Hap_2   TACATCAGGGTAC
Hap_3   TACATTAGGGTAC
Hap_4   TCCACCAGGGAAC
Hap_5   TCCATCAGGGTAC
Hap_6   TACATCAGGGTCC
Hap_7   TACAACAGCTTAC
Hap_8   TACAACAGGGTAC
Hap_9   CACCACCACGTAC
Hap_10  TATAACAAGGTAC
Hap_11  TACTTCAGGGAAC
Hap_12  TACAACAGGGAAC
复制<br>雷区
注意，第一列与第二列之间需要以空格分隔，其余列必须字数相等，其它地区不能出现多余的空格。不允许使用逗号和制表符。
<br>现在修改模板：将黑色部分替换成自己的数据。可以用TXT打开，然后将后缀改为nex。<br>
<img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041251837.png" referrerpolicy="no-referrer"><br>雷区
注意，需要修改 ntax、nchar、NTraits、TrautLables。
<br>ntax：有多少行数据。<br>
nchar：每一条序列有多少个字符。<br>
NTraits：每一行频率有多少特征。<br>
TrautLables：每一行频率的每个特征分别对应的标签。<br>
例如，上述中NTraits的数目是4，对应着TrautLables中的4个地区。<br>雷区
注意NTraits和TrautLables的数目必须一一对应。<br>
注意TrautLables之间的分隔必须为空格。不允许使用逗号和制表符。
<br><br>打开PopART，点击File→Open。打开刚才修改了后缀的nex文件。<br>如果这一步出现报错，可能的原因很多，但应该都是触碰到了上述雷区。仔细核对是否存在多余的空格。<br>还有一件事，文件名和路径不能出现中文！！！<br>
<br>点击Network→TCS Network。
<br>点击Edit→Set trait colour，自定义喜欢的颜色。
<br>完成！
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041322305.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\其它生信软件\k-s\popart：基于fst绘制network图.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/PopArt：基于Fst绘制Network图.md</guid><pubDate>Fri, 23 Aug 2024 08:26:29 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041250489.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402041250489.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[软件下载]]></title><description><![CDATA[ 
 <br>这个版本由Exelixis实验室于2024年4月30日发布。软件开发者包括Alexey M. Kozlov和Alexandros Stamatakis，还有多位贡献者如Diego Darriba、Tomas Flouri等。软件的最新版本可以在其GitHub页面找到，使用过程中的问题或建议可以通过Google论坛讨论。<br><br>这是一个预编译的版本，Linux 应该直接可以使用：<a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hflVSbd_Xz0UIOzsZw?e=ezKznm" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hflVSbd_Xz0UIOzsZw?e=ezKznm" target="_blank">raxml-ng</a><br><br>命令行输入 raxml-ng [选项] 来运行程序。<br><br>
<br>--help：显示帮助信息。
<br>--version：显示版本信息。
<br>--evaluate：评估一棵树的似然值（含模型和枝长优化）。
<br>--search：最大似然树搜索（默认：10棵拟合树和10棵随机起始树）。
<br>--bootstrap：自动检测重复次数的Bootstrap分析。
<br>--all：一体化分析（ML树搜索 + Bootstrap）。
<br>--support：对一棵参考树（如最佳ML树）和一组重复树（如Bootstrap分析所得）计算双分支支持。
<br>更多命令及详细参数见使用说明。
<br><br>
<br>--tree：指定起始树，可以是随机、拟合或指定的新疆文件。
<br>--msa：指定比对文件。
<br>--msa-format：指定比对文件格式，如FASTA、PHYLIP等。
<br>--data-type：指定数据类型，如DNA、氨基酸等。
<br>更多选项如输出文件前缀、日志详细程度、精度等也可设置。
<br><br>
<br>--model：模型设置或指定分区文件。
<br>--brlen：分区间枝长的关联性。
<br>--opt-model：是否优化所有模型参数。
<br>--opt-branches：是否优化所有枝长。
<br>更多模型相关的设置包括优化方法、似然值精度等。
<br><br>使用 GTR+G 模型建树<br>./raxml-ng --msa testDNA.fa --model GTR+G
复制<br>一步到位<br>./raxml-ng --all --msa testAA.fa --model LG+G8+F --tree pars{10} --bs-trees 200
复制<br># 进入工作目录
cd Raxml-ng/

# 运行 RAxML-NG 构建最大似然树
raxml-ng --all \
         --msa trim_out.fasta \
         --model GTR+G+I \
         --threads 32 \
         --bs-trees 20 \ # 生成 20 个 Bootstrap 树以评估树的稳健性。
         --prefix ml_tree
复制]]></description><link>软件\其它生信软件\k-s\raxml-ng：使用基础.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/RAxML-NG：使用基础.md</guid><pubDate>Mon, 09 Sep 2024 02:37:32 GMT</pubDate></item><item><title><![CDATA[软甲下载]]></title><description><![CDATA[ 
 <br><br>可以谷歌搜索下载，也可以用我这个版本，该版本包括了 win和linux，但是没有 GUI： <a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hoZgWzZl00vhYAj7wg?e=upQntR" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hoZgWzZl00vhYAj7wg?e=upQntR" target="_blank">SLiM.zip</a><br><br>SLiM home page: <a rel="noopener nofollow" class="external-link" href="http://messerlab.org/slim/" target="_blank">http://messerlab.org/slim/</a><br>
slim-announce mailing list: <a rel="noopener nofollow" class="external-link" href="https://groups.google.com/d/forum/slim-announce" target="_blank">https://groups.google.com/d/forum/slim-announce</a><br>
slim-discuss mailing list: <a rel="noopener nofollow" class="external-link" href="https://groups.google.com/d/forum/slim-discuss" target="_blank">https://groups.google.com/d/forum/slim-discuss</a><br><br>首先在 Linux 解压，然后按照如下进行安装：<br>cd /SLiM # 进入目录
mkdir build # 创建文件夹
# 编译
cd build
cmake ..
cmake --build .
# 安装
sudo make install
# 运行软件
/mnt/e/Scientifc_software/SLiM/build/slim
复制<br>如果运行成功，就会出现如下提示：<br>SLiM version 4.3, built Sep 23 2024 14:49:28.
Git commit SHA-1: unknown (built from a non-Git source archive)
This is a RELEASE build of SLiM.
This is a NON-PARALLEL (SINGLE-THREADED) build of SLiM.

SLiM is a product of the Messer Lab, http://messerlab.org/
Copyright 2013-2024 Philipp Messer.  All rights reserved.

By Benjamin C. Haller, http://benhaller.com/, and Philipp Messer.

---------------------------------------------------------------------------------

SLiM home page: http://messerlab.org/slim/
slim-announce mailing list: https://groups.google.com/d/forum/slim-announce
slim-discuss mailing list: https://groups.google.com/d/forum/slim-discuss

---------------------------------------------------------------------------------

SLiM is free software: you can redistribute it and/or modify it under the terms
of the GNU General Public License as published by the Free Software Foundation,
either version 3 of the License, or (at your option) any later version.

SLiM is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR
PURPOSE.  See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
SLiM.  If not, see &lt;http://www.gnu.org/licenses/&gt;.

---------------------------------------------------------------------------------

usage: slim -v[ersion] | -u[sage] | -h[elp] | -testEidos | -testSLiM |
   [-l[ong] [&lt;l&gt;]] [-s[eed] &lt;seed&gt;] [-t[ime]] [-m[em]] [-M[emhist]] [-x]
   [-d[efine] &lt;def&gt;] [&lt;script file&gt;]
复制]]></description><link>软件\其它生信软件\k-s\slim：群体演化历史建模软件安装教程.html</link><guid isPermaLink="false">软件/其它生信软件/K-S/SLiM：群体演化历史建模软件安装教程.md</guid><pubDate>Tue, 24 Sep 2024 08:54:03 GMT</pubDate></item><item><title><![CDATA[1.染色体长度信息]]></title><description><![CDATA[ 
 <br>原教程地址：<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E8%A7%86%E9%A2%91/Advanced%20Circos-iMeta.mp4?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=10000000001718845000&amp;Signature=KTKSIONYKnseaqz8Q%2BEOtqenC7M%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E8%A7%86%E9%A2%91/Advanced%20Circos-iMeta.mp4?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=10000000001718845000&amp;Signature=KTKSIONYKnseaqz8Q%2BEOtqenC7M%3D" target="_blank">Advanced Circos-iMeta.mp4</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091343.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091411.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
通过这里可以获得以制表符分隔的长度信息文件，例如 length.txt：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091514.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
将这个文件放进 Set Input ChrLen File (ChrlD It Length) 选项中。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091837.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
颜色可以通过 RGB 来表示。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091943.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
你可以通过自动化的流程生成需要的颜色配色：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620092059.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
例如：<br>Sometimes, we may need to prepare a color scheme for classification. 
To facilate this process, this function is developed.
Users feed a tab-delimited file and set corresponding color column index in TBtools.
TBtools will append random color code 
to each line at the last column.
For example,
====input====
Chr1 10000 20000
Chr2 40000 50000
====output===
Chr1 10000 20000 124,56,88
Chr2 40000 50000 23,124,56
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620092447.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
然后我们通过 Excel 软件删除多余的信息，只保留如下信息：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620092545.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第二列是 基因名字，第三和第四列是 基因的起始和终止位置。当然，你肯定只需要从中提取出你需要的基因，你可以通过 Excel 表格来实现操作。<br>基因文件到哪里下载？
在 NCBI 官网可以下载 gff 文件。
<br>微微调整一下文字的颜色等：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620092933.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>由于在这里我只想要绘制 NUMTs 的情况，所以就不运行常规的程序了。<br>什么是常规的程序？
One Step McScanX（被包含在 TBtools 中），可以通过 fasta 文件和 gff 文件查找基因的调控关系，从而获得不同位置之间的 link。
<br>总而言之，我们可以获得的最终格式是这样的：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620093253.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
其中：<br>
<br>第一列是一条染色体（例如调控基因所在的染色体）
<br>第四列是与第一列有关系的另一条染色体（例如被调控基因所在的染色体）
<br>第二三列是调控基因所在的染色体的基因的起始和终止的位置
<br>第五六列是被调控基因所在的染色体的基因的起始和终止的位置
<br>第七列是颜色。
<br>你可以在下面添加各种各样的图形（例如热图、组装质量图等等）：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620094016.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620094113.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
\<br>这个软件似乎绘制出来的每个染色体的长度是按照碱基长度来的。不能把 mtDNA 给放大，其他染色体缩小吗？]]></description><link>软件\其它生信软件\t-z\tbtools：advanced-circos：绘制numt弦图教程.html</link><guid isPermaLink="false">软件/其它生信软件/T-Z/TBtools：Advanced Circos：绘制NUMT弦图教程.md</guid><pubDate>Fri, 21 Jun 2024 01:46:45 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091343.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240620091343.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[安装 trimal]]></title><description><![CDATA[ 
 <br><br>trimal 可以从官方网站或者使用包管理工具（如 conda）安装：<br>conda install -c bioconda trimal
复制<br><br>trimal 的基本命令结构如下：<br>trimal -in inputfile -out outputfile [options]
复制<br>其中 inputfile 是输入文件，outputfile 是输出文件，[options] 是你可以添加的其他选项。<br><br>
<br>-automated1：自动选择过滤的参数，通常能够达到不错的平衡。
<br>-gt：设定一个阈值，过滤掉那些在序列中缺失数据百分比高于该阈值的列。例如，-gt 0.9 会保留至少有90%有效数据的列。
<br>-st：与 -gt 类似，但适用于非缺失的序列。
<br>-cons：生成一个基于整体保守性的修剪版本。
<br>-resoverlap 和 -seqoverlap：设定最小重叠的百分比，用于保留或剔除某些序列或位置。
<br><br>假设你有一个名为 example.fasta 的 FASTA 格式文件，并想删除那些至少有20%缺失数据的列，你可以运行：<br>trimal -in example.fasta -out trimmed.fasta -gt 0.8
复制<br><br>
<br>-h：打印此信息并显示一些示例。
<br>--version：打印 trimAl 版本。
<br>-in &lt;inputfile&gt;：输入文件，支持多种格式（clustal, fasta, NBRF/PIR, nexus, phylip3.2, phylip）。
<br>-compareset &lt;inputfile&gt;：输入包含要比较的比对的文件路径的列表。
<br>-forceselect &lt;inputfile&gt;：在文件比较方法中强制选择给定的输入文件。
<br>-backtrans &lt;inputfile&gt;：使用编码序列文件对给定的氨基酸比对进行反向翻译。
<br>-ignorestopcodon：在输入的编码序列中忽略停止密码子。
<br>-splitbystopcodon：将输入的编码序列根据第一个出现的停止密码子进行分割。
<br>-matrix &lt;inputfile&gt;：输入用户定义的相似性矩阵文件（默认为 Blosum62）。
<br>--alternative_matrix &lt;name&gt;：选择一个已加载的替代相似性矩阵。目前仅提供 'degenerated_nt_identity'。
<br>-out &lt;outputfile&gt;：输出比对，格式与输入文件相同（默认为标准输出）。
<br>-htmlout &lt;outputfile&gt;：以 HTML 文件形式获取 trimal 工作的摘要。
<br>-keepheader：保留原始序列头部，包括非字母数字字符。仅适用于输入的 FASTA 格式文件。
<br>-nbrf：以 NBRF/PIR 格式输出文件。
<br>-mega：以 MEGA 格式输出文件。
<br>-nexus：以 NEXUS 格式输出文件。
<br>-clustal：以 CLUSTAL 格式输出文件。
<br>-fasta：以 FASTA 格式输出文件。
<br>-fasta_m10：以 FASTA 格式输出文件，序列名称长度最多 10 个字符。
<br>-phylip：以 PHYLIP/PHYLIP4 格式输出文件。
<br>-phylip_m10：以 PHYLIP/PHYLIP4 格式输出文件，序列名称长度最多 10 个字符。
<br>-phylip_paml：输出与 PAML 兼容的 PHYLIP 格式文件。
<br>-phylip_paml_m10：输出与 PAML 兼容的 PHYLIP 格式文件，序列名称长度最多 10 个字符。
<br>-phylip3.2：以 PHYLIP3.2 格式输出文件。
<br>-phylip3.2_m10：以 PHYLIP3.2 格式输出文件，序列名称长度最多 10 个字符。
<br>-complementary：获取互补比对。
<br>-colnumbering：获取旧比对与新比对之间列的对应关系。
<br>-selectcols { n,l,m-k }：选择要从比对中移除的列。范围：[0 - (列数 - 1)]。
<br>-selectseqs { n,l,m-k }：选择要从比对中移除的序列。范围：[0 - (序列数 - 1)]。
<br>-gt -gapthreshold &lt;n&gt;：允许具有缺口的序列的分数（1 - 允许的序列缺口比例）。范围：[0 - 1]
<br>-st -simthreshold &lt;n&gt;：允许的最小平均相似性。范围：[0 - 1]
<br>-ct -conthreshold &lt;n&gt;：允许的最小一致性值。范围：[0 - 1]
<br>-cons &lt;n&gt;：在原始比对中要保留的位置的最小百分比。范围：[0 - 100]
<br>-nogaps：删除比对中所有带有缺口的位置。
<br>-noallgaps：删除仅由缺口组成的列。
<br>-keepseqs：即使序列仅由缺口组成也保留序列。
<br>-gappyout：使用基于缺口分布信息的“gappyout”模式的自动选择。（见用户指南）
<br>-strict：使用“strict”模式的自动选择。（见用户指南）
<br>-strictplus：使用“strictplus”模式的自动选择。（见用户指南）
<br>-automated1：使用基于相似性统计的自动方法的启发式选择。（见用户指南）
<br>-terminalonly：仅外部边界（没有缺口的第一列和最后一列）的列可以根据所选方法被修剪
<br>--set_boundaries { l,r }：手动设置左（l）和右（r）边界 - 仅边界外的列可以根据所选方法被修剪。范围：[0 - (列数 - 1)]
<br>-block &lt;n&gt;：在修剪的比对中保留的最小列块大小。可与手动和自动（gappyout）方法一起使用
<br>-resoverlap：位置与列中其他位置的最小重叠，以被视为“好位置”。范围：[0 - 1]。
<br>-seqoverlap：序列必须保留的“好位置”的最小百分比。范围：[0 - 100]。
<br>-clusters &lt;n&gt;：从给定比对中获取最具代表性的 N 个序列。范围：[1 - (序列数)]
<br>-maxidentity &lt;n&gt;：为给定身份阈值获取代表性序列。范围：[0 - 1].
<br>-w &lt;n&gt;：（半）窗口大小，位置 i 的得分是窗口（i - n）到（i + n）的平均值。
<br>-gw &lt;n&gt;：（半）窗口大小仅适用于基于缺口的统计/方法。
<br>-sw &lt;n&gt;：（半）窗口大小仅适用于基于相似性的统计/方法。
<br>-cw &lt;n&gt;：（半）窗口大小仅适用于基于一致性的统计/方法。
<br>-sgc：为输入比对的每一列打印缺口得分。
<br>-sgt：为输入比对打印累积缺口得分。
<br>-ssc：为每一列打印相似性得分。
<br>-sst：为输入比对打印累积相似性得分。
<br>-sfc：为选定比对的每一列打印成对总和得分
<br>-sft：为选定比对打印累积成对总和得分
<br>-sident：为输入比对中所有序列打印身份得分矩阵。（见用户指南）
<br>-soverlap：为输入比对中所有序列打印重叠得分矩阵。（见用户指南）
<br>这些参数涵盖了从基本的文件输入输出格式选择到更复杂的比对修剪和统计评分方法。]]></description><link>软件\其它生信软件\t-z\trimal：自动修剪序列不可靠区域.html</link><guid isPermaLink="false">软件/其它生信软件/T-Z/Trimal：自动修剪序列不可靠区域.md</guid><pubDate>Sun, 28 Jul 2024 08:36:15 GMT</pubDate></item><item><title><![CDATA[利用代理使Onedrive下载速度起飞]]></title><description><![CDATA[ 
 <br>我发现下面两个网址似乎为 onedrive 的下载提供验证，直连能够提供很快的下载速度：<br>2024/07/13 15:57:52 127.0.0.1:50827 accepted //ecs.office.com:443 [http -&gt; direct]
2024/07/13 15:57:56 127.0.0.1:50830 accepted //self.events.data.microsoft.com:443 [http -&gt; direct]
复制<br>只需要在 v2rayn 软件的路由设置中添加 direct 规则：<br>ecs.office.com,
self.events.data.microsoft.com,
inputsuggestions.msdxcdn.microsoft.com
复制<br>然后开始同步，速度就会起飞。]]></description><link>软件\生活窍门\利用代理使onedrive下载速度起飞.html</link><guid isPermaLink="false">软件/生活窍门/利用代理使Onedrive下载速度起飞.md</guid><pubDate>Sat, 13 Jul 2024 08:00:47 GMT</pubDate></item><item><title><![CDATA[一些尝试]]></title><description><![CDATA[ 
 <br><br>我尝试了很多种方法想要构建自己的个人网址，例如网上比较火热的Digital garden,但是很显然，我失败了。因为通过vercel部署会导致DNS污染特别快。因此，我在寻求更加合适的方法。<br>
现在，我利用另一个插件和Github desktop进行发布。<br><br>第三方库中下载 Webpage HTML Export，然后进行如下设置：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155015.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155026.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里有2个地方可以进行设置，仓库的标题可以设置为自己想要的名字。另外，建议不要删除旧的文件，因为我发现一旦开启这个选项，似乎会导致没有修改的文件被删除。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155331.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里每次导出之后点击save，就可以保存上次导出的文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155419.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里设置导出仓库位置。<br><br>下载一个Github Desktop，然后创建一个Repo，选定文件夹内作为仓库。例如创建在 F:\obsidian-publish，创建成功之后会出现下列文件：<br>F:\obsidian-publish\obsidian-notebook\.gitattributes
F:\obsidian-publish\obsidian-notebook\README.md
复制<br>然后手动创建一个文件夹 F:\obsidian-publish\obsidian-notebook\docs，现在，应该存在如下文件：<br>F:\obsidian-publish\obsidian-notebook\docs
F:\obsidian-publish\obsidian-notebook\.gitattributes
F:\obsidian-publish\obsidian-notebook\README.md
复制<br>我们需要把刚才导出来的文件全部放在 F:\obsidian-publish\obsidian-notebook\docs 中，然后我们可以得到类似如下文件：<br>F:\obsidian-publish\obsidian-notebook\docs
F:\obsidian-publish\obsidian-notebook\docs\主页\
F:\obsidian-publish\obsidian-notebook\docs\lib
复制<br>然后打开 Github Desktop：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155954.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621160110.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>现在你的文件全部在这里：<br>
<a data-tooltip-position="top" aria-label="https://victory-hugo.github.io/obsidian-notebook/%E4%BD%A0%E7%9A%84obsidian%E6%96%87%E4%BB%B6.html" rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/obsidian-notebook/%E4%BD%A0%E7%9A%84obsidian%E6%96%87%E4%BB%B6.html" target="_blank">https://victory-hugo.github.io/obsidian-notebook/你的obsidian文件.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/obsidian-notebook/home.html" target="_blank">https://victory-hugo.github.io/obsidian-notebook/home.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/obsidian-notebook/homepage.html" target="_blank">https://victory-hugo.github.io/obsidian-notebook/homepage.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/obsidian-notebook/Home/homepage.html" target="_blank">https://victory-hugo.github.io/obsidian-notebook/Home/homepage.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://github.com/Victory-Hugo/obsidian-notebook/blob/main/docs/home/homepage.html" target="_blank">https://github.com/Victory-Hugo/obsidian-notebook/blob/main/docs/home/homepage.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/obsidian-notebook/home/homepage.html" target="_blank">https://victory-hugo.github.io/obsidian-notebook/home/homepage.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://victory-hugo.github.io/biglin/home/homepage.html" target="_blank">https://victory-hugo.github.io/biglin/home/homepage.html</a>]]></description><link>软件\生活窍门\如何利用obsidian构建自己的网址？.html</link><guid isPermaLink="false">软件/生活窍门/如何利用Obsidian构建自己的网址？.md</guid><pubDate>Mon, 04 Nov 2024 12:44:31 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155015.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240621155015.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[安装 VSC]]></title><description><![CDATA[ 
 <br>教程来源：<a data-tooltip-position="top" aria-label="https://mdnice.com/writing/7c6fe4c344b645f7867d29580d7616c4" rel="noopener nofollow" class="external-link" href="https://mdnice.com/writing/7c6fe4c344b645f7867d29580d7616c4" target="_blank"><span></span> VS Code | 在VS Code中搭建你的R语言运行环境吧！~（图文介绍超详细） - mdnice 墨滴</a><img class="emoji" draggable="false" alt="🤯" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f92f.svg" height="18px" style="max-width: 100%;"><br><br>官网： <a rel="noopener nofollow" class="external-link" href="https://code.visualstudio.com/" target="_blank">https://code.visualstudio.com/</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520112315.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我只建议安装这一个插件。因为这个插件已经整合了 R LSP Client 的插件，现在已经整合到这个里面了，无需单独安装。<br><br>接着我们再 R 中输入下面这段代码，选择好离你比较近的镜像后，安装 languageserver。<br>install.packages("languageserver")
复制<br><br>自从 R 语言的版本升级之后，radian 就会出现一些莫名其妙的报错，作者也无法解决。因此我们不建议安装 radian。<br><br>找到你安装 R 的路径，在这里我是如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520112630.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>接着我们装一下图形输出包，httpgd，在R中运行下面这段代码吧。<br>
install.packages("httpgd") <br><br>接着我们在VS Code的设置中输入 r.plot.useHttpgd，启用 httpgd。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520112752.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>由于VS Code和我们在R studio中用的快捷键会不同，所以我们需要做一些修改，打开快捷键设置我们会进入下面这个界面，点击这个<img class="emoji" draggable="false" alt="👇" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f447.svg" height="18px" style="max-width: 100%;">。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520112822.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520112850.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用Windows的小伙伴可以复制下面这段代码，粘贴即可。<img class="emoji" draggable="false" alt="😘" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f618.svg" height="18px" style="max-width: 100%;"><br>[  
{  
"key": "alt+-",  
"command": "type",  
"when": "editorLangId == r &amp;&amp; editorTextFocus || editorLangId == rmd &amp;&amp; editorTextFocus",  
// if you want using quarto, try this  
// "when": "editorLangId =~ /r|rmd|qmd/ &amp;&amp; editorTextFocus",  
"args": {"text": " &lt;- "}  
},  
{  
"key": "ctrl+shift+m",  
"command": "type",  
"when": "editorLangId == r &amp;&amp; editorTextFocus || editorLangId == rmd &amp;&amp; editorTextFocus",  
"args": {"text": " %&gt;% "}  
},  
{  
"key": "ctrl+shift+m",  
"command": "-workbench.actions.view.problems"  
},  
  
// input indicative of r markdown code chunk  
{  
"key": "ctrl+shift+i",  
"command": "editor.action.insertSnippet",  
"when": "editorTextFocus &amp;&amp; editorLangId == 'rmd'",  
"args": {  
"snippet": "```{r}\n${TM_SELECTED_TEXT}$0\n```"  
},  
"label": "input indicative of r markdown code chunk"  
},  
  
// you can also input indicative of code chunk in `r` file by inserting "# %% ":  
// specifics in `https://github.com/REditorSupport/vscode-R/pull/662`  
{  
"key": "ctrl+shift+i",  
"command": "editor.action.insertSnippet",  
"when": "editorTextFocus &amp;&amp; editorLangId == 'r'",  
"args": {  
"snippet": "$LINE_COMMENT %% "  
},  
"label": "input indicative of code chunk"  
},  
  
// open help panel for selection  
{  
"key": "f1",  
"command": "r.helpPanel.openForSelection",  
"when": "editorTextFocus &amp;&amp; editorLangId == 'r' || editorTextFocus &amp;&amp; editorLangId == 'rmd'"  
},  
  
// RStudio keybinding for R Package development  
{  
"key": "ctrl+shift+b",  
"command": "r.install",  
"when": "resourceLangId == 'r'"  
},  
{  
"key": "ctrl+shift+e",  
"command": "r.check",  
"when": "resourceLangId == 'r'"  
},  
{  
"key": "ctrl+shift+t",  
"command": "r.test",  
"when": "resourceLangId == 'r'"  
},  
{  
"key": "ctrl+shift+d",  
"command": "r.document",  
"when": "resourceLangId == 'r'"  
},  
{  
"key": "ctrl+shift+l",  
"command": "r.loadAll",  
"when": "resourceLangId == 'r'"  
},  
{  
"key": "ctrl+alt+p",  
"command": "r.runCommand",  
"when": "editorTextFocus &amp;&amp; editorLangId == 'r'",  
"args": ".vsc.browser(httpgd::hgd_url(), viewer = \"Beside\")"  
}  
]
复制<br>]]></description><link>软件\生活窍门\在vs-code中搭建你的r语言运行环境.html</link><guid isPermaLink="false">软件/生活窍门/在VS Code中搭建你的R语言运行环境.md</guid><pubDate>Sat, 29 Jun 2024 13:34:30 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f92f.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f92f.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[寻找 easyconnect 使用的端口]]></title><description><![CDATA[ 
 <br>Easyconnect 是一个流氓。一个很好的方法是不使用它。如果非要使用，你可以尝试使用 docker 来隔离它，但是太麻烦了。<br>这里有一个曲线救国的办法，让代理软件不要和它冲突：<br><br>点击 控制面板\网络和 Internet\网络连接\更改适配器设置：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409131516352.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里看到 Sangfor 就是该软件专用的接口。<br><br>我们需要让其它软件不要使用这个接口与之冲突即可：<br>
以 Singlebox 为例，在 出站接口名称 换为你正在使用的网络，我在这里是网线连接的因此是 以太网。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409131515076.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这可以治标，但是不治本。]]></description><link>软件\生活窍门\easyconnect：如何设置不与代理冲突.html</link><guid isPermaLink="false">软件/生活窍门/EasyConnect：如何设置不与代理冲突.md</guid><pubDate>Fri, 13 Sep 2024 07:18:44 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409131516352.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409131516352.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[1. <strong>筛选状态下删除数据</strong>]]></title><description><![CDATA[ 
 <br><br>
<br>错误描述：用户在筛选状态下删除了一些可见行，但实际上Excel也会删除隐藏的行。这可能导致不必要的数据丢失。
<br>解决方法：不要筛选，多用排序。
<br><br>
<br>错误描述：在编写公式时，用户可能不理解相对引用（如A1）和绝对引用（如$A$1）的区别，导致复制公式时出错。
<br>解决方法：理解相对引用和绝对引用的含义。在拖动公式或复制公式前，确保使用正确的引用方式，避免公式在其他单元格中计算错误。
<br><br>
<br>错误描述：初学者常使用“合并单元格”来美化表格，但这会影响筛选、排序、复制粘贴等操作，甚至可能导致公式错误。
<br>解决方法：老老实实的复制→ 仅仅粘贴值。
<br><br>
<br>错误描述：Excel默认是自动计算模式，但用户在某些时候手动更改了计算模式为“手动”，导致新输入的数据没有立即更新计算结果。
<br>解决方法：在“公式”选项卡中，确保将计算模式设为“自动”。如果发现计算结果未及时更新，可以按F9手动重新计算。
<br><br>
<br>错误描述：初学者在排序时只选择了一列进行排序，导致其他列的数据没有同步变化，最终数据错乱。
<br>解决方法：在进行排序操作时，一定要选择整个数据区域，确保所有相关列一同排序，保持数据的一致性。或者保证完整的表格内不存在空行空列！
<br><br>
<br>错误描述：不正确设置单元格格式（如文本、数字、日期等）可能导致公式无法正确计算。例如，将文本格式的数字用于数值计算时会出现错误。
<br>解决方法：在输入数据或编写公式时，确保每个单元格的格式正确。如果需要，可以右键单元格，选择“设置单元格格式”，并选择合适的格式类型。针对完全是数字的样本名字，点击右键，设置单元格格式，点击 自定义→类型→0。
<br><br>
<br>错误描述：在使用VLOOKUP或INDEX/MATCH函数时，初学者可能会错误使用这些函数，导致查找失败或返回错误数据。

<br>常见错误包括VLOOKUP的查找列未正确指定、数据未排序时使用VLOOKUP的近似匹配模式等。


<br>解决方法：确保VLOOKUP的查找列是第一个列，并在匹配模式中设置为FALSE以进行精确匹配。如果需要更灵活的查找方式，可以学习使用INDEX和MATCH函数。
<br><br>
<br>错误描述：隐藏列或行后，初学者可能忘记它们的存在，导致在处理数据（如复制、粘贴、计算）时忽略了隐藏的数据。
<br>解决方法：在处理数据前，始终检查是否有隐藏的行或列。如果有，确保自己已经意识到它们的存在并适当处理它们。或者不要隐藏列，尤其不要筛选！
<br><br>
<br>错误描述：用户常将SUM函数误用于非数值数据，或者在需要计算平均值时误用SUM函数，而不是AVERAGE。
<br>解决方法：理解每个聚合函数的用途，比如：

<br>SUM：用于求和。
<br>AVERAGE：用于求平均值。
<br>COUNT：用于计数（只统计数值）。
<br>COUNTA：用于统计非空单元格。


<br><br>
<br>错误描述：用户在插入或删除行列时，常常忽略现有公式中的引用，这会导致公式的计算范围不正确。
<br>解决方法：插入或删除行列后，检查相关公式，确保引用范围已自动更新，或手动调整引用范围。
<br><br>
<br>错误描述：有时用户会忘记保存工作，或在没有备份的情况下频繁覆盖文件，导致数据丢失或不可恢复。
<br>解决方法：养成定期保存文件的习惯，并开启自动保存功能。另外，重要的文件可以使用不同的版本进行保存，避免在覆盖时丢失重要数据。
<br><br>
<br>错误描述：初学者常常为了视觉效果使用合并单元格，尤其是在表格标题中，但这会严重影响数据分析操作（如筛选、排序）。
<br>解决方法：使用“跨列居中”而不是“合并单元格”来保持数据整洁且方便操作。
<br><br>
<br>错误描述：Excel中的空格会导致看似空的单元格实际并不空白，从而影响公式计算（如COUNTA函数）。
<br>解决方法：在数据整理时，可以使用函数 TRIM 去除多余的空格，确保单元格内容与预期一致。最好保证单元格之内不存在空格，直接全部替换！再 excel 内最好使用 _ 替代单元格。
]]></description><link>软件\生活窍门\excel：避雷指南.html</link><guid isPermaLink="false">软件/生活窍门/Excel：避雷指南.md</guid><pubDate>Wed, 04 Sep 2024 03:44:50 GMT</pubDate></item><item><title><![CDATA[软件下载]]></title><description><![CDATA[ 
 <br>NGDC 可以使用 FTP 格式传输。<br><br>链接：  <a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hflDBmgAVpkH5t24_Q?e=wJUcPR" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hflDBmgAVpkH5t24_Q?e=wJUcPR" target="_blank">FileZilla Pro .zip</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409081645755.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409081643321.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\生活窍门\filezilla：如何传输下载ngdc数据？.html</link><guid isPermaLink="false">软件/生活窍门/FileZilla：如何传输下载NGDC数据？.md</guid><pubDate>Sun, 08 Sep 2024 13:01:58 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409081645755.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409081645755.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[设置]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407091502507.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>搜索 latex-workshop tools：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407091503696.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>将 "latex-workshop.latex.tools" 内容进行替换：<br>"latex-workshop.latex.tools": [
        
    
        {
            "name": "latexmk",
            "command": "latexmk",
            // "args": [
            //     "-synctex=1",
            //     "-interaction=nonstopmode",
            //     "-file-line-error",
            //     "-pdf",
            //     "-outdir=%OUTDIR%",
            //     "%DOC%"
            // ],
            "args": [
               "-xelatex",
               "-synctex=1",
               "-interaction=nonstopmode",
               "-file-line-error",
               "%DOC%"
            ],           
            "env": {}
        },
......其余不变
复制]]></description><link>软件\生活窍门\latex：修改vscode默认为xelatex.html</link><guid isPermaLink="false">软件/生活窍门/Latex：修改VScode默认为xelatex.md</guid><pubDate>Tue, 09 Jul 2024 07:04:40 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407091502507.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407091502507.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[WSL是什么]]></title><description><![CDATA[ 
 <br>并不是每个人都能够拥有服务器。即使是服务器也有宕机的时候。<br>为了应急，可以选择在自己的win电脑上安装一个Linux，这个被称为WSL。<br><br>适用于 Linux 的 Windows 子系统 (WSL) 是 Windows 的一项功能，可用于在 Windows 计算机上运行 Linux 环境，而无需单独的虚拟机或双引导。 WSL 旨在为希望同时使用 Windows 和 Linux 的开发人员提供无缝高效的体验。<br><br><br>微软官方给出的安装方式很直接，但是由于太直接了，所以直接给升级到最高的版本。<br>具体可以参见<a data-tooltip-position="top" aria-label="https://learn.microsoft.com/zh-cn/windows/wsl/install" rel="noopener nofollow" class="external-link" href="https://learn.microsoft.com/zh-cn/windows/wsl/install" target="_blank">这里</a>。<br>刚开始我并没有发现什么问题，但是由于我需要使用代理发现WSL会导致Win系统频繁断网，这让我苦不堪言。不得已我只能够卸载WSL。<br><br>手动安装旧版本是我觉得靠谱一点的方式。<br>需要先启用“适用于 Linux 的 Windows 子系统”可选功能，然后才能在 Windows 上安装 Linux 分发。<br>以管理员身份打开 PowerShell（“开始”菜单 &gt;“PowerShell” &gt;单击右键 &gt;“以管理员身份运行”），然后输入以下命令：<br>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
复制<br>下一步千万不要装WSL2.0，照着这样做：<br>
<br>打开微软商店，选择最早的Ubantu版本。

<br>单击以下链接会打开每个分发版的 Microsoft Store 页面：


<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9N9TNGVNDL3Q" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9N9TNGVNDL3Q" target="_blank">Ubuntu 18.04 LTS</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9n6svws3rx71" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9n6svws3rx71" target="_blank">Ubuntu 20.04 LTS</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9PN20MSR04DW" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9PN20MSR04DW" target="_blank">Ubuntu 22.04 LTS</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9NJFZK00FGKV" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9NJFZK00FGKV" target="_blank">openSUSE Leap 15.1</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9MZ3D1TRP8T1" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9MZ3D1TRP8T1" target="_blank">SUSE Linux Enterprise Server 12 SP5</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9PN498VPMF3Z" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9PN498VPMF3Z" target="_blank">SUSE Linux Enterprise Server 15 SP1</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9PKR34TNCV07" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9PKR34TNCV07" target="_blank">Kali Linux</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9MSVKQC78PK6" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9MSVKQC78PK6" target="_blank">Debian GNU/Linux</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9n6gdm4k2hnc" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9n6gdm4k2hnc" target="_blank">Fedora Remix for WSL</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9NV1GV1PXZ6P" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9NV1GV1PXZ6P" target="_blank">Pengwin</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9N8LP0X93VCP" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9N8LP0X93VCP" target="_blank">Pengwin Enterprise</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9p804crf0395" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9p804crf0395" target="_blank">Alpine WSL</a>
<br><a data-tooltip-position="top" aria-label="https://www.microsoft.com/store/apps/9msmjqd017x7" rel="noopener nofollow" class="external-link" href="https://www.microsoft.com/store/apps/9msmjqd017x7" target="_blank">Raft（免费试用版）</a>
<br><a data-tooltip-position="top" aria-label="https://apps.microsoft.com/search?query=alma+linux" rel="noopener nofollow" class="external-link" href="https://apps.microsoft.com/search?query=alma+linux" target="_blank">Alma Linux</a>


<br>在分发版的页面中，选择“获取”。

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041500485.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br>首次启动新安装的 Linux 分发版时，将打开一个控制台窗口，系统会要求你等待一分钟或两分钟，以便文件解压缩并存储到电脑上。 未来的所有启动时间应不到一秒。
<br><br>我在知乎上看到另一个大佬，可以把WSL系统放到D盘去，但是因为我的C盘本身很大，所以就没必要了。<br>原文链接：<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/466001838" rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/466001838" target="_blank">Windows10/11 三步安装wsl2 Ubuntu20.04（任意盘） - 知乎 (zhihu.com)</a>]]></description><link>软件\生活窍门\win10安装wsl1.0.html</link><guid isPermaLink="false">软件/生活窍门/Win10安装WSL1.0.md</guid><pubDate>Fri, 21 Jun 2024 01:04:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041500485.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041500485.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[创建宏]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://blog.csdn.net/weixin_40575956/article/details/127960138" rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/weixin_40575956/article/details/127960138" target="_blank">word批量修改交叉引用颜色_word交叉引用变蓝色-CSDN博客</a><br>
为增加学术论文可读性，论文中的文献引用和图表的交叉引用字体常设置为特殊颜色。手动一个个设置相当繁琐，我们可以利用Word的宏实现批量修改交叉引用字体颜色。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041534981.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041535181.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041535203.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>我在原作者的基础上添加了一些功能，不需要的时候可以直接删掉：<br>Sub CitingColor()
    For i = 1 To ActiveDocument.Fields.Count '遍历文档所有域
        If Left(ActiveDocument.Fields(i).Code, 4) = " REF" Then 'Word自带的交叉引用的域代码起始4位是" REF"（注意空格）
            ActiveDocument.Fields(i).Select '选中上述几类域
            With Selection.Font
                .Color = RGB(205, 226, 241) '设置字体颜色
                .Bold = True '设置字体为粗体
                .Italic = True '设置字体为斜体
                .Name = "仿宋" '设置字体为仿宋
                .Underline = wdUnderlineSingle '设置下划线
            End With
        End If
    Next
End Sub
复制<br><br>保存关闭，然后双击下列：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041536613.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
即可获得最终结果！]]></description><link>软件\生活窍门\word：批量调整交叉引用的字体.html</link><guid isPermaLink="false">软件/生活窍门/Word：批量调整交叉引用的字体.md</guid><pubDate>Thu, 04 Jul 2024 07:39:34 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041534981.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041534981.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[原始文档]]></title><description><![CDATA[ 
 <br>针对一个使用了Endnote软件插入参考文献的 Word 文件，可以使用如下方式快速替换字体颜色等。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041503747.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041503049.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041504669.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
现在，你就获得了具备颜色的引用：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041505764.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>如何取消下划线以及设置其他的样式呢？<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041507469.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041507705.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041508681.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041508985.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>完成！]]></description><link>软件\生活窍门\word：批量将endnote引用的参考文献字体调整为蓝色.html</link><guid isPermaLink="false">软件/生活窍门/Word：批量将Endnote引用的参考文献字体调整为蓝色.md</guid><pubDate>Thu, 04 Jul 2024 07:09:41 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041503747.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407041503747.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[WSL1 和 WSL2 网络的区别]]></title><description><![CDATA[ 
 <br>在 WSL2 环境中 clone 一个很大的 git 项目，不走代理速度很慢，所以研究了一下怎么让 WSL2 走 Windows 的代理客户端。<br><br>在 WSL1 时代，由于 Linux 子系统和 Windows 共享了网络端口，所以访问 Windows 的代理非常简单。例如 Windows 的代理客户端监听了 8000 端口，那么只需要在 Linux 子系统中执行如下命令，就可以让当前 session 中的请求通过代理访问互联网。<br>export ALL_PROXY="http://127.0.0.1:8000"
复制<br>但是 WSL2 基于 Hyper-V 运行，导致 Linux 子系统和 Windows 在网络上是两台各自独立的机器，从 Linux 子系统访问 Windows 首先需要找到 Windows 的 IP。<br><br><br>有两个关键步骤： 1. WSL2 中配置的代理要指向 Windows 的 IP； 2. Windows 上的代理客户端需要允许来自本地局域网的请求；<br>由于 Linux 子系统也是通过 Windows 访问网络，所以 Linux 子系统中的网关指向的是 Windows，DNS 服务器指向的也是 Windows，基于这两个特性，我们可以将 Windows 的 IP 读取出来。<br>例如，在 Ubuntu 子系统中，通过&nbsp;cat /etc/resolv.conf&nbsp;查看 DNS 服务器 IP。<br># This file was automatically generated by WSL. To stop 
automatic generation of this file, add the following entry to /etc/wsl.conf:# [network]# generateResolvConf = falsenameserver 172.19.80.1
复制<br>可以看到 DNS 服务器是&nbsp;172.19.80.1，通过环境变量&nbsp;ALL_PROXY&nbsp;配置代理：<br>export ALL_PROXY="http://172.19.80.1:7890"
复制<br>7890 是 Windows 上运行的代理客户端的端口，记得要在 Windows 代理客户端上配置允许本地局域网请求。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409132127022.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>另外的网友发现一个更加简单的方法。<br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/iftodayhappy/article/details/137236279" rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/iftodayhappy/article/details/137236279" target="_blank">为WSL2.0设置本机代理的配置方案（通过隧道）_wsl 代理-CSDN博客</a>
<br>
<br>把之前在.bashrc 启动文件中配置 http_proxy 和 https_proxy 的逻辑删去，并且关闭 WSL
<br>wsl --shutdown
复制<br>
<br>在我的 window 主机编辑 ~\.wslconfig：去 C:\Users\你的名字\ 新建一个 .wslconfig，然后用记事本打开往里面放这些内容：如"C:\Users\fangy.wslconfig"
<br>[wsl2]
memory=8GB
processors=8
[experimental]
autoMemoryReclaim=gradual
networkingMode=mirrored
dnsTunneling=true
firewall=true
autoProxy=true
sparseVhd=true
复制<br>
<br>重启 WSL
<br>确实自动设置了代理，代理正常工作，很好，不必再手动设置 http_proxy 和 https_proxy 了。]]></description><link>软件\生活窍门\wsl2：为linux设置网络代理.html</link><guid isPermaLink="false">软件/生活窍门/WSL2：为Linux设置网络代理.md</guid><pubDate>Fri, 13 Sep 2024 13:30:59 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409132127022.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409132127022.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第二节 倾斜工具]]></title><description><![CDATA[ 
 <br>使用矩形绘图工具来绘制正方形或者长方形，并通过倾斜工具进行扭曲变形。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
倾斜工具<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151800525.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151800246.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
按Alt拖动复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151801221.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
通过不断复制来铺满画面。<br><br>新建一个图层，置于底层，然后画一个大矩形填充颜色。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151802973.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>]]></description><link>软件\adobe-illustrator绘图\第二节-倾斜工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第二节 倾斜工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:11 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第六节 直线工具]]></title><description><![CDATA[ 
 <br>直线工具包括以下：<br>
<br>直线工具（在使用的时候，如果按住~键，就可以拖拽出多条直线，很方便！
<br>弧线工具
<br>螺旋线工具
<br>矩形线工具
<br>极坐标工具
<br>效果演示：<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212032136.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
在实际使用过程中，一定要注意↑ ↓ ← →方向键的使用，可以迅速调整生成图形的数量和密度。]]></description><link>软件\adobe-illustrator绘图\第六节-直线工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第六节 直线工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:11 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212032136.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212032136.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第七节 混合工具]]></title><description><![CDATA[ 
 <br>混合工具可以将多个对象的属性产生混合。<br>混合工具可以进行多种操作：<br>
<br>两个或者多个形状进行混合：

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212046339.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br>双击对象进入隔离模式以调整细节：<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212047522.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>文字与文字进行混合

<br>我发现某些版本的Ai可能在进行上述操作时可能遇到问题，我的建议是不要在使用文字工具的时候拖动文本框，直接点击进行键入文字。
<br>然后对图层复制一份，进行移动。
<br>通过混合工具进行混合。
<br>按下enter回车有助于调整混合强度！<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212123687.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>文字与形状进行混合
]]></description><link>软件\adobe-illustrator绘图\第七节-混合工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第七节 混合工具.md</guid><pubDate>Thu, 20 Jun 2024 12:28:15 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212046339.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212046339.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第三节 钢笔工具]]></title><description><![CDATA[ 
 <br>钢笔工具绘制矢量图形。其绘制出来的线条叫做路径。<br>
路径分为：<br>
<br>闭合路径
<br>开放路径
<br>钢笔工具是一种手感工具，需要多加练习。如果练习过程中出现困难，无法判断落点，则可以开启橡皮筋功能。快捷键是：ctrl+K。<br>当我们使用钢笔工具勾画出路径之后，可以通过以下方式退出勾画：按住ctrl，点击空白处。<br>以下快捷键可能比较有用：<br><br>熟练生巧，<a data-tooltip-position="top" aria-label="http://bezier.method.ac" rel="noopener nofollow" class="external-link" href="http://bezier.method.ac" target="_blank">练习网址</a>。<br>其他好用的快捷键：<br>]]></description><link>软件\adobe-illustrator绘图\第三节-钢笔工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第三节 钢笔工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:11 GMT</pubDate></item><item><title><![CDATA[第四节 文字工具]]></title><description><![CDATA[ 
 <br>我们在固定某些图层的时候，可以使用锁定工具来固定，快捷键是ctrl+2。<br>在我们使用文字工具键入文字之后，可以设置其字体和段落等。可以使用快捷键ctrl+T。<br>文字是可以按照路径去排列的。举例，先用钢笔进行路径勾画。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211952984.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>文字是一种图形，所以可以通过shift+T进行单个部首修饰。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211955738.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">]]></description><link>软件\adobe-illustrator绘图\第四节-文字工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第四节 文字工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:11 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211952984.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211952984.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第五节 形状工具]]></title><description><![CDATA[ 
 <br>在Ai里面有很多形状工具，例如：<br>
<br>矩形
<br>三角形
<br>圆形
<br>多边形
<br>直线<br>
选择多边形工具,拖动建立形状，保持左键不松开，按↑↓方向键，就可以调整边数。
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212016133.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>实战演练：<br>
<br>绘制一个六边形
<br>对其施加变化

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212018320.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212020190.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212022169.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>对其施加渐变效果

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212025353.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>大功告成！
]]></description><link>软件\adobe-illustrator绘图\第五节-形状工具.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第五节 形状工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:11 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212016133.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212016133.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[第一节 基础导入导出功能]]></title><description><![CDATA[ 
 <br>Adobe公司致力于开发各种创意和设计工具。<br>
<br>Adobe Photoshop：Adobe Photoshop是一款广泛使用的图像编辑软件，它包含许多AI功能，如内容感知填充、智能选择工具、自动改善功能等。这些功能可以帮助用户更轻松地编辑和改善照片和图像。
<br>Adobe Illustrator：Adobe Illustrator是一款矢量图形设计软件，它也包括一些AI功能，如自动图像跟踪、智能路径简化等，以帮助设计师创建精美的矢量图形。
<br>Ai和PS有着几乎相同的操作逻辑。Ai输出格式有以下常用的几种格式：<br><br>操作快捷键：<br>]]></description><link>软件\adobe-illustrator绘图\第一节-基础导入导出功能.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/第一节 基础导入导出功能.md</guid><pubDate>Thu, 20 Jun 2024 12:28:11 GMT</pubDate></item><item><title><![CDATA[思维总览]]></title><description><![CDATA[ 
 <br>Adobe公司致力于开发各种创意和设计工具。<br>
<br>Adobe Photoshop：Adobe Photoshop是一款广泛使用的图像编辑软件，它包含许多AI功能，如内容感知填充、智能选择工具、自动改善功能等。这些功能可以帮助用户更轻松地编辑和改善照片和图像。
<br>Adobe Illustrator：Adobe Illustrator是一款矢量图形设计软件，它也包括一些AI功能，如自动图像跟踪、智能路径简化等，以帮助设计师创建精美的矢量图形。
<br>Ai和PS有着几乎相同的操作逻辑。Ai输出格式有以下常用的几种格式：<br><br>操作快捷键：<br>第一节 基础导入导出功能<br>使用矩形绘图工具来绘制正方形或者长方形，并通过倾斜工具进行扭曲变形。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
倾斜工具<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151800525.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151800246.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
按Alt拖动复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151801221.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
通过不断复制来铺满画面。<br><br>新建一个图层，置于底层，然后画一个大矩形填充颜色。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151802973.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>第二节 倾斜工具<br>钢笔工具绘制矢量图形。其绘制出来的线条叫做路径。<br>
路径分为：<br>
<br>闭合路径
<br>开放路径
<br>钢笔工具是一种手感工具，需要多加练习。如果练习过程中出现困难，无法判断落点，则可以开启橡皮筋功能。快捷键是：ctrl+K。<br>当我们使用钢笔工具勾画出路径之后，可以通过以下方式退出勾画：按住ctrl，点击空白处。<br>以下快捷键可能比较有用：<br><br>熟练生巧，<a data-tooltip-position="top" aria-label="http://bezier.method.ac" rel="noopener nofollow" class="external-link" href="http://bezier.method.ac" target="_blank">练习网址</a>。<br>其他好用的快捷键：<br>第三节 钢笔工具<br>直线工具包括以下：<br>
<br>直线工具（在使用的时候，如果按住~键，就可以拖拽出多条直线，很方便！
<br>弧线工具
<br>螺旋线工具
<br>矩形线工具
<br>极坐标工具
<br>效果演示：<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212032136.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
在实际使用过程中，一定要注意↑ ↓ ← →方向键的使用，可以迅速调整生成图形的数量和密度。第六节 直线工具<br>混合工具可以将多个对象的属性产生混合。<br>混合工具可以进行多种操作：<br>
<br>两个或者多个形状进行混合：

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212046339.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br>双击对象进入隔离模式以调整细节：<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212047522.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>文字与文字进行混合

<br>我发现某些版本的Ai可能在进行上述操作时可能遇到问题，我的建议是不要在使用文字工具的时候拖动文本框，直接点击进行键入文字。
<br>然后对图层复制一份，进行移动。
<br>通过混合工具进行混合。
<br>按下enter回车有助于调整混合强度！<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212123687.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>文字与形状进行混合
第七节 混合工具<br>我们在固定某些图层的时候，可以使用锁定工具来固定，快捷键是ctrl+2。<br>在我们使用文字工具键入文字之后，可以设置其字体和段落等。可以使用快捷键ctrl+T。<br>文字是可以按照路径去排列的。举例，先用钢笔进行路径勾画。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211952984.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>文字是一种图形，所以可以通过shift+T进行单个部首修饰。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312211955738.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">第四节 文字工具<br>在Ai里面有很多形状工具，例如：<br>
<br>矩形
<br>三角形
<br>圆形
<br>多边形
<br>直线<br>
选择多边形工具,拖动建立形状，保持左键不松开，按↑↓方向键，就可以调整边数。
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212016133.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>实战演练：<br>
<br>绘制一个六边形
<br>对其施加变化

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212018320.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212020190.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212022169.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>对其施加渐变效果

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312212025353.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">


<br>大功告成！
第五节 形状工具]]></description><link>软件\adobe-illustrator绘图\思维总览.html</link><guid isPermaLink="false">软件/Adobe illustrator绘图/思维总览.canvas</guid><pubDate>Tue, 29 Oct 2024 09:54:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312151759474.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[古代 DNA]]></title><description><![CDATA[ 
 <br><br>古代 DNA 对于校正系统发育树具有重要的意义，通过 C14测定的年龄往往比通过分子钟推算出来的共同祖先时间更精准，因此可以认为古代 DNA 是校正时间的“金标准”。<br><br>我们需要准备3个文件：<br>
<br>古代 DNA 序列文件
<br>现代 DNA 序列文件
<br>古代 DNA 时间文件
<br><br>序列文件通常采用 fasta 格式，也可以使用 nex 格式。我们可以使用 geneious 软件轻松转换。序列比对推荐使用 <a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a>。我们需要保证古代 DNA 和现代 DNA 的序列的长度一致。<br><br>我们需要制作 新建 Text Document.txt 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122209592.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>第一列是 ID，第二列是年龄。
注意，ID 必须与 fasta 文件中的 ID 一样！另外，推荐我们使用 BP（before present） 作为时间的单位。原因在于古代 DNA 的时间跨度很长，使用 BCE,BC 等单位可能会弄混。另外，注意，BP 不能出现负数！BP 与 BCE 的换算关系是：
<br><br><br>进行系统发育树建树的前提是：<a data-tooltip-position="top" aria-label="构建系统发育树的重要步骤.canvas" data-href="构建系统发育树的重要步骤.canvas" href="软件\其它生信软件\使用心得\构建系统发育树的重要步骤.html" class="internal-link" target="_self" rel="noopener nofollow">构建系统发育树的重要步骤</a>。<br>
请按照上述操作进行保守区选择和饱和度检测。<br><br>我们需要进行<a data-href="日期随机化检验（Date Randomization Tests, DRTs）" href="术语\日期随机化检验（date-randomization-tests,-drts）.html" class="internal-link" target="_self" rel="noopener nofollow">日期随机化检验（Date Randomization Tests, DRTs）</a>，具体的操作步骤在这里：<a data-href="BEAST：日期随机化检测" href="软件\beast\beast：日期随机化检测.html" class="internal-link" target="_self" rel="noopener nofollow">BEAST：日期随机化检测</a>。<br><br>现在可以将文件导入 BEAUTi 了。其它的设置和仅分析现代 DNA是一致的。我们需要注意的是 Tip ——这个参数一般我们在分析现代 DNA 的时候没有设置，但是在这里我们需要设置：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122218792.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
依次点击图中，注意这里我们选择的是 Before the present!!!<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122219996.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
然后我们选择 read from file ，将刚才我们准备好的古代 DNA 时间文件 新建 Text Document.txt 导入进去。<br>为什么我没有选择 use everything？
事实上，这里的选项都可以选，但是，某些 ID 中存在的特殊字符例如 @#_!/ 等都会导致这些方法失效，因此还是按我说的做吧。<br>
现代 DNA 可以不指定时间，若要指定，请指定为 0.
<br>最终，我们获得了 xml 文件，然后放到 BEAST2 中运行即可。<br><br>我发现序列中存在一些特殊字符或者参数设定有些不合理时，该软件会闪退，但是如果我们选择 Scaling 为 None 就不会。我也不知道为什么。
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122223883.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>这一步我们烧掉前 25% 的树。<br>当然，你也可以按照树的 数量 而非百分比烧，也可以烧掉 10% 等。随便你。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122225822.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>略。<br><br>由于这次我们仅作演示，因此 MCMC 并未设定过长，这导致我们的 ESS 比较低，树的结构有些紊乱，这可以通过增大迭代次数来解决。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122227377.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\beast\beast：加入古代dna建树校正时间.html</link><guid isPermaLink="false">软件/BEAST/BEAST：加入古代DNA建树校正时间.md</guid><pubDate>Thu, 12 Sep 2024 14:30:27 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122209592.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409122209592.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[介绍]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://user.qzone.qq.com/58001704/2" rel="noopener nofollow" class="external-link" href="https://user.qzone.qq.com/58001704/2" target="_blank">Raindy Qzone</a>
<br><br>病毒或其他病原的进化速率（rate）和时间尺度（timescale）可以通过对具有时间结构（Time-structured）的序列进行系统发育分析获得，应用病毒序列的采样时间进行分子钟校准前，通常需要进行日期随机化检验（date-randomization test）判断分析的数据集中是否有足够的时间信号（Temporal signal）可用于校准。<br>
在人类的古代 DNA 研究中，时间跨度较长导致序列之间的变异很大，另外，我们无法排除测序质量对序列的干扰，因此在进行线粒体 DNA 和 Y 染色体研究时，为了构建带有古代 DNA 的<a data-href="贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）" href="术语\贝叶斯进化树分析（bayesian-evolutionary-analysis-by-sampling-trees，beast）.html" class="internal-link" target="_self" rel="noopener nofollow">贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）</a>，有必要进行<a data-href="日期随机化检验（Date Randomization Tests, DRTs）" href="术语\日期随机化检验（date-randomization-tests,-drts）.html" class="internal-link" target="_self" rel="noopener nofollow">日期随机化检验（Date Randomization Tests, DRTs）</a>。<br><br>
<br>
两个检验标准（CR1和CR2）：<br>
1. 通过原始（正确的）采样时间进行校准估算获得的进化速率平均值（Mean）与应用日期随机化后的数据获得的进化速率95%置信区间（CI）进行比较（如下图中的小红点与黑色线段的上下限）。如果两者之间没有重合，则通过相对宽松的标准1（即：CR1）；<br>
2. 通过原始（正确的）采样时间估算获得进化速率的95%CI与随机化后的数据获得进化速率的任何区间（如下图的红色虚线段与黑色线段的任意一个值）均没有重合，则能通过更为严格的标准2（即：CR2）；<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111512994.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
两个不同方法：&nbsp;

<br>标准的Bayesian dating permutation tests&nbsp;
<br>相同采样时间归组的&nbsp;Clustered permutation tests。


<br>本文以标准的方法为示例，应用 R 包 TipDatingBeast 进行 DRTs 简明图解分析。<br><br><br>
<br>配置用于BEAST运行的XML文件：应用 BEAUti 1.8.x&nbsp;配置时，在"Tip"标签下，启用&nbsp;Tip-date&nbsp;功能，将每条序列的采样时间根据不同规则提取出来用于校准（如下图所示），其他根据规范逐一配置完成，最好导出配置好的xml文件（本例为PMMoV_DRTs.xml），注意该xml文件中的时期是正确采样时间（非随机化的）：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111515117.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>随机化 XML 文件的采样日期：
<br><br>使用 R 语言进行：<br># 载入TipDatingBeast库，如果没有安装则先安装
if (!require("TipDatingBeast")) {
  install.packages("TipDatingBeast", dependencies = TRUE)
  library(TipDatingBeast)
}
# 指定工作路径
setwd("C:/Users/victo/Desktop")
# 指定你要产生多少份xml文件用于检测，默认20个
rep = 20
# 执行 RandomDates 函数，生成随机化日期的 XML 文件
RandomDates(name="TIP", rep, writeTrees=FALSE)
复制<br><br>应用BEAST分别运行前两步得到的.xml，获得运行日志文件.log。<br>在大批量运行这些xml文件前，建议先随机抽个随机化的xml文件，看看程序是否会存在如下类似报错信息。
SEVERE: Parsing error - poorly formed XML (possibly not an XML file):<br>
The string "--" is not permitted within comments.<br>
java.lang.RuntimeException: Terminate
<br>解决办法： <br>用文本编辑器搜索字符串&nbsp; &nbsp;&nbsp;&lt;!-- write tree log to file&nbsp;，将它后一行的&nbsp;&lt;!--&nbsp;&nbsp;的字符串删除后删除，保存xml 文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111518068.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>获得的21个 xxx.log&nbsp;复制到当前工作目录，输入以下命令，对DRTs结果进行绘制：<br># 载入TipDatingBeast库，如果没有安装则先安装
if (!require("TipDatingBeast")) {
  install.packages("TipDatingBeast", dependencies = TRUE)
  library(TipDatingBeast)
}
# 指定工作路径
setwd("C:/Users/victo/Desktop")
# 指定你要烧掉前%多少的树，默认0.1即10%
burnin = 0.1 
# 使用 PlotDRT 函数分析和绘制结果
PlotDRT(name="TIP", rep, burnin=0.1)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111519067.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>绘制完成，工作目录会生成三个文件，meanRate.stats.csv 和两个 PDF 文件，均为DRTs的结果，一个是原数绘制的，另一个是数值取Log10后绘制的结果。<br><br>可以使用R对 meanRate.stats.csv 进行绘制，效果图如下图所示，DRTs结果表明目的数据集通过DRTs检验，具有时间信号，其采样时间用于校准分子钟。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111520265.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\beast\beast：日期随机化检测.html</link><guid isPermaLink="false">软件/BEAST/BEAST：日期随机化检测.md</guid><pubDate>Thu, 12 Sep 2024 03:07:30 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111512994.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111512994.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[BEAST：替换模型]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="BEAST：使用指南.canvas" data-href="BEAST：使用指南.canvas" href="BEAST：使用指南.canvas" class="internal-link" target="_self" rel="noopener nofollow">BEAST使用指南</a><br>
<a data-href="贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）" href="术语\贝叶斯进化树分析（bayesian-evolutionary-analysis-by-sampling-trees，beast）.html" class="internal-link" target="_self" rel="noopener nofollow">贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）</a><br>
<a data-href="MrBayes：贝叶斯方法建树" href="软件\beast\mrbayes：贝叶斯方法建树.html" class="internal-link" target="_self" rel="noopener nofollow">MrBayes：贝叶斯方法建树</a><br>BEAST1软件包里面BEAUti程序图形用户界面只给出了3种目前最流行的DNA替换模型: HKY、TrN和GTR。目前，标准的DNA替换模型有11种，也就是说还有8种（JC69、TrNef、K80、F81、SYM、TIM、TVM和TVMef）DNA替换模型没有指定，需要自己想办法解决。<br><br>更多有关BEAST的内容可以访问：<a data-tooltip-position="top" aria-label="https://user.qzone.qq.com/2049968646/main" rel="noopener nofollow" class="external-link" href="https://user.qzone.qq.com/2049968646/main" target="_blank">李老师的个人QQ空间日志</a><br>核苷酸替换模型参数 NST 修正：<br>JC		   nst=1  base=equal  rates=equal  pinv=0
JC+I 	   nst=1  base=equal  rates=equal  pinv=est
JC+G	   nst=1  base=equal  rates=gamma  shape=est  pinv=0
JC+I+G	   nst=1  base=equal  rates=gamma  shape=est  pinv=est
F81		   nst=1  base=est  rates=equal  pinv=0
F81+I	   nst=1  base=est  rates=equal  pinv=est
F81+G	   nst=1  base=est  rates=gamma  shape=est  pinv=0
F81+I+G	   nst=1  base=est  rates=invgamma  shape=est  pinv=est
K80		   nst=2  base=equal  tratio=est  rates=equal  pinv=0
K80+I	   nst=2  base=equal  tratio=est  rates=equal  pin=est
K80+G	   nst=2  base=equal  tratio=est  rates=gamma  shape=est  pinv=0
K80+I+G	   nst=2  base=equal  tratio=est  rates=invgamma  shape=est pinv=est
HKY	   nst=2  base=est  tratio=est  rates=equal  pinv=0
HKY+I	   nst=2  base=est  tratio=est  rates=equal  pinv=est
HKY+G	   nst=2  base=est  tratio=est  rates=gamma  shape=est  pinv=0  
HKY+I+G  nst=2  base=est  tratio=est  rates=invgamma  shape=est  pinv=est  
TrNef	   nst=6  base=equal  rmat=est  rclass=(a b a a e a)  rates=equal  pinv=0
TrNef+I	   nst=6  base=equal  rmat=est  rclass=(a b a a e a) rates=equal  pinv=est
TrNef+G	   nst=6  base=equal  rmat=est  rclass=(a b a a e a) rates=gamma  shape=est  pinv=0
TrNef+I+G	nst=6  base=equal  rmat=est rclass=(a b a a e a) rates=invgamma  shape=est  pinv=est
TrN 	   nst=6  base=est  rmat=est  rclass=(a b a a e a) rates=equal  pinv=0
TrN+I 	   nst=6  base=est  rmat=est  rclass=(a b a a e a) rates=equal  pinv=est
TrN+G	   nst=6  base=est  rmat=est  rclass=(a b a a e a) rates=gamma  shape=est  pinv=0
TrN+I+G	   nst=6  base=est  rmat=est  rclass=(a b a a e a) rates=invgamma  shape=est  pinv=est
K81		   nst=6  base=equal  rmat=est  rclass=(a b c c b a)  rates=equal  pinv=0
K81+I	   nst=6  base=equal  rmat=est  rclass=(a b c c b a)  rates=equal  pinv=est
K81+G	   nst=6  base=equal rmat=est  rclass=(a b c c b a)  rates=gamma  shape=est  pinv=0
K81+I+G	   nst=6  base=equal  rmat=est rclass=(a b c c b a)  rates=invgamma  shape=est  pinv=est
K81uf	   nst=6  base=est  rmat=est  rclass=(a b c c b a)  rates=equal  pinv=0
K81uf+I	   nst=6  base=est  rmat=est  rclass=(a b c c b a)  rates=equal  pinv=est
K81uf+G	   nst=6  base=est  rmat=est  rclass=(a b c c b a)  rates=gamma  shape=est  pinv=0
K81uf+I+G	nst=6  base=est  rmat=est  rclass=(a b c c b a)  rates=invgamma  shape=est  pinv=est
TIMef	   nst=6  base=equal  rmat=est  rclass=(a b c c e a)  rates=equal  pinv=0
TIMef+I	   nst=6  base=equal  rmat=est  rclass=(a b c c e a)  rates=equal  pinv=est
TIMef+G	   nst=6  base=equal rmat=est  rclass=(a b c c e a)  rates=gamma  shape=est  pinv=0
TIMef+I+G	nst=6  base=equal  rmat=est rclass=(a b c c e a)  rates=invgamma  shape=est  pinv=est
TIM		   nst=6  base=est  rmat=est  rclass=(a b c c e a)  rates=equal  pinv=0
TIM+I 	   nst=6  base=est  rmat=est  rclass=(a b c c e a)  rates=equal  pinv=est
TIM+G	   nst=6  base=est  rmat=est  rclass=(a b c c e a)  rates=gamma  shape=est  pinv=0
TIM+I+G	nst=6  base=est  rmatrix=est  rclass=(a b c c e a)  rates=invgamma  shape=est  pinv=est
TVMef	   nst=6  base=equal  rmat=est  rclass=(a b c d b e)  rates=equal  pinv=0
TVMef+I	   nst=6  base=equal  rmat=est  rclass=(a b c d b e)  rates=equal  pinv=est
TVMef+G	nst=6  base=equal rmat=est  rclass=(a b c d b e)  rates=gamma  shape=est  pinv=0
TVMef+I+G	nst=6  base=equal  rmat=est rclass=(a b c d b e)  rates=invgamma  shape=est  pinv=est
TVM		nst=6  base=est  rmat=est  rclass=(a b c d b e)  rates=equal  pinv=0
TVM+I	    nst=6  base=est  rmat=est  rclass=(a b c d b e)  rates=equal  pinv=est
TVM+G	    nst=6  base=est  rmat=est  rclass=(a b c d b e)  rates=gamma  shape=est  pinv=0
TVM+I+G	nst=6  base=est  rmat=est  rclass=(a b c d b e)  rates=invgamma  shape=est  pinv=est
SYM		nst=6  base=equal  rmat=est  rclass= (a b c d e f)  rates=equal  pinv=0
SYM+I	    nst=6  base=equal  rmat=est  rclass= (a b c d e f)  rates=equal  pinv=est
SYM+G	    nst=6  base=equal  rmat=est  rclass= (a b c d e f)  rates=gamma  shape=est pinv=0  
SYM+I+G	nst=6  base=equal  rmat=est  rclass= (a b c d e f)  rates=invgamma  shape=est pinv=est  
GTR		nst=6  base=est  rmat=est  rclass= (a b c d e f)  rates=equal  pinv=0
GTR+I	    nst=6  base=est  rmat=est  rclass= (a b c d e f)  rates=equal  pinv=est
GTR+G	    nst=6  base=est  rmat=est  rclass= (a b c d e f)  rates=gamma  shape=est pinv=0  
GTR+I+G	nst=6  base=est  rmat=est  rclass= (a b c d e f)  rates=invgamma  shape=est pinv=est  
复制<br>注意：其中参数 base 和 rmat 是 paup 模块里的，mrbayes 里不需要设置，rclas 也不是 mrbayes 里的参数。<br>
在 mrbayes 里的 GTR 模型示例如下：<br>
lset  nst=6  rates=equal ;<br>
Prset statefreqpr=dirichlet(1,1,1,1)  pinv=fix(0);<br>BEGIN MRBAYES;
outgroup  name1
outgroup  name2
lset nst=6 rates=invgamma ngammacat=4;
Prset statefreqpr=dirichlet(1,1,1,1);
mcmc ngen=2000000 printfreq=1000 nruns=2 diagnfreq=5000 samplefreq=100 nchains=4 temp=0.1 burninfrac=0.25 checkpoint=yes savebrlens=yes;
sump burnin=5000;
sumt burnin=5000 contype=allcompat showtreeprobs=yes;
END;
复制<br>例如，我希望：<br>
<br>编码区：GTR+F+I+G4
<br>高变区：HKY+F+I 
]]></description><link>软件\beast\beast：替换模型.html</link><guid isPermaLink="false">软件/BEAST/BEAST：替换模型.md</guid><pubDate>Fri, 21 Jun 2024 07:31:17 GMT</pubDate></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>一个对齐的 fasta 文件，可以使用如下脚本把 coding 和 control 区分开。<br>import os
import re
from Bio import SeqIO

# Define input and output file paths
input_fasta_path = r"原始的.fasta"
output_nexus_path = r"TK.nex" # 这里会生成分区的nex文件，大概率不会用到
output_dloop_fasta_path = r"TKDLOOP.FASTA" # 控制区
output_coding_fasta_path = r"TKCODING.FASTA" # 编码区
input_txt_path = r"新建 Text Document.txt" # 输入的提取文件，第一列是ID，第二列是单倍群（也可以只要ID） 

def clean_special_characters(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    
    # Remove special characters @, ', and "
    cleaned_content = re.sub(r"[\'\"@]", "", content)
    
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(cleaned_content)

def read_sample_info(txt_path):
    sample_info = {}
    with open(txt_path, 'r', encoding='utf-8') as txt_file:
        # Skip the header
        next(txt_file)
        for line in txt_file:
            parts = line.strip().split('\t')
            if len(parts) == 2:
                sample_id, haplogroup = parts
                sample_info[sample_id] = haplogroup
    return sample_info

def write_nexus(sequences, file_path):
    num_taxa = len(sequences)
    num_characters = len(next(iter(sequences.values())))

    with open(file_path, 'w') as file:
        file.write("#NEXUS\n\n")
        file.write("Begin data;\n")
        file.write(f"Dimensions ntax={num_taxa} nchar={num_characters};\n")
        file.write("Format datatype=dna missing=? gap=-;\n")
        file.write("Matrix\n\n")
        
        for sequence_id, sequence_data in sequences.items():
            file.write(f"{sequence_id}\t{sequence_data}\n")
        
        file.write(";\n")
        file.write("End;\n\n")
        
        file.write("begin assumptions;\n")
        file.write("charset DLOOP = 1-599 16001-16594;\n")
        file.write("charset CODING = 600-16000;\n")
        file.write("end;\n")

def write_fasta(sequences, file_path):
    with open(file_path, 'w') as file:
        for sequence_id, sequence_data in sequences.items():
            file.write(f"&gt;{sequence_id}\n")
            file.write(f"{sequence_data}\n")

def extract_sequences(fasta_path, sample_info, include_haplogroup):
    extracted_sequences = {}
    for record in SeqIO.parse(fasta_path, "fasta"):
        original_id = record.id
        if not sample_info or original_id in sample_info:
            if include_haplogroup and original_id in sample_info:
                new_id = f"{original_id}_{sample_info[original_id]}"
                record.id = new_id
                record.description = new_id
            extracted_sequences[record.id] = str(record.seq)
    return extracted_sequences

def split_sequences(sequences):
    dloop_sequences = {}
    coding_sequences = {}
    for seq_id, seq in sequences.items():
        dloop_seq = seq[:599] + seq[16000:] # 在这里指定区域的位置！
        coding_seq = seq[599:16000] # 在这里指定区域的位置！
        dloop_sequences[seq_id] = dloop_seq
        coding_sequences[seq_id] = coding_seq
    return dloop_sequences, coding_sequences

def calculate_chain_and_log_parameters(num_sequences):
    length_of_chain = 3000 * (num_sequences ** 2)
    log_parameters_every = length_of_chain // 30000
    return length_of_chain, log_parameters_every

def main():
    read_sequence_list = input("是否要读取序列ID列表？请回答YES或者NO: ").strip().upper() == 'YES'
    
    if read_sequence_list:
        clean_special_characters(input_txt_path)
        sample_info = read_sample_info(input_txt_path)
    else:
        sample_info = {}

    include_haplogroup = input("是否需要添加单倍群信息至ID？请输入YES或者NO: ").strip().upper() == 'YES'
    
    extracted_sequences = extract_sequences(input_fasta_path, sample_info, include_haplogroup)
    
    if not extracted_sequences:
        print("没有找到任何序列。")
        return
    
    num_sequences = len(extracted_sequences)
    length_of_chain, log_parameters_every = calculate_chain_and_log_parameters(num_sequences)
    
    print(f"序列数目: {num_sequences}")
    print(f"总代数 (Length of chain): {length_of_chain}")
    print(f"样本容量 (Log parameters every): {log_parameters_every}")
    
    write_nexus(extracted_sequences, output_nexus_path)

    dloop_sequences, coding_sequences = split_sequences(extracted_sequences)
    
    write_fasta(dloop_sequences, output_dloop_fasta_path)
    write_fasta(coding_sequences, output_coding_fasta_path)

    print(f"提取已经完成. 提取的序列已经转为NEX并保存至 {output_nexus_path}")
    print(f"DLOOP序列已经保存至 {output_dloop_fasta_path}")
    print(f"CODING序列已经保存至 {output_coding_fasta_path}")
    print(f"M单倍群60500年前;#N单倍群58200年前;#R单倍群54300年前")

if __name__ == "__main__":
    main()

复制<br><br><br>
<br>导入你的mtDNA序列数据：

<br>启动BEaUTI。
<br>在“File”菜单中选择“Import Alignment”来导入你的mtDNA序列（包括编码区的位置 577–16023）。


<br><br>
<br>设置替代模型：

<br>转到“Partitions”选项卡。
<br>选择你的数据分区，然后点击“Site Model”。
<br>将替代模型设置为“GTR+I”（General Time-Reversible model with Invariant sites）。


<br><br>
<br>设置时钟模型：

<br>转到“Clock Models”选项卡。
<br>选择“Strict Clock”。
<br>在“Clock Rate”字段中输入 1.691 × 10⁻⁸。


<br><br>
<br>设置树的先验信息：

<br>转到“Priors”选项卡。
<br>将“Tree Prior”设置为“Coalescent: Bayesian Skyline”。
<br>将“Number of groups”设置为20。
<br>将“Skyline Model”选择为“Piecewise-linear”。


<br><br>
<br>设置MCMC参数：

<br>转到“MCMC”选项卡。
<br>将“Chain Length”设置为 40,000,000。
<br>将“Store Every”设置为 4000。
<br>将“Pre Burnin”设置为 4,000,000。
<br>为每个群体进行三次独立的运行。


<br><br>
<br>保存设置并生成XML文件：

<br>确认所有设置都已正确配置。
<br>在“File”菜单中选择“Save As”来保存设置并生成BEAST的输入XML文件。


<br>完成这些步骤后，你可以使用生成的XML文件在BEAST中进行分析。这样可以确保你再现文献中描述的BSP分析参数。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033728.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033418.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033395.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033852.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022034907.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022035950.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在最后这一步，请生成3个名字不同但参数相同的 xml 文件。并分别用 BEAST 软件运行，最后使用 LogCombiner 软件进行合并即可。<br>注意，没有截图的参数保持原样不要动！<br><br><a data-href="贝叶斯天际线图（Bayesian Skyline Plot, BSP）" href="术语\贝叶斯天际线图（bayesian-skyline-plot,-bsp）.html" class="internal-link" target="_self" rel="noopener nofollow">贝叶斯天际线图（Bayesian Skyline Plot, BSP）</a> 的基础人口规模函数可以指定为人口规模变化的逐步函数或分段线性函数。主要数据是通过分段线性模型获得的。有效种群规模的估计是根据随时间推断的谱系合并率得出的。祖先基因树是根据一般时间可逆（GTR）替换模型推断的，该模型具有第一、第二和第三密码子位置的位点特异性比率。通过重要性采样计算的贝叶斯因子（Newton et al. 1994）表明该模型比常用的 GTR + Γ + I 模型（log Bayes 因子 = 19.1）更好地拟合数据。每个 MCMC 样本均基于 40,000,000 代的运行，每 4,000 代采样一次，前 4,000,000 代作为老化被丢弃。对 MCMC 图的自相关时间的检查表明运行已收敛到平衡分布，并且此时提供了足够的后验样本。所有运行的聚结先验有效样本量至少为 1,​​000<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a> <a data-footref="2" href="about:blank#fn-2-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[2]</a>。<br>使用 BEAST（Drummond 和 Rambaut 2007）基于13个蛋白质编码区重建了有效种群大小（Nef）随时间变化的 BSP，如其他地方所述（Atkinson 等人，2008;Atkinson 等人，2009 年）。在 BSP 分析中，采用第一、第二和第三密码子的通用时间可逆替换模型来推断祖先基因树。为了估计 Nef 变化的时间尺度，选择了一个严格的分子钟，其固定速率为每年每个位点1.691e 8次替换（Ho 和 Endicott 2008）。每个马尔可夫链蒙特卡洛模拟运行了 40,000,000 代，每 4,000 代采样一次，前 40,000 代作为老化丢弃。结果使用 Tracer v1.5（<a data-tooltip-position="top" aria-label="http://tree.bio.ed.ac.uk/software/tracer/%EF%BC%8C2009%E5%B9%B412%E6%9C%881%E6%97%A5%E8%AE%BF%E9%97%AE%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E3%80%82%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E7%8E%87%E5%92%8C%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E6%97%B6%E9%97%B4%E6%98%AF%E6%A0%B9%E6%8D%AE%E5%85%88%E5%89%8D%E7%A0%94%E7%A9%B6%E5%90%8E%E7%9A%84%E5%A4%A9%E9%99%85%E7%BA%BF%E5%9B%BE%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%88Gignoux" rel="noopener nofollow" class="external-link" href="http://tree.bio.ed.ac.uk/software/tracer/%EF%BC%8C2009%E5%B9%B412%E6%9C%881%E6%97%A5%E8%AE%BF%E9%97%AE%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E3%80%82%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E7%8E%87%E5%92%8C%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E6%97%B6%E9%97%B4%E6%98%AF%E6%A0%B9%E6%8D%AE%E5%85%88%E5%89%8D%E7%A0%94%E7%A9%B6%E5%90%8E%E7%9A%84%E5%A4%A9%E9%99%85%E7%BA%BF%E5%9B%BE%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%88Gignoux" target="_blank">http://tree.bio.ed.ac.uk/software/tracer/，2009年12月1日访问）可视化。人口增长率和人口增长时间是根据先前研究后的天际线图计算的（Gignoux</a> 等人，2011 年）<a data-footref="3" href="about:blank#fn-3-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[3]</a>。<br><br><br><br><br>
<br>
<br>mtDNA Variation Predicts Population Size in Humans and Reveals a Major Southern Asian Chapter in Human Prehistory<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
<br>Bayesian coalescent inference of major human mitochondrial DNA haplogroup expansions in Africa<a href="about:blank#fnref-2-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
<br>River Valleys Shaped the Maternal Genetic Landscape of Han Chinese<a href="about:blank#fnref-3-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\beast\beast：线粒体dna的bsp设置.html</link><guid isPermaLink="false">软件/BEAST/BEAST：线粒体DNA的BSP设置.md</guid><pubDate>Thu, 18 Jul 2024 08:22:44 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033728.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407022033728.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[安装必要的软件 java]]></title><description><![CDATA[ 
 <br><br>sudo apt-get update
sudo apt-get install cmake build-essential autoconf automake libtool git pkg-config openjdk-11-jdk
复制<br><br>git clone --depth=1 https://github.com/beagle-dev/beagle-lib.git
cd beagle-lib
mkdir build
cd build
复制<br><br><br>cmake -DCMAKE_INSTALL_PREFIX:PATH=$HOME ..
make install
复制<br><br>首先需要安装英伟达驱动：<a data-tooltip-position="top" aria-label="https://developer.nvidia.com/cuda-downloads" rel="noopener nofollow" class="external-link" href="https://developer.nvidia.com/cuda-downloads" target="_blank">链接</a><br>
如果安装成功，可以通过 nvidia-smi 和 nvcc --version 命令查看。<br>cmake -DCMAKE_INSTALL_PREFIX:PATH=$HOME -DBUILD_CUDA=ON -DBUILD_OPENCL=OFF ..
make install
复制<br><br>export LD_LIBRARY_PATH=$HOME/lib:$LD_LIBRARY_PATH
export PKG_CONFIG_PATH=$HOME/lib/pkgconfig:$PKG_CONFIG_PATH
echo 'export LD_LIBRARY_PATH=$HOME/lib:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc
echo 'export PKG_CONFIG_PATH=$HOME/lib/pkgconfig:$PKG_CONFIG_PATH' &gt;&gt; ~/.bashrc
source ~/.bashrc
复制<br>如果你使用的是 WSL，那么需要修改一下beagle-lib/CMakeLists.txt：<br>修改前：<br>link_directories(
    ${PROJECT_SOURCE_DIR}/libhmsbeagle
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/CPU
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/GPU
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/JNI
)
复制<br>修改后：<br>link_directories(
    ${PROJECT_SOURCE_DIR}/libhmsbeagle
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/CPU
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/GPU
    ${PROJECT_SOURCE_DIR}/libhmsbeagle/JNI
    /usr/lib/wsl/lib
    /usr/local/cuda/lib64
)
复制<br><br>export LD_LIBRARY_PATH=$HOME/lib:$LD_LIBRARY_PATH
复制<br><br>#!/bin/bash
export PATH=/home/luolintao/SRA_download/sratoolkit.3.1.0-ubuntu64/bin:/home/zhiyongwang/.aspera/cli/bin:/home/bin:/home/guanglinhe/biosoftware/AdmixTools/bin:/home/anaconda3/include:/usr/include:/usr/share/R/include:/home/biosoftware/OpenBLAS:/home/anaconda3/bin:/home/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
export LD_LIBRARY_PATH=$HOME/lib:$LD_LIBRARY_PATH
export PERL5LIB="/home/luolintao/perl5/lib/perl5${PERL5LIB:+:${PERL5LIB}}"
export PERL_LOCAL_LIB_ROOT="/home/luolintao/perl5${PERL_LOCAL_LIB_ROOT:+:${PERL_LOCAL_LIB_ROOT}}"
export PERL_MB_OPT="--install_base \"/home/luolintao/perl5\""
export PERL_MM_OPT="INSTALL_BASE=/home/luolintao/perl5"

# 启动 beast
/home/luolintao/BEASTv1.10.4/BEASTv1.10.4/bin/beast -beagle_cpu -beagle_order 1 /home/luolintao/BEASTv1.10.4/BEASTv1.10.4/INPUT/Han_N.xml

复制<br><br>
<br>-beagle_off: 不使用BEAGLE库进行计算。
<br>-beagle: 如果可用，使用BEAGLE库（默认开启）。
<br>-beagle_info: 显示可用的BEAGLE资源信息。
<br>-beagle_order: 设置资源使用顺序。例如，-beagle_order 1意味着使用资源1。
<br>-beagle_instances: 在多个实例之间划分站点模式。
<br>-beagle_CPU: 使用CPU实例进行计算。
<br>-beagle_GPU: 如果可用，使用GPU实例进行计算。
<br>-beagle_SSE: 如果可用，使用SSE扩展进行计算。
<br>-beagle_SSE_off: 禁用SSE扩展。
<br>-beagle_cuda: 如果可用，使用CUDA并行化进行计算。
<br>-beagle_opencl: 如果可用，使用OpenCL并行化进行计算。
<br>-beagle_single: 如果可用，使用单精度计算。
<br>-beagle_double: 如果可用，使用双精度计算。
<br>-beagle_async: 如果可用，使用异步内核。
<br>-beagle_scaling: 指定使用的缩放方案。
<br>-beagle_rescale: 动态缩放时指定重新缩放的频率。
<br><br>beast -beagle_cuda -beagle_order 1 data.xml
复制<br>/to/your/path/beast/bin/beast -beagle_GPU -beagle_SSE /to/your/path/输入文件.xml
复制<br><br>输入命令 /to/your/path/beast/bin/beast -h 可以查看完整的命令行参数，需要注意的几点有<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>：  <br>
<br>beast 会根据机器的计算资源，来自动的配置线程数，通常情况下，这个线程数会是你的机器能开启的最大线程，所以，除非你十分了解你的机器或希望约束beast 所调用的资源，那么就你不应该设置一个 --threads 参数来约束 beast 所开启的线程数目。  
<br>beast_instances 的作用是给输入文件划分子分区，注意，这是子分区而不是分区，分区是输入文件中定义的，beast 计算程序不会给输入文件划分分区，根据，如果划分了子分区，一般都会导致计算性能的下降。  
<br>beast_order 用来指定各个分区或子分区依次使用哪些计算资源，如果不指定该参数，那么程序将只会调用起一块计算资源。参数就是1,2,3,4这样的自然数，分别对应机器上的第一块CPU（GPU），第二块CPU（GPU）。  
<br>如果输入文件没有分区，那么即使指定了 beagle_order 选项，也无法调用起多块计算资源，这时可以通过beagle_instances划分子分区来强行调用多块资源来进行计算，但是计算效率未必尽如人意。  
<br>如果输入文件有 4 个分区，那么只有指定了 beagle_order 选项，才会调用起多块资源来并行计算。  
线粒体 DNA 分区为2，可使用 beagle_order 2

<br>执行 /to/your/path/beast/bin/beast -beagle_info 可以查看 beast 识别到的机器上的计算资源。  
<br>每次beast开始计算时，都会在执行命令的目录下产生程序的日志输出文件，如果不删除该日志文件来重新计算同一个输入文件，那么会报错说日志文件已存在，这时可以手动删除该文件或添加 -overwrite 参数来强行覆盖原来的日志输出文件。  
<br>查看英伟达显卡显存和利用效率的命令为：nvidia-smi
<br><br><br>
<br>
<br><a data-tooltip-position="top" aria-label="https://blog.qiql.net/archives/beast" rel="noopener nofollow" class="external-link" href="https://blog.qiql.net/archives/beast" target="_blank"><span></span>Linux 平台 BEAST 安装及调试指南【附安装脚本】 (qiql.net)</a><img class="emoji" draggable="false" alt="🔥" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f525.svg" height="18px" style="max-width: 100%;"><a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\beast\beast：在服务器内运行.html</link><guid isPermaLink="false">软件/BEAST/BEAST：在服务器内运行.md</guid><pubDate>Sat, 29 Jun 2024 11:50:01 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f525.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f525.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[MrBayes：贝叶斯方法建树]]></title><description><![CDATA[ 
 <br>贝叶斯方法建树 MrBayes<br>网址： <a rel="noopener nofollow" class="external-link" href="http://mrbayes.sourceforge.net/" target="_blank">http://mrbayes.sourceforge.net/</a><br>MrBayes 是一款利用贝叶斯方法进行进化树构建的软件。贝叶斯方法建树与最大似然法建树有密切联系，最大似然法是寻找合适的参数（树型、枝长和进化模型，），使得到数据（多序列比对结果）的似然率最大，最大化 P (Data | Tree)；而贝叶斯方法则是利用给定数据，寻找概率最大的树型，最大化 P (Tree | Data)。不仅如此，贝叶斯方法还提供给定数据条件下，各种树型出现的概率，也称后验概率。但实际上各种树型的的后验概率很难直接计算，一般是采用 MCMC 方法来近似。图 3.1 是利用 MCMC(Markov Chain Monte Carlo)方法构建贝叶斯树的流程图。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154503.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>MrBayes 是 DOS 界面的，不需安装可直接使用。它默认读入 nexus 格式的多序列比对结果文件进行分析，我们首先用程序提供的演示序列学习使用方法。<br>3.1 运行 MrBayes，读入 nex 文件，确定参数。<br>解压程序文件夹，可以看到里面有两个可执行文件，mrbayes_x64 适用于 64 位操作系统的电脑，mrbayes_X86 适合 32 位电脑。双击应用程序，打开界面（图 3.2）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154509.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在”MrBayes &gt;”控制符后输入 execute examples/primates.nex&nbsp; 点击回车，程序读入演示用的多序列比对文件 primates.nex，并输出相关数据供核对。（图 3.3）  <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154515.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图 3.3 MrBayes 读入序列<br>如果要分析自己的序列，需先用序列比对软件（如 ClustalX)进行多序列比对，输出比对结果为 MrBayes 可识别的为 nexus 文件，或者用比对结果转换软件将其它格式的比对结果转换成 nexus 格式。<br>Tips: 在用 ClustalX 做多序列比对前，先将要比对的序列 fasta 格式第一行（“&gt;”后）只保留物种名_序列名，这样方便后续分析，用 ClustalX 读入序列后，在 output format option 选中 nexus 格式，对序列进行比对（Do complete alignment）。生成的 nxs 文件可以用写字板打开浏览，里面内容是多条序列比对结果。将它与 mrbayes 可执行文件放在同一目录下即可读入分析。<br>可以在控制符后输入 help lset，查看默认的参数(图 3.4)。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154520.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图 3.4 默认参数（部分）<br>Nucmodel 设定核酸模型，doublet 是二联子核苷酸，有 16 种状态，用于分析核糖体 DNA 茎环结构，codon 是将核酸作为三联体密码子分析的，默认的 4by4 就是将核酸按其四种核苷酸（ACGT）进行分析.<br>Nst 设定替换模型，1 是 JC Model，所有核苷酸替换速率相同，2 是 kimura Model，将转换和颠换区分开来，6 是 General Time Reversible (GTR) model，任意两个核苷酸之间替换速率均不相等，有 6 个速率参数。&nbsp;<br>Code 是选择密码子编码方式的，只有在核酸模型（Nucmodel）选择为 codon 时需要设定。<br>Ploidy 有两个选项，单倍体 haploid 和多倍体 diploid, 只有在设定了 coalescence prior 时需要设定。<br>Rates是设定替换速率的，默认的是equal(所有位点替换速率相同）, 还可选择gamma（位点间替换速率变化服从gamma分布），propinv（有一部分位点的替换速率是不变的），<br>Invgamma (有一部分位点的替换速率是不变的，其余位点的替换速率服从gamma分布)&nbsp; adgamma（相邻位点的替换速率是相关的，其边缘分布为gamma分布）.<br>Ngammacat：由于 gamma 分布是连续的，无法计算某个点的概率，所以这里采用了近似的方法，将连续的 gamma 分布分成几个单元（categories）,每个单元的平均速率作为此单元的替换速率。这里设定的就是独立的单元数目(the number of discrete categories)，默认为 4，这个数字越高，近似效果越好，但需要的时间也越长。<br>Nbetacat：当使用表型数据（morphological data）建树时，替换速率常采用 beta 分布近似，与 gamma 分布相同，需要将连续的分布近似为几个离散的分类，这里设定的就是独立的分类数目。<br>Omegavar 设定各位点间的非同义替换与同义替换速率的比值。Equal 假定各位点间的<br>omega (nonsynonymous/synonymous rate)相同，Ny98 和M3各位点间omega值不同，两个模型的取值范围有所不同。<br>Covarion 设定在进化历史上各位点替换速率是否恒定。选择 yes，则各位点突变速率随时间会发生变化。&nbsp;<br>Coding 设定数据抽样方式，all 适用于所有类型的数据，所有的字符都会被抽中；variable 只适合表型数据和限制性位点（restriction site）数据，只有发生了变化的数据会被抽中； noabsencesite 适合限制性位点数据，所有物种中不存在的位点不抽取。<br>Parsmodel 是否利用简约模型（parsimony model）。<br>在控制符后输入 lset nst=6 rates=invgamma，点击回车(图 3.5)。将进化模型设定为<br>GTR 模型，4 种核苷酸替换速率各不相同，序列中有部分位点不发生替换，其它位点的替换服从 Gamma 分布。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154530.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这时在控制符后输入 Help lset，查看刚才设定的参数是改变（图 3.6）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154535.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>如果你分析的是蛋白质序列，需要提前设定氨基酸替换模型，可选的氨基酸替换模型包<br>括 poisson(20 种氨基酸频率相同，替换速率相同)，jones（Jones，1992）, dayhoff(Dayhoff,<br>Schwartz and Orcutt, 1978), mtrev(Adachi and Hasegawa, 1996), mtmam(Cao et al., 1998; Yang, Nielsen, and Hasegawa, 1998), wag(Wheland and Goldman, 2001), rtrev(Dimmic et al., 2002), cprev(Adachi et al., 2000)，vt(Muller and Vingron, 2000)，<br>blosum(Henikoff and Henikoff, 1992)，equalin，gtr。由于蛋白质替换符合哪个模型事<br>先未知，可以设定 prset aamodelpr=mixed，则 MCMC 算法对前 10 个模型分别尝试<br>（jumping between fixed-rate amino acid models），每个模型对结果的贡献与其后验<br>概率成正比。<br>prset 命令是用来设定各个参数的先验概率分布，可以根据你对自己数据的了解来设定，也可以设定一个无信息的先验概率分布（所有模型的概率都均等）。MrBayes 默认的是无信息的先验分布，所以如果对自己的数据没有先验经验，可以不做设定。想了解系统对各个参数的先验设定，在控制符后输入 help prset 即可查看。<br>3.2 MCMC<br>在控制符后输入 mcmc ngen=100000 samplefreq=100 printfreq=100 diagnfreq=1000，点击回车，程序开始运行（图3.7），首先输出你设定的参数和待分析数据情况，然后输出抽样结果。<br>Ngen设定MCMC算法进行的循环代数，也就是对树型、枝长或进化模型参数改变的次数，这里设定为10万次；默认值为100万。因为每次对树型、枝长或进化模型参数改变的并不大，如果每个循环（每次改变）都抽样，输出文件太大，样本相似度太高；所以这里samplefreq 抽样频率设定为100个循环抽一次，100000个循环将会从后验概率分布中抽得1000个样本；默认抽样频率为500代抽一次。printfreq是分析结果输出屏幕的频率，默认是500代输出一次。MCMC默认同时运行两个独立的分析（run），也就是从八个不同的起始树（initial tree，每个分析4棵起始树）开始运行。同时运行两个独立的分析，可以在运行过程中诊断结果是否收敛，刚开始两个分析抽样出来的树差异比较大，但随着代数的增加，它们会收敛，越来越相似，说明我们已经得到了一个好的后验概率分布样本；诊断频率diagnfreq设定每过多少代，检查一下两个分析结果的差异，默认为5000代一次。如果你序列数目比较少，很快就达到收敛状态，ngen可以设置的小一点儿，抽样和诊断频率高一点儿（samplefreq小一点儿）；反之如果你的样本比较大，可能需要更多代数才能达到收敛，这时抽样和诊断频率可以低一点儿（samplefreq大一点儿）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154546.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图 3.7 MCMC 运行界面（部分）<br>图 3.7 第一列是代数（也即循环数），后面四列数字分别为第一轮运行的四条链各自的对数概率值（log likelihood），[ ]内的对应的是冷链(cold chain)，（）内的是热链（heated chains）。注意冷热链间应该能变换状态（图中上下两行间[]的位置发生变化），如果不能，则算法效率不高，可能需要延长代数或降低冷热链间温度差距（Temp）。*号后的是第二轮运行的四条链的情况。最后一列是估计的完成设定代数还需要的时间。<br>运行结束后如果 standard deviation of split frequencies 小于 0.01 ，在程序询问<br>“Continue the analysis?”时回答 no,否则答 yes,增加世代数继续运行至小于 0.01。<br>运行结束可以看到在输出界面有一个链交换信息（chain swap information），矩阵左上部分是交换频率，这些值如果在 0.1~0.8 之间，说明结果合理，否则要重新设置增加参数，如增加代数 ngen，降低 Temp 等。<br>这时在 MrBayes 可执行程序文件夹内可看到生成五个文件，扩展名为 mcmc 的文件一个，扩展名为 p 和 t 的文件各两个,每个分析（run）一个，mcmc 文件记录的是抽样的信息， p 文件记录了每个抽样的模型参数，可以用写字板打开查看；t 文件是树型和枝长数据，可以用 TreeView 打开，每个 run 有一个树文件，里面包含了 1000 棵树。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154600.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图 3.8 MCMC 运行结束输出界面（部分）<br>3.3 归纳参数和树型（sump &amp; sumt）<br>在控制符后输入sump burnin=250，点击回车运行。<br>Sump对MCMC抽样的替换模型参数（(p文件)）进行总结，由于起始阶段的树型概率并<br>不高，不是最优树，所以要将这部分样本抛弃。burnin后面的250就是这里要抛弃的样本数量。程序默认是将样本的前25％抛弃(（burninfrac = 0.25）)后再总结。你可以改变burnin<br>的数量或比例，如果就用默认的25%，则可以只输入sump即可。输出结果包括三部分内容：首先是数据的log-probability随世代数变化的图，图上的点应该没有明显趋势，说明抽样到达了平稳状态（stationarity）（左图）如果你的分析数据显示随世代数增加，log-probability 有上升或下降的趋势（右图），那么需要增加代数，继续分析；第二部分是两轮MCMC分析的边缘概率估计；第三部分是用到的各个参数取值情况。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154607.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154615.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154620.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154626.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>&nbsp;图 3.9 Summarize the parameter 结果<br>图 3.9 第三张图片共输出了 13 个参数的情况，第一行 TL 是进化树的总枝长，接下来 6 个 r 是两两核苷酸替换速率，再下来 4 个 pi 是四种核苷酸频率，alpha 是 gamma 分布的形状参数，pinvar 是不变位点所占比例。每个参数分别输出其均值 mean，方差 variance， 95％置信区间的 lower 和 upper 值，中位数 median，如果 MCMC 运行结果收敛，最后一栏<br>PSRF(potential scale reduction factor)值应该在 1 左右。<br>然后可以通过 sumt burnin=250 对所有抽样树型进行总结 (同样前 25％的样本被抛弃)，程序对树型进行总结，输出所有枝长参数，同时输出 cladogram，上面标注了每个分枝(clade)的后验概率，还输出 phylogram，各枝的长度代表对应的进化距离（图 3.10）。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154632.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154641.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154644.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图 3.10 Summarize Trees 结果<br>观察程序所在文件夹，可以看到生成了六个文件，扩展名为 parts 的文件里记录的是构<br>建出来的进化树的分枝类型（branch pattern, partition）。扩展名为 tstat、vstat 和 lstat 的文件分别总结了各种树型（topology）、枝长(branch length)和似然值（marginal likelihood）的统计数据，这三个文件可用写字板打开查看；扩展名为 con.tre 的文件就是总结出来的一致树，可用 Treeview 软件打开查看，可看到各分枝的后验概率及枝长（standard deviation from posterior probability）信息；trprobs 文件包含了 MCMC搜索过程中得到的树，且按照后验概率大小排序。]]></description><link>软件\beast\mrbayes：贝叶斯方法建树.html</link><guid isPermaLink="false">软件/BEAST/MrBayes：贝叶斯方法建树.md</guid><pubDate>Fri, 21 Jun 2024 07:31:17 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154503.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606154503.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Mrbayes：系统发育树构建流程]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://user.qzone.qq.com/58001704/2" rel="noopener nofollow" class="external-link" href="https://user.qzone.qq.com/58001704/2" target="_blank">Raindy Qzone</a>
<br>贝叶斯法（ Bayesian inference, BI）是基于进化模型的统计推论法，具有完整而坚实的数学和统计学基础，可以处理复杂而接近实际情况的进化模型，可以将现有的系统发育知识整合或体现在先验概率中，通过后验概率直观反映出各分支的可靠性而不需要通过自举法检验。Bi法适用于大或复杂的数据集，其缺点是对进化模型比较敏感，BI法中指定的每个氨基酸的后验概率建立在许多假说条件下，在现实中可能不成立。<br>重要必看！重要必看！重要必看！
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111529889.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br><br>在这里不阐述了，多个软件都可以进行：<a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a>，<a data-href="序列比对（Align）软件评测" href="软件\其它生信软件\使用心得\序列比对（align）软件评测.html" class="internal-link" target="_self" rel="noopener nofollow">序列比对（Align）软件评测</a>，<a data-href="序列比对的思考" href="软件\其它生信软件\使用心得\序列比对的思考.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="❔" aria-label="❔" data-icon="❔" aria-hidden="true" style="transform: translateY(0px);"></span>序列比对的思考</a><img class="emoji" draggable="false" alt="❔" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/2754.svg" height="18px" style="max-width: 100%;">。得到一个比对完成的 fasta 文件。<br><br>将得到的序列提交Gblock在线服务器（<a rel="noopener nofollow" class="external-link" href="http://www.phylogeny.fr/one_task.cgi?task_type=gblocks" target="_blank">http://www.phylogeny.fr/one_task.cgi?task_type=gblocks</a>），得到保守区的序列align_cured.fasta。<br>这个网站目前缺乏维护，已经出现了很多替代品。这里推荐Trimal软件，可以自动修剪去除不可靠的区域：<a data-href="Trimal：自动修剪序列不可靠区域" href="软件\其它生信软件\t-z\trimal：自动修剪序列不可靠区域.html" class="internal-link" target="_self" rel="noopener nofollow">Trimal：自动修剪序列不可靠区域</a>。<br><br>通常有两种方法：  <br>
<br>第一种是PAUP 软件验证替换饱和，简要操作如下：  在PAUP中分别计算p距离和GTR+I+G距离，然后在Excel中做散点图。（a）如果散点分别在y=x直线上，就说明没达到饱和；（b）如果GTR+I+G距离&gt;p距离就说明饱和了。
<br>第二种用DAMBE 软件验证替换饱和，该法最方便实用，推荐使用。只要比较ISS和ISS.c 值大小及显著与否，即可。当ISS小于ISS.c 且p=0.0000（极显著），就说明没序列替换未饱和，可以建树。<br>
在这里使用第二种方法：DAMBE：核苷酸替代饱和度检测<br><br><br><a data-tooltip-position="top" aria-label="http://dambe.bio.uottawa.ca/DAMBE/dambe.aspx" rel="noopener nofollow" class="external-link" href="http://dambe.bio.uottawa.ca/DAMBE/dambe.aspx" target="_blank">XiaLab (uottawa.ca)</a><br>或者你也可以使用如下连接下载，在 Win10及以上电脑：<a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hflO2Qz-mZKR2fkGFQ?e=kJEmlp" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hflO2Qz-mZKR2fkGFQ?e=kJEmlp" target="_blank">DAMBE.msi</a><br>注意，这里有个 bug：
我们需要在属性中选择兼容模式运行，否则可能在 win11电脑闪退！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111713579.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>通常有两种方法：  <br>
<br>第一种是PAUP 软件验证替换饱和，简要操作如下：  在 PAUP 中分别计算 p 距离和 GTR+I+G 距离，然后在 Excel 中做散点图。（a）如果散点分别在 y=x 直线上，就说明没达到饱和；（b）如果 GTR+I+G 距离&gt;p 距离就说明饱和了。
<br>第二种用DAMBE 软件验证替换饱和，该法最方便实用，推荐使用。只要比较 ISS 和 ISS.c 值大小及显著与否，即可。当 ISS 小于 ISS.c 且 p=0.0000（极显著），就说明没序列替换未饱和，可以建树。<br>
我们在这里展示第二种方法的具体操作：
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111706837.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111706165.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111705181.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111533427.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我们发现了，Iss 小于 Iss.c 且 P&lt;0.05，因此替换没有饱和，适合建树。
<br><br>多种软件都可以计算：<a data-href="iqtree：寻找最优模型及分区" href="软件\iqtree\iqtree：寻找最优模型及分区.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：寻找最优模型及分区</a>，<a data-href="图解核苷酸替换模型（Nucleotide substitution models）" href="术语\图解核苷酸替换模型（nucleotide-substitution-models）.html" class="internal-link" target="_self" rel="noopener nofollow">图解核苷酸替换模型（Nucleotide substitution models）</a>, <a data-tooltip-position="top" aria-label="http://user.qzone.qq.com/58001704/blog/1369539093" rel="noopener nofollow" class="external-link" href="http://user.qzone.qq.com/58001704/blog/1369539093" target="_blank">其它的软件</a><br><br>我们可以在如下模板中替换：将其添加（2）中的 *.nex 文件中。<br>begin mrbayes;
outgroup TuMV;
lset nst=6 rates=invgamma ngammacat=16; 
Prset statefreqpr=dirichlet(1,1,1,1);
mcmcp savebrlens=yes ngen=2000000 samplefreq=100 nchains=4; # MCMC长度和样本容量等，与BEAST相似。
mcmc;
sumt contype=allcompat burnin=5000;
end;
复制<br><br>输入 exe BI-ref.nex 回车，最后生成 *.tre，即最终的BI树。在运行1000代后都会显示 Average standard deviation of split frequencies。<br>
<br>当这个值 &lt; 0.01 时，说明两次运行的结果差异显著，Convergence 已经达到，这时可以输入 no 终止运行；  
<br>当这个值 &gt;0.01 时，说明两次运行的结果差异不显著，可以输入 yes ，并输入继续运行的代数，直至上述值&lt; 0.01为止。<br>
这个指标不是绝对的，当Average standard deviation of split frequencies值一直达不到理想值，可以通过 tracer 查看 *.p 文件，当所有参数ESS &gt;200时，说明参数已收敛。
<br><br>用 Figtree 查看生成&nbsp;.tre，美化修饰。]]></description><link>软件\beast\mrbayes：系统发育树构建流程.html</link><guid isPermaLink="false">软件/BEAST/Mrbayes：系统发育树构建流程.md</guid><pubDate>Thu, 12 Sep 2024 03:22:18 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111529889.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409111529889.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[文件准备]]></title><description><![CDATA[ 
 <br><br>
<br>fasta 文件
<br>META 文件（也就是分组文件）<br>
如下所示：<br>
fasta 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405052256925.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
META 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405052256353.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>我更新了一个脚本，使用 Python 提取相应的序列，然后再进行运行。Python 如下：<br>import sys
import pandas as pd

def read_ids(filename):
    ids = pd.read_csv(filename, sep='\t', header=None, usecols=[0])
    return ids[0].tolist()

def filter_fasta(input_fasta, ids, output_fasta):
    with open(input_fasta, 'r') as fasta_file, open(output_fasta, 'w') as outfile:
        write_sequence = False
        for line in fasta_file:
            if line.startswith('&gt;'):
                sequence_id = line.split('&gt;')[1].strip().split(' ')[0]
                if sequence_id in ids:
                    write_sequence = True
                    outfile.write(line)
                else:
                    write_sequence = False
            elif write_sequence:
                outfile.write(line)

def main(input_ids_file, input_fasta_file, output_fasta_file):
    ids = read_ids(input_ids_file)
    filter_fasta(input_fasta_file, ids, output_fasta_file)
    print("Filtered fasta file has been created.")

if __name__ == '__main__':
    if len(sys.argv) != 4:
        print("Usage: python extract_fasta_sequences.py &lt;meta_file&gt; &lt;fasta_file&gt; &lt;output_file&gt;")
        sys.exit(1)
    main(sys.argv[1], sys.argv[2], sys.argv[3])

复制<br>要把我写的 Python 脚本放在/mnt/e/Scientifc_software/fastHaN-main/fastHaN-main 文件夹下，并且名字不能更改。<br><br>#!/bin/bash

# 定义常用变量
#################################把下面的改了################################
FASTA_PATH="/mnt/c/Users/victo/Desktop/Illumina_mtDNA_hap.fasta" # 你的fasta文件
OUTPUT_PATH="/mnt/c/Users/victo/Desktop" # 输出结果路径
SOFTWARE_PATH="/mnt/e/Scientifc_software/fastHaN-main/fastHaN-main/" # 软件所在路径
META_FILE="/mnt/c/Users/victo/Desktop/新建TextDocument.txt" # META文件所在路径
##################################把上面的改了################################
OUTPUT_FASTA="${OUTPUT_PATH}/filtered_sequences.fasta" #这个不用更改。

# 运行 Python 脚本（需要把我写的Python脚本放在/mnt/e/Scientifc_software/fastHaN-main/fastHaN-main文件夹下，并且名字不能更改#######
###############################################################################################
python3 "${SOFTWARE_PATH}extract_fasta_sequences.py" "$META_FILE" "$FASTA_PATH" "$OUTPUT_FASTA"

# 定义常用变量
EXTRACTED_FASTA_PATH=$OUTPUT_FASTA # 你的fasta文件


# 将fasta序列转为phy并压缩为gz
GZ_FILE="$OUTPUT_PATH/fasta2phylip.phy.gz" # 输出.phy.gz文件
python $SOFTWARE_PATH/Pipline/Fasta2Phylip.py $EXTRACTED_FASTA_PATH | gzip &gt; $GZ_FILE

# 进行network运算
# TCS算法
$SOFTWARE_PATH/fastHaN_linux original_tcs -i $GZ_FILE -t 8 -a 1 -m 1 -o $OUTPUT_PATH/ORIGIN_TCS

# mjn算法
$SOFTWARE_PATH/fastHaN_linux mjn -i $GZ_FILE -t 8 -e 0 -o $OUTPUT_PATH/MJN

# 改良TCS算法
$SOFTWARE_PATH/fastHaN_linux modified_tcs -i $GZ_FILE -t 8 -o $OUTPUT_PATH/TCS

# msn算法
$SOFTWARE_PATH/fastHaN_linux msn -i $GZ_FILE -e 0 -o $OUTPUT_PATH/MSN

# 分组文件整理
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/ORIGIN_TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/MJN.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/MSN.json $META_FILE $OUTPUT_PATH/Groupinformation
复制<br><br>修改脚本的 ########################################################## 之前的路径，一键运行。<br>#!/bin/bash

# 定义常用变量
FASTA_PATH="/mnt/c/Users/a/Desktop/Illumina_mtDNA_Filter_recode.fasta" # 你的fasta文件
OUTPUT_PATH="/mnt/c/Users/a/Desktop" # 输出结果路径
SOFTWARE_PATH="/mnt/e/OneDrive/文档（科研）/分析软件/fastHaN-main/fastHaN-main" # 软件所在路径
META_FILE="/mnt/c/Users/a/Desktop/illumina.meta" # META文件所在
##########################################################
# 将fasta序列转为phy并压缩为gz
GZ_FILE="$OUTPUT_PATH/fasta2phylip.phy.gz" # 输出.phy.gz文件
python $SOFTWARE_PATH/Pipline/Fasta2Phylip.py $FASTA_PATH | gzip &gt; $GZ_FILE

# 进行network运算
# TCS算法
$SOFTWARE_PATH/fastHaN_linux original_tcs -i $GZ_FILE -t 8 -a 1 -m 1 -o $OUTPUT_PATH/illumina_ORIGIN_TCS

# mjn算法
$SOFTWARE_PATH/fastHaN_linux mjn -i $GZ_FILE -t 8 -e 0 -o $OUTPUT_PATH/illumina_MJN

# 改良TCS算法
$SOFTWARE_PATH/fastHaN_linux modified_tcs -i $GZ_FILE -t 8 -o $OUTPUT_PATH/illumina_TCS

# msn算法
$SOFTWARE_PATH/fastHaN_linux msn -i $GZ_FILE -e 0 -o $OUTPUT_PATH/illumina_MSN

# 分组文件整理
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_ORIGIN_TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_MJN.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_MSN.json $META_FILE $OUTPUT_PATH/Groupinformation
复制]]></description><link>软件\fasthan\自制fasthan脚本——2024年5月13日更新.html</link><guid isPermaLink="false">软件/FastHaN/自制FastHaN脚本——2024年5月13日更新.md</guid><pubDate>Fri, 21 Jun 2024 07:31:04 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405052256925.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405052256925.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[新功能]]></title><description><![CDATA[ 
 <br><br>将下列2个 python 脚本放在软件目录下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240516175448.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意，名字必须一样：<br>
color_replace.py<br>
extract_fasta_sequences.py<br><br>import pandas as pd
import sys

# 获取传入的文件路径
input_file = sys.argv[1]
output_file = sys.argv[2]

# 定义新的颜色列表
new_colors = [+
    '#EA1F1F', '#E88421', '#E5C923',
    '#FFF924', '#9DEF1B', '#42D726',
    '#449657', '#4CCCB3', '#369BA8',
    '#2B7EBC', '#3626D1', '#A128CE','#2D2D2D'
]

# 读取CSV文件
df = pd.read_csv(input_file, delimiter=';', header=None, names=['Group', 'Color', 'Value'])

# 创建一个字典来跟踪颜色的分配以避免重复
assigned_colors = {}

# 替换颜色的函数并处理重复情况
def replace_colors_v3(row, color_index):
    color = new_colors[color_index % len(new_colors)]
    if color in assigned_colors:
        occurrence = assigned_colors[color]
        row['Value'] = f'lines-{occurrence * 2 - 1}'
        assigned_colors[color] += 1
    else:
        assigned_colors[color] = 1
    row['Color'] = color
    return row

# 替换数据框中的颜色
color_index = 0
for i in range(len(df)):
    df.iloc[i] = replace_colors_v3(df.iloc[i], color_index)
    color_index += 1

# 保存修改后的数据框到新的CSV文件
df.to_csv(output_file, sep=';', index=False, header=False)

复制<br><br>import sys
import pandas as pd

def read_ids(filename):
    ids = pd.read_csv(filename, sep='\t', header=None, usecols=[0])
    return ids[0].tolist()

def filter_fasta(input_fasta, ids, output_fasta):
    with open(input_fasta, 'r') as fasta_file, open(output_fasta, 'w') as outfile:
        write_sequence = False
        for line in fasta_file:
            if line.startswith('&gt;'):
                sequence_id = line.split('&gt;')[1].strip().split(' ')[0]
                if sequence_id in ids:
                    write_sequence = True
                    outfile.write(line)
                else:
                    write_sequence = False
            elif write_sequence:
                outfile.write(line)

def main(input_ids_file, input_fasta_file, output_fasta_file):
    ids = read_ids(input_ids_file)
    filter_fasta(input_fasta_file, ids, output_fasta_file)
    print("Filtered fasta file has been created.")

if __name__ == '__main__':
    if len(sys.argv) != 4:
        print("Usage: python extract_fasta_sequences.py &lt;meta_file&gt; &lt;fasta_file&gt; &lt;output_file&gt;")
        sys.exit(1)
    main(sys.argv[1], sys.argv[2], sys.argv[3])

复制<br><br>#!/bin/bash

# 基础路径设置
FASTA_PATH="/mnt/c/Users/victo/Desktop/Illumina_mtDNA_hap.fasta"
OUTPUT_PATH="/mnt/c/Users/victo/Desktop/output"
SOFTWARE_PATH="/mnt/e/Scientifc_software/fastHaN-main/fastHaN-main/"
META_FILE="/mnt/c/Users/victo/Desktop/文本.txt"
META_HAP_FILE="/mnt/c/Users/victo/Desktop/文本_HAP.txt"

# 输出的 FASTA 文件路径
OUTPUT_FASTA="${OUTPUT_PATH}/filtered_sequences.fasta"

# 提取序列并保存
python3 "${SOFTWARE_PATH}extract_fasta_sequences.py" "$META_FILE" "$FASTA_PATH" "$OUTPUT_FASTA"

# 文件转换并压缩
GZ_FILE="$OUTPUT_PATH/fasta2phylip.phy.gz"
python "${SOFTWARE_PATH}Pipline/Fasta2Phylip.py" "$OUTPUT_FASTA" | gzip &gt; "$GZ_FILE"

# 定义一个函数来执行网络计算
run_network_algorithm() {
    algorithm=$1
    input_file=$2
    output_dir=$3
    extra_args=$4
    "${SOFTWARE_PATH}fastHaN_linux" $algorithm -i "$input_file" $extra_args -o "$output_dir"
}

# 执行各种网络计算算法
run_network_algorithm "original_tcs" "$GZ_FILE" "$OUTPUT_PATH/ORIGIN_TCS" "-t 8 -a 1 -m 1"
run_network_algorithm "mjn" "$GZ_FILE" "$OUTPUT_PATH/MJN" "-t 8 -e 0"
run_network_algorithm "modified_tcs" "$GZ_FILE" "$OUTPUT_PATH/TCS" "-t 8"
run_network_algorithm "msn" "$GZ_FILE" "$OUTPUT_PATH/MSN" "-e 0"

# 生成网络配置文件
generate_network_config() {
    json_file=$1
    meta_file=$2
    output_dir=$3
    python "${SOFTWARE_PATH}Script/GenNetworkConfig.py" "$json_file" "$meta_file" "$output_dir"
}

for algorithm in "ORIGIN_TCS" "MJN" "TCS" "MSN"; do
    generate_network_config "$OUTPUT_PATH/${algorithm}.json" "$META_FILE" "$OUTPUT_PATH/Groupinformation"
    generate_network_config "$OUTPUT_PATH/${algorithm}.json" "$META_HAP_FILE" "$OUTPUT_PATH/HAPINFORMATION"
done

# 替换颜色配置文件
replace_color_config() {
    input_file=$1
    output_file=$2
    python3 "${SOFTWARE_PATH}color_replace.py" "$input_file" "$output_file"
}

replace_color_config "$OUTPUT_PATH/Groupinformation_groupconf.csv" "$OUTPUT_PATH/Groupinformation_groupconf_updated.csv"
replace_color_config "$OUTPUT_PATH/HAPINFORMATION_groupconf.csv" "$OUTPUT_PATH/HAPINFORMATION_groupconf_updated_HAP.csv"

复制]]></description><link>软件\fasthan\自制fasthan脚本——2024年5月16日更新.html</link><guid isPermaLink="false">软件/FastHaN/自制FastHaN脚本——2024年5月16日更新.md</guid><pubDate>Fri, 21 Jun 2024 07:31:04 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240516175448.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240516175448.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[详细使用指南]]></title><description><![CDATA[ 
 <br><br><br>首先，下载自制的脚本的软件包。<br>
链接：<a data-href="自制FastHaN脚本——2024年8月16日更新" href="软件\fasthan\自制fasthan脚本——2024年8月16日更新.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>自制FastHaN脚本——2024年8月16日更新</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br><br>脚本需要 Linux 系统运行。<br><br>你需要准备三个文件：<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163203.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163248.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
用 制表符 分割，没有标题。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163330.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
用 制表符 分割，没有标题。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163416.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>FASTA_PATH 修改为 fasta 文件路径
<br>OUTPUT_PATH 修改为输出文件路径
<br>SOFTWARE_PATH 修改为软件所在路径，具体可以看看下图：<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163527.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>META_FILE 修改为分组文件（按照其他条件分）
<br>META_HAP_FILE 修改为分组文件（按照单倍群分）
<br><br>省略<br><br>运行成功之后，会得到如下文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163718.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
其中我们需要的是 csv, gml 文件。带 updated 的文件是我配的彩虹色，可以用也可以不用。为了修改颜色，可以到软件的 color_replace.py 文件中修改。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163851.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>找到如下文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522163942.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>用浏览器打开。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522164122.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522164206.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240522164306.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\fasthan\自制fasthan脚本——2024年5月22日更新.html</link><guid isPermaLink="false">软件/FastHaN/自制FastHaN脚本——2024年5月22日更新.md</guid><pubDate>Fri, 16 Aug 2024 02:24:33 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[更新内容]]></title><description><![CDATA[ 
 <br><a data-href="FastHaN软件——陈华" href="软件\fasthan\fasthan软件——陈华.html" class="internal-link" target="_self" rel="noopener nofollow">FastHaN软件——陈华</a><br>
<a data-href="自制FastHaN脚本——2024年5月13日更新" href="软件\fasthan\自制fasthan脚本——2024年5月13日更新.html" class="internal-link" target="_self" rel="noopener nofollow">自制FastHaN脚本——2024年5月13日更新</a><br>
<a data-href="自制FastHaN脚本——2024年5月16日更新" href="软件\fasthan\自制fasthan脚本——2024年5月16日更新.html" class="internal-link" target="_self" rel="noopener nofollow">自制FastHaN脚本——2024年5月16日更新</a><br>
<a data-href="自制FastHaN脚本——2024年5月22日更新" href="软件\fasthan\自制fasthan脚本——2024年5月22日更新.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>自制FastHaN脚本——2024年5月22日更新</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
不想自己配置文件？<br>
下载这个，包含所有脚本：<a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hflPAj37JsvZf3tSFA?e=zJJd0G" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hflPAj37JsvZf3tSFA?e=zJJd0G" target="_blank">fastHaN-main.zip</a>
<br><br>现在你不需要拆开来准备多个 META 文件了，你只需要一个 META 文件，我们自动将其拆分。<br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408161016797.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408161131175.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>第一列是 ID，ID 必须在 FASTA 文件中存在，且不能错误。
<br>其余几列是你的分类标准。分类标准可以无限多。
<br>以 制表符 进行分隔。
请注意，表包含标题。

<br><br>#!/bin/bash

# 基础路径设置
FASTA_PATH="M7_all.fasta"
OUTPUT_PATH="output"
SOFTWARE_PATH="fastHaN-main/fastHaN-main/"
META_FILE="Group.txt"

# 获取META文件的列数（不包括第一列）
META_COLS=$(awk '{print NF-1; exit}' "$META_FILE")

# 动态生成META_DIRS数组
META_DIRS=()
for ((i=1; i&lt;=META_COLS; i++)); do
    META_DIRS+=("Column_$i")
done

# 创建输出文件夹
mkdir -p "$OUTPUT_PATH"
for dir in "ORIGIN_TCS" "MJN" "TCS" "MSN" "${META_DIRS[@]}"; do
    mkdir -p "$OUTPUT_PATH/$dir"
done

# 拆分META文件
split_meta_files() {
    input_file=$1
    output_dir=$2
    # 读取每一行（除去第一行标题）
    while IFS= read -r line || [[ -n "$line" ]]; do
        id=$(echo "$line" | awk '{print $1}')
        # 获取列数（包括第一列）
        num_columns=$(awk '{print NF}' &lt;&lt;&lt; "$line")
        # 遍历每一列
        for ((i=2; i&lt;=num_columns; i++)); do
            value=$(echo "$line" | awk -v col="$i" '{print $col}')
            echo -e "$id\t$value" &gt;&gt; "${output_dir}/Column_$((i-1)).txt"
        done
    done &lt; &lt;(tail -n +2 "$input_file")
}

split_meta_files "$META_FILE" "$OUTPUT_PATH"

# 输出的 FASTA 文件路径
OUTPUT_FASTA="${OUTPUT_PATH}/filtered_sequences.fasta"

# 提取序列并保存
python3 "${SOFTWARE_PATH}extract_fasta_sequences.py" "$META_FILE" "$FASTA_PATH" "$OUTPUT_FASTA"

# 文件转换并压缩
GZ_FILE="$OUTPUT_PATH/fasta2phylip.phy.gz"
python3 "${SOFTWARE_PATH}Pipline/Fasta2Phylip.py" "$OUTPUT_FASTA" | gzip &gt; "$GZ_FILE"

# 定义一个函数来执行网络计算
run_network_algorithm() {
    algorithm=$1
    input_file=$2
    output_dir=$3
    extra_args=$4
    "${SOFTWARE_PATH}fastHaN_linux" $algorithm -i "$input_file" $extra_args -o "$output_dir"
}

# 执行各种网络计算算法
run_network_algorithm "original_tcs" "$GZ_FILE" "$OUTPUT_PATH/ORIGIN_TCS" "-t 16 -a 1 -m 1"
run_network_algorithm "mjn" "$GZ_FILE" "$OUTPUT_PATH/MJN" "-t 16 -e 0"
run_network_algorithm "modified_tcs" "$GZ_FILE" "$OUTPUT_PATH/TCS" "-t 16"
run_network_algorithm "msn" "$GZ_FILE" "$OUTPUT_PATH/MSN" "-e 0"

# 生成网络配置文件
generate_network_config() {
    json_file=$1
    meta_file=$2
    output_dir=$3
    python3 "${SOFTWARE_PATH}Script/GenNetworkConfig.py" "$json_file" "$meta_file" "$output_dir"
}

for algorithm in "ORIGIN_TCS" "MJN" "TCS" "MSN"; do
    for dir in "${META_DIRS[@]}"; do
        generate_network_config "$OUTPUT_PATH/${algorithm}.json" "$OUTPUT_PATH/${dir}.txt" "$OUTPUT_PATH/$dir"
    done
done

# 替换颜色配置文件
replace_color_config() {
    input_file=$1
    output_file=$2
    python3 "${SOFTWARE_PATH}color_replace.py" "$input_file" "$output_file"
    echo "正在为你的群组或单倍群创建彩虹色"
}

for dir in "${META_DIRS[@]}"; do
    replace_color_config "$OUTPUT_PATH/${dir}_groupconf.csv" "$OUTPUT_PATH/${dir}_groupconf_updated.csv"
done

复制<br><br><a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/fastHaN-main.zip?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1001723774867&amp;Signature=Cj7LZPKCqzEQkA4COojc9XxvEaA%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/fastHaN-main.zip?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1001723774867&amp;Signature=Cj7LZPKCqzEQkA4COojc9XxvEaA%3D" target="_blank">fastHaN-main</a> 如果过期了可以发邮件给我 <a data-href="Homepage" href="home\homepage.html" class="internal-link" target="_self" rel="noopener nofollow">Homepage</a>。]]></description><link>软件\fasthan\自制fasthan脚本——2024年8月16日更新.html</link><guid isPermaLink="false">软件/FastHaN/自制FastHaN脚本——2024年8月16日更新.md</guid><pubDate>Sun, 08 Sep 2024 13:03:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[网址]]></title><description><![CDATA[ 
 <br><br><br>
<br>文件计算： <a data-tooltip-position="top" aria-label="https://github.com/ChenHuaLab/fastHaN" rel="noopener nofollow" class="external-link" href="https://github.com/ChenHuaLab/fastHaN" target="_blank">ChenHuaLab/fastHaN: fastHaN is a fast and scalable program for constructing haplotype networks for large samples (github.com)</a>
<br>可视化：<a data-tooltip-position="top" aria-label="https://github.com/sairum/tcsBU" rel="noopener nofollow" class="external-link" href="https://github.com/sairum/tcsBU" target="_blank">sairum/tcsBU: a TCS network beautifier (github.com)</a>
<br><br>
<br>名称: fastHaN
<br>功能: 构建大样本序列数据集的单倍型网络
<br>算法支持: 最小连接网络（MJN）和Templeton-Crandall-Sing（TCS）算法
<br><br>
<br>速度: 单线程模式运行速度远超现有软件
<br>可扩展性: 支持多线程模式，具有良好的扩展性
<br><br>
<br>Linux系统:

<br>下载软件： <a rel="noopener nofollow" class="external-link" href="https://github.com/ChenHuaLab/fastHaN/archive/refs/heads/main.zip" target="_blank">https://github.com/ChenHuaLab/fastHaN/archive/refs/heads/main.zip</a>
<br>并赋予执行权限 (chmod +x fastHaN_linux)
<br>运行测试数据 (./fastHaN_linux mjn -i 测试数据.phy.gz -t 8 -o Test1000)


<br>Windows系统:

<br>在 CMD 窗口运行 (fastHaN_win.exe mjn -i 测试数据.phy.gz -t 8 -o Test1000)


<br><br><br>#!/bin/bash

# 定义常用变量
FASTA_PATH="/mnt/c/Users/a/Desktop/Illumina_mtDNA_Filter_recode.fasta" # 你的fasta文件
OUTPUT_PATH="/mnt/c/Users/a/Desktop" # 输出结果路径
SOFTWARE_PATH="/mnt/e/OneDrive/文档（科研）/分析软件/fastHaN-main/fastHaN-main" # 软件所在路径
META_FILE="/mnt/c/Users/a/Desktop/illumina.meta" # META文件所在

# 将fasta序列转为phy并压缩为gz
GZ_FILE="$OUTPUT_PATH/fasta2phylip.phy.gz" # 输出.phy.gz文件
python $SOFTWARE_PATH/Pipline/Fasta2Phylip.py $FASTA_PATH | gzip &gt; $GZ_FILE

# 进行network运算
# TCS算法
$SOFTWARE_PATH/fastHaN_linux original_tcs -i $GZ_FILE -t 8 -a 1 -m 1 -o $OUTPUT_PATH/illumina_ORIGIN_TCS

# mjn算法
$SOFTWARE_PATH/fastHaN_linux mjn -i $GZ_FILE -t 8 -e 0 -o $OUTPUT_PATH/illumina_MJN

# 改良TCS算法
$SOFTWARE_PATH/fastHaN_linux modified_tcs -i $GZ_FILE -t 8 -o $OUTPUT_PATH/illumina_TCS

# msn算法
$SOFTWARE_PATH/fastHaN_linux msn -i $GZ_FILE -e 0 -o $OUTPUT_PATH/illumina_MSN

# 分组文件整理
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_ORIGIN_TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_MJN.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_TCS.json $META_FILE $OUTPUT_PATH/Groupinformation
python $SOFTWARE_PATH/Script/GenNetworkConfig.py $OUTPUT_PATH/illumina_MSN.json $META_FILE $OUTPUT_PATH/Groupinformation
复制<br><br>python Fasta2Phylip.py Example50.fasta &gt;  out.phylip

# 压缩
gzip -c out.phylip &gt; out.phy.gz
复制<br><br>original_tcs: 使用Clement等（2000）提出的原始TCS算法<br>
modified_tcs: 使用 Leigh（2015）在 PopART 中实现的 TCS 算法优化<br>
msn: 使用PopART实现的MSN算法优化<br>
mjn: 使用PopART实现的MJN算法优化（Bandelt等，1999）<br><br><br>- `usage: fastHaN original_tcs [arguments]`：使用`fastHaN`软件的`original_tcs`模式时所需输入的命令格式。

input:
    -i    输入的phylip格式文件
options:
    -t    (int)设置线程数，默认为8。这是多线程执行的配置，可以提升程序处理的速度。
    -a    (int)标记含有不明确碱基的位点，`1`表示遮蔽这些位点，`0`表示不遮蔽，默认值为`0`。这个选项用于处理序列数据中的不确定性。
    -m    (int)是否合并中间顶点，`1`表示合并，`0`表示不合并，默认值为`0`。合并中间顶点可以简化网络结构，使得结果更加清晰易读。
output:
    -o    指定输出文件的路径前缀。输出文件将以GML和json格式保存，这些文件格式用于后续的网络可视化或其他形式的数据分析。
复制<br><br>usage: fastHaN modified_tcs [arguments]

input:
    -i    输入的phylip格式文件
options:
    -t    (int)设置线程数，默认为8。这是多线程执行的配置，可以提升程序处理的速度。
output:
    -o    指定输出文件的路径前缀。输出文件将以GML和json格式保存，这些文件格式用于后续的网络可视化或其他形式的数据分析。
复制<br><br>usage: fastHaN msn [arguments]
     
input:
    -i    输入的phylip格式文件
options:
    -e    (int)设定epsilon参数，默认值为0。在构建最小生成树网络时，epsilon参数用于控制在网络构建过程中顶点之间连接的灵活性。较大的epsilon值允许在构建网络时考虑更远的距离，从而可能连接更多的节点，影响网络的密集度和结构。
output:
    -o    输出文件的路径前缀。生成的文件将以GML和JSON格式保存。这些格式主要用于后续的网络可视化或数据分析。
复制<br><br>usage: fastHaN mjn [arguments]
     
input:
    -i    输入的phylip格式文件
options:
    -t    (int)设置线程数，默认为8。这是多线程执行的配置，可以提升程序处理的速度。
    -e    (int)(int)设定epsilon参数，默认值为0。在构建最小生成树网络时，epsilon参数用于控制在网络构建过程中顶点之间连接的灵活性。较大的epsilon值允许在构建网络时考虑更远的距离，从而可能连接更多的节点，影响网络的密集度和结构。
output:
    -o    输出文件的路径前缀。生成的文件将以GML和JSON格式保存。这些格式主要用于后续的网络可视化或数据分析。
复制<br><br>
<br>输入文件 (PHYLIP格式): 包含单倍型的数量和每个单倍型的字符数
<br>输出文件 (GML和Json格式): 表示构建的单倍型网络
<br><br>
<br>使用tcsBU软件对构建的单倍型网络进行可视化，该软件包含在fastHaN软件包中
<br>需要提供META文件指定每个单倍型的组信息（如国家、城市等）
<br>配置文件生成<br>
<br>使用GenNetworkConfig.py Python脚本或手动生成配置文件，用于tcsBU软件的单倍型网络可视化
<br><br>举例：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404291718897.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>3: 单倍型的数量
<br>75: 每个单倍型的字符数
<br>Hap385899: Hap385899的单倍型名称
<br>GATCTG...GTAT：单倍型 Hap385899 的序列
<br><br>GML 文件和 Json 文件都代表构建的单倍型网络：<br>
<br>GML文件用作tcsBU软件的输入以可视化haplotye网络
<br>Json 文件用于为构建的单倍型网络准备颜色主题（请参阅“可视化”部分）。
<br><br>使用 fastHaN 构建单倍型网络后，我们需要使用软件 tcsBU 对其进行可视化。<br>
csBU (<a rel="noopener nofollow" class="external-link" href="https://github.com/sairum/tcsBU" target="_blank">https://github.com/sairum/tcsBU</a>) 是一个基于浏览器的 JavaScript 程序，包含在 fastHaN 的包中。您所需要做的就是解压缩 tcsBU.zip 并双击 index.html 文件。<br>
为了显示一个节点内单倍型的组成，您需要提供一个 META 文件（两列），其中指定每个单倍型的组信息（例如国家、城市等）。<br><br>fastHaN/Visualization/Example”目录中有一个示例文件（Test167.meta）：<br>Hap6825391      SouthAfrica
Hap6825397      SouthAfrica
Hap6825546      Austria
Hap6795850      SouthAfrica
Hap6795842      SouthAfrica
复制<br>
<br>hap6825391：单倍型名称，必须与 phylip 文件中的一致
<br>SouthAfrican：单倍体 Hap6825391 的群体信息
<br><br>您可以使用 GenNetworkConfig.py  的 Python 脚本生成 tcsBU 所需的两个配置文件。<br># generate the config files
python GenNetworkConfig.py Test167.json Test167.meta Test167Conf

# Two config files will be generaged by above command
1. Test167Conf_groupconf.csv
2. Test167Conf_hapconf.csv
复制<br>
<br>Test167Conf_groupconf.csv（3列，分号分隔）,内容如下：<br>
第 1 个：组名称，例如：南非。<br>
第 2 个：由 GenNetworkConfig.py 生成的随机颜色。<br>
第3个：值为“none”的固定列.
<br>SouthAfrica;#4C3D35;none
Austria;#D56234;none
Botswana;#DCBD4D;none
复制<br>
<br>Test167Conf_hapconf.csv（2列，分号分隔）
<br>Hap6825390;SouthAfrica
Hap6825391;SouthAfrica
Hap6825397;SouthAfrica
Hap6825546;Austria
Hap6795850;SouthAfrica
Hap6795842;SouthAfrica
复制<br>第1列：单倍型名称，必须与 phylip 文件中的一致；<br>
第2列：每个单倍型的组名称。<br><br>#!/usr/bin/python3
# -*- coding: utf-8 -*-

#*************************************************************************
#    &gt; File Name: GenNetworkConfig.py
#    &gt; Author: xlzh
#    &gt; Mail: xiaolongzhang2015@163.com 
#    &gt; Created Time: 2021年12月02日 星期四 22时20分30秒
#*************************************************************************

'''
generate the config file for tcsBU webserver (group.csv and haplotype.csv)

Input: 
   1. json file generated by fastHaN
   2. meta file prepared by the user (column-1: sampleID, column-2: sample group)
   
Output:
   1. group.csv (color of each group)
   2. haplotype.csv (group of each haplotype)
'''


import sys
import json
from random import randint


def json_parse(json_file):
    ''' func: parse the json file to obtain the haplotye
    '''
    hap_list = []
    json_wraper = json.load(open(json_file, 'r'))

    # node -&gt; {'id': 0, 'frequency': 1.0, 'title1': 'Sample1', 'title2': 'Sample1;Sample2'}
    for node in json_wraper['nodes']: 
        hap_list.extend(node['title2'].split(';'))

    return hap_list


def read_meta_file(meta_file):
    ''' func: read the meta file and obtain the country for each individual
        meta_dict = {'EPI_ISL_6814923': 'Australia', ...}
    '''
    meta_dict = {}
    meta_fp = open(meta_file, 'r')

    for line in meta_fp:
        l = line.rstrip().split('\t')
        meta_dict[l[0]] = l[1]  # l[0] -&gt; sampleid; l[1] -&gt; country

    return meta_dict


def gen_hap_config(hap_list, meta_dict):
    ''' func: generate the haplotype config
        hap_conf_list = [(sample1, China), (sample2, Korean), ...]
    '''
    hap_conf_list = []

    for hap in hap_list:  # EPI_ISL_6832737 or 'IN12'
        if hap.startswith('IN'):  # skip the intermediate node
            continue

        elif hap not in meta_dict:
            sys.stderr.write("[warning] the group of %s is not given in the meta file\n" % hap)

        else:
            country = meta_dict[hap].replace(' ', '')
            hap_conf_list.append((hap, country))

    return hap_conf_list


def _random_color():
    ''' func: generate color randomly
    '''
    colorArr = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F']
    color_list = []

    for i in range(6):
        color_list.append(colorArr[randint(0, 14)])

    return '#' + ''.join(color_list)


def gen_group_config(hap_conf_list):
    ''' func: generate the group config
        group_conf_list = [(China, '#FF011B', 'none'), (Korean, '2693FF', 'none'), ...]
    '''
    group_conf_list = []
    count_dict = {}

    for item in hap_conf_list:
        if item[1] != 'Default' and item[1] not in count_dict:
            rand_color = _random_color()
            count_dict[item[1]] = rand_color
            group_conf_list.append((item[1], rand_color, 'none'))

    return group_conf_list


def write_conf(hap_conf_list, group_conf_list, out_prefix):
    ''' func: write the config file
    '''
    # write the haplotype config file
    hap_fp = open(out_prefix + '_hapconf.csv', 'w')
    for hap in hap_conf_list:
        hap_fp.write("%s;%s\n" % (hap[0], hap[1]))

    # write the group config file
    group_fp = open(out_prefix + '_groupconf.csv', 'w')
    for group in group_conf_list:
        group_fp.write("%s;%s;%s\n" % (group[0], group[1], group[2]))

    hap_fp.close()
    group_fp.close()


def main():
    args = sys.argv
    if len(args) != 4:
        sys.stderr.write("usage: python GetNetworkConfig.py &lt;in.json&gt; &lt;in.meta&gt; &lt;out_prefix&gt;\n")
        sys.exit(-1)

    json_file = args[1]
    meta_file = args[2]
    out_prefix = args[3]

    hap_list = json_parse(json_file)
    meta_dict = read_meta_file(meta_file)
    hap_conf_list = gen_hap_config(hap_list, meta_dict)
    group_conf_list = gen_group_config(hap_conf_list)
    write_conf(hap_conf_list, group_conf_list, out_prefix)
    

if __name__ == '__main__':
    main()

复制]]></description><link>软件\fasthan\fasthan软件——陈华.html</link><guid isPermaLink="false">软件/FastHaN/FastHaN软件——陈华.md</guid><pubDate>Fri, 21 Jun 2024 07:31:05 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404291718897.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404291718897.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[教程一]]></title><description><![CDATA[ 
 <br><br>IQtree 是利用最大似然法构建系统发生树的，具有高效建树、模型选择、超快自展等特点，适用于大数据的系统发育分析。<br><br>
<br>首先在NCBI下载用于建树的序列，把这些序列放到同一个fasta文件里，每条序列的标签名要以“&gt;”开始并且序列标签名中不要有空格和标点等，不然后续无法识别. IQtree支持多种序列数据格式，如:Phylip/ fasta/fas/nexus/clustlw等格式，程序也会自动转为Phylip格式，省去格式转换的步骤。
<br>将上述的 fasta 格式序列文件进行序列比对，<a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a>。对高度变异区域进行裁剪。
<br>保存成 fasta 格式。
<br><br>软件下载地址：<a data-tooltip-position="top" aria-label="http://www.iqtree.org/" rel="noopener nofollow" class="external-link" href="http://www.iqtree.org/" target="_blank">IQ-TREE: Efficient phylogenomic software by maximum likelihood (iqtree.org)</a><br>Command-line examples (replace 'iqtree2 ...' by actual path to executable):

1. Infer maximum-likelihood tree from a sequence alignment (example.phy)
   with the best-fit model automatically selected by ModelFinder:
     iqtree2 -s example.phy

2. Perform ModelFinder without subsequent tree inference:
     iqtree2 -s example.phy -m MF
   (use '-m TEST' to resemble jModelTest/ProtTest)

3. Combine ModelFinder, tree search, ultrafast bootstrap and SH-aLRT test:
     iqtree2 -s example.phy --alrt 1000 -B 1000

4. Perform edge-linked proportional partition model (example.nex):
     iqtree2 -s example.phy -p example.nex
   (replace '-p' by '-Q' for edge-unlinked model)

5. Find best partition scheme by possibly merging partitions:
     iqtree2 -s example.phy -p example.nex -m MF+MERGE
   (use '-m TESTMERGEONLY' to resemble PartitionFinder)

6. Find best partition scheme followed by tree inference and bootstrap:
     iqtree2 -s example.phy -p example.nex -m MFP+MERGE -B 1000

7. Use 4 CPU cores to speed up computation: add '-T 4' option

8. Polymorphism-aware model with HKY nucleotide model and Gamma rate:
     iqtree2 -s counts_file.cf -m HKY+P+G

9. PoMo mixture with virtual popsize 5 and weighted binomial sampling:
     iqtree2 -s counts_file.cf -m "MIX{HKY+P{EMP},JC+P}+N5+WB"

To show all available options: run 'iqtree2 -h'

Have a look at the tutorial and manual for more information:
     http://www.iqtree.org
复制<br>需要哪条指令，就复制上述的指令即可。完成之后，会出现几个文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405081630868.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
举个例子：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405081631373.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>建树完成后，会在序列文件夹中生成多个文件，覆盖之前产生的文件，主要包括程序运行日志、一致树文件(example.fasta.contree)、ML 树文件(example.fasta.treefile)即 TREEFILE 文件（含有 Bootstrap/UFBoot/SH-aLRT 值的 BP 评估分支置信度），可以用 Figtree 进行查看和赋根，使用 Adode Photoshop 对进化树进行编辑和美化.<br><br><a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/408382758" rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/408382758" target="_blank">如何构建进化树 | 基于IQ-TREE - 知乎 (zhihu.com)</a><br><br>是否去除 gap？
是否需要去除对齐序列中的gap取决于具体的分析需求和数据特征。<br>
保留gap：在某些情况下提供重要的信息，尤其是当gap表示插入或缺失事件时,gap可能有助于反映真实的进化历史。<br>
去除gap：对齐中的gap较多且分布不均匀，它们可能引入噪声，影响树的准确性。高度保守的区域或者当gap被认为是对分析有害时，去除它们可能会提高树的质量
]]></description><link>软件\iqtree\iqtree：基础操作.html</link><guid isPermaLink="false">软件/IQtree/iqtree：基础操作.md</guid><pubDate>Sun, 08 Sep 2024 13:16:04 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405081630868.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405081630868.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[文件准备]]></title><description><![CDATA[ 
 <br>可以看看: <a data-href="iqtree：基础操作" href="软件\iqtree\iqtree：基础操作.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：基础操作</a><br><br><br>partition_file.txt：<br>DNA, TIM2+F+I: HighVar = 1-600 16001-16594
DNA, GTR+F+I+G4: Coding = 601-16000
复制<br>注意，你需要根据自己的数据选择适合的模型，具体你可以参考这个：<br>
<a data-href="BEAST：替换模型" href="软件\beast\beast：替换模型.html" class="internal-link" target="_self" rel="noopener nofollow">BEAST：替换模型</a><br>
<a data-href="iqtree：寻找最优模型及分区" href="软件\iqtree\iqtree：寻找最优模型及分区.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：寻找最优模型及分区</a><br><br>一个普通的 nex 文件：<br>#NEXUS
Begin data;
Dimensions ntax=8368 nchar=16594;
Format datatype=dna missing=? gap=-;
Matrix
ID1	GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCT-CCATGCATTTGGTA
ID2	GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCT-CCATGCATTTGGTA
......
;
End;
复制<br><br>iqtree -s 你的nex文件.nex -spp 分区文件/partition_file.txt -b 1000 -nt AUTO  
复制<br>结果解读可以看这里：<a data-href="iqtree：基础操作" href="软件\iqtree\iqtree：基础操作.html" class="internal-link" target="_self" rel="noopener nofollow">iqtree：基础操作</a>]]></description><link>软件\iqtree\iqtree：线粒体最大似然树.html</link><guid isPermaLink="false">软件/IQtree/iqtree：线粒体最大似然树.md</guid><pubDate>Sat, 13 Jul 2024 08:00:54 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5e3.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5e3.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备工作]]></title><description><![CDATA[ 
 <br><br><br>我们需要对原始的 fasta 文件进行对齐，因为在建立系统发育树必须使用对齐的文件。我们推荐使用 [[MAFFT]] 软件进行操作。<br><br>sudo apt-get update
sudo apt-get install mafft
复制<br><br>mafft --auto input.fasta &gt; output_aligned.fasta
复制<br>注意一下，我们使用的是自动策略，可能对精确度有一定的影响，我们在这里不展开叙述。<br>
如果为了寻求更加精确的对齐，请参考如下教程。<br>
<br><a data-href="GUIDANCE2 Server ：多序列比对置信软件" href="软件\其它生信软件\a-j\guidance2-server-：多序列比对置信软件.html" class="internal-link" target="_self" rel="noopener nofollow">GUIDANCE2 Server ：多序列比对置信软件</a>。
<br><a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a>。<br>
总之，多序列比对，还是推荐MAFFT，基因编码区的多序列比对，更加推荐使用MAFFT基于密码子的方式比对。
<br><br>mtDNA 不同的区域的遵循不同的进化模型，因此，我们需要将不同的区域切割出来，寻找每个区域对应的最优模型，然后再进行 BEAST。<br>得到比对完成之后的 aln.fasta 之后，就可以进行切割了。 在这里，我做了一个 python，可以交互式切割指定的序列。<br>from Bio import SeqIO

def print_sequence_lengths(sequences):
    """打印每个序列的长度"""
    for record in sequences:
        print(f"ID: {record.id}, Length: {len(record.seq)}")

def get_ranges_from_user():
    """交互式获取用户输入的范围"""
    ranges = []
    while True:
        user_input = input("Enter range (e.g., 1-200) or 'OK' to finish: ")
        if user_input.strip().upper() == 'OK':
            break
        try:
            start, end = map(int, user_input.split('-'))
            ranges.append((start, end))
        except ValueError:
            print("Invalid range. Please enter again.")
    return ranges

def split_fasta(input_file, sequences, ranges):
    """
    根据指定的位置范围切割FASTA文件，并保存到单独的文件中。

    :param input_file: 输入的FASTA文件路径
    :param sequences: 序列列表
    :param ranges: 切割范围列表，例如 [(1, 200), (201, 400)]
    """
    base_name = input_file.rsplit(".", 1)[0]
    
    for start, end in ranges:
        output_file = f"{base_name}_{start}-{end}.fasta"
        with open(output_file, "w") as out_f:
            for record in sequences:
                # 提取指定范围的序列
                sub_seq = record.seq[start-1:end]
                sub_record = record[:0]  # 创建一个空记录
                sub_record.seq = sub_seq  # 设置序列
                sub_record.id = record.id  # 保留原ID
                sub_record.description = f"{record.description} [{start}-{end}]"  # 添加范围信息
                # 写入到输出文件
                SeqIO.write(sub_record, out_f, "fasta")
        print(f"Generated {output_file} for range {start}-{end}")

# 输入文件
input_file = "C:/Users/victo/Desktop/最终版本ALN的编码区.fasta"

# 读取FASTA文件中的序列
sequences = list(SeqIO.parse(input_file, "fasta"))

# 打印每个序列的长度
print_sequence_lengths(sequences)

# 获取用户输入的范围
ranges = get_ranges_from_user()

# 调用函数进行切割
split_fasta(input_file, sequences, ranges)

复制<br>得到一些切割之后的文件，我们就开始寻找最优模型。<br>
注意！切割之前的原始文件需要保留，我们后续进行 BEAST 需要原始文件。<br><br>在这里，有许多软件可以帮助寻找最优模型，我们现在使用 iqtree2，因为这个软件经过了更新迭代之后，获得了更强、更广泛的功能，属于建模的万金油。所以我们选择使用它来寻找：<br>
iqtree 的下载地址如下： <a data-tooltip-position="top" aria-label="http://www.iqtree.org/" rel="noopener nofollow" class="external-link" href="http://www.iqtree.org/" target="_blank">IQ-TREE: Efficient phylogenomic software by maximum likelihood (iqtree.org)</a><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605174230.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在这里，我推荐使用 Linux版本，因为操作相对来说比较简单。点击 ALL Downloads，可以查看所有的版本。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605174320.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
下载安装包之后，放到 Linux 环境下，进行解压：<br>tar -xzf 您的压缩包.gz
复制<br>运行相当简单，在目录中寻找 iqtree/iqtree-2.3.4-Linux-intel/bin/iqtree2，然后直接运行它输入参数即可。<br>
在运行之前，建议对已有的 fasta 文件进行删减，只保留你需要进行分析的位点，因为如果位点太多，会导致建树非常慢，而且模型寻找也慢。<br>现在开始运行 iqtree：<br>#!/bin/bash

# 定义输入文件和输出前缀
declare -A FILES # 下面更改为你切割出来的每一个文件
FILES["编码区"]="/home/luolintao/Mydata/10K/最终版本ALN的编码区.fasta"
FILES["D环_"]="/home/luolintao/Mydata/10K/最终版本ALN的D环.fasta"
# 设置IQ-TREE可执行文件的路径
IQTREE_EXEC="/home/luolintao/iqtree/iqtree-2.3.4-Linux-intel/bin/iqtree2"

# 遍历每个文件并运行IQ-TREE的ModelFinder功能
for NAME in "${!FILES[@]}"; do
    INPUT_FILE="${FILES[$NAME]}"
    OUTPUT_PREFIX="/home/luolintao/Mydata/10K/${NAME}_model_selection"

    echo "Running IQ-TREE ModelFinder for $INPUT_FILE..."

    nohup $IQTREE_EXEC -s "$INPUT_FILE" -m MF -nt AUTO -pre "$OUTPUT_PREFIX" &gt; "$OUTPUT_PREFIX.log" 2&gt;&amp;1 &amp;

    # 检查命令是否成功启动
    if [ $? -eq 0 ]; then
        echo "IQ-TREE ModelFinder for $NAME successfully started."
        echo "Output files are prefixed with $OUTPUT_PREFIX"
    else
        echo "IQ-TREE ModelFinder for $NAME encountered an error." &gt;&amp;2
        exit 1
    fi
done

echo "All IQ-TREE ModelFinder tasks have been started."

复制<br>运行完成之后，我们找到后缀为 .iqtree 的文件，文件的内容大致如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605181328.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里我们观察：<br>
<br>AIC (Akaike Information Criterion)：用于模型选择，较低的AIC值表示较好的模型拟合。
<br>AICc (Corrected Akaike Information Criterion)：对于小样本量的修正AIC。
<br>BIC (Bayesian Information Criterion)：另一个用于模型选择的准则，也较低的BIC值表示较好的模型。
<br>根据表格数据：<br>
<br>AIC 最低：GTR+F+I+G4 (-22875.459, 46120.919)
<br>AICc 最低：GTR+F+I+G4 (-22875.459, 46125.444)
<br>BIC 最低：TIM3e+I+G4 (-23625.409, 48986.330)
<br>我们选择的样本会影响到最优模型的构建，在这里我们选择最低的值。这里出现了3个最低，我们需要选择一个。我们选择 GTR+F+I+G4模型。<br>另外的分割序列也如法炮制，记住我们选择的最优模型。<br>
例如，在我这里，高变区我认为 TIM2+F+I 是最优的模型。<br>
AIC 最低模型：<br>
<br>TIM2+F+I (-1083.148, 2390.296)<br>
AICc 最低模型：
<br>TIM2+F+I (-1083.148, 2442.379)<br>
BIC 最低模型：
<br>HKY+F+I (-1085.821, 2875.122)
<br>后面附带构建的系统发育树，可以目测结构是否大致正确，但是在这里我们不重点关注：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605181515.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>现在，使用我们一开始的原始文件。将原始文件导入 BEAUTi 软件，<br>
在导入之前，我们需要先把 ALN 文件转为 NEX 文件，转的方式很多，最简单的方法就是直接把 FASTA 导入 MEGA 软件，然后导出来的时候，点击 NEX 就可以了。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605203652.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在这里，我们可以看到 NEX 文件的末尾什么都没有，但是我们需要增加分区，分区依然按照我们之前分区进行。<br>
例如：<br>begin assumptions;
charset DLOOP = 1-599;
charset CODING = 600-16594;
end;
复制<br>将这段内容修改为自己的分区，追加到 nex 文件中去。再把新的 nex 文件导入 BEAUti。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605204158.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605204235.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以看到，我们已经得到了对应的区域。现在，选择2行内容，取消链接：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605204355.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
用红色方框标记的每个区域都有2个选项：<br>
<br>连接：2个区域用 同一套参数
<br>取消连接：2个区域 不用同一套参数。
<br>所以，我们在这里最好取消连接。得到如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605204747.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
当然，这样的分类并不是绝对的，需要根据自己的情况决定是否要设定为相同的 site model 和 clock model。]]></description><link>软件\iqtree\iqtree：寻找最优模型及分区.html</link><guid isPermaLink="false">软件/IQtree/iqtree：寻找最优模型及分区.md</guid><pubDate>Sun, 08 Sep 2024 13:16:04 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605174230.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240605174230.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[一、引言]]></title><description><![CDATA[ 
 <br><br>在很多生信分析文章中，我们总能看到这样的图，如下图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031230270.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>弦图，主要也是用于描述一个对象在另一个对象中的占比与流向，整体和前面提到的桑基图有些类似；如何用Origin来绘制弦图。<br>二、教学部分<br>
1.&nbsp;首先输入我们提前准备好的数据（数据和我们原来绘制桑基图的数据比较相近）<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031230369.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
2. 然后选择上方的“Plot-Categorical-Chord Diagram（弦图）”<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031240947.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
在Origin中有两种弦图，图标标红部分的左侧为弦图，右侧为比例弦图。关于两者之间的差异，请查看文章最后的部分。我们这里直接选择左侧的按钮进行绘图.<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031240141.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
3. 而如果我们选择右侧的弦图绘制按钮，我们则会得到下面的图，即比例弦图。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031240306.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
从弦图和比例弦图的绘图结果，我们可以看到对于两种弦图来说，我们都能从中看到数据起点到终点的对应关系，比如我们能看到BOC银行把钱借给了yama、saki和kawa。但是我们无法从比例弦图中看到kawa从BOC和ICBC中借到的钱的比例关系，这也正是两者的差别。<br>
<br>最后我们调整下样式和演示，针对弦图可以得到下图。
]]></description><link>软件\origin绘图\origin：教学—弦图绘制.html</link><guid isPermaLink="false">软件/origin绘图/Origin：教学—弦图绘制.md</guid><pubDate>Fri, 21 Jun 2024 07:31:19 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031230270.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312031230270.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[根据经纬度求国家]]></title><description><![CDATA[ 
 <br><br>直接 txt 文件，表不包含标题。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405051401197.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import googlemaps
import csv

# 替换为你的Google Maps API密钥
api_key = ''
gmaps = googlemaps.Client(key=api_key)

# 从文件中读取市区列表
file_path = 'C:/Users/a/Desktop/新建文本文档.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    countries = [line.strip() for line in file]

# 查询市区的经纬度并实时写入CSV
output_file = 'C:/Users/a/Desktop/result.csv'
with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    # 写入标题行
    writer.writerow(['Country', 'Latitude', 'Longitude'])
    
    for country in countries:
        geocode_result = gmaps.geocode(country)
        if geocode_result:
            location = geocode_result[0]['geometry']['location']
            writer.writerow([country, location['lat'], location['lng']])
        else:
            writer.writerow([country, None, None])
        csvfile.flush()
        print(f"Processed {country},{location}")

print("Data has been saved to CSV.")


复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405051414873.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用 txt 文件保存，用制表符分割。优先查询后者，提高查询准确度。<br>import googlemaps
import csv

# 替换为你的Google Maps API密钥
api_key = ''
gmaps = googlemaps.Client(key=api_key)

# 从文件中读取省份和市区列表
file_path = 'C:/Users/a/Desktop/地区列表.txt'
with open(file_path, 'r', encoding='utf-8') as file:
    # 分割每一行来获取市区名称
    regions = [line.strip().split('\t')[1] for line in file if '\t' in line]

# 查询市区的经纬度并实时写入CSV
output_file = 'C:/Users/a/Desktop/result.csv'
with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    # 写入标题行
    writer.writerow(['Region', 'Latitude', 'Longitude'])
    
    for region in regions:
        geocode_result = gmaps.geocode(region)
        if geocode_result:
            location = geocode_result[0]['geometry']['location']
            writer.writerow([region, location['lat'], location['lng']])
            print(f"Processed {region},{location}")
        else:
            writer.writerow([region, None, None])
            print(f"Processed {region}, no location found")
        csvfile.flush()

print("Data has been saved to CSV.")

复制]]></description><link>软件\python\地理经纬\python：根据国家、地区求经纬度.html</link><guid isPermaLink="false">软件/Python/地理经纬/python：根据国家、地区求经纬度.md</guid><pubDate>Fri, 21 Jun 2024 07:31:19 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405051401197.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202405051401197.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备]]></title><description><![CDATA[ 
 <br>地图上经纬度定位可以直接获取该地区海拔，进而探索海拔对人群的影响。<br><br>为了获取更准确的数值，一个可以使用的 API 至关重要。<br>
<br>Open Elevation API：Open Elevation是一个免费的开源项目，提供了一个简单的API来查找地球上任意位置的海拔高度。虽然它是免费的，但可能在数据精度和请求限制方面与商业服务相比有所不足。
<br>MapQuest Elevation API：MapQuest提供了一个可以查询地点海拔的API服务。MapQuest的开发者服务包括免费和付费计划，具体费用取决于请求的数量和所需服务的类型。
<br>NASA SRTM (Shuttle Radar Topography Mission) 数据：虽然不是一个实时API，NASA的SRTM项目提供了全球大部分地区详细的高程数据。这些数据可以被下载并用于应用程序中，适合需要批量处理海拔数据的情况。
<br>Google Elevation API：一个比较精准 API，但是定价较高。每个月有 200$的免费额度。
<br><br>使用 csv 文件，格式应该如下：<br>Object_ID,Latitude,Longtitude
Tibetan_Amdo10,29.653,91.138
Tibetan_Amdo16,29.653,91.138
Tibetan_Amdo17,29.653,91.138
Tibetan_Amdo9,29.653,91.138
Tibetan_Baima1,29.653,91.138
Tibetan_Baima10,29.653,91.138
Tibetan_Baima11,29.653,91.138
Tibetan_Baima15,29.653,91.138
Tibetan_Baima16,29.653,91.138
Tibetan_Baima17,29.653,91.138
Tibetan_Baima18,29.653,91.138
Tibetan_Baima20,29.653,91.138
Tibetan_Baima21,29.653,91.138
Tibetan_Baima26,29.653,91.138
Tibetan_Baima3,29.653,91.138
Tibetan_Baima4,29.653,91.138
Tibetan_Baima5,29.653,91.138
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403271324577.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
import requests

# 替换为您的Google API密钥
api_key = '?????????'

# 定义获取海拔的函数
def get_elevation(lat, lon, api_key):
    url = f"https://maps.googleapis.com/maps/api/elevation/json?locations={lat},{lon}&amp;key={api_key}"
    response = requests.get(url)
    if response.status_code == 200:
        result = response.json()
        # 提取海拔值
        elevation = result['results'][0]['elevation']
        print(f"{lat},{lon}海拔：{elevation}")
        return elevation
    else:
        print("Error:", response.status_code)
        return None

# 读取CSV文件
df = pd.read_csv('C:/Users/a/Desktop/分析表.csv')

# 打开一个新的CSV文件以写入海拔信息
with open('C:/Users/a/Desktop/芯片分析表_HI.csv', 'w') as f:
    f.write("Latitude,Longitude,Elevation\n")  # 写入表头
    for index, row in df.iterrows():
        elevation = get_elevation(row['Latitude'], row['Longitude'], api_key)
        if elevation is not None:
            f.write(f"{row['Latitude']},{row['Longitude']},{elevation}\n")

复制<br>对于大量请求，考虑批量处理和缓存结果以减少 API 调用次数可能会有帮助。<br><br>我们希望把海拔信息放到所有的样本中去，可以继续使用 Python 解决。<br>import pandas as pd

# 读取海拔信息文件
elevation_df = pd.read_csv('/mnt/data/海拔信息.csv')

# 读取需要填充海拔信息的总表文件
main_df = pd.read_csv('/mnt/data/总表.csv')

# 根据经纬度合并海拔信息到总表中
# 注意这里使用了拼写错误的'Lontitude'以保持数据一致性
merged_df = pd.merge(main_df, elevation_df[['Longitude', 'Latitude', 'Elevation']], on=['Longitude', 'Latitude'], how='left')

# 保存合并后的数据到新的CSV文件
merged_file_path = '/mnt/data/总表_带海拔信息.csv'
merged_df.to_csv(merged_file_path, index=False)

复制]]></description><link>软件\python\地理经纬\python：根据经纬度获取海拔.html</link><guid isPermaLink="false">软件/Python/地理经纬/python：根据经纬度获取海拔.md</guid><pubDate>Fri, 21 Jun 2024 07:31:19 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403271324577.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403271324577.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br><br>你需要准备一个 csv 文件，格式如下所示：<br>
请注意，原作者把经度错误拼写成了 Longtitude，请你也在这个 csv 文件中如此拼写。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404211638100.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>以下代码需要使用 谷歌API，因此需要注册。<br>import pandas as pd
import requests

def get_country_name(latitude, longitude, api_key):
    """根据经纬度使用Google Maps Geocoding API获取国家名称"""
    url = "https://maps.googleapis.com/maps/api/geocode/json"
    params = {
        'latlng': f"{latitude},{longitude}",
        'key': api_key
    }
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            results = response.json().get('results', [])
            if results:
                for result in results:
                    for component in result.get('address_components', []):
                        if 'country' in component.get('types', []):
                            return component.get('long_name')
    except requests.RequestException as e:
        return f"Error: {e}"
    return "Country not found"

def process_coordinates(file_path, output_file, api_key):
    data = pd.read_csv(file_path)
    with open(output_file, 'w', newline='', encoding='utf-8') as file:
        # 写入列标题
        file.write(','.join(list(data.columns) + ['Country']) + '\n')
        
        for index, row in data.iterrows():
            country_name = get_country_name(row['Latitude'], row['Longtitude'], api_key)
            print(f"Processing Record {index + 1}: {country_name}")  # 打印每个记录的处理状态
            # 写入更新后的行并确保每次都写入磁盘
            file.write(','.join(map(str, row.tolist() + [country_name])) + '\n')
            file.flush()  # 确保数据实时写入磁盘

    print("All records have been processed and saved.")

# 示例使用
api_key = '????????'  # 替换为您的Google Maps API密钥
file_path = 'C:/Users/a/Desktop/求国家.csv'  # 替换为您的CSV文件路径
output_file = 'C:/Users/a/Desktop/updated_file_with_countries.csv'  # 输出文件路径
process_coordinates(file_path, output_file, api_key)

复制]]></description><link>软件\python\地理经纬\python：根据经纬度求国家.html</link><guid isPermaLink="false">软件/Python/地理经纬/python：根据经纬度求国家.md</guid><pubDate>Fri, 21 Jun 2024 07:31:19 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404211638100.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404211638100.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：根据中文求拼音]]></title><description><![CDATA[ 
 <br>import pypinyin
import pandas as pd

# 读取文本文件
with open('C:/Users/a/Desktop/拼音.txt', 'r', encoding='utf-8') as file:
    lines = file.read().splitlines()

# 转换为拼音，并首字母大写
pinyin_cities = [
    ''.join(pypinyin.lazy_pinyin(city, style=pypinyin.Style.NORMAL)).capitalize()
    for city in lines
]
# 转换为带声调的拼音
pinyin_cities_toned = [
    ' '.join(pypinyin.lazy_pinyin(city, style=pypinyin.Style.TONE))
    for city in lines
]
# 创建DataFrame
df = pd.DataFrame({'City': lines, 'Pinyin': pinyin_cities,'Pinyin_Toned': pinyin_cities_toned})

# 保存到CSV文件
df.to_csv('C:/Users/a/Desktop/Cities_Pinyin.csv', index=False)
复制]]></description><link>软件\python\地理经纬\python：根据中文求拼音.html</link><guid isPermaLink="false">软件/Python/地理经纬/python：根据中文求拼音.md</guid><pubDate>Fri, 21 Jun 2024 07:31:20 GMT</pubDate></item><item><title><![CDATA[Python：古代DNA饼图准备文件的生成]]></title><description><![CDATA[ 
 <br>绘制古代 DNA 的分布常见两种图形：<br>
<br>散点图：着重强调某种古代 DNA 单倍群是否存在。
<br>饼图：着重强调某种古代 DNA 单倍群的占比。
<br>散点图很简单，这里不阐述。我们开始准备饼图的绘制。<br><br>首先需要整理过后的数据：<br>haplogroup	Lat_int	Long_int	Date_BP
C1b	-56	-69	1369
D1g	-56	-70	1000
C1b	-56	-68	910
D4h	-56	-70	910
C1b	-56	-68	906
C1b	-56	-68	659
C1b	-56	-68	200
C1b	-55	-66	590
C	-55	-66	440
D1g	-55	-66	420
D1g	-55	-66	400
D1g	-55	-66	330
复制<br>表包含标题，请严格按照标题名！haplogroup	Lat_int	Long_int	Date_BP
建议经纬度全部取整，在 excel 中可以通过如下公式进行：取整=int(单元格)
<br><br>import pandas as pd

# 读取数据
file_path = r'C:/Users/victo/Desktop/新建 Text Document.txt'
output_path = r'C:/Users/victo/Desktop'
data = pd.read_csv(file_path, delimiter='\t')

# 交互式询问用户关注的单倍群
interested_haplogroups = []
print("请输入您关注的单倍群，输入空或'OK'结束输入：")
while True:
    print("请继续输入您关注的单倍群，输入空或'OK'结束输入：")
    haplogroup = input().strip()
    if haplogroup.lower() == 'ok' or haplogroup == '':
        break
    interested_haplogroups.append(haplogroup)

# 筛选Date_BP &lt;= 10000的数据，可以自定义
data_filtered = data[data['Date_BP'] &lt;= 10000].copy()

# 将不在用户关注列表中的单倍群分类为 'Other'
data_filtered['haplogroup'] = data_filtered['haplogroup'].apply(lambda x: x if x in interested_haplogroups else 'Other')

# 用户输入时间范围
try:
    data_range = int(input("请输入时间范围（如：2000）："))
except ValueError:
    print("输入错误，请输入一个有效的整数。")
    data_range = 2000  # 默认值或重新输入

# 定义时间区间（每data_range年一个区间）
bins = list(range(int(data_filtered['Date_BP'].min() // data_range * data_range),
                 int(data_filtered['Date_BP'].max()) + data_range, data_range))

# 为数据添加一个区间列
data_filtered.loc[:, 'Year_Bin'] = pd.cut(data_filtered['Date_BP'], bins=bins, right=False)

# 按时间区间、经纬度和单倍群分组，计算每组的数量
grouped_data = data_filtered.groupby(['Year_Bin', 'Lat_int', 'Long_int', 'haplogroup'], observed=True).size().reset_index(name='Count')

# 将数据透视，使每个单倍群成为一个列
pivot_table = grouped_data.pivot_table(index=['Year_Bin', 'Lat_int', 'Long_int'], 
                                       columns='haplogroup', 
                                       values='Count', 
                                       aggfunc='sum',
                                       observed=True).fillna(0).reset_index()

# 确保每个地理位置至少有一个用户感兴趣的单倍群数量大于0
pivot_table = pivot_table[(pivot_table[interested_haplogroups].sum(axis=1) &gt; 0)]

# 计算总数列
pivot_table['Total'] = pivot_table[interested_haplogroups + ['Other']].sum(axis=1)

# 写入过滤后的数据到CSV文件
for period in pivot_table['Year_Bin'].unique():
    subset = pivot_table[pivot_table['Year_Bin'] == period]
    file_name = f'{output_path}/{interested_haplogroups}_{period.left}_{period.right}.csv'
    subset.to_csv(file_name, index=False)

复制<br>现在得到了多个时间段的 csv 文件，现在打开 Arcgis Pro。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251526241.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251526552.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251527979.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251527938.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>在下图这里添加排除规则，例如我们可以排除某个经纬度下总数小于某个数值的饼图，将其转为散点图，避免对结果的误判和误读。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251528925.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\地理经纬\python：古代dna饼图准备文件的生成.html</link><guid isPermaLink="false">软件/Python/地理经纬/Python：古代DNA饼图准备文件的生成.md</guid><pubDate>Fri, 25 Oct 2024 07:29:36 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251526241.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410251526241.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：经纬度和地址互转]]></title><description><![CDATA[ 
 <br>import googlemaps
from datetime import datetime

# 用你的API密钥替换YOUR_API_KEY
gmaps = googlemaps.Client(key='AIzaSyCKaJ8zNApYur8NwZM7-EKEC7QZmggXAVQ')

# 地理编码示例，将地址转换为经纬度
geocode_result = gmaps.geocode('北京天安门')

# 打印结果
print(geocode_result)

import googlemaps
from datetime import datetime

# 用你的API密钥替换YOUR_API_KEY
gmaps = googlemaps.Client(key='AIzaSyCKaJ8zNApYur8NwZM7-EKEC7QZmggXAVQ')

# 设置你要进行逆地理编码的经纬度坐标
latitude = 30
longitude = 116

# 进行逆地理编码查询
reverse_geocode_result = gmaps.reverse_geocode((latitude, longitude))

# 打印结果
print(reverse_geocode_result)

复制]]></description><link>软件\python\地理经纬\python：经纬度和地址互转.html</link><guid isPermaLink="false">软件/Python/地理经纬/python：经纬度和地址互转.md</guid><pubDate>Fri, 21 Jun 2024 07:31:20 GMT</pubDate></item><item><title><![CDATA[问题]]></title><description><![CDATA[ 
 <br><br>NGDC 的下载需要调用 API，如果手动去点实在是太慢了。这一点做得没有 <a data-href="python：下载NCBI数据" href="软件\python\数据获取\python：下载ncbi数据.html" class="internal-link" target="_self" rel="noopener nofollow">python：下载NCBI数据</a>好。因此，我们需要分成2个步骤完成。<br><br>登录网站： <a data-tooltip-position="top" aria-label="https://ngdc.cncb.ac.cn/gsa-human/" rel="noopener nofollow" class="external-link" href="https://ngdc.cncb.ac.cn/gsa-human/" target="_blank">Genome Sequence Archive for Human (cncb.ac.cn)</a><br>
例如，我需要下载 HRA000117 项目的所有样本：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408272239687.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>import requests
import csv

# 输入和输出文件路径
input_file = r'ID.txt'
output_file = r'下载详情.csv'

# 打开输出文件
with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    # 创建CSV写入对象
    csvwriter = csv.writer(csvfile)
    
    # 初始化标志，确保表头只写入一次
    header_written = False
    
    # 读取ID列表并逐个处理
    with open(input_file, 'r') as file:
        for id in file:
            id = id.strip()  # 去除ID前后的空白字符（如换行符）
            if id:  # 如果ID不为空
                # 构造请求URL
                url = f'https://ngdc.cncb.ac.cn/gwh/api/public/assembly/{id}'
                
                try:
                    # 发送GET请求
                    response = requests.get(url)
                    response.raise_for_status()  # 如果请求失败则抛出异常
                    
                    # 将JSON响应转为Python字典
                    response_json = response.json()
                    
                    # 解析JSON对象并转换为适合CSV格式的行
                    # 这里只是一个示例，具体字段需要根据返回的JSON内容调整
                    if not header_written:
                        # 写入表头
                        headers = response_json.keys()
                        csvwriter.writerow(['ID'] + list(headers))
                        header_written = True
                    
                    # 写入数据行
                    row = [id] + list(response_json.values())
                    csvwriter.writerow(row)
                    
                    print(f"ID: {id} 已经成功获取并写入CSV")
                except requests.exceptions.RequestException as e:
                    # 如果请求出错，记录错误信息到CSV中
                    csvwriter.writerow([id, f'请求失败: {str(e)}'])
                    print(f"ID: {id} 获取失败！")

print("任务完成，结果已保存到下载详情.csv")

复制<br>以上脚本可以获得每个 ID 的下载地址。将 json 转换为 csv 文件。<br><br>以下脚本将 csv 文件中提取 ftp 下载的链接，然后依次访问这些链接。<br>import os
import pandas as pd
import requests

# 定义CSV文件路径
csv_file_path = r'下载详情.csv'

# 定义保存文件的目录
download_dir = r'古代DNA'
os.makedirs(download_dir, exist_ok=True)

# 读取CSV文件
df = pd.read_csv(csv_file_path)

# 获取“ftpPathDna”列的所有链接
urls = df['ftpPathDna'].tolist()

# 下载文件
for url in urls:
    file_name = os.path.join(download_dir, url.split('/')[-1])
    print(f"Downloading {file_name}...")
    try:
        response = requests.get(url)
        response.raise_for_status()  # 检查请求是否成功
        with open(file_name, 'wb') as file:
            file.write(response.content)
        print(f"{file_name} downloaded successfully.")
    except requests.exceptions.RequestException as e:
        print(f"Failed to download {file_name}. Error: {e}")

print("All files downloaded.")

复制<br>完成！]]></description><link>软件\python\数据获取\python：下载国家基因组科学数据中心数据（ngdc）.html</link><guid isPermaLink="false">软件/Python/数据获取/python：下载国家基因组科学数据中心数据（NGDC）.md</guid><pubDate>Tue, 27 Aug 2024 14:42:30 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f62e-200d-1f4a8.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f62e-200d-1f4a8.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[下载序列本身]]></title><description><![CDATA[ 
 <br><br>下载 NCBI 数据的常用脚本，绕过了直接使用浏览器下载，避免了经常断线的尴尬问题。<br><br><br>需要准备的是样本 ID 组成的 txt 文件，内容如下所示：<br>OQ707084
OQ707085
OQ707086
OQ707087
OQ707088
OQ707089
OQ707090
OQ707091
OQ707092
OQ707093
复制<br><br>from Bio import Entrez, SeqIO

# 替换为你的电子邮件地址
Entrez.email = "your_email@example.com"

# 从文件读取Accession号列表
accession_list = []
with open("C:/Users/a/Desktop/补充下载NCBI数据.txt", "r") as file:
    for line in file:
        # 移除每行末尾的换行符并添加到列表
        accession_list.append(line.strip())

# 指定保存文件的目录
save_directory = r"C:/Users/a/Desktop/下载序列"

for accession in accession_list:
    # 获取记录
    handle = Entrez.efetch(db="nucleotide", id=accession, rettype="fasta", retmode="text")
    record = SeqIO.read(handle, "fasta")
    handle.close()
    
    # 保存文件，包括完整路径
    filename = f"{save_directory}\\{accession}.fasta"
    SeqIO.write(record, filename, "fasta")
    print(f"Downloaded and saved {filename}")

print("All downloads complete.")

复制<br><br>大多数时候为了下载 NCBI 的序列信息，会选择 fasta 格式，但是这样的格式并不带有样本的信息，例如 经纬度、参考文献 等。这个脚本可以根据 序列号 取回其对应的 参考文献、经纬度、样本别名（即在文献中作者所赋予的 ID），如果 NCBI 没有收录，则返回 invalid。<br><br>需要准备的是样本 ID 组成的 txt 文件，内容如下所示：<br>OQ707084
OQ707085
OQ707086
OQ707087
OQ707088
OQ707089
OQ707090
OQ707091
OQ707092
OQ707093
复制<br><br># 进行代码之前请先准备好你的.txt文件，文件每一行都是一个序列ID

from Bio import Entrez, SeqIO

# 设置您的邮箱地址，可写可不写
Entrez.email = "your_email@example.com"

def download_and_format_features_references(id_list):
    for seq_id in id_list:
        handle = Entrez.efetch(db="nucleotide", id=seq_id, rettype="gb", retmode="text")
        record = SeqIO.read(handle, "genbank")
        country = isolate = lat_lon = "Not Available"
        for feature in record.features:
            if feature.type == "source":
                qualifiers = feature.qualifiers
                country = qualifiers.get("country", ["Not Available"])[0]
                isolate = qualifiers.get("isolate", ["Not Available"])[0]
                lat_lon = qualifiers.get("lat_lon", ["Not Available"])[0]
                break

        reference_titles = ", ".join([ref.title for ref in record.annotations.get("references", []) if ref.title])
        with open ("C:/Users/a/Desktop/下载序列/信息.txt", "a") as output_file:
            output_file.write(f"{seq_id}\t{country}\t{isolate}\t{lat_lon}\t{reference_titles}\n")
        print(f"{seq_id}\t信息已经打印！")
        handle.close()

# 从文件读取序列ID
with open("C:/Users/a/Desktop/新建文本文档.txt", "r") as file:
    example_ids = [line.strip() for line in file.readlines()]

# 调用函数
download_and_format_features_references(example_ids)

复制<br><br>结果应该如下所示：<br>OQ707084	China	Agangrong_M12SK	Not Available	Maternal genetic history of ancient Tibetans over the past 4000 years, Direct Submission
OQ707085	China	Agangrong_M12T	Not Available	Maternal genetic history of ancient Tibetans over the past 4000 years, Direct Submission
OQ707086	China	Agangrong_M13	Not Available	Maternal genetic history of ancient Tibetans over the past 4000 years, Direct Submission
OQ707087	China	Agangrong_M5	Not Available	Maternal genetic history of ancient Tibetans over the past 4000 years, Direct Submission

复制<br>全部以制表符分隔。<br><br>综合以上 2 种功能，可以使用以下脚本：<br><br>需要准备的是样本 ID 组成的 txt 文件，内容如下所示：<br>OQ707084
OQ707085
OQ707086
OQ707087
OQ707088
OQ707089
OQ707090
OQ707091
OQ707092
OQ707093
复制<br><br>from Bio import Entrez, SeqIO

# 设置您的邮箱地址
Entrez.email = "your_email@example.com"

# 指定保存文件的目录
save_directory = r"C:/Users/a/Desktop/下载序列"

# 定义函数以下载序列并提取特征信息
def download_and_process_sequences(id_list):
    for seq_id in id_list:
        # 下载fasta序列
        handle_fasta = Entrez.efetch(db="nucleotide", id=seq_id, rettype="fasta", retmode="text")
        record_fasta = SeqIO.read(handle_fasta, "fasta")
        handle_fasta.close()

        # 保存fasta文件
        filename_fasta = f"{save_directory}\\{seq_id}.fasta"
        SeqIO.write(record_fasta, filename_fasta, "fasta")
        print(f"Downloaded and saved {filename_fasta}")

        # 下载genbank记录以提取特征信息
        handle_gb = Entrez.efetch(db="nucleotide", id=seq_id, rettype="gb", retmode="text")
        record_gb = SeqIO.read(handle_gb, "genbank")
        country = isolate = lat_lon = "Not Available"
        for feature in record_gb.features:
            if feature.type == "source":
                qualifiers = feature.qualifiers
                country = qualifiers.get("country", ["Not Available"])[0]
                isolate = qualifiers.get("isolate", ["Not Available"])[0]
                lat_lon = qualifiers.get("lat_lon", ["Not Available"])[0]
                break

        reference_titles = ", ".join([ref.title for ref in record_gb.annotations.get("references", []) if ref.title])
        with open(f"{save_directory}/信息.txt", "a") as output_file:
            output_file.write(f"{seq_id}\t{country}\t{isolate}\t{lat_lon}\t{reference_titles}\n")
        print(f"{seq_id}\t信息已经打印！")
        handle_gb.close()

# 从同一个文件读取序列ID
id_list = []
with open("C:/Users/a/Desktop/测试.txt", "r") as file:
    for line in file:
        id_list.append(line.strip())

# 调用函数
download_and_process_sequences(id_list)

复制]]></description><link>软件\python\数据获取\python：下载ncbi数据.html</link><guid isPermaLink="false">软件/Python/数据获取/python：下载NCBI数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:30 GMT</pubDate></item><item><title><![CDATA[打开网址]]></title><description><![CDATA[ 
 <br><br><a data-tooltip-position="top" aria-label="https://www.ncbi.nlm.nih.gov/Traces/wgs/JAHWGD01?display=contigs" rel="noopener nofollow" class="external-link" href="https://www.ncbi.nlm.nih.gov/Traces/wgs/JAHWGD01?display=contigs" target="_blank">JAHWGD000000000.1 Helicobacter pylori :: NCBI (nih.gov)</a><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404092239106.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>得到一个 tsv 文件：<br>sequence_name	accession	length	prot_count	title	status	taxid	organism
Scaffold1	JAHWGD010000001.1	253687	0	Scaffold1, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold2	JAHWGD010000002.1	231945	0	Scaffold2, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold3	JAHWGD010000003.1	227650	0	Scaffold3, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold4	JAHWGD010000004.1	183301	0	Scaffold4, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold5	JAHWGD010000005.1	164631	0	Scaffold5, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold6	JAHWGD010000006.1	93579	0	Scaffold6, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold7	JAHWGD010000007.1	68164	0	Scaffold7, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold8	JAHWGD010000008.1	64157	0	Scaffold8, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold9	JAHWGD010000009.1	58982	0	Scaffold9, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold10	JAHWGD010000010.1	46861	0	Scaffold10, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold11	JAHWGD010000011.1	37050	0	Scaffold11, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold12	JAHWGD010000012.1	36849	0	Scaffold12, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold13	JAHWGD010000013.1	30940	0	Scaffold13, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold14	JAHWGD010000014.1	27846	0	Scaffold14, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold15	JAHWGD010000015.1	26854	0	Scaffold15, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold16	JAHWGD010000016.1	16661	0	Scaffold16, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold17	JAHWGD010000017.1	12852	0	Scaffold17, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold18	JAHWGD010000018.1	12496	0	Scaffold18, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold19	JAHWGD010000019.1	10991	0	Scaffold19, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold20	JAHWGD010000020.1	10050	0	Scaffold20, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold21	JAHWGD010000021.1	7178	0	Scaffold21, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold22	JAHWGD010000022.1	5423	0	Scaffold22, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold23	JAHWGD010000023.1	3892	0	Scaffold23, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold24	JAHWGD010000024.1	3555	0	Scaffold24, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold25	JAHWGD010000025.1	2506	0	Scaffold25, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold26	JAHWGD010000026.1	2057	0	Scaffold26, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold27	JAHWGD010000027.1	1848	0	Scaffold27, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold28	JAHWGD010000028.1	1665	0	Scaffold28, whole genome shotgun sequence	Live	210	Helicobacter pylori
Scaffold29	JAHWGD010000029.1	1113	0	Scaffold29, whole genome shotgun sequence	Live	210	Helicobacter pylori
复制<br>去除不需要的信息，只保留 Accession 列，然后将这一列放进一个 txt 文件中。<br>
将这个文件放到 linux 系统中，保存。<br><br>把 SRA 下载软件放到 linux 中，然后解压：<br>tar -zxvf sratoolkit.3.1.0-ubuntu64.tar.gz
复制<br>然后返回自己的目录,添加到系统路径：<br>vim ~/.bashrc
export PATH=$PATH:/home/luolintao/SRA_download/sratoolkit.3.1.0-ubuntu64/bin
复制<br>例如路径可能是：/home/luolintao/SRA_download/sratoolkit.3.1.0-ubuntu64<br><br>fastq-dump -v
prefetch -v
复制<br>如果成功，就会出现<br>
2024-04-09T 13:51:42 fastq-dump.3.1.0 err: param empty while validating argument list - expected accession
<br><br> while read -r line; do
prefetch "$line"
done &lt; 下载accession.txt
复制<br><br>fastq-dump --fasta 0 SRA_Accession -O output_directory
复制<br><br>创建一个 shell 脚本，然后执行它。<br>#!/bin/bash

# 指定包含.sra文件的目录
sra_dir="/home/luolintao/SRA_download/sratoolkit.3.1.0-ubuntu64"
# 指定输出FASTA文件的目录
output_dir="/home/luolintao/SRA_download/fasta_files"

# 创建输出目录，如果不存在的话
mkdir -p "$output_dir"

# 遍历目录中的所有没有后缀的文件
for sra_file in $sra_dir/*; do
  if [ -f "$sra_file" ]; then  # 确保是文件
    filename=$(basename -- "$sra_file")
    extension="${filename##*.}"
    if [ "$extension" = "$filename" ]; then  # 没有后缀的文件
      echo "正在处理文件：$filename"
      # 使用fastq-dump转换为FASTA，并输出到指定目录
      fastq-dump --fasta 0 "$sra_file" -O "$output_dir" --split-files
    fi
  fi
done

echo "所有文件处理完成。"
复制<br><br>
<br>
将上面的脚本保存到一个文件中，例如convert_sra_to_fasta.sh。

<br>
打开终端，导航到脚本所在的目录。

<br>
赋予脚本执行权限：
bashCopy code
chmod +x convert_sra_to_fasta.sh

<br>
执行脚本：
bashCopy code
./convert_sra_to_fasta.sh

]]></description><link>软件\python\数据获取\python：下载sra.html</link><guid isPermaLink="false">软件/Python/数据获取/python：下载SRA.md</guid><pubDate>Fri, 23 Aug 2024 08:22:32 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404092239106.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404092239106.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）]]></title><description><![CDATA[ 
 <br><a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a> 一种用于估算物种或群体的最近共同祖先（MRCA）时间的分子钟方法。该方法基于假设遗传标记（通常是线粒体 DNA）中的差异累积与时间成正比。通过测量遗传距离（即基因序列中的差异数量）并结合已知的分子钟速率，可以估计 MRCA 的时间。<br>
理论部分在这里：<a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a><br><a data-href="σ方法" href="术语\σ方法.html" class="internal-link" target="_self" rel="noopener nofollow">σ方法</a> 是 <a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a>的一种补充，为其增添了一个上下限的功能，作为方差的估计。<br>
我们可以通过一个脚本进行计算：<br>注意！
经过了一些询问和求证，我发现 <a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a> 的计算步骤与我之前想象的不同。请仔细阅读。
<br><br>假如我希望计算支系的ρ值（举例来说，我希望算 D4），那么我应该拿 D4和哪个支系比较才能得到距今多少年前呢？
不用比较的，只需要计算 D4这个支系内部的平均核苷酸差异数就可以了
<br>D4这个支系内部是否包括 D4的下游单倍群呢？（例如 D4a 等等）
要计算整个 D4，包括下游所有的单倍群
<br>以下代码将实现一个功能，从准备好的 txt 文档中提取指定单倍群的下游单倍群及对应的 ID。<br><br>准备一个文档：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040916633.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>所用到的系统发育树的文件可以：<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1725416291&amp;Signature=qJQ20OzkHnsAHqlcn9eOexHeYy8%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1725416291&amp;Signature=qJQ20OzkHnsAHqlcn9eOexHeYy8%3D" target="_blank">下载</a><br>import os

# 定义桌面路径
DESKTOP_PATH = os.path.join(os.path.expanduser("~"), 'Desktop')
HAPLOGROUP_FILE_PATH = os.path.join('线粒体单倍群phylotree(version17)2024年8月19日.txt')
INPUT_TXT_PATH = r'1.txt'

def parse_haplogroup_file(lines):
    haplogroups = []
    for line in lines:
        stripped_line = line.strip()
        if stripped_line:
            level = line.count('\t')
            haplogroups.append((level, stripped_line))
    return haplogroups

def find_downstream_haplogroups(haplogroup_name, haplogroups):
    downstream_haplogroups = []
    found = False
    base_level = None

    for level, haplogroup in haplogroups:
        if haplogroup == haplogroup_name:
            found = True
            base_level = level
            downstream_haplogroups.append(haplogroup)
        elif found and level &gt; base_level:
            downstream_haplogroups.append(haplogroup)
        elif found and level &lt;= base_level:
            break

    return downstream_haplogroups

def extract_ids_for_haplogroups(downstream_haplogroups, input_lines):
    haplogroup_to_ids = {}
    for line in input_lines:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            id, haplogroup = parts
            if haplogroup in downstream_haplogroups:
                if haplogroup not in haplogroup_to_ids:
                    haplogroup_to_ids[haplogroup] = []
                haplogroup_to_ids[haplogroup].append(id)
    return haplogroup_to_ids

def main():
    with open(HAPLOGROUP_FILE_PATH, 'r', encoding='utf-8') as file:
        refined_lines = file.readlines()

    haplogroups = parse_haplogroup_file(refined_lines)

    # 交互式要求用户输入单倍群名称
    haplogroup_name = input("请输入单倍群名称：")

    # 查找该单倍群及其所有下游单倍群
    downstream_haplogroups = find_downstream_haplogroups(haplogroup_name, haplogroups)

    # 读取1.txt文件
    with open(INPUT_TXT_PATH, 'r', encoding='utf-8') as input_file:
        input_lines = input_file.readlines()

    # 提取下游单倍群对应的ID
    haplogroup_to_ids = extract_ids_for_haplogroups(downstream_haplogroups, input_lines)

    # 输出文件路径
    output_file_path = os.path.join(DESKTOP_PATH, f'{haplogroup_name}_下游单倍群_IDs.txt')

    # 将结果写入到桌面的文件中
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        for haplogroup, ids in haplogroup_to_ids.items():
            for id in ids:
                output_file.write(f'{id}\t{haplogroup}\n')

    print(f"已将结果输出到: {output_file_path}")

if __name__ == "__main__":
    main()

复制<br>现在，你得到了输出的文件，可以使用 TBtools 等软件提取对应的 fasta 了。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040934929.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409040935579.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>现在，准备好了 fasta 文件，是时候计算了！<br><br>from Bio import AlignIO
from itertools import combinations
from multiprocessing import Pool
import math

def calculate_rho_sigma(fasta_file):
"""
计算群体中个体间的平均核苷酸差异 ρ 和 σ
"""
# 读取FASTA文件并进行序列比对
alignment = AlignIO.read(fasta_file, "fasta")

# 初始化变量
total_differences = 0
total_squared_differences = 0
total_comparisons = 0

# 准备所有配对组合
seq_pairs = [(seq1.seq, seq2.seq) for seq1, seq2 in combinations(alignment, 2)]
num_comparisons = len(seq_pairs)

# 使用多进程池进行并行计算
with Pool() as pool:
    differences = pool.starmap(calculate_nucleotide_difference, seq_pairs)

# 计算ρ和σ
for difference in differences:
    total_differences += difference  # n_i
    total_squared_differences += difference ** 2  # n_i^2

# 根据文献公式计算ρ和σ²
rho = total_differences / num_comparisons
sigma_squared = total_squared_differences / (num_comparisons ** 2)
sigma = math.sqrt(sigma_squared)

return rho, sigma


if __name__ == '__main__':
    # 指定FASTA文件路径
    fasta_file = r'C:/Users/victo/Desktop/1.fasta'

    # 计算平均核苷酸差异 π 和 σ
    rho, sigma = calculate_rho_sigma(fasta_file)
    print(f"平均核苷酸差异 ρ: {rho}")
    print(f"σ值: {sigma}")

复制<br><br>from Bio import AlignIO, SeqIO
from itertools import combinations
from multiprocessing import Pool
from pathlib import Path
import math

# 全局变量，定义输入文件和输出文件路径
INPUT_FASTA_FILE = Path('alignment.fasta')
OUTPUT_TXT_FILE = Path('output_results.txt')

# 计算核苷酸差异
def calculate_nucleotide_difference(seq1, seq2):
    """计算两个序列之间的核苷酸差异"""
    return sum(1 for a, b in zip(seq1, seq2) if a != b)

def calculate_rho_sigma(fasta_file):
    """计算群体中个体间的平均核苷酸差异 ρ 和 σ"""
    alignment = AlignIO.read(fasta_file, "fasta")

    seq_pairs = [(seq1.seq, seq2.seq) for seq1, seq2 in combinations(alignment, 2)]
    num_comparisons = len(seq_pairs)

    # 并行处理计算核苷酸差异
    with Pool() as pool:
        differences = pool.starmap(calculate_nucleotide_difference, seq_pairs)

    total_differences = sum(differences)
    total_squared_differences = sum(diff ** 2 for diff in differences)

    rho = total_differences / num_comparisons
    sigma_squared = total_squared_differences / (num_comparisons ** 2)
    sigma = math.sqrt(sigma_squared)

    return rho, sigma

def remove_bases(seq, positions):
    """从后往前删除指定位置的碱基"""
    for pos in sorted(positions, reverse=True):
        seq = seq[:pos-1] + seq[pos:]
    return seq

def write_fasta(fasta_sequences, output_file):
    """将处理后的序列写入FASTA文件"""
    with open(output_file, 'w') as f:
        for header, seq in fasta_sequences:
            f.write(f"&gt;{header}\n")
            for i in range(0, len(seq), 60):
                f.write(f"{seq[i:i+60]}\n")

def process_fasta(input_file):
    """处理FASTA文件并生成4个新的FASTA文件"""
    fasta_sequences = list(SeqIO.parse(input_file, 'fasta'))

    output_files = {
        "Complete_sequence.fasta": [],
        "HVS-I_(16051-16400).fasta": [],
        "HVS-II_(68-263).fasta": [],
        "Control_Region.fasta": []
    }

    positions_to_remove = [16519, 16194, 16183, 16182]

    for fasta in fasta_sequences:
        header, sequence = fasta.id, str(fasta.seq)

        # 第一种情况
        seq_1 = remove_bases(sequence, positions_to_remove)
        output_files["Complete_sequence.fasta"].append((header, seq_1))

        # 第二种情况
        seq_2 = seq_1[16051-1:16397]
        output_files["HVS-I_(16051-16400).fasta"].append((header, seq_2))

        # 第三种情况
        seq_3 = sequence[68-1:263]
        output_files["HVS-II_(68-263).fasta"].append((header, seq_3))

        # 第四种情况
        seq_4 = sequence[16024-1:16569] + sequence[0:576]
        output_files["Control_Region.fasta"].append((header, seq_4))

    # 写入生成的4个FASTA文件
    for output_file, sequences in output_files.items():
        write_fasta(sequences, output_file)

def calculate_for_all_files(output_txt_file):
    """对4个新生成的FASTA文件分别进行ρ和σ的计算，并将结果写入txt文件"""
    fasta_files = [
        'Complete_sequence.fasta',
        'HVS-I_(16051-16400).fasta',
        'HVS-II_(68-263).fasta',
        'Control_Region.fasta'
    ]

    results = {}

    for fasta_file in fasta_files:
        rho, sigma = calculate_rho_sigma(fasta_file)
        region_key = fasta_file.split('_')[0].upper()
        results[f'RHO_{region_key}'] = f"{rho:.6f}"
        results[f'RHO_{region_key}_SE'] = f"{sigma:.6f}"

    # 写入结果到txt文件
    with open(output_txt_file, 'w') as f:
        f.write("SAMPLE\tRHO_CS\tRHO_CS_SE\tRHO_SYN\tRHO_SYN_SE\t"
                "RHO_HVSI\tRHO_HVSI_SE\tRHO_HVSI_TRANSI\t"
                "RHO_HVSI_TRANSI_SE\tRHO_HVSII\tRHO_HVSII_SE\t"
                "RHO_CR\tRHO_CR_SE\n")
        f.write(f"1\t{results.get('RHO_COMPLETE', '0')}\t"
                f"{results.get('RHO_COMPLETE_SE', '0')}\t0\t0\t"
                f"{results.get('RHO_HVS-I', '0')}\t"
                f"{results.get('RHO_HVS-I_SE', '0')}\t0\t0\t"
                f"{results.get('RHO_HVS-II', '0')}\t"
                f"{results.get('RHO_HVS-II_SE', '0')}\t"
                f"{results.get('RHO_CONTROL', '0')}\t"
                f"{results.get('RHO_CONTROL_SE', '0')}\n")

    # 删除临时产生的FASTA文件
    for fasta_file in fasta_files:
        Path(fasta_file).unlink()
        print(f"已经删除了临时文件： {fasta_file}，如需保留删掉相应代码")

if __name__ == '__main__':
    # 处理输入FASTA文件并生成新的FASTA文件
    process_fasta(INPUT_FASTA_FILE)

    # 对新生成的FASTA文件计算 ρ 和 σ 并写入结果文件
    calculate_for_all_files(OUTPUT_TXT_FILE)

复制<br>为了提升计算速度，本代码可以调用 CPU 多核性能导致卡顿。<br>推荐使用第二个代码进行计算，第二个代码将会得到一个 txt 文件，该文件可以直接用于计算：<a data-href="python：ρ（rho）方法计算mtDNA共同祖先时间" href="软件\python\数据科学与格式转换\ρ方法\python：ρ（rho）方法计算mtdna共同祖先时间.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：ρ（rho）方法计算mtDNA共同祖先时间</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。]]></description><link>软件\python\数据科学与格式转换\ρ方法\python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/ρ方法/python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）.md</guid><pubDate>Tue, 08 Oct 2024 03:33:15 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[背景]]></title><description><![CDATA[ 
 <br>这个代码没有包括 同义突变率 的计算，因为我暂时没想好如何写这样的代码，之后可能会进一步的改进。<br><br>首先，我们需要知道什么是 <a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a>和 <a data-href="σ方法" href="术语\σ方法.html" class="internal-link" target="_self" rel="noopener nofollow">σ方法</a>。请自行复习理论知识。<br>
该代码将以下代码的功能全部集中起来了：<br>
<a data-href="python：ρ（rho）方法计算mtDNA共同祖先时间" href="软件\python\数据科学与格式转换\ρ方法\python：ρ（rho）方法计算mtdna共同祖先时间.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：ρ（rho）方法计算mtDNA共同祖先时间</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
<a data-href="python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）" href="软件\python\数据科学与格式转换\ρ方法\python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br><br>需要3个数据：<br><br>我们需要一个线粒体全基因组测序数据，并且经过了比对。由于我加入了删除特定位点的功能<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>，所以建议使用 <a data-href="MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)" href="软件\其它生信软件\a-j\mafft：(multiple-alignment-program-for-amino-acid-or-nucleotide-sequences).html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：(Multiple alignment program for amino acid or nucleotide sequences)</a> 软件进行如下代码的比对：<br>mafft --add 未比对的文件.fasta --keeplength rCRS.fasta
复制<br>这个代码可以保持长度为16569。<br><br>ID	MT_HAP
FJ748724	A
OM194256	A
HG00565	A
HG00566	A
HG00567	A
KX456634	A
HGDP01227	A
KU682910	A
KU682924	A
KU682951	A
KU682973	A
KU682988	A
KU683271	A
KU683494	A
KU683587	A
.......
复制<br>如上所示，必须将 ID 放在第一列，单倍群放在第二列。其中第一列的 ID 应该和 FASTA 文件中的 ID 对应。<br><br>点击这里下载： <a data-tooltip-position="top" aria-label="https://vip.123pan.cn/1835545223/%E6%96%87%E6%A1%A3%EF%BC%88%E5%85%B1%E4%BA%AB%EF%BC%89/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree%28version17%292024%E5%B9%B48%E6%9C%8819%E6%97%A5.txt" rel="noopener nofollow" class="external-link" href="https://vip.123pan.cn/1835545223/%E6%96%87%E6%A1%A3%EF%BC%88%E5%85%B1%E4%BA%AB%EF%BC%89/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree%28version17%292024%E5%B9%B48%E6%9C%8819%E6%97%A5.txt" target="_blank">发育树</a><br><br>运行如下代码，终端会提示你输入希望计算的单倍群以及质量控制指标。<br>import os
from Bio import AlignIO, SeqIO
from Bio.Seq import Seq  # 导入Seq类
from itertools import combinations
from multiprocessing import Pool
from pathlib import Path
import math
import pandas as pd
import numpy as np
import shutil

# 定义桌面路径
DESKTOP_PATH = os.path.join(os.path.expanduser("~"), 'Desktop')
HAPLOGROUP_FILE_PATH = os.path.join('线粒体单倍群phylotree(version17).txt')
INPUT_TXT_PATH = r'基本信息文件.txt'
FASTA_FILE_PATH = r'古代现代对齐.fasta'
OUTPUT_FASTA_PATH = None  # 根据用户输入的单倍群动态生成

# 输出文件路径
OUTPUT_TXT_FILE = Path('output_results.txt')
FINAL_OUTPUT_FILE = Path(f'Rho统计共祖时间.txt')
RHO_RESULT_FILE = Path('ρ结果.txt')

# ------------------- 单倍群提取部分 -------------------
def parse_haplogroup_file(lines):
    print("正在解析Haplogroup文件...")
    haplogroups = []
    for line in lines:
        stripped_line = line.strip()
        if stripped_line:
            level = line.count('\t')
            haplogroups.append((level, stripped_line))
    return haplogroups

def find_downstream_haplogroups(haplogroup_name, haplogroups):
    print("正在查找下级单倍群...")
    downstream_haplogroups = []
    found = False
    base_level = None

    for level, haplogroup in haplogroups:
        if haplogroup == haplogroup_name:
            found = True
            base_level = level
            downstream_haplogroups.append(haplogroup)
        elif found and level &gt; base_level:
            downstream_haplogroups.append(haplogroup)
        elif found and level &lt;= base_level:
            break

    return downstream_haplogroups

def extract_ids_for_haplogroups(downstream_haplogroups, input_lines):
    print("正在提取ID...")
    haplogroup_to_ids = {}
    for line in input_lines:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            id, haplogroup = parts
            if haplogroup in downstream_haplogroups:
                if haplogroup not in haplogroup_to_ids:
                    haplogroup_to_ids[haplogroup] = []
                haplogroup_to_ids[haplogroup].append(id)
    return haplogroup_to_ids

def extract_matching_sequences(fasta_path, ids_to_extract):
    print("正在提取符合条件的序列...")
    """从FASTA文件中提取符合条件的序列"""
    sequences = {}
    with open(fasta_path, 'r') as fasta_file:
        for record in SeqIO.parse(fasta_file, "fasta"):
            if record.id in ids_to_extract:
                sequences[record.id] = record
    return sequences

# ------------------- 新增的序列检查和过滤功能 -------------------
def check_and_filter_sequences(sequences):
    print("正在检查序列质量...")
    """检查序列长度，替换特殊字符，并根据N比例过滤"""
    sequence_lengths = [len(record.seq) for record in sequences.values()]

    # 检查所有序列长度是否一致
    if len(set(sequence_lengths)) &gt; 1:
        print("错误：序列长度不一致，程序终止。")
        exit(1)

    # 检查特殊字符并替换为N
    valid_chars = set("AGCTN-")
    print("正在检查特殊字符并替换为N...")
    for record in sequences.values():
        seq_str = str(record.seq)  # 获取序列的字符串表示
        # 替换非标准碱基字符为N
        new_seq = ''.join([base if base in valid_chars else 'N' for base in seq_str])
        record.seq = Seq(new_seq)  # 使用Bio.Seq模块的Seq类将字符串转换为Seq对象

    # 计算每个序列的N_ratio
    print("正在计算每个序列的N比例...")
    n_ratios = {record.id: record.seq.count('N') / len(record.seq) for record in sequences.values()}

    # 获取用户输入的阈值
    threshold = float(input("请输入需要去除的N比例阈值(例如0.1)："))

    # 过滤N比例超过阈值的序列
    filtered_sequences = {id: seq for id, seq in sequences.items() if n_ratios[id] &lt;= threshold}

    # 提示哪些序列被移除
    removed_ids = [id for id in sequences if id not in filtered_sequences]
    if removed_ids:
        print(f"以下序列因N比例过高被移除: {', '.join(removed_ids)}")
    else:
        print("所有序列的N比例都低于阈值，无需移除。")

    return filtered_sequences

def write_fasta_file(output_path, sequences):
    """将提取的序列写入新的FASTA文件"""
    print("正在写入FASTA文件...")
    with open(output_path, 'w') as output_file:
        SeqIO.write(sequences.values(), output_file, "fasta")

# ------------------- 核苷酸差异计算和ρ、σ部分 -------------------
def calculate_nucleotide_difference(seq1, seq2):
    """计算两个序列之间的核苷酸差异"""
    return sum(1 for a, b in zip(seq1, seq2) if a != b)

def calculate_rho_sigma(fasta_file):
    alignment = AlignIO.read(fasta_file, "fasta")
    seq_pairs = [(seq1.seq, seq2.seq) for seq1, seq2 in combinations(alignment, 2)]
    num_comparisons = len(seq_pairs)

    with Pool() as pool:
        differences = pool.starmap(calculate_nucleotide_difference, seq_pairs)

    total_differences = sum(differences)
    total_squared_differences = sum(diff ** 2 for diff in differences)

    rho = total_differences / num_comparisons
    sigma_squared = total_squared_differences / (num_comparisons ** 2)
    sigma = math.sqrt(sigma_squared)

    return rho, sigma

def remove_bases(seq, positions):
    for pos in sorted(positions, reverse=True):
        seq = seq[:pos-1] + seq[pos:]
    return seq

def write_fasta(fasta_sequences, output_file):
    with open(output_file, 'w') as f:
        for header, seq in fasta_sequences:
            f.write(f"&gt;{header}\n")
            for i in range(0, len(seq), 60):
                f.write(f"{seq[i:i+60]}\n")

def process_fasta(input_file):
    fasta_sequences = list(SeqIO.parse(input_file, 'fasta'))

    output_files = {
        "Complete_sequence.fasta": [],
        "HVS-I_(16051-16400).fasta": [],
        "HVS-II_(68-263).fasta": [],
        "Control_Region.fasta": []
    }
    positions_to_remove = [16519, 16194, 16183, 16182]
    for fasta in fasta_sequences:
        header, sequence = fasta.id, str(fasta.seq)

        seq_1 = remove_bases(sequence, positions_to_remove)
        output_files["Complete_sequence.fasta"].append((header, seq_1))

        seq_2 = seq_1[16051-1:16397]
        output_files["HVS-I_(16051-16400).fasta"].append((header, seq_2))

        seq_3 = sequence[68-1:263]
        output_files["HVS-II_(68-263).fasta"].append((header, seq_3))

        seq_4 = sequence[16024-1:16569] + sequence[0:576]
        output_files["Control_Region.fasta"].append((header, seq_4))

    for output_file, sequences in output_files.items():
        write_fasta(sequences, output_file)

def calculate_for_all_files(output_txt_file, haplogroup_name):
    fasta_files = [
        'Complete_sequence.fasta',
        'HVS-I_(16051-16400).fasta',
        'HVS-II_(68-263).fasta',
        'Control_Region.fasta'
    ]

    results = {}

    for fasta_file in fasta_files:
        print("正在计算，请稍后")
        rho, sigma = calculate_rho_sigma(fasta_file)
        region_key = fasta_file.split('_')[0].upper()
        results[f'RHO_{region_key}'] = f"{rho:.6f}"
        results[f'RHO_{region_key}_SE'] = f"{sigma:.6f}"

    # 使用haplogroup_name作为SAMPLE列
    with open(output_txt_file, 'w') as f:
        f.write("SAMPLE\tRHO_CS\tRHO_CS_SE\tRHO_SYN\tRHO_SYN_SE\t"
                "RHO_HVSI\tRHO_HVSI_SE\tRHO_HVSI_TRANSI\t"
                "RHO_HVSI_TRANSI_SE\tRHO_HVSII\tRHO_HVSII_SE\t"
                "RHO_CR\tRHO_CR_SE\n")
        f.write(f"{haplogroup_name}\t{results.get('RHO_COMPLETE', '0')}\t"
                f"{results.get('RHO_COMPLETE_SE', '0')}\t0\t0\t"
                f"{results.get('RHO_HVS-I', '0')}\t"
                f"{results.get('RHO_HVS-I_SE', '0')}\t0\t0\t"
                f"{results.get('RHO_HVS-II', '0')}\t"
                f"{results.get('RHO_HVS-II_SE', '0')}\t"
                f"{results.get('RHO_CONTROL', '0')}\t"
                f"{results.get('RHO_CONTROL_SE', '0')}\n")

    for fasta_file in fasta_files:
        Path(fasta_file).unlink()

# ------------------- 共祖时间计算部分 -------------------
def calculate_ages(df, rho_col, se_col, multiplier):
    df[f'{rho_col}_AGE'] = df[rho_col] * multiplier
    df[f'{rho_col}_AGE_95LB'] = (df[rho_col] - 1.96 * df[se_col]) * multiplier
    df[f'{rho_col}_AGE_95HB'] = (df[rho_col] + 1.96 * df[se_col]) * multiplier

def process_rho_results(output_txt_file, final_output_file, haplogroup_name, sequence_count):
    df = pd.read_csv(output_txt_file, sep='\t')

    required_columns = [
        'Haplogroup', 'RHO_CS', 'RHO_CS_SE',
        'RHO_SYN', 'RHO_SYN_SE',
        'RHO_HVSI', 'RHO_HVSI_SE',
        'RHO_HVSI_TRANSI', 'RHO_HVSI_TRANSI_SE',
        'RHO_HVSII', 'RHO_HVSII_SE',
        'RHO_CR', 'RHO_CR_SE'
    ]

    for col in required_columns:
        if col not in df.columns:
            df[col] = 0

    # 插入用户输入的单倍群名称
    df['Haplogroup'] = haplogroup_name

    # 插入序列数量
    df['Number'] = sequence_count

    cs_multiplier = 3624
    cs_base = -40.2789
    cs_exp = -0.0263
    df['CS_AGE'] = cs_multiplier * (((np.exp(-(np.exp((df['RHO_CS'] - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * df['RHO_CS'])
    df['CS_AGE_95LB'] = cs_multiplier * (((np.exp(-(np.exp(((df['RHO_CS'] - 1.96 * df['RHO_CS_SE']) - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * (df['RHO_CS'] - 1.96 * df['RHO_CS_SE']))
    df['CS_AGE_95HB'] = cs_multiplier * (((np.exp(-(np.exp(((df['RHO_CS'] + 1.96 * df['RHO_CS_SE']) - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * (df['RHO_CS'] + 1.96 * df['RHO_CS_SE']))

    multipliers = {
        'RHO_SYN': 7872,
        'RHO_HVSI': 16677,
        'RHO_HVSI_TRANSI': 18845,
        'RHO_HVSII': 22388,
        'RHO_CR': 9058
    }

    for rho_col, multiplier in multipliers.items():
        calculate_ages(df, rho_col, f'{rho_col}_SE', multiplier)

    result_columns = [
        'Haplogroup', 'Number', 'CS_AGE', 'CS_AGE_95LB', 'CS_AGE_95HB',
        'RHO_SYN_AGE', 'RHO_SYN_AGE_95LB', 'RHO_SYN_AGE_95HB',
        'RHO_HVSI_AGE', 'RHO_HVSI_AGE_95LB', 'RHO_HVSI_AGE_95HB',
        'RHO_HVSI_TRANSI_AGE', 'RHO_HVSI_TRANSI_AGE_95LB', 'RHO_HVSI_TRANSI_AGE_95HB',
        'RHO_HVSII_AGE', 'RHO_HVSII_AGE_95LB', 'RHO_HVSII_AGE_95HB',
        'RHO_CR_AGE', 'RHO_CR_AGE_95LB', 'RHO_CR_AGE_95HB'
    ]
    result = df[result_columns]
    result.to_csv(final_output_file, sep='\t', index=False, encoding='UTF-8', mode='a', header=False)
    print(result)

# ------------------- 主程序 -------------------
def main():
    # 读取母系单倍群文件
    with open(HAPLOGROUP_FILE_PATH, 'r', encoding='utf-8') as file:
        refined_lines = file.readlines()

    haplogroups = parse_haplogroup_file(refined_lines)

    # 交互式要求用户输入单倍群名称
    haplogroup_name = input("请输入单倍群名称：")

    # 查找该单倍群及其所有下游单倍群
    downstream_haplogroups = find_downstream_haplogroups(haplogroup_name, haplogroups)

    # 读取输入的txt文件
    with open(INPUT_TXT_PATH, 'r', encoding='utf-8') as input_file:
        input_lines = input_file.readlines()

    # 提取下游单倍群对应的ID
    haplogroup_to_ids = extract_ids_for_haplogroups(downstream_haplogroups, input_lines)

    # 收集所有的ID
    all_ids = []
    for ids in haplogroup_to_ids.values():
        all_ids.extend(ids)

    # 从FASTA文件中提取匹配的序列
    matching_sequences = extract_matching_sequences(FASTA_FILE_PATH, all_ids)

    # 进行序列检查和过滤
    filtered_sequences = check_and_filter_sequences(matching_sequences)

    # 输出新的FASTA文件
    global OUTPUT_FASTA_PATH
    OUTPUT_FASTA_PATH = os.path.join(DESKTOP_PATH, f'{haplogroup_name}_sequences.fasta')
    write_fasta_file(OUTPUT_FASTA_PATH, filtered_sequences)

    print(f"已将符合条件的序列输出到: {OUTPUT_FASTA_PATH}")

    # 处理生成的FASTA文件并计算ρ和σ
    process_fasta(OUTPUT_FASTA_PATH)

    # 调用 calculate_for_all_files 时传递 haplogroup_name 参数
    calculate_for_all_files(OUTPUT_TXT_FILE, haplogroup_name)

    # 获取过滤后FASTA文件中的序列数量
    sequence_count = len(filtered_sequences)

    # 处理 output_results.txt 文件并计算共祖时间，添加序列数量和单倍群名称
    process_rho_results(OUTPUT_TXT_FILE, FINAL_OUTPUT_FILE, haplogroup_name, sequence_count)

    # 追加模式将 OUTPUT_TXT_FILE 的内容复制到 ρ结果.txt 文件
    with open(OUTPUT_TXT_FILE, 'r') as source_file:
        with open(RHO_RESULT_FILE, 'a') as target_file:
            shutil.copyfileobj(source_file, target_file)

    print(f"已将 {OUTPUT_TXT_FILE} 的内容追加到 {RHO_RESULT_FILE}")

if __name__ == "__main__":
    main()

复制<br><br>Rho统计共祖时间.txt：<br>D5a	108	28862.656901081224	27976.497605014723	29751.893280052132	0	0.0	0.0	47661.348393	46171.08633636	49151.61044963999	0	0.0	0.0	8539.835436	8154.38929968	8925.28157232	55323.0937	53751.555700079996	56894.63169991999
复制<br>ρ结果.txt：<br>SAMPLE	RHO_CS	RHO_CS_SE	RHO_SYN	RHO_SYN_SE	RHO_HVSI	RHO_HVSI_SE	RHO_HVSI_TRANSI	RHO_HVSI_TRANSI_SE	RHO_HVSII	RHO_HVSII_SE	RHO_CR	RHO_CR_SE
D5a	10.369851	0.151796	0	0	2.857909	0.045592	0	0	0.381447	0.008784	6.107650	0.088519
复制<br>其中 Rho统计共祖时间.txt 中的年份分别对应 ρ结果.txt 中的列标题。<br><br><br>
<br>
<br>删除的位点是极易回复突变的位点，具体你可以在这篇文献的方法中找到：<a data-href="2009 AJHG 净化选择校正：改进的人类线粒体分子钟" href="文献及报道\文献\2024年阅读\7-12月\2009-ajhg-净化选择校正：改进的人类线粒体分子钟.html" class="internal-link" target="_self" rel="noopener nofollow">2009 AJHG 净化选择校正：改进的人类线粒体分子钟</a>、<a data-href="2024 JGG 中国云南新石器时代至青铜时代的母系遗传史" href="文献及报道\文献\2024年阅读\7-12月\2024-jgg-中国云南新石器时代至青铜时代的母系遗传史.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="LiThumbsUp" aria-label="LiThumbsUp" data-icon="LiThumbsUp" aria-hidden="true" style="transform: translateY(20%);"><svg xmlns="http://www.w3.org/2000/svg" width="18px" height="18px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-thumbs-up"><path d="M7 10v12"></path><path d="M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2a3.13 3.13 0 0 1 3 3.88Z"></path></svg></span>2024 JGG 中国云南新石器时代至青铜时代的母系遗传史</a> 、<a data-href="2024 PO 调查越南南岛语传播中的文化扩散和性别偏倚" href="文献及报道\文献\2024年阅读\7-12月\2024-po-调查越南南岛语传播中的文化扩散和性别偏倚.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="LiQuestionMarkGlyph" aria-label="LiQuestionMarkGlyph" data-icon="LiQuestionMarkGlyph" aria-hidden="true" style="transform: translateY(20%);"><svg xmlns="http://www.w3.org/2000/svg" width="18px" height="18px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="question-mark-glyph"><path d="M12 22C17.5228 22 22 17.5228 22 12C22 6.47715 17.5228 2 12 2C6.47715 2 2 6.47715 2 12C2 17.5228 6.47715 22 12 22Z"></path><path d="M9.09009 9.00003C9.32519 8.33169 9.78924 7.76813 10.4 7.40916C11.0108 7.05019 12.079 6.94542 12.7773 7.06519C13.9093 7.25935 14.9767 8.25497 14.9748 9.49073C14.9748 11.9908 12 11.2974 12 14"></path><path d="M12 17H12.01"></path></svg></span>2024 PO 调查越南南岛语传播中的文化扩散和性别偏倚</a>的方法部分。<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\python\数据科学与格式转换\ρ方法\python：一次性计算σ+ρ.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/ρ方法/Python：一次性计算σ+ρ.md</guid><pubDate>Wed, 30 Oct 2024 01:30:27 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[理论]]></title><description><![CDATA[ 
 <br>以下代码源自该文献：<br>
<a data-href="2009 AJHG 净化选择校正：改进的人类线粒体分子钟" href="文献及报道\文献\2024年阅读\7-12月\2009-ajhg-净化选择校正：改进的人类线粒体分子钟.html" class="internal-link" target="_self" rel="noopener nofollow">2009 AJHG 净化选择校正：改进的人类线粒体分子钟</a>
<br><br>理论部分在这里：<a data-href="ρ（rho）方法" href="术语\ρ（rho）方法.html" class="internal-link" target="_self" rel="noopener nofollow">ρ（rho）方法</a><br>
除了使用 <a data-href="贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）" href="术语\贝叶斯进化树分析（bayesian-evolutionary-analysis-by-sampling-trees，beast）.html" class="internal-link" target="_self" rel="noopener nofollow">贝叶斯进化树分析（Bayesian Evolutionary Analysis by Sampling Trees，BEAST）</a>，还可以使用 ρ方法计算<a data-href="最近共同祖先时间(Time to most recent common ancestor，MRCA)" href="术语\最近共同祖先时间(time-to-most-recent-common-ancestor，mrca).html" class="internal-link" target="_self" rel="noopener nofollow">最近共同祖先时间(Time to most recent common ancestor，MRCA)</a> 时间。该方法提供了一种无偏的估计<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>。尽管该方法受到了广泛的质疑，但是一些文献依然证明了其合理性<a data-footref="2" href="about:blank#fn-2-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[2]</a>。<br>注意，在进行下列之前，请确认已经计算了ρ值和σ值。
如果没有，可以<a data-href="python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）" href="软件\python\数据科学与格式转换\ρ方法\python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：计算ρ（rho）和σ（sigma）（2024年10月8日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">
<br><br>该文献的附加材料中给到了一个 Excel 表格。<a data-tooltip-position="top" aria-label="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2694979/bin/mmc2.xls" rel="noopener nofollow" class="external-link" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2694979/bin/mmc2.xls" target="_blank">表格下载</a><br>
表格本身是一个计算器，但是针对大规模数据，手动填入耗费时间。因此，可以使用如下代码计算。<br><br>需要填入这样一个表格：<br>SAMPLE	RHO_CS	RHO_CS_SE	RHO_SYN	RHO_SYN_SE	RHO_HVSI	RHO_HVSI_SE	RHO_HVSI_TRANSI	RHO_HVSI_TRANSI_SE	RHO_HVSII	RHO_HVSII_SE	RHO_CR	RHO_CR_SE
复制<br><a data-tooltip-position="top" aria-label="https://1drv.ms/t/s!AnGqDjyiZ5t_hflG99fptnbUGFAJjQ?e=5utxw2" rel="noopener nofollow" class="external-link" href="https://1drv.ms/t/s!AnGqDjyiZ5t_hflG99fptnbUGFAJjQ?e=5utxw2" target="_blank">Rho方法计算模板.txt</a><br><br>import pandas as pd
import numpy as np

# 读取数据
file_path = 'ρ+σ_result.txt'
output_path = 'Rho统计共祖时间.txt'
df = pd.read_csv(file_path, sep='\t')

# 需要的列名
required_columns = [
    'Haplogroup', 'RHO_CS', 'RHO_CS_SE',
    'RHO_SYN', 'RHO_SYN_SE',
    'RHO_HVSI', 'RHO_HVSI_SE',
    'RHO_HVSI_TRANSI', 'RHO_HVSI_TRANSI_SE',
    'RHO_HVSII', 'RHO_HVSII_SE',
    'RHO_CR', 'RHO_CR_SE'
]

# 检查并添加缺失的列，值全部设为0
for col in required_columns:
    if col not in df.columns:
        df[col] = 0

# 定义函数
def calculate_ages(df, rho_col, se_col, multiplier):
    df[f'{rho_col}_AGE'] = df[rho_col] * multiplier
    df[f'{rho_col}_AGE_95LB'] = (df[rho_col] - 1.96 * df[se_col]) * multiplier
    df[f'{rho_col}_AGE_95HB'] = (df[rho_col] + 1.96 * df[se_col]) * multiplier

# 定义计算
cs_multiplier = 3624
cs_base = -40.2789
cs_exp = -0.0263
df['CS_AGE'] = cs_multiplier * (((np.exp(-(np.exp((df['RHO_CS'] - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * df['RHO_CS'])
df['CS_AGE_95LB'] = cs_multiplier * (((np.exp(-(np.exp(((df['RHO_CS'] - 1.96 * df['RHO_CS_SE']) - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * (df['RHO_CS'] - 1.96 * df['RHO_CS_SE']))
df['CS_AGE_95HB'] = cs_multiplier * (((np.exp(-(np.exp(((df['RHO_CS'] + 1.96 * df['RHO_CS_SE']) - cs_base) * cs_exp)))) * 0.4794) / 0.4794 * (df['RHO_CS'] + 1.96 * df['RHO_CS_SE']))

# 定义倍率
multipliers = {
    'RHO_SYN': 7872,
    'RHO_HVSI': 16677,
    'RHO_HVSI_TRANSI': 18845,
    'RHO_HVSII': 22388,
    'RHO_CR': 9058
}

# 循环计算 
for rho_col, multiplier in multipliers.items():
    calculate_ages(df, rho_col, f'{rho_col}_SE', multiplier)

# 生成输出文件
result_columns = [
    'Haplogroup', 'CS_AGE', 'CS_AGE_95LB', 'CS_AGE_95HB',
    'RHO_SYN_AGE', 'RHO_SYN_AGE_95LB', 'RHO_SYN_AGE_95HB',
    'RHO_HVSI_AGE', 'RHO_HVSI_AGE_95LB', 'RHO_HVSI_AGE_95HB',
    'RHO_HVSI_TRANSI_AGE', 'RHO_HVSI_TRANSI_AGE_95LB', 'RHO_HVSI_TRANSI_AGE_95HB',
    'RHO_HVSII_AGE', 'RHO_HVSII_AGE_95LB', 'RHO_HVSII_AGE_95HB',
    'RHO_CR_AGE', 'RHO_CR_AGE_95LB', 'RHO_CR_AGE_95HB'
]
result = df[result_columns]
result.to_csv(output_path, sep='\t', index=False, encoding='UTF-8')

# 打印
print(result)


复制<br><br><br><br>
<br>
<br>Origin and Evolution of Native American mtDNA Variation: A Reappraisal<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
<br>Correcting for Purifying Selection: An Improved Human Mitochondrial Molecular Clock<a href="about:blank#fnref-2-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\python\数据科学与格式转换\ρ方法\python：ρ（rho）方法计算mtdna共同祖先时间.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/ρ方法/python：ρ（rho）方法计算mtDNA共同祖先时间.md</guid><pubDate>Tue, 08 Oct 2024 03:33:49 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br>把在一个群体中出现的<br><br>def find_terminal_haplogroups(input_file_path, output_file_path):
    import pandas as pd
    
    # 读取文件并忽略第一行标题
    df = pd.read_csv(input_file_path, sep='\t', header=0)
    
    # 提取所有单倍群
    all_haplogroups = set(df.iloc[:, 0])
    upstream_haplogroups = set(df.iloc[:, 1:].stack())
    
    # 找出末端单倍群（仅在第一列出现的单倍群）
    terminal_haplogroups = all_haplogroups - upstream_haplogroups
    
    # 保存结果
    with open(output_file_path, 'w') as f:
        for haplogroup in terminal_haplogroups:
            f.write(f"{haplogroup}\n")

# 输入和输出文件路径
input_file_path = '逆序等级.txt'
output_file_path = 'terminal_haplogroups.txt'

# 调用函数处理文件
find_terminal_haplogroups(input_file_path, output_file_path)

复制<br><br>]]></description><link>软件\python\数据科学与格式转换\python：查找末端单倍群.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：查找末端单倍群.md</guid><pubDate>Mon, 24 Jun 2024 11:00:29 GMT</pubDate></item><item><title><![CDATA[Python：查找mtDNA上游单倍型（2024年10月23日更新）]]></title><description><![CDATA[ 
 <br>2024年6月更新内容：选定一个级别，可以查询该支系的该级别的单倍群。<br>2024年10月23日更新内容：调整查找逻辑，极大提高速度。
为什么要一行一行去找单倍群呢？首先查重，仅保留一个；查找；还原重复项；完美。
<br>这个脚本的作用是，给定任何一个线粒体单倍型，能够将该单倍型上游的所有单倍型查找出来并放置在一行。<br><br>
<br>请将本文末尾的文件保存为 线粒体单倍群phylotree(version17).txt
<br>请准备好需要查询的文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520171304.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
表不包含标题。
<br><br>import os

# 定义输入、输出文件的路径
BASE_PATH = r'C:/Users/victo/Desktop/'
INPUT_FILE_PATH = os.path.join(BASE_PATH, '需要查询的单倍群.txt')
OUTPUT_FILE_PATH = os.path.join(BASE_PATH, '正序等级.txt')
REVERSED_OUTPUT_FILE_PATH = os.path.join(BASE_PATH, '逆序等级.txt')
NOT_FOUND_FILE_PATH = os.path.join(BASE_PATH, '没有查询到请核实.txt')
HAPLOGROUP_FILE_PATH = os.path.join('F:/OneDrive/文档（科研）/脚本/我的科研脚本/Python/母系专用/线粒体单倍群phylotree(version17).txt')

def parse_haplogroup_file(lines):
    haplogroups = []
    for line in lines:
        stripped_line = line.strip()
        if stripped_line:
            level = line.count('\t')
            haplogroups.append((level, stripped_line))
    return haplogroups

def find_correct_upstream_haplogroups(haplogroup_name, haplogroups):
    upstream_haplogroups = []
    current_haplogroup = None

    for level, haplogroup in reversed(haplogroups):
        if haplogroup == haplogroup_name:
            current_haplogroup = haplogroup
            current_level = level
            upstream_haplogroups.append((level, haplogroup))
        elif current_haplogroup and level &lt; current_level:
            upstream_haplogroups.insert(0, (level, haplogroup))
            current_level = level

    return upstream_haplogroups

def main():
    with open(HAPLOGROUP_FILE_PATH, 'r', encoding='utf-8') as file:
        refined_lines = file.readlines()

    haplogroups = parse_haplogroup_file(refined_lines)

    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as input_file:
        haplogroup_names = input_file.read().splitlines()

    not_found_haplogroups = []

    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as output_file, \
         open(REVERSED_OUTPUT_FILE_PATH, 'w', encoding='utf-8') as reversed_output_file:
        
        max_level = max(len(find_correct_upstream_haplogroups(name, haplogroups)) for name in haplogroup_names)
        header = [f"Level_{i}" for i in range(max_level)]
        output_file.write("\t".join(header) + "\n")
        reversed_header = [f"Level_{i}" for i in range(max_level-1, -1, -1)]
        reversed_output_file.write("\t".join(reversed_header) + "\n")

        for haplogroup_name in haplogroup_names:
            correct_upstream_haplogroups = find_correct_upstream_haplogroups(haplogroup_name, haplogroups)
            if not correct_upstream_haplogroups:
                not_found_haplogroups.append(haplogroup_name)
                continue

            haplogroup_list = [haplogroup for _, haplogroup in correct_upstream_haplogroups]
            result_line = "\t".join(haplogroup_list) + "\n"
            output_file.write(result_line)
            reversed_result_line = "\t".join(haplogroup_list[::-1]) + "\n"
            reversed_output_file.write(reversed_result_line)

    if not_found_haplogroups:
        with open(NOT_FOUND_FILE_PATH, 'w', encoding='utf-8') as not_found_file:
            for haplogroup_name in not_found_haplogroups:
                not_found_file.write(haplogroup_name + "\n")

if __name__ == "__main__":
    main()
复制<br><br>import os
from concurrent.futures import ProcessPoolExecutor

# 定义输入、输出文件的路径
BASE_PATH = r'C:/Users/victo/Desktop/'
INPUT_FILE_PATH = os.path.join(BASE_PATH, '新建 Text Document.txt')
OUTPUT_FILE_PATH = os.path.join(BASE_PATH, '正序等级.txt')
REVERSED_OUTPUT_FILE_PATH = os.path.join(BASE_PATH, '逆序等级.txt')
NOT_FOUND_FILE_PATH = os.path.join(BASE_PATH, '没有查询到请核实.txt')
HAPLOGROUP_FILE_PATH = os.path.join('F:/OneDrive/文档（科研）/脚本/我的科研脚本/Python/母系专用/线粒体单倍群phylotree(version17)2024年8月19日.txt')

def parse_haplogroup_file(lines):
    haplogroups = []
    for line in lines:
        stripped_line = line.strip()
        if stripped_line:
            level = line.count('\t')
            haplogroups.append((level, stripped_line))
    return haplogroups

def find_correct_upstream_haplogroups(haplogroup_name, haplogroups):
    upstream_haplogroups = []
    current_haplogroup = None

    for level, haplogroup in reversed(haplogroups):
        if haplogroup == haplogroup_name:
            current_haplogroup = haplogroup
            current_level = level
            upstream_haplogroups.append((level, haplogroup))
        elif current_haplogroup and level &lt; current_level:
            upstream_haplogroups.insert(0, (level, haplogroup))
            current_level = level

    return upstream_haplogroups

def process_haplogroup(haplogroup_name, haplogroups):
    correct_upstream_haplogroups = find_correct_upstream_haplogroups(haplogroup_name, haplogroups)
    if not correct_upstream_haplogroups:
        return haplogroup_name, None, None

    haplogroup_list = [haplogroup for _, haplogroup in correct_upstream_haplogroups]
    result_line = "\t".join(haplogroup_list)
    reversed_result_line = "\t".join(haplogroup_list[::-1])
    return haplogroup_name, result_line, reversed_result_line

# 将 haplogroups 作为参数传递
def process_wrapper(args):
    haplogroup_name, haplogroups = args
    return process_haplogroup(haplogroup_name, haplogroups)

def main():
    # 读取单倍群文件
    with open(HAPLOGROUP_FILE_PATH, 'r', encoding='utf-8') as file:
        refined_lines = file.readlines()

    haplogroups = parse_haplogroup_file(refined_lines)

    # 读取输入文件中的单倍群名称并去重
    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as input_file:
        haplogroup_names = input_file.read().splitlines()

    # 去重处理，只保留一个重复值
    unique_haplogroup_names = list(set(haplogroup_names))

    # 准备要传递给进程的参数 (haplogroup_name, haplogroups)
    args = [(name, haplogroups) for name in unique_haplogroup_names]

    # 用进程池并行处理每个单倍群名称
    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:
        results = list(executor.map(process_wrapper, args))

    # 构建一个哈希表，用于快速查找处理结果
    results_dict = {name: (result_line, reversed_result_line) for name, result_line, reversed_result_line in results}

    not_found_haplogroups = [name for name, result_line, _ in results if result_line is None]

    # 写入正序和逆序文件
    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as output_file, \
         open(REVERSED_OUTPUT_FILE_PATH, 'w', encoding='utf-8') as reversed_output_file:

        # 先获取最大层级数量
        max_level = max(len(find_correct_upstream_haplogroups(name, haplogroups)) for name, result_line, _ in results if result_line)

        # 写入表头
        header = [f"Level_{i}" for i in range(max_level)]
        output_file.write("\t".join(header) + "\n")
        reversed_header = [f"Level_{i}" for i in range(max_level-1, -1, -1)]
        reversed_output_file.write("\t".join(reversed_header) + "\n")

        # 遍历原始文件的每一行，按照去重前的顺序写入对应结果
        for haplogroup_name in haplogroup_names:
            if haplogroup_name in results_dict:
                result_line, reversed_result_line = results_dict[haplogroup_name]
                if result_line:
                    output_file.write(result_line + "\n")
                    reversed_output_file.write(reversed_result_line + "\n")
            else:
                not_found_haplogroups.append(haplogroup_name)

    # 写入没有查询到的单倍群
    if not_found_haplogroups:
        with open(NOT_FOUND_FILE_PATH, 'w', encoding='utf-8') as not_found_file:
            for haplogroup_name in set(not_found_haplogroups):
                not_found_file.write(haplogroup_name + "\n")

if __name__ == "__main__":
    main()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520171444.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>点击这里下载发育树文件： <a rel="noopener nofollow" class="external-link" href="https://vip.123pan.cn/1835545223/9626446" target="_blank">https://vip.123pan.cn/1835545223/9626446</a><br>
当然，你也可以自己按照这个要求进行制作：<a data-href="python：phylotree多余信息删除" href="软件\python\数据科学与格式转换\python：phylotree多余信息删除.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：phylotree多余信息删除</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。]]></description><link>软件\python\数据科学与格式转换\python：查找mtdna上游单倍型（2024年10月23日更新）.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/Python：查找mtDNA上游单倍型（2024年10月23日更新）.md</guid><pubDate>Wed, 23 Oct 2024 07:35:54 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[差异性单倍群]]></title><description><![CDATA[ 
 <br><br>不同的群体中，同一种单倍群可能有不同的频率。例如四川的汉族和苗族之间，M7单倍群的频率可能不同。这代表了群体的遗传结构的差异。<br><br>公式很简单：<br><br><br><br>需要准备2个文件：这2个文件可以从这里获得：<a data-href="python：绘制差异性单倍群的曼哈顿图" href="软件\python\数据科学与格式转换\python：绘制差异性单倍群的曼哈顿图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：绘制差异性单倍群的曼哈顿图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
<br>Category_totals.txt：每个群体的总数量。<br>
例如：
<br>Manchu	5356
Mongolian	3399
Kazak	494
复制<br>
<br>Haplogroup_counts.txt 每个单倍群在每个群体的数量。<br>
例如：
<br>    Manchu  Kazak   Mongolian
A   432 31  271
A1  166 11  131
A10 1   0   1
A11 15  0   6
A11b    2   0   0
A12a    0   0   2
复制<br><br>import pandas as pd
from scipy.stats import chi2_contingency, fisher_exact

# 读取数据
data = pd.read_csv('/Haplogroup_counts.txt', sep='\t')

# 从Category_totals.txt中读取总和数据
category_totals = pd.read_csv('/Category_totals.txt', sep='\t', header=None, index_col=0)
category_totals.columns = ['Total']

# 提供列名供用户选择
columns = data.columns[1:]  # 排除第一列（单倍群名称）

# 显示可选列名
print("可用的列名如下：")
for i, col in enumerate(columns, start=1):
    print(f"{i}: {col}")

# 交互式选择列
col1_index = int(input("请选择第一列的编号：")) - 1
col2_index = int(input("请选择第二列的编号：")) - 1

col1_name = columns[col1_index]
col2_name = columns[col2_index]

# 获取外部总和数据
total_col1 = category_totals.loc[col1_name, 'Total']
total_col2 = category_totals.loc[col2_name, 'Total']

# 计算每个单倍群在所选群体中的频率
data[f'{col1_name}_freq'] = data[col1_name] / total_col1
data[f'{col2_name}_freq'] = data[col2_name] / total_col2

# 计算频率差异
data['freq_diff'] = abs(data[f'{col1_name}_freq'] - data[f'{col2_name}_freq'])

# 找出频率差异最大的前十个单倍群
rank_number = int(input("请输入需要计算的前几名差异单倍群：（例如输入10）"))
top_10_diff = data.nlargest(rank_number, 'freq_diff')

# 进行卡方检验
top_10_diff['chi2_p_value'] = top_10_diff.apply(
    lambda row: chi2_contingency([[row[col1_name], total_col1 - row[col1_name]], 
                                  [row[col2_name], total_col2 - row[col2_name]]])[1],
    axis=1
)

# 进行Fisher精确检验
top_10_diff['fisher_p_value'] = top_10_diff.apply(
    lambda row: fisher_exact([[row[col1_name], total_col1 - row[col1_name]], 
                              [row[col2_name], total_col2 - row[col2_name]]])[1],
    axis=1
)

# 保存结果至桌面的TXT文件
output_path = '/haplogroup_difference_result.txt'
top_10_diff.to_csv(output_path, sep='\t', index=False)

print(f"结果已保存至 {output_path}")

复制<br><br>将其整理为三线表：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408241546067.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：差异性单倍群统计学检验.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：差异性单倍群统计学检验.md</guid><pubDate>Sat, 24 Aug 2024 07:46:29 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：多个fasta、txt合并为一个]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！
更好的办法建议使用 TBtools 软件！
<br>这个脚本是把多个 fasta 合并为一个  fasta。<br>from Bio import SeqIO
import os

def merge_fasta_files(input_dir, output_file):
    """
    Merge multiple FASTA files from a specified directory into a single FASTA file, remove duplicates using BioPython,
    and print the number of unique sequences in the new fasta file.

    :param input_dir: Directory containing the input FASTA files.
    :param output_file: Path to the output file where the merged and deduplicated FASTA will be saved.
    """
    unique_sequences = {}
    for filename in os.listdir(input_dir):
        if filename.endswith('.fasta') or filename.endswith('.fa'):
            file_path = os.path.join(input_dir, filename)
            # Use BioPython to parse the fasta files
            for record in SeqIO.parse(file_path, "fasta"):
                # Use sequence as key to avoid duplicates
                if record.seq not in unique_sequences:
                    unique_sequences[record.seq] = record.description

    # Write deduplicated sequences to the output file
    with open(output_file, 'w') as outfile:
        for seq, desc in unique_sequences.items():
            SeqIO.write(SeqIO.SeqRecord(seq, id=desc, description=""), outfile, "fasta")

    # Print the number of unique sequences saved
    print("Number of unique sequences in the merged file:", len(unique_sequences))

if __name__ == "__main__":
    input_directory = "C:/Users/a/Desktop"  # Replace with your input directory
    output_filename = "C:/Users/a/Desktop/10K_UNIQ"  # Enter your output file path
    merge_fasta_files(input_directory, output_filename)

复制<br>import os

# 定义文件夹路径
directory = '/home/luolintao/Haplogrep/outcome'

# 定义输出文件路径
output_file = '/home/luolintao/Haplogrep/merged_output.txt'

# 收集所有txt文件
files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.txt')]

# 合并文件
with open(output_file, 'w') as outfile:
    for i, file_path in enumerate(files):
        with open(file_path, 'r') as infile:
            if i == 0:
                # 对于第一个文件，保留标题行
                outfile.write(infile.read())
            else:
                # 从后续文件中跳过标题行
                next(infile)
                outfile.write(infile.read())

复制]]></description><link>软件\python\数据科学与格式转换\python：多个fasta、txt合并为一个.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：多个fasta、txt合并为一个.md</guid><pubDate>Fri, 23 Aug 2024 08:23:42 GMT</pubDate></item><item><title><![CDATA[输入文件]]></title><description><![CDATA[ 
 <br><br>你需要准备2个表格，第一个是mtDNA 单倍群的共享表格，第二个是Y 单倍群的共享表格，顺序不重要。<br>如何计算共享单倍群？
看这里：<a data-href="python：根据共享单倍群频率绘制人群柱状图" href="软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群柱状图.html" class="internal-link" target="_self" rel="noopener nofollow">python：根据共享单倍群频率绘制人群柱状图</a>，<a data-href="python：根据共享单倍群频率绘制人群热图" href="软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群热图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：根据共享单倍群频率绘制人群热图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。
<br>如图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410121115505.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap

# 设置plt.rcParams以确保输出为矢量图格式并且字体为 Arial
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42
plt.rcParams['font.family'] = 'Arial'  # 统一设置字体为Arial

# 第一步：加载第一个文件（用实际文件路径替换）
first_file_path = 'C:/Users/victo/Desktop/新建 Text Document.txt'
df = pd.read_csv(first_file_path, sep='\t', index_col=0)
df = df.apply(pd.to_numeric, errors='coerce')  # 将所有值转换为数值类型，无法转换的变为NaN

# 第二步：加载第二个文件（用实际文件路径替换）
second_file_path = 'C:/Users/victo/Desktop/新建 Text Document (2).txt'
second_df = pd.read_csv(second_file_path, sep='\t', index_col=0)
second_df = second_df.apply(pd.to_numeric, errors='coerce')

# 第三步：确保两个DataFrame（数据表）对齐，以避免索引和列错位
aligned_second_df = second_df.reindex(index=df.index, columns=df.columns)

# 第四步：创建上三角掩码和下三角掩码（包括对角线）
upper_tri_mask = np.triu(np.ones_like(df, dtype=bool))  # 上三角掩码
lower_tri_mask = np.tril(np.ones_like(aligned_second_df, dtype=bool))  # 下三角掩码

# 第五步：将两个矩阵合并
# - 使用第一个DataFrame的上三角（包括对角线）
# - 使用对齐后的第二个DataFrame的下三角（包括对角线）
combined_matrix_aligned = df.where(upper_tri_mask, aligned_second_df)

# 第六步：创建具有100种渐变颜色的自定义颜色映射
colors = ["#20364F", "#31646C", "#4E9280", "#96B89B", "#EEEFFF", "#ECD9CF", 
          "#D49C87", "#B86265", "#8B345E", "#50184E"]  # 基础颜色列表
n_colors = 100  # 颜色渐变的总数量

# 使用LinearSegmentedColormap生成颜色渐变映射
cmap = LinearSegmentedColormap.from_list("custom_cmap", colors, N=n_colors)

# 第七步：绘制组合后的热图，并调整纵横比和标签间距
plt.figure(figsize=(14, 12))  # 设置图形尺寸，使间距更大
ax = sns.heatmap(combined_matrix_aligned, cmap=cmap, annot=False, cbar=True, square=False)  # 绘制热图

# 将X轴移动到右侧
ax.xaxis.set_ticks_position('top')
ax.xaxis.set_label_position('top')

# 将Y轴保留在左侧
ax.yaxis.set_ticks_position('left')
ax.yaxis.set_label_position('left')

# 调整标签的间距和方向
plt.xticks(rotation=90, ha='left', fontsize=10)  # X轴标签
plt.yticks(rotation=0, fontsize=10)  # Y轴标签

# 设置图形标题和标签
plt.title('Sharing haplogroups of mtDNA and ChrY', fontsize=14)  # 设置图标题，字体大小14
plt.xlabel('', fontsize=12)  # X轴标签
plt.ylabel('', fontsize=12)  # Y轴标签
plt.tight_layout(pad=2.0)  # 调整布局，增加一些间距，防止标签重叠

# 显示图形
plt.show()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410121119530.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>为什么不使用聚类？
你猜。
]]></description><link>软件\python\数据科学与格式转换\python：父系母系共享单倍群.py.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：父系母系共享单倍群.py.md</guid><pubDate>Sat, 12 Oct 2024 03:19:11 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br>你可以在这里找到具体的参数设置向导： <a rel="noopener nofollow" class="external-link" href="https://scikit-learn.org.cn/view" target="_blank">https://scikit-learn.org.cn/view</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404252118650.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
频率代表每个群体的不同单倍群占这个群体中的比例。<br><br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding
import plotly.express as px

# 加载数据
file_path = 'C:/Users/victo/Desktop/PCA.csv'
data = pd.read_csv(file_path)
pca_data = data.drop(columns=data.columns[0])

# 执行PCA
pca = PCA(n_components=2)
pca_results = pca.fit_transform(pca_data.T)  # T表示转置
explained_variance_ratio = pca.explained_variance_ratio_

with open('C:/Users/victo/Desktop/PCA降维结果解释度.txt', 'w') as f:
    # 将解释度写入文件
    f.write('Explained Variance Ratio:\n')
    for i, ratio in enumerate(explained_variance_ratio):
        f.write(f'Component {i+1}: {ratio:.4f}\n')
        f.write('\n')

# 生成PCA结果的txt文件
pca_results_df = pd.DataFrame({
    'Sample Name': pca_data.columns,
    'PC1': pca_results[:, 0],
    'PC2': pca_results[:, 1]
})
pca_results_df.to_csv('C:/Users/victo/Desktop/PCA结果.txt', sep='\t', index=False)

# 执行t-SNE
tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)
tsne_results = tsne.fit_transform(pca_data.T)

# 执行Isomap
isomap = Isomap(n_components=2, n_neighbors=5)
isomap_results = isomap.fit_transform(pca_data.T)

# 执行LLE
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, method='standard')
lle_results = lle.fit_transform(pca_data.T)

# 准备结果数据
results_df = pd.DataFrame({
    'PCA1': pca_results[:, 0],
    'PCA2': pca_results[:, 1],
    't-SNE1': tsne_results[:, 0],
    't-SNE2': tsne_results[:, 1],
    'Isomap1': isomap_results[:, 0],
    'Isomap2': isomap_results[:, 1],
    'LLE1': lle_results[:, 0],
    'LLE2': lle_results[:, 1],
    'Ethnic Group': pca_data.columns
})

# 静态可视化
# plt.figure(figsize=(24, 12))
# for i, (label, comp1, comp2) in enumerate(zip(['PCA', 't-SNE', 'Isomap', 'LLE'], 
#                                               ['PCA1', 't-SNE1', 'Isomap1', 'LLE1'], 
#                                               ['PCA2', 't-SNE2', 'Isomap2', 'LLE2'])):
#     plt.subplot(2, 2, i+1)
#     sns.scatterplot(x=comp1, y=comp2, hue='Ethnic Group', data=results_df, palette='viridis')
#     plt.title(f'{label} Visualization of Ethnic Groups')
#     plt.xlabel(f'{label} 1')
#     plt.ylabel(f'{label} 2')
#     plt.legend([],[], frameon=False)  # 隐藏图例以节省空间

# plt.tight_layout()
# plt.show()

# Plotly可视化
def plot_with_plotly(data):
    fig = px.scatter_matrix(
        data,
        dimensions=['PCA1', 't-SNE1', 'Isomap1', 'LLE1', 'PCA2', 't-SNE2', 'Isomap2', 'LLE2'],
        color='Ethnic Group',
        labels={col: col for col in data.columns},
        width=3000,  # 调整宽度
        height=2500  # 调整高度
    )
    fig.update_layout(
        legend=dict(orientation='h', y=1.02, x=1, xanchor='right', yanchor='bottom')
    )
    fig.show()

plot_with_plotly(results_df)

复制]]></description><link>软件\python\数据科学与格式转换\python：根据单倍群频率使用sklearn进行降维大礼包.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：根据单倍群频率使用sklearn进行降维大礼包.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[原理]]></title><description><![CDATA[ 
 <br>文献出处是 <a data-href="早期南岛人：进出台湾" href="文献及报道\文献\2023年阅读\早期南岛人：进出台湾.html" class="internal-link" target="_self" rel="noopener nofollow">早期南岛人：进出台湾</a>。可以根据单倍群频率分布来计算不同群体之间的亲缘关系，类似于 <a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a>。<br><br>我们通过识别共有单倍群的方法来比较不同人群的遗传关系。我们通过成对矩阵的方式，把成对人群共享单倍群的最小频率数值视为该两个人群的共享单倍群值。而这两个人群非共享单倍群的值，即某一单倍群在成对比较中该人群有但是另个一人群没有的单倍群，视为独有单倍群频率。每个人群共享、非共享单倍群的值都除以总计数，从而归一化数据。据此得到的共享单倍群数值来衡量人群之间的基因相似性的程度。<br><br>以 csv 文件准备每个 群体 中的单倍型的分布频率。<br>,Ami,Atayal,Bunun,Paiwan,Puyuma,Rukai,Saisiat,Tsou
A5b1,10,4,2,2,2.6,4,4.2,8.3
B4a1a,0,0,0,8,0.0,0,0.0,0.0
B4a1a3,12,0,0,24,0.0,4,0.0,0.0
B4a2a,0,0,0,0,0.0,0,0.0,0.0
B4b1a2,8,2,34,8,0.0,8,16.7,10.4
B5a2a,2,0,12,0,0.0,0,0.0,2.1
C7a,0,0,0,4,0.0,0,0.0,0.0
D4a,0,0,0,0,0.0,0,0.0,2.1
D4i,8,2,2,12,2.6,2,12.5,4.2
D5a2,0,0,0,0,0.0,0,0.0,0.0
D5b,0,0,0,0,0.0,48,0.0,0.0
D5b1c1,0,4,0,0,2.6,0,0.0,0.0
D6a2,10,0,4,0,5.1,0,0.0,0.0
E1a,0,2,0,0,0.0,0,0.0,0.0
E1a1,4,6,0,0,0.0,0,0.0,0.0
E1a1a,0,0,0,0,0.0,0,0.0,0.0
E1a1a2,2,8,2,2,20.5,2,4.2,2.1
E2b1,0,0,0,0,0,0,0.0,0.0
F1a1d,0,0,4,2,7.7,0,0.0,8.3
F1a3,0,0,4,0,17.9,2,0.0,8.3
F1a3a,0,0,0,0,0.0,0,0.0,0.0
F1a4a1,2,22,2,2,28.2,20,12.5,10.4
F2,2,0,8,0,0.0,0,0.0,4.2
F3b1,0,0,0,12,0.0,0,0.0,0.0
F4b,2,0,18,0,0.0,0,0.0,2.1
M7b3a,4,40,4,10,2.6,6,37.5,10.4
M7c'e'f,0,0,0,0,0.0,0,0.0,0.0
M7c1,8,0,0,0,0.0,0,0.0,0.0
M7c3a,0,0,0,0,0.0,0,0.0,0.0
M7c3b,2,6,2,2,7.7,0,8.3,4.2
M7c3c,0,0,0,12,0.0,0,0.0,0.0
M8a2,2,0,0,0,0.0,0,0.0,0.0
R9c1a,0,4,2,0,2.6,4,4.2,22.9
复制<br><br># 导入所需的库
# 导入所需的库
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 载入数据，数据为CSV格式，编码为UTF-8
file_path = '人群单倍群频率分布.csv' # 数据文件路径
save_path = '频率表归一化.csv' # 保存文件路径
data = pd.read_csv(file_path, encoding='utf-8') 

# 将单倍群列设置为DataFrame的索引
data.set_index('Unnamed: 0', inplace=True)

# 在这里进行归一化处理，使得每一列的总和不是1而是100
data = data * 100 / data.sum()

# 定义计算共享单倍群矩阵的函数
def calculate_shared_haplogroups_matrix(haplogroups_data):
    # 获取人群数量
    n_populations = haplogroups_data.shape[1]
    # 初始化共享单倍群矩阵，初始值为0
    shared_haplogroups_matrix = np.zeros((n_populations, n_populations))
    
    # 获取人群名称列表
    populations = haplogroups_data.columns
    for i in range(n_populations):
        for j in range(i + 1, n_populations):
            # 计算两个人群之间共享单倍群的最小频率值
            shared_haplogroups = np.minimum(haplogroups_data[populations[i]], haplogroups_data[populations[j]])
            # 注意这里我们不需要再次归一化shared_haplogroups的值，因为它已经根据前面的要求进行了调整
            shared_value = np.sum(shared_haplogroups)
            shared_haplogroups_matrix[i, j] = shared_value
            shared_haplogroups_matrix[j, i] = shared_value
    
    # 将对角线的值设置为100，因为每个人群与自己的共享单倍群值是最大的，并且我们想让它反映出100倍的增加
    np.fill_diagonal(shared_haplogroups_matrix, 100)
    
    # 返回一个新的DataFrame，其索引和列都是人群名称
    return pd.DataFrame(shared_haplogroups_matrix, index=populations, columns=populations)

# 使用函数计算共享单倍群矩阵
shared_matrix = calculate_shared_haplogroups_matrix(data)

# 输出计算得到的共享单倍群矩阵
shared_matrix.to_csv(save_path, encoding='utf-8')
print('共享单倍群矩阵已经保存')

# 读取保存的文件并绘制热图
df = pd.read_csv(save_path, encoding='utf-8', index_col=0)

sns.heatmap(df, cmap='YlGnBu')
plt.show()


复制<br>根据上述的代码可以得到矩阵：<br>,Ami,Atayal,Bunun,Paiwan,Puyuma,Rukai,Saisiat,Tsou
Ami,1.0,0.12359550561797752,0.1797752808988764,0.2247191011235955,0.10612015721504772,0.14606741573033707,0.16956765861875348,0.2050561797752809
Atayal,0.12359550561797752,1.0,0.09,0.11,0.24187906046976512,0.2,0.3608195902048975,0.1955
Bunun,0.1797752808988764,0.09,1.0,0.12,0.13293353323338333,0.12,0.1634182908545727,0.214
Paiwan,0.2247191011235955,0.11,0.12,1.0,0.07596201899050474,0.13,0.18990504747626186,0.161
Puyuma,0.10612015721504772,0.24187906046976512,0.13293353323338333,0.07596201899050474,1.0,0.16891554222888555,0.17382617382617385,0.21539230384807598
Rukai,0.14606741573033707,0.2,0.12,0.13,0.16891554222888555,1.0,0.1924037981009495,0.192
Saisiat,0.16956765861875348,0.3608195902048975,0.1634182908545727,0.18990504747626186,0.17382617382617385,0.1924037981009495,1.0,0.25037481259370314
Tsou,0.2050561797752809,0.1955,0.214,0.161,0.21539230384807598,0.192,0.25037481259370314,1.0
复制<br><br>随便找个网站或者 R 代码也可以可视化。具体可看 <a data-href="R：Fst绘制频率热图矩阵" href="软件\r语言语法\r：fst绘制频率热图矩阵.html" class="internal-link" target="_self" rel="noopener nofollow">R：Fst绘制频率热图矩阵</a>。<br>
<img alt="Figure_1.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403262141478.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
Figure X. Pairwise matrix showing the proportion of shared haplogroups between different populations. The matrix is calculated by identifying the minimum count of shared haplogroups between each pair of populations. Each cell in the matrix represents the amount of haplogroups shared between the two populations indicated by the corresponding row and column. Diagonal elements (self-comparisons) are excluded or set to zero. Higher values in the matrix indicate greater genetic similarity in terms of shared haplogroups, while lower values indicate fewer haplogroups in common. Non-shared haplogroups, specific to one population, are not represented in this matrix. The color gradient (or numerical values) illustrates the degree of sharing across populations.
<br><br>通过观察矩阵中数值的整体分布，可以识别出共享单倍群的普遍趋势。数值较高的区域意味着这些人群之间有更多的遗传相似性，反之则说明遗传差异较大。<br>一个需要注意的问题是共享单倍群的计算基于单倍群，而非序列。这在很大程度上受限于对单倍群的定义。
不建议生成聚类树（结果难以解释）。<br>
建议将共享单倍群矩阵和柱状图联合起来进行解释和解读。重点在于柱状图。<a data-href="python：根据共享单倍群频率绘制人群柱状图" href="软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群柱状图.html" class="internal-link" target="_self" rel="noopener nofollow">python：根据共享单倍群频率绘制人群柱状图</a>。
]]></description><link>软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群热图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：根据共享单倍群频率绘制人群热图.md</guid><pubDate>Tue, 15 Oct 2024 02:37:34 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[方法一（简单粗暴）]]></title><description><![CDATA[ 
 <br><br>在进行这个教程之前请先完成这个： <a data-href="python：根据共享单倍群频率绘制人群热图" href="软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群热图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：根据共享单倍群频率绘制人群热图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。得到的数据文件是我们这一步需要的输入文件。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616143341.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
不明白的可以看<a data-href="python：根据共享单倍群频率绘制人群热图" href="软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群热图.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：根据共享单倍群频率绘制人群热图</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。<br><br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# Step 1: Load the CSV file
file_path = '共享.csv'  # Replace with your actual file path
data = pd.read_csv(file_path)

# Step 2: Remove the diagonal elements by setting them to zero
data_without_self = data.copy()
for index in data.index:
    group = data.loc[index, 'Unnamed: 0']
    data_without_self.loc[index, group] = 0

# Step 3: Normalize the data for each group
normalized_data = data_without_self.copy()
categories = data.columns[1:]  # Define the categories
for index in normalized_data.index:
    total = normalized_data.loc[index, categories].sum()
    for category in categories:
        normalized_data.loc[index, category] = normalized_data.loc[index, category] / total

# Step 4: Define the color palette
base_colors = ['#EA1F1F', '#E88421', '#E5C923', '#FFF924', '#9DEF1B', '#42D726', '#449657', '#4CCCB3', '#369BA8', '#2B7EBC', '#3626D1', '#A128CE', '#999999']
num_categories = len(categories)

if num_categories &gt; len(base_colors):
    cmap = plt.get_cmap('tab20', num_categories)  # Use 'tab20' colormap to generate more colors
    colors = [cmap(i) for i in range(num_categories)]
    colors = [f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}' for r, g, b, _ in colors]  # Convert to hex
else:
    colors = base_colors

# Step 5: Plot the normalized stacked bar chart with specified colors using Matplotlib
fig, ax = plt.subplots(figsize=(10, 6))
bottom = np.zeros(len(normalized_data))
for i, category in enumerate(categories):
    ax.bar(normalized_data['Unnamed: 0'], normalized_data[category], bottom=bottom, label=category, color=colors[i % len(colors)])
    bottom += normalized_data[category].values
ax.set_xlabel('Group')
ax.set_ylabel('Proportion')
ax.set_title('Normalized Comparison of Group Composition')
ax.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')
ax.grid(False)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Step 6: Plot the normalized stacked bar chart with specified colors using Plotly
# fig = go.Figure()

# for i, category in enumerate(categories):
#     fig.add_trace(go.Bar(
#         x=normalized_data['Unnamed: 0'],
#         y=normalized_data[category],
#         name=category,
#         marker_color=colors[i % len(colors)]
#     ))

# fig.update_layout(
#     barmode='stack',
#     xaxis_title='Group',
#     yaxis_title='Proportion',
#     title='Normalized Comparison of Group Composition',
#     legend_title='Group'
# )

# fig.show()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616143556.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>通过 NETWORK 分析（例如<a data-href="中值连接网络（Median-Joining Network，MJN）" href="术语\中值连接网络（median-joining-network，mjn）.html" class="internal-link" target="_self" rel="noopener nofollow">中值连接网络（Median-Joining Network，MJN）</a> ），基于西藏个体在网络中的位置，对西藏样本中的所有单倍型进行了分类。设定了一个标准：如果某一单倍型的西藏个体位于独立的分支上、形成一个聚类（cluster），并且这个聚类中的西藏个体占到75%以上，则该单倍型被视为“西藏相关单倍型”。这种单倍型代表了一些和西藏人群有特定联系的遗传特征<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>。<br>
这就很头痛，不能用一次性的代码进行解决。所以，只能自己手算。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410151040667.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Haplogroup sharing analysis among ancient Tibet groups, Nepal and Pakistan.
<br><br><br>
<br>
<br>Maternal genetic history of ancient Tibetans over the past 4,000&nbsp;years<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\python\数据科学与格式转换\python：根据共享单倍群频率绘制人群柱状图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：根据共享单倍群频率绘制人群柱状图.md</guid><pubDate>Tue, 15 Oct 2024 02:41:07 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><a data-href="2019 MBE 河流流域塑造了汉族的母系遗传景观" href="文献及报道\文献\2024年阅读\1-6月\2019-mbe-河流流域塑造了汉族的母系遗传景观.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="😀" aria-label="😀" data-icon="😀" aria-hidden="true" style="transform: translateY(0px);"></span>2019 MBE 河流流域塑造了汉族的母系遗传景观</a><img class="emoji" draggable="false" alt="😀" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f600.svg" height="18px" style="max-width: 100%;">文献提到的单倍群频率分布并未直接使用数据本身，而是经过了 z-score 转化。如图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408131741293.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408131742224.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
from scipy.stats import zscore

# Load the data from the file
file_path = '新建 Text Document.txt'
data = pd.read_csv(file_path, sep='\t', index_col=0)

# Transpose the matrix
data_transposed = data.transpose()

# Calculate the z-score for each column
z_scores = data_transposed.apply(zscore, axis=0)
z_scores_transposed = z_scores.transpose()
z_scores_transposed.index.name = 'population'
# Define the output path
output_path = 'Z-score_Data.txt'

# Save the z-score data to a text file
z_scores_transposed.to_csv(output_path, sep='\t')

print(f"Z-scores saved to {output_path}")
复制<br><br>得到 Z-score_Data.txt 之后，可以使用 <a data-href="R：Fst绘制频率热图矩阵" href="软件\r语言语法\r：fst绘制频率热图矩阵.html" class="internal-link" target="_self" rel="noopener nofollow">R：Fst绘制频率热图矩阵</a>进行可视化。效果基本如文献中所示。]]></description><link>软件\python\数据科学与格式转换\python：根据频数求z-score.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：根据频数求z-score.md</guid><pubDate>Sun, 29 Sep 2024 09:00:37 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f62e-200d-1f4a8.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f62e-200d-1f4a8.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a>是一个衡量种群间遗传差异的指标。
具体可看<a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a><br>
我们可以通过人群的序列来计算 Fst，之前的操作可以看 <a data-href="python：将FASTA转换成Arp" href="软件\python\数据科学与格式转换\python：将fasta转换成arp.html" class="internal-link" target="_self" rel="noopener nofollow">python：将FASTA转换成Arp</a>
<br>当我们缺少序列信息，只有单倍群信息的时候也可以计算 Fst，具体操作如下。<br><br>准备一个 txt 文件，第一列是 单倍群的名称，第二列是 样本的名称。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410232037017.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>注意，标题必须是 haplogroup 和 name！<br>haplogroup	name
A	Kazak_Xinjiang
A	Kazak_Xinjiang
A	Kazak_Xinjiang
A	Kazak_Xinjiang
A	Kazak_Xinjiang
A	Manchu_Hebei
A	Manchu_Beijing
复制<br><br>import pandas as pd

# 载入数据
file_path = '新建 Text Document.txt'
output_arp_file_path = 'mtDNA.arp'
data = pd.read_csv(file_path, delimiter='\t')

# Step 1: 为单倍群创建字典
unique_haplogroups = data['haplogroup'].unique()
hapl_dict = {haplogroup: idx + 1 for idx, haplogroup in enumerate(unique_haplogroups)}

# Step 2: 计算单倍群在人群中的出现频率
grouped_data = data.groupby(['name', 'haplogroup']).size().reset_index(name='count')

# Step 3: 生成样本频率文件
samples_file_content = ""
for group_name in grouped_data['name'].unique():
    group_data = grouped_data[grouped_data['name'] == group_name]
    sample_size = group_data['count'].sum()
    samples_block = f"\n\n\tSampleName=\"{group_name}\"\n\tSampleSize={sample_size}\n\tSampleData= {{\n"
    
    for haplogroup, idx in hapl_dict.items():
        count = group_data[group_data['haplogroup'] == haplogroup]['count'].sum()
        frequency = count / sample_size if sample_size &gt; 0 else 0
        samples_block += f"\t\t{idx}\t{frequency:.6f}\n"
    
    samples_block += "\t}\n"
    samples_file_content += samples_block

# Step 4: 生成ARP文件头部内容
nb_samples = len(grouped_data['name'].unique())
profile_content = f"""[Profile]

    Title="The population fixation index(Fst) of mtDNA"

    NbSamples={nb_samples}
    DataType=FREQUENCY
    GenotypicData=0
    LocusSeparator=' '
    MissingData="? "
    Frequency= REL
"""

# Step 5: 生成单倍群定义部分
hapl_list_content = "\n".join([f"{idx}\t{haplogroup}" for haplogroup, idx in hapl_dict.items()])

# Step 6: 合并所有内容为最终的mtDNA.arp文件
combined_content = f'''{profile_content}\n
[Data]\n
[HaplotypeDefinition]\n
\tHaplList={{\n{hapl_list_content}}}\n
[Samples]\n{samples_file_content}'''

# Step 7: 写入最终的ARP文件
with open(output_arp_file_path, 'w') as example_arp_file:
    example_arp_file.write(combined_content)
复制<br><br>输出的是一个 arp 文件，内容大致如下所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410232040636.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>我们使用 Arlequin 软件：<br>
其中距离选项可以勾选也可以不勾选，勾选了会额外计算，不勾选能节约时间。<br>
permutation 次数一般默认即可。显著性水平一般认为0.05为阈值。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410232041633.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：根据人群单倍群频率生成arp文件计算fst.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/Python：根据人群单倍群频率生成arp文件计算Fst.md</guid><pubDate>Wed, 23 Oct 2024 12:45:44 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410232037017.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410232037017.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>饼图反应的是一个整体内部各成分的比例。<br><br>第一列是成分，第二列是类别。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407042146225.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib import rcParams

# Load the dataset
file_path = '10K数据集.csv'
data = pd.read_csv(file_path)

# Filter the data for Macro == 'M'
filtered_data = data[data['Macro'] == 'N']

# Count the occurrences of each Haplogroup
haplogroup_counts = filtered_data['Haplogroup'].value_counts()

# Get the top 50 Haplogroups
top_50_haplogroups = haplogroup_counts.head(50)

# Sum the counts of the remaining Haplogroups
other_count = haplogroup_counts[50:].sum()

# Prepare data for pie chart
pie_data = pd.concat([top_50_haplogroups, pd.Series({'Other': other_count})])

# Calculate the total number of Haplogroup 'M'
total_m_count = filtered_data.shape[0]

# Create a natural gradient of colors between the specified SCI colors
def create_color_gradient(start_color, mid_color1, mid_color2, end_color, n):
    gradient_colors = np.linspace(mcolors.hex2color(start_color), mcolors.hex2color(mid_color1), n//3)
    gradient_colors = np.vstack((gradient_colors, np.linspace(mcolors.hex2color(mid_color1), mcolors.hex2color(mid_color2), n//3)))
    gradient_colors = np.vstack((gradient_colors, np.linspace(mcolors.hex2color(mid_color2), mcolors.hex2color(end_color), n - 2*(n//3))))
    return [mcolors.rgb2hex(c) for c in gradient_colors]

# Generate the gradient color palette
# gradient_colors = create_color_gradient('#440054', '#414F8C', '#2C8D8E', '#FDE724', len(pie_data))
gradient_colors = create_color_gradient('#090387','#9611A1','#DD5E66','#FDE724', len(pie_data))
# Set font to vector font
rcParams['pdf.fonttype'] = 42
rcParams['ps.fonttype'] = 42

# Plotting the pie chart with the gradient colors
fig, ax = plt.subplots(figsize=(12, 8))
wedges, texts = ax.pie(pie_data, labels=None, colors=gradient_colors, startangle=140)

# Adding legend
ax.legend(pie_data.index, title="Haplogroups", loc="center left", bbox_to_anchor=(1, 0.5))

plt.title(f'Top 50 Haplogroups in Macro M ({total_m_count})')

# Save the plot as an SVG file with editable text
plt.savefig('C:/Users/victo/Desktop/haplogroup_pie_chart.svg', format='svg')

plt.show()

复制<br><br>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib import rcParams

# Load the dataset
file_path = '10K数据集.csv'
data = pd.read_csv(file_path)

# Filter the data for Macro == 'M'
filtered_data = data[data['Macro'] == 'M']

# Count the occurrences of each Haplogroup
haplogroup_counts = filtered_data['Haplogroup'].value_counts()

# Get the top 50 Haplogroups
top_50_haplogroups = haplogroup_counts.head(50)

# Remove 'Other' category
bar_data = top_50_haplogroups

# Calculate the total number of Haplogroup 'M'
total_m_count = filtered_data.shape[0]

# Create a natural gradient of colors between the specified SCI colors
def create_color_gradient(start_color, mid_color1, mid_color2, end_color, n):
    gradient_colors = np.linspace(mcolors.hex2color(start_color), mcolors.hex2color(mid_color1), n//3)
    gradient_colors = np.vstack((gradient_colors, np.linspace(mcolors.hex2color(mid_color1), mcolors.hex2color(mid_color2), n//3)))
    gradient_colors = np.vstack((gradient_colors, np.linspace(mcolors.hex2color(mid_color2), mcolors.hex2color(end_color), n - 2*(n//3))))
    return [mcolors.rgb2hex(c) for c in gradient_colors]

# Generate the gradient color palette
gradient_colors = create_color_gradient('#440054', '#414F8C', '#2C8D8E', '#FDE724', len(bar_data))

# Set font to vector font
rcParams['pdf.fonttype'] = 42
rcParams['ps.fonttype'] = 42

# Plotting the bar chart with the gradient colors
plt.figure(figsize=(14, 7))
bars = plt.bar(bar_data.index, bar_data.values, color=gradient_colors)

# Adding title and labels
plt.xlabel('Haplogroup')
plt.ylabel('Count')
plt.title(f'Top 50 Haplogroups in Macro M ({total_m_count})')
plt.xticks(rotation=90)

# Save the plot as an SVG file with editable text
plt.tight_layout()
plt.savefig('C:/Users/victo/Desktop/haplogroup_bar_chart.svg', format='svg')

plt.show()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407042147048.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407042205464.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：绘制饼图和柱状图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：绘制饼图和柱状图.md</guid><pubDate>Thu, 04 Jul 2024 14:05:24 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407042146225.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407042146225.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备工作]]></title><description><![CDATA[ 
 <br>这个代码针对的是 mtDNA，Y 染色体可以去看：[这里](<a rel="noopener nofollow" class="external-link" href="https://www.yuque.com/yuqueyonghuyzhkuy/fi68oz/prgpziv5gkdy2ggq?singleDoc#" target="_blank">https://www.yuque.com/yuqueyonghuyzhkuy/fi68oz/prgpziv5gkdy2ggq?singleDoc#</a> 《从单倍群频率一键出曼哈顿图》)<br><br>首先，你需要用到这个代码：<a data-href="Python：查找mtDNA上游单倍型（2024年10月23日更新）" href="软件\python\数据科学与格式转换\python：查找mtdna上游单倍型（2024年10月23日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Python：查找mtDNA上游单倍型（2024年10月23日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
然后，你需要把上个代码的输出文件（逆序版本）与自己的文件合并形成下列的列：<br>ID	Population	Province	Location_Classfication	Language	Level_18	Level_17	Level_16	Level_15	Level_14	Level_13	Level_12	Level_11	Level_10	Level_9	Level_8	Level_7	Level_6	Level_5	Level_4	Level_3	Level_2	Level_1	Level_0
复制<br>说明一下
并不是说每一列都需要，但是至少 ID 列和单倍群列是必须的。另外，由于你需要进行差异性的统计，因此至少还应该有一列（例如 Province 列）作为差异的计算。使用上述代码的生成的 level 要全部贴过来。注意不要错位。
<br><br>我其实不太想写这玩意儿，这里就我一个人做 mtDNA 的。<br>import pandas as pd
from collections import defaultdict
import matplotlib.pyplot as plt
from adjustText import adjust_text
import os

# 输入文件
data = pd.read_csv('C:/Users/victo/Desktop/新建 Text Document.txt', sep='\t')

# 输出文件路径
outputfile_path = "C:/Users/victo/Desktop"

# 定义函数，找到最下游的单倍群
def find_most_downstream_level(data):
    # 找到最下游的单倍群列
    level_columns = [col for col in data.columns if col.startswith('Level_')]
    level_numbers = [int(col.split('_')[1]) for col in level_columns]
    most_downstream_level = f"Level_{max(level_numbers)}"
    return most_downstream_level

# 定义函数，计算每个类别的总数
def calculate_category_totals(data, column_name):
    # 去除缺失值再计算每个类别的总数
    category_totals = data[column_name].dropna().value_counts().to_dict()
    return category_totals

# 定义函数，计算单倍群的频率
def calculate_haplogroup_frequencies(data, column_name, category_totals):
    # 动态确定最下游的单倍群
    most_downstream_level = find_most_downstream_level(data)
    
    # 清除列名中的空白并检查指定的列是否存在
    data.columns = data.columns.str.strip()
    column_name = column_name.strip()
    if (column_name not in data.columns):
        return f"列名 '{column_name}' 不存在于数据中，请检查输入。"

    # 按最下游的单倍群列和指定列进行分组，然后计算出现次数
    distribution = data.groupby([most_downstream_level, column_name]).size().unstack(fill_value=0).reset_index()

    # 获取指定列中的实际类别
    valid_categories = data[column_name].dropna().unique()  # 去除缺失值

    # 初始化一个字典来存储下游单倍群和单倍群本身
    haplogroup_totals = defaultdict(lambda: defaultdict(int))

    # 计算单倍群和下游单倍群的数量
    for haplogroup in distribution[most_downstream_level]:
        for idx, group in distribution.iterrows():
            if group[most_downstream_level].startswith(haplogroup):
                for category in valid_categories:
                    if category in group:  # 检查类别是否在行中
                        haplogroup_totals[haplogroup][category] += group.get(category, 0)

    # 转为Pandas DataFrame并保存为文件
    total_distribution = pd.DataFrame(haplogroup_totals).T.reset_index()
    total_distribution.columns = [most_downstream_level] + valid_categories.tolist()

    # 输出到桌面的txt文件
    total_distribution.to_csv(f"{outputfile_path}/Haplogroup_counts.txt", sep="\t", encoding="utf-8", index=False)

    # 计算频率
    frequencies = total_distribution.copy()
    for category in valid_categories:
        total_count = category_totals.get(category, 0)  # 使用预先计算的类别总数，并对不存在的类别返回0
        if total_count &gt; 0:
            frequencies[category] = frequencies[category] / total_count
    return frequencies

def calculate_frequency_difference(frequency_result):
    # 询问用户输入两个列名以计算差值
    column1 = input("请输入第一个要计算差值的列名：").strip()
    column2 = input("请输入第二个要计算差值的列名：").strip()

    # 检查列是否存在于频率结果中
    if column1 not in frequency_result.columns or column2 not in frequency_result.columns:
        return f"输入的列名 '{column1}' 或 '{column2}' 不存在于频率结果中，请检查输入。"

    # 计算两个列之间的差值
    frequency_result['Difference'] = frequency_result[column1] - frequency_result[column2]
    return frequency_result, column1, column2

# 定义曼哈顿图的绘制
def plot_manhattan(frequency_result, column1, column2):
    # 确保生成的字体为矢量可编辑
    plt.rcParams['pdf.fonttype'] = 42
    plt.rcParams['ps.fonttype'] = 42

    # 提取绘图所需数据
    haplogroups = frequency_result.columns[0]
    differences = frequency_result['Difference']

    # 设定颜色和形状
    colors = ['#bb2649','#f35d74','#E88421','#E5C923','#ded82d','#8FBC8F', '#37945F',
              '#c6ffe6','#4CCCB3', '#369BA8','#2B7EBC','#d4eaf7','#003f8f','#ffc7ff','#A128CE','#6c35de','#999999','#1F1F1F' ]
    
    # 设置阈值
    thresholds = [-0.01, 0.01]
    print(f"阈值已经被设定为{thresholds}")
    plt.figure(figsize=(15, 5))

    # 为首字母分配颜色
    letter_to_color = {}
    current_color_idx = 0

    # 计算x轴位置
    x_positions = []
    for haplogroup in frequency_result[haplogroups]:
        first_letter = haplogroup[0]
        if (first_letter not in letter_to_color):
            letter_to_color[first_letter] = colors[current_color_idx % len(colors)]
            current_color_idx += 1
        x_positions.append(first_letter)

    unique_letters = list(letter_to_color.keys())
    x_ticks = [x_positions.index(letter) for letter in unique_letters]

    # 绘制单倍群的差值散点图
    texts = []
    for i, haplogroup in enumerate(frequency_result[haplogroups]):
        color = letter_to_color[haplogroup[0]]
        
        # 只绘制差异不为零的散点
        if differences[i] != 0:
            plt.scatter(i, differences[i], color=color, s=50, marker='o')
        
            # 标记超出阈值的点并附上全名标签
            if differences[i] &lt; thresholds[0] or differences[i] &gt; thresholds[1]:
                text = plt.text(i, differences[i], haplogroup, fontsize=8, ha='right')
                texts.append(text)
                # 画线连接点和文本标签
                plt.plot([i, i], [differences[i], differences[i]], 'k-', lw=0.5, color='gray')

    # 添加阈值线
    plt.axhline(y=thresholds[0], color='#1F1F1F', linestyle='--')
    plt.axhline(y=thresholds[1], color='#1F1F1F', linestyle='--')

    # 调整文本避免重叠
    adjust_text(texts, 
                arrowprops=dict(arrowstyle='-&gt;', color='#1F1F1F', lw=0.5, shrinkA=5),
                expand_text=(1.5, 1.5),
                expand_objects=(1.5, 1.5),
                force_text=(0.5, 0.5),
                force_objects=(0.5, 0.5),
                only_move={'points':'y', 'text':'xy'})

    # 设置x轴只显示单倍群的首字母
    plt.xticks(ticks=x_ticks, labels=unique_letters, rotation=0)
    plt.xlabel('Haplogroup')
    plt.ylabel('Frequency Difference')
    plt.title(f"Frequency Difference: {column1} - {column2}")
    plt.tight_layout()
    plt.show()

# 询问用户输入要进行分类计算的列名
column_name = input("请输入要分类计算统计的列名（例如 'Province'）：").strip()

# 计算每个类别的总数
category_totals = calculate_category_totals(data, column_name)

# 将类别总数写入一个新的txt文件
with open(f"{outputfile_path}/Category_totals.txt", 'w', encoding='utf-8') as f:
    for category, total in category_totals.items():
        f.write(f"{category}\t{total}\n")

# 计算单倍群的频率，包括所有下游单倍群
frequency_result = calculate_haplogroup_frequencies(data, column_name, category_totals)

# 显示结果并询问要计算差异的列名
if isinstance(frequency_result, pd.DataFrame):
    print(frequency_result)
    frequency_result.to_csv(f"{outputfile_path}/Haplogroup_frequency.txt", sep="\t", encoding="utf-8")
    
    # 根据用户输入计算频率差异
    frequency_difference_result, column1, column2 = calculate_frequency_difference(frequency_result)
    
    # 显示并保存频率差异结果
    if isinstance(frequency_difference_result, pd.DataFrame):
        print(frequency_difference_result)
        frequency_difference_result.to_csv(f"{outputfile_path}/Frequency_difference.txt", sep="\t", encoding="utf-8")
        
        # 绘制频率差异曼哈顿图
        plot_manhattan(frequency_difference_result, column1, column2)
    else:
        print(frequency_difference_result)
else:
    print(frequency_result)

os.remove(f"{outputfile_path}/Haplogroup_frequency.txt")
print("中间文件已经被删除了，如果你不需要删除，请你删除代码最后2行。")


复制<br>运行代码，会让你输入一些关键的参数，按照提示输入即可。我懒得写了。<br>
绘图的参数可以自己定义颜色之类的。<br>
尽管代码绘图还有瑕疵，但是可以在 Adobe illustrator 改。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408032214544.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
展示两类群体之间单倍群频率差异的曼哈顿图。该图展示了在两类指定群体之间，不同单倍群频率的差异。x轴代表不同的单倍群，点的位置由单倍群名称的首字母决定，y轴表示两个群体之间的频率差异值。每个标记点显示对应单倍群在两个群体中的差异，不同的颜色代表不同的宏单倍群，超出预设阈值的单倍群用标签标注。
]]></description><link>软件\python\数据科学与格式转换\python：绘制差异性单倍群的曼哈顿图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：绘制差异性单倍群的曼哈顿图.md</guid><pubDate>Wed, 23 Oct 2024 07:12:27 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>为了探究一个群体中有多少种单倍群，以及每种单倍群的比例，可以绘制柱状图，类似常染色体的ADMIXTURE。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407230859072.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一列是群体，其余列是单倍群的比例。计算公式如下：<br><br><br>import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import numpy as np

# 设置 rcParams 以确保导出的图表是可编辑的
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42

# 加载数据
file_path = 'C:\\Users\\victo\\Desktop\\新建 Text Document.txt'
data = pd.read_csv(file_path, sep='\t')

# 定义一个函数以生成更多颜色（如果需要）
def generate_colors(num_colors):
    colors = list(mcolors.TABLEAU_COLORS.values())
    if num_colors &gt; len(colors):
        # 如果需要更多颜色，则使用颜色映射生成它们
        colormap = plt.get_cmap('hsv', num_colors)
        colors = [colormap(i) for i in range(num_colors)]
    return colors

# 删除非数值列以便进行绘图
plot_data = data.select_dtypes(include=['float64', 'int64'])

# 检查是否存在 Population 列，并将其设置为索引
if 'Population' in data.columns:
    plot_data.index = data['Population']
else:
    raise ValueError("The 'Population' column is missing from the data.")

# 确定属性的数量
num_attributes = len(plot_data.columns)

# 生成所需数量的颜色
colors = generate_colors(num_attributes)

# 绘制堆积条形图并应用新的颜色
plt.figure(figsize=(12, 8))
plot_data.plot(kind='bar', stacked=True, color=colors)
plt.title('Stacked Bar Chart with Generated Colors')
plt.xlabel('Population')
plt.ylabel('Values')
plt.legend(title='Attributes', bbox_to_anchor=(1.05, 1), loc='upper left')

# 显示图表
plt.show()

复制<br><br>出来的图的颜色是随机的，但是可以在 Ai 中进行修改至喜欢的颜色：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407230902257.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：绘制单倍群成分柱状图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：绘制单倍群成分柱状图.md</guid><pubDate>Tue, 23 Jul 2024 01:02:49 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407230859072.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407230859072.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>准备一个 csv 文件，表头包含 Province，Haplogroup。例如：<br>ID,Province,Ethnicity,Haplogroup,Latitude,Longitude,Elevation
4,重庆市,汉族,M7c1a2a,29.5656843,106.5511838,219.9486542
6,河北省,汉族,F4a,38.0427599,114.5143,72.94680786
复制<br>准备好 shp 文件以及配套的文件，例如：<br>省面.CPG
省面.dbf
省面.prj
省面.sbn
省面.sbx
省面.shp
省面.shx
复制<br><br>import pandas as pd
import geopandas as gpd
import folium
from folium.features import GeoJson, GeoJsonTooltip

# 加载数据
file_path = 'C:/Users/victo/Desktop/数据清洗整理.csv'
data = pd.read_csv(file_path)

# 过滤指定单倍群并统计各省数量
def count_haplogroup_by_province(haplogroup_name, data):
    filtered_data = data[data['Haplogroup'] == haplogroup_name]
    province_counts = filtered_data['Province'].value_counts().reset_index()
    province_counts.columns = ['Province', 'Count']
    return province_counts

# 计算各省的单倍群频率
def calculate_frequency(data, haplogroup_name):
    total_counts = data['Province'].value_counts().reset_index()
    total_counts.columns = ['Province', 'Total']
    haplogroup_counts = count_haplogroup_by_province(haplogroup_name, data)
    frequency = pd.merge(haplogroup_counts, total_counts, on='Province')
    frequency['Frequency'] = frequency['Count'] / frequency['Total']
    return frequency[['Province', 'Frequency']]

# 获取单倍群 M9a1b1c 在各省的频率
haplogroup_name = 'M9a1b1c'
province_frequency = calculate_frequency(data, haplogroup_name)

# 加载中国省份地理数据
china_map = gpd.read_file('C:/Users/victo/Desktop/省面.shp')

# 检查列名
print(china_map.columns)

# 确认地理数据中的省份名称列
province_column = '省全名'  # 根据实际情况调整列名

# 合并统计数据和地理数据
china_map = china_map.merge(province_frequency, left_on=province_column, right_on='Province', how='left').fillna(0)

# 创建 folium 地图
m = folium.Map(location=[35.8617, 104.1954], zoom_start=4)

# 添加 GeoJson 数据
folium.Choropleth(
    geo_data=china_map,
    name='choropleth',
    data=china_map,
    columns=[province_column, 'Frequency'],
    key_on=f'feature.properties.{province_column}',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name=f'Haplogroup {haplogroup_name} Frequency by Province'
).add_to(m)

# 添加工具提示
folium.GeoJson(
    china_map,
    style_function=lambda x: {'fillColor': '#ffffff00', 'color': 'black', 'weight': 1},
    tooltip=GeoJsonTooltip(
        fields=[province_column, 'Frequency'],
        aliases=['Province: ', 'Frequency: '],
        localize=True
    )
).add_to(m)

# 保存地图
m.save('haplogroup_distribution_map.html')

# 打印 M9a1b1c 单倍群在各省的频率
print(province_frequency)
复制<br><br>
import pandas as pd
import matplotlib.pyplot as plt
import geopandas as gpd
from pypinyin import pinyin, Style

# 加载数据
file_path = 'C:/Users/victo/Desktop/数据清洗整理.csv'
data = pd.read_csv(file_path)

# 过滤指定单倍群并统计各省数量
def count_haplogroup_by_province(haplogroup_name, data):
    filtered_data = data[data['Haplogroup'] == haplogroup_name]
    province_counts = filtered_data['Province'].value_counts().reset_index()
    province_counts.columns = ['Province', 'Count']
    return province_counts

# 获取单倍群在各省的数量
haplogroup_name = 'Z'
province_counts = count_haplogroup_by_province(haplogroup_name, data)

# 计算全国B5a1c的总数量
total_haplogroup_count = province_counts['Count'].sum()

# 计算各省的B5a1c频率
province_counts['Frequency'] = province_counts['Count'] / total_haplogroup_count

# 将中文省份名称转换为拼音
province_counts['Province_pinyin'] = province_counts['Province'].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 加载中国省份地理数据
china_map = gpd.read_file('C:/Users/victo/Desktop/省面.shp')

# 确认地理数据中的省份名称列
province_column = '省全名'

# 将地理数据中的省份名称转换为拼音
china_map['Province_pinyin'] = china_map[province_column].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 合并统计数据和地理数据
china_map = china_map.merge(province_counts, left_on='Province_pinyin', right_on='Province_pinyin', how='left')
china_map = china_map.infer_objects()  # 确保数据类型正确
china_map = china_map.fillna(0)  # 填充缺失值

# 检查province_counts是否为空
if province_counts.empty:
    print("Error: The haplogroup data is empty. Please check the input data and haplogroup name.")
else:
    # 获取频率的最大值和最小值
    max_frequency = province_counts['Frequency'].max()
    min_frequency = province_counts['Frequency'].min()

    # 绘制热图
    fig, ax = plt.subplots(1, 1, figsize=(10, 18))  # 调整图形高度
    china_map.boundary.plot(ax=ax, linewidth=0.03, color='#8B8B8B')
    china_map.plot(column='Frequency', ax=ax, legend=True,
                   cmap='Spectral_r',  # 使用配色
                   legend_kwds={'label': f"Haplogroup {haplogroup_name} Frequency by Province",
                                'orientation': "vertical"},
                   vmin=min_frequency, vmax=max_frequency)  # 设置颜色条范围

    # 隐藏地图外框的横轴、纵轴的数字标尺
    ax.set_xticks([])
    ax.set_yticks([])

    plt.title(f'Distribution of Haplogroup {haplogroup_name} in China by Province')

    # 添加表格
    table_data = province_counts[['Province_pinyin', 'Count', 'Frequency']].values
    column_labels = ['Province', 'Count', 'Frequency']
    table = plt.table(cellText=table_data,
                      colLabels=column_labels,
                      cellLoc='center',
                      loc='bottom',
                      bbox=[0, -1.4, 1, 1.2])  # 调整表格位置和大小

    table.auto_set_font_size(False)
    table.set_fontsize(8)  # 调整字体大小

    # 调整单元格的高度
    cell_dict = table.get_celld()
    for i in range(len(table_data) + 1):  # 包括标题行
        for j in range(len(column_labels)):
            cell_dict[(i, j)].set_height(0.15)  # 调整单元格高度

    plt.subplots_adjust(left=0.2, bottom=0.4)  # 调整图形布局
    plt.show()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606111507.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：绘制省份频率地图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：绘制省份频率地图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:29 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606111507.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240606111507.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br>我们有时候需要计算每一种单倍群（不是仅仅包括宏单倍群）在特定字段（例如省份、语系等）的数量与频率。这对于 mtDNA 来说是一件非常麻烦的事情，因为很多单倍群的命名极其混乱。为了完成这个工作，需要分成2步骤。<br><br>首先按照 <a data-href="Python：查找mtDNA上游单倍型（2024年10月23日更新）" href="软件\python\数据科学与格式转换\python：查找mtdna上游单倍型（2024年10月23日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Python：查找mtDNA上游单倍型（2024年10月23日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">步骤，获得一个这样的文件。你可以从生成的 正序 文件中获得，并将对应的 ID 与基本信息补齐。例如省份等。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409091136058.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>然后按照如下代码进行操作：<br>import pandas as pd
import numpy as np
from multiprocessing import Pool, cpu_count

# 加载数据集
file_path = '正序等级.txt'  # 替换为你的数据文件路径
data = pd.read_csv(file_path,sep="\t")

# 列出单倍群字段
haplogroup_columns = [f'Level_{i}' for i in range(20)]  # Level_0到Level_19

# 获取所有单倍群
def get_unique_haplogroups(df, columns):
    unique_haplogroups = set()
    for col in columns:
        unique_haplogroups.update(df[col].dropna().unique())  # 将各列中的非空唯一值加入集合
    return unique_haplogroups

# 单个省份的统计工作（并行执行的任务）
def calculate_haplogroup_for_province(args):
    province, province_data, haplogroups = args
    province_haplogroup_data = province_data[haplogroup_columns].to_numpy()

    haplogroup_counts = {}
    for haplogroup in haplogroups:
        # 使用 NumPy 的向量化操作来检查单倍群是否出现在该省份的某行中
        mask = np.any(province_haplogroup_data == haplogroup, axis=1)
        # 统计出现次数（每一行最多计数一次）
        haplogroup_counts[haplogroup] = np.sum(mask)
    
    total_samples = len(province_data)
    haplogroup_frequencies = {haplogroup: count / total_samples for haplogroup, count in haplogroup_counts.items()}
    
    return province, haplogroup_counts, haplogroup_frequencies

# 多核计算函数
def calculate_haplogroup_counts_parallel(df, province_col):
    # 获取所有单倍群
    haplogroups = get_unique_haplogroups(df, haplogroup_columns)
    
    # 按省份分组数据
    province_groups = [(province, df[df[province_col] == province], haplogroups) for province in df[province_col].unique()]

    # 使用多核CPU进行并行计算
    with Pool(cpu_count()) as pool:
        results = pool.map(calculate_haplogroup_for_province, province_groups)

    # 汇总结果
    haplogroup_counts = {}
    haplogroup_frequencies = {}

    for province, counts, frequencies in results:
        haplogroup_counts[province] = counts
        haplogroup_frequencies[province] = frequencies

    return haplogroup_counts, haplogroup_frequencies

if __name__ == '__main__':
    # 交互式输入省份字段
    province_field = input("请输入要计算的字段名称，例如 Province_cn: ")

    # 计算结果
    haplogroup_counts, haplogroup_frequencies = calculate_haplogroup_counts_parallel(data, province_field)

    # 将数量结果转换为 DataFrame 以表格形式展示
    haplogroup_counts_df = pd.DataFrame(haplogroup_counts).T.fillna(0)

    # 将频率结果转换为 DataFrame 以表格形式展示
    haplogroup_frequencies_df = pd.DataFrame(haplogroup_frequencies).T.fillna(0)

    # 重置索引，使用户输入的字段成为第一列
    haplogroup_counts_df.index.name = province_field
    haplogroup_frequencies_df.index.name = province_field
    haplogroup_counts_df.reset_index(inplace=True)
    haplogroup_frequencies_df.reset_index(inplace=True)

    # 保存数量和频率表格为CSV文件
    haplogroup_counts_df.to_csv("haplogroup_counts.csv", index=False)
    haplogroup_frequencies_df.to_csv("haplogroup_frequencies.csv", index=False)

    # 输出数量和频率表格
    print("单倍群数量统计表格:")
    print(haplogroup_counts_df)
    print("\n单倍群频率表格:")
    print(haplogroup_frequencies_df)

#####以下是在普通计算机中使用的脚本################
# import pandas as pd
# import numpy as np

# # 加载数据集
# file_path = '你的数据路径.csv'  # 替换为你的数据文件路径
# data = pd.read_csv(file_path)

# # 列出单倍群字段
# haplogroup_columns = [f'Level_{i}' for i in range(20)]  # Level_0到Level_19

# # 获取所有单倍群
# def get_unique_haplogroups(df, columns):
#     unique_haplogroups = set()
#     for col in columns:
#         unique_haplogroups.update(df[col].dropna().unique())  # 将各列中的非空唯一值加入集合
#     return unique_haplogroups

# # 高效计算每个单倍群在每个省份的出现次数
# def calculate_haplogroup_counts(df, province_col):
#     # 创建一个字典来存储结果
#     haplogroup_counts = {}
    
#     # 获取所有单倍群
#     haplogroups = get_unique_haplogroups(df, haplogroup_columns)
    
#     # 对每个省份进行处理
#     for province in df[province_col].unique():
#         haplogroup_counts[province] = {}
#         # 筛选出该省份的数据
#         province_data = df[df[province_col] == province]
#         province_haplogroup_data = province_data[haplogroup_columns].to_numpy()
        
#         # 创建一个省份对应的哈希表，用于存储该省份中每个单倍群的计数
#         for haplogroup in haplogroups:
#             # 使用 NumPy 的向量化操作来检查单倍群是否出现在该省份的某行中
#             mask = np.any(province_haplogroup_data == haplogroup, axis=1)
            
#             # 统计出现次数（每一行最多计数一次）
#             count = np.sum(mask)
            
#             # 保存计数结果
#             haplogroup_counts[province][haplogroup] = count
    
#     return haplogroup_counts

# # 交互式输入省份字段
# province_field = input("请输入要计算的字段名称，例如 Province_cn: ")

# # 计算结果
# haplogroup_counts = calculate_haplogroup_counts(data, province_field)

# # 将结果转换为 DataFrame 以表格形式展示
# haplogroup_df = pd.DataFrame(haplogroup_counts).T.fillna(0)

# # 重置索引，使用户输入的字段成为第一列
# haplogroup_df.index.name = province_field
# haplogroup_df.reset_index(inplace=True)

# # 显示表格结果
# print(haplogroup_df)

# # 如果需要将结果保存为文件，可以使用以下命令：
# # haplogroup_df.to_csv("haplogroup_counts.csv", index=False)

复制<br>如上代码可以获得2个文件：<br>
<br>数量文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409091139437.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>频率文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409091139438.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>你可能发现了，为什么最后的每个省份的频率之和不为1？
这是因为上下游单倍群之间存在包含关系，而非独立的，因此在计算时上游单倍群已经包含了下游单倍群，固然频率之和就不为1了。
]]></description><link>软件\python\数据科学与格式转换\python：计算各级别线粒体单倍群数量和频率.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：计算各级别线粒体单倍群数量和频率.md</guid><pubDate>Wed, 23 Oct 2024 07:12:27 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[原理]]></title><description><![CDATA[ 
 <br><br>我们使用以下公式来计算单倍型/单倍群的法医学参数。单倍型/单倍群多样性(Haplotype/haplogroup diversity, HD)是基于 Nei 和 Tajima 提出来的公式进行计算：<br>
其中 n 表示观察到单倍型/单倍群的数量，pi 表示第 i个单倍型/单倍群的频率。分辨能力(Discrimination capacity,DC)使用观察的单倍型/单倍群数量和所有单倍型/单倍群数量的比值。<br><a data-href="序列多态性分析（Sequence Polymorphism Analysis）" href="术语\序列多态性分析（sequence-polymorphism-analysis）.html" class="internal-link" target="_self" rel="noopener nofollow">序列多态性分析（Sequence Polymorphism Analysis）</a>中不包含这项。<br><br>准备一个 txt 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240624155053.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import kruskal

# 读取数据
file_path = '新建 Text Document.txt'
output_file_path = 'group_haplotype_diversities.csv'
data = pd.read_csv(file_path, sep='\t')

# 分组并计算每个群体的单倍型多样性
diversities = []
group_sizes = data['Population_Province'].value_counts()

for group, group_data in data.groupby('Population_Province'):
    n = len(group_data)
    if n &gt;= 20:
        haplotype_counts = group_data['Haplo'].value_counts()
        haplotype_frequencies = haplotype_counts / n
        HD = n * (1 - sum(haplotype_frequencies**2)) / (n - 1)
        diversities.append((group, HD, n))

# 转为DataFrame
diversity_df = pd.DataFrame(diversities, columns=['Group', 'Haplotype Diversity', 'Sample Size'])

# 统计学检验
groups = diversity_df['Group'].unique()
group_data = [diversity_df[diversity_df['Group'] == group]['Haplotype Diversity'].values for group in groups]
stat, p = kruskal(*group_data)

# 输出统计学检验结果
print(f"Kruskal-Wallis H检验结果: H-statistic = {stat}, p-value = {p}")

# 保存结果到CSV文件
with open(output_file_path, 'w') as f:
    f.write(f"Kruskal-Wallis H检验结果: H-statistic = {stat}, p-value = {p}\n")
diversity_df.to_csv(output_file_path, mode='a', index=False)

# 排序数据
diversity_df = diversity_df.sort_values(by='Haplotype Diversity')

# 添加群体数量到群体名称
diversity_df['Group'] = diversity_df.apply(lambda row: f"{row['Group']} ({row['Sample Size']})", axis=1)

# 可视化结果：柱状图
plt.figure(figsize=(14, 7))
sns.barplot(x='Group', y='Haplotype Diversity', data=diversity_df, palette='hsv')
plt.xlabel('Group')
plt.ylabel('Haplotype Diversity')
plt.title('Haplotype Diversity of Different Groups')
plt.xticks(rotation=90)  # 群体名称旋转90度便于阅读
plt.tight_layout()

# 展示图表
plt.show()

# 可选：保存图表
# output_chart_path = 'group_haplotype_diversities_barplot.png'
# plt.savefig(output_chart_path)

复制<br><br><img alt="Figure_1.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/Figure_1.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：计算群体单倍群单倍型多样性.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：计算群体单倍群单倍型多样性.md</guid><pubDate>Mon, 28 Oct 2024 09:41:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240624155053.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240624155053.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>这个脚本的作用是，给定一个 txt 文档，里面只有线粒体单倍群，根据 phylotree 计算得到该单倍群下游的共有多少个单倍群（包括自身）。<br><br>一个 txt 文档，如图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240627164132.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>需要的是 线粒体单倍群phylotree(version17).txt，可以通过<a data-tooltip-position="top" aria-label="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1720477816&amp;Signature=P3fhsQWh2K5Qn%2BFe2ET%2F4WlgSNc%3D" rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1720477816&amp;Signature=P3fhsQWh2K5Qn%2BFe2ET%2F4WlgSNc%3D" target="_blank">这里下载</a><br>
如果过期请提醒我。<br><br># 文件路径
haplogroups_file = '新建 Text Document.txt'
phylogroup_file = '线粒体单倍群phylotree(version17).txt'
output_file = 'haplogroup_counts.txt'

def read_haplogroups(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return lines

def find_downstream_haplogroups(haplogroup, lines):
    downstream_haplogroups = []
    haplogroup_level = None
    capture = False

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
        
        current_level = len(line) - len(line.lstrip('\t'))
        if haplogroup_level is None and stripped_line.startswith(haplogroup):
            haplogroup_level = current_level
            capture = True
            continue
        
        if capture:
            if current_level &gt; haplogroup_level:
                downstream_haplogroups.append(stripped_line)
            elif current_level == haplogroup_level:
                break

    return downstream_haplogroups

def count_haplogroups(haplogroups_list, target_haplogroup, phylogroup_lines):
    count = 0
    visited = set()

    def recursive_count(haplogroup):
        nonlocal count
        if haplogroup in visited:
            return
        visited.add(haplogroup)
        haplogroup_count = haplogroups_list.count(haplogroup)
        count += haplogroup_count
        downstream_haplogroups = find_downstream_haplogroups(haplogroup, phylogroup_lines)
        for downstream in downstream_haplogroups:
            recursive_count(downstream)
    
    recursive_count(target_haplogroup)
    return count

def main():
    # 读取目标单倍群
    haplogroups = read_haplogroups(haplogroups_file)
    haplogroups_list = [haplo.strip() for haplo in haplogroups]
    
    # 读取所有单倍群数据
    phylogroup_lines = read_haplogroups(phylogroup_file)
    
    # 查找每个单倍群的下游单倍群并统计数量
    results = {}
    for haplogroup in haplogroups_list:
        if haplogroup not in results:
            count = count_haplogroups(haplogroups_list, haplogroup, phylogroup_lines)
            downstream = find_downstream_haplogroups(haplogroup, phylogroup_lines)
            results[haplogroup] = {
                'count': count,
                'downstream': downstream
            }
    
    # 输出结果到文件
    with open(output_file, 'w', encoding='utf-8') as f:
        for haplogroup, data in results.items():
            f.write(f'{haplogroup}\t{data["count"]}\n')

    print(f'结果已保存到 {output_file}')

if __name__ == '__main__':
    main()

复制]]></description><link>软件\python\数据科学与格式转换\python：计算下游单倍群数量.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：计算下游单倍群数量.md</guid><pubDate>Thu, 27 Jun 2024 08:45:08 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5a5.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5a5.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br>如果我们已经获得了一个 ALN 文件，同时还需要把一些新的 FASTA 文件放进去。同时我们不希望改变 CONSENSUS 序列的长度，换言之：<br>将多个新的 FASTA 文件中的序列与现有的 ALN 文件中的对齐结果进行整合，并确保在保持原有对齐结果不变的同时，新的序列能够准确地与现有比对对齐。<br><br>
<br>没有对齐的 fasta 文件。
<br>已经对齐的 Aln 文件。
<br><br>我们尽量在 Linux 环境下运行，因为我们需要使用比对的软件，在这里，我们使用 MAFFT。<br><br><br>import subprocess
from Bio import AlignIO, SeqIO

# 定义文件路径
aln_file = "已经比对的文件"
fasta_files = [
    "/mnt/c/Users/victo/Desktop/1.fasta",
    "/mnt/c/Users/victo/Desktop/2.fasta"
    # 这里可以添加更多的FASTA文件路径
]
output_file = "/mnt/c/Users/victo/Desktop/UPDATED_MSA.aln"

# 将所有新的FASTA文件合并到一个临时文件中
temp_fasta_file = "/mnt/c/Users/victo/Desktop/temp_new_sequences.fasta"
with open(temp_fasta_file, "w") as temp_f:
    for fasta_file in fasta_files:
        with open(fasta_file) as fasta_f:
            temp_f.write(fasta_f.read())

# 使用MAFFT的--add选项将新的序列与现有的比对文件进行对齐
mafft_command = ["mafft", "--add", temp_fasta_file, aln_file]
with open(output_file, "w") as output_f:
    subprocess.run(mafft_command, stdout=output_f, check=True)

###############################################
#这里提供了更多的参数来追求更精致的比对###########
###############################################
# import subprocess
# # 使用MAFFT的--addfull选项
# mafft_command = ["mafft", "--addfull", temp_fasta_file, aln_file]
# with open(output_file, "w") as output_f:
#     subprocess.run(mafft_command, stdout=output_f, check=True)
# print(f"新的对齐文件已保存到 {output_file}")


# import subprocess
# # 使用MAFFT的--add选项，带--keeplength
# mafft_command = ["mafft", "--add", temp_fasta_file, "--keeplength", aln_file]
# with open(output_file, "w") as output_f:
#     subprocess.run(mafft_command, stdout=output_f, check=True)
# print(f"新的对齐文件已保存到 {output_file}")

print(f"新的对齐文件已保存到 {output_file}")
复制<br>
<br>
--addprofile：

<br>参数：mafft --addprofile new_sequences.fasta existing_alignment.aln
<br>描述：将新序列作为一个独立的多序列比对文件，与现有比对文件进行全局比对。这种方法通常是最精确的，因为它考虑了所有新序列的上下文。
<br>使用场景：适用于有较多新序列的情况，需要高精度的对齐。


<br>
--addfull：

<br>参数：mafft --addfull new_sequences.fasta existing_alignment.aln
<br>描述：将新序列添加到现有比对中，进行全局比对，考虑整个序列的上下文。
<br>使用场景：适用于需要较高精度对齐但新序列不多的情况。


<br>
--add：

<br>参数：mafft --add new_sequences.fasta existing_alignment.aln
<br>描述：将新序列逐个添加到现有比对中，进行局部比对。这种方法比 --addfull 快，但精度较低。
<br>使用场景：适用于需要快速对齐且新序列较少的情况。


<br>
--keeplength：

<br>参数：mafft --add new_sequences.fasta --keeplength existing_alignment.aln
<br>描述：类似 --add，但确保现有比对的长度保持不变。精度最低，但对已有比对影响最小。
<br>使用场景：适用于需要确保现有比对完全不变的情况，即使新序列的对齐精度较低。


<br><br>除了 MAFFT，还有其他一些常用的多序列比对工具可以用来将新的序列添加到现有的比对中。这些工具包括:<br>
<br>Clustal Omega：一个适合大规模数据的多序列比对工具。
<br>T-Coffee：一个灵活且强大的多序列比对工具，支持多种对齐策略。
<br><br>import subprocess
from Bio import AlignIO, SeqIO

# 定义文件路径
aln_file = "existing_alignment.aln"
fasta_files = ["new_sequences1.fasta", "new_sequences2.fasta"]
temp_fasta_file = "temp_new_sequences.fasta"
output_file = "UPDATED_MSA.aln"

# 合并所有新的FASTA文件到一个临时文件中
with open(temp_fasta_file, "w") as temp_f:
    for fasta_file in fasta_files:
        with open(fasta_file) as fasta_f:
            temp_f.write(fasta_f.read())

# 使用Clustal Omega的--p1和--p2选项
clustalo_command = ["clustalo", "--in1", aln_file, "--in2", temp_fasta_file, "--out", output_file, "--outfmt", "clustal"]
subprocess.run(clustalo_command, check=True)

print(f"新的对齐文件已保存到 {output_file}")

复制<br><br>import subprocess
from Bio import AlignIO, SeqIO

# 定义文件路径
aln_file = "existing_alignment.aln"
fasta_files = ["new_sequences1.fasta", "new_sequences2.fasta"]
temp_fasta_file = "temp_new_sequences.fasta"
output_file = "UPDATED_MSA.aln"

# 合并所有新的FASTA文件到一个临时文件中
with open(temp_fasta_file, "w") as temp_f:
    for fasta_file in fasta_files:
        with open(fasta_file) as fasta_f:
            temp_f.write(fasta_f.read())

# 使用T-Coffee进行比对
t_coffee_command = ["t_coffee", "-other_pg", "aln2tree", "-mode", "quickaln", "-aln", aln_file, "-seq", temp_fasta_file, "-outfile", output_file]
subprocess.run(t_coffee_command, check=True)

print(f"新的对齐文件已保存到 {output_file}")

复制]]></description><link>软件\python\数据科学与格式转换\python：将fasta文件与aln文件合并.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：将FASTA文件与Aln文件合并.md</guid><pubDate>Fri, 21 Jun 2024 07:31:20 GMT</pubDate></item><item><title><![CDATA[准备工作]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！<br><br>
<br>需要进行分组的FASTA文件。
<br>进行分组的TXT文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402011320563.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>分组文件应该符合如下格式：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402011319719.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br># 这个脚本将FASTA文件中的序列分组到指定的组中

# 该脚本需要两个参数：

# 1. 分组文件的路径

# 2. FASTA文件的路径

# 3. 输出文件的路径

# 4. 分组文件的格式为：

#     样本名称\t分组名称

# 5. FASTA文件的格式为：

#     &gt;sample_name

# 6. 输出文件的格式为：

#     &gt;sample_name\t分组名称

  

# 这样做的目的是为了将多个FASTA文件中的序列分组到一起，以便于使用DnaSP快速分组

  
  

def read_group_file(group_file_path):

    """

    读取分组文件并创建一个映射关系的字典

    """

    group_map = {}

    with open(group_file_path, 'r', encoding='utf-8') as file:

        for line in file:

            parts = line.strip().split('\t')

            if len(parts) == 2:

                sample_name, Group_name = parts

                group_map[sample_name] = Group_name

    return group_map

  

def read_fasta_file(fasta_file_path):

    """

    读取fasta文件并返回一个包含样本名称和序列的字典

    """

    fasta_data = {}

    current_sample = None

    with open(fasta_file_path, 'r') as file:

        for line in file:

            if line.startswith('&gt;'):

                current_sample = line.strip()[1:]

                fasta_data[current_sample] = ''

            else:

                fasta_data[current_sample] += line.strip()

    return fasta_data

  

def write_sorted_fasta_file(fasta_data, output_file_path):

    """

    将排序后的fasta数据写入新文件

    """

    with open(output_file_path, 'w') as output_file:

        for sample in sorted(fasta_data, key=lambda x: fasta_data[x]['province']):

            output_file.write(f'&gt;{sample}\n{fasta_data[sample]["sequence"]}\n')

  

def replace_and_sort_fasta_samples(fasta_file_path, group_file_path, output_file_path):

    """

    替换fasta文件中的样本名称并排序，然后保存到新的文件中

    """

    group_map = read_group_file(group_file_path)

    fasta_data = read_fasta_file(fasta_file_path)

  

    # 替换样本名称并添加省份信息

    replaced_fasta_data = {}

    for name, seq in fasta_data.items():

        Group_name = group_map.get(name, 'Unknown')

        replaced_fasta_data[f'{Group_name} {name}'] = {'sequence': seq, 'province': Group_name}

  

    # 写入排序后的fasta数据到新文件

    write_sorted_fasta_file(replaced_fasta_data, output_file_path)

  

# 使用示例

fasta_file_path = 'C:/Users/a/Desktop/.fasta' # Fasta文件路径

  

group_file_path = 'C:/Users/a/Desktop/.txt' # 分组文件路径

  

output_file_path = 'C:/Users/a/Desktop/.fasta' # 输出文件路径

  

replace_and_sort_fasta_samples(fasta_file_path, group_file_path, output_file_path)
复制]]></description><link>软件\python\数据科学与格式转换\python：将fasta文件中的序列分组到指定的组中.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：将FASTA文件中的序列分组到指定的组中.md</guid><pubDate>Fri, 23 Aug 2024 08:25:16 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402011320563.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402011320563.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：将FASTA转换成Arp]]></title><description><![CDATA[ 
 <br>这个脚本可以把FASTA文件转换成Arp，跳过了DnaSP的操作，但是这增加了Arlequin的运算量。需要准备2个文件：<br>
<br>FASTA文件
<br>分组文件<br>
如图所示：
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402201357957.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
用制表符分隔<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402201358078.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>我知道你很急，但你先别急。如果你直接把完整序列的fasta文件拿来处理，也不是不行，只是你可能会遇到Arlequin歇菜。<br>为了防止这样的情况发生，我们有必要对上面的fasta文件进行简化，从而减轻计算量。<br><br>
<br>打开<a data-href="MEGA" href="MEGA" class="internal-link" target="_self" rel="noopener nofollow">MEGA</a>
<br>导入原始的fasta文件
<br>点击Analyze
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211402609.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
4. 根据实际情况选择<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211403215.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
5. 打开序列<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211404745.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
6. 点击Data→Export data。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211404470.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
7. 删去gap等。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211405590.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
8. 点击OK。导出新的fasta文件，再用这个新的文件进行后续分析。<br><br>这个版本将不会输出.hap文件，而是会强制将所有的序列信息和ID原封不动全部保留在.arp文件。这对于Arlequin运算时间会变长。<br>from collections import defaultdict

# Function to read the file content efficiently
def read_file(file_path, encoding='utf-8'):
    try:
        with open(file_path, 'r', encoding=encoding) as file:
            return file.readlines()
    except UnicodeDecodeError:
        with open(file_path, 'r', encoding='latin1') as file:
            return file.readlines()

# Read the FASTA and Group files
fasta_file_path = 'All_pubilc.fasta'
group_file_path = '新建 Text Document.txt'
arp_file_path = 'All.ALN.arp'

fasta_content = read_file(fasta_file_path)
group_content = read_file(group_file_path)

# Preview the files
fasta_preview = fasta_content[:5]
group_preview = group_content[:5]

fasta_preview, group_preview

# Process the group information
group_dict = defaultdict(list)
for line in group_content:
    sample_name, group_name = line.strip().split('\t')
    group_dict[group_name].append(sample_name)

# Number of unique groups (NbSamples)
nb_samples = len(group_dict)

# Prepare to match FASTA sequences to their respective groups
sequence_dict = {}
current_sample_name = None
for line in fasta_content:
    if line.startswith('&gt;'):
        current_sample_name = line[1:].strip().split()[0]
        sequence_dict[current_sample_name] = []
    elif current_sample_name is not None:
        sequence_dict[current_sample_name].append(line.strip())

# Combine sequence lines
for sample in sequence_dict:
    sequence_dict[sample] = ''.join(sequence_dict[sample])

# Create the .arp file content
arp_content = [
    "[Profile]",
    "   Title = \"Genetic Diversity Analysis\"",
    f"   NbSamples = {nb_samples}",
    "   DataType = DNA",
    "   GenotypicData = 0",
    "   LocusSeparator = NONE",
    "   MissingData = \"N\"",
    "   CompDistMatrix = 1",
    "",
    "[Data]",
    ""
]

# Adding each group's sample information to the .arp file content
for group_name, samples in group_dict.items():
    sample_size = len(samples)
    arp_content.append("[[Samples]]")
    arp_content.append(f"   SampleName = \"{group_name}\"")
    arp_content.append(f"   SampleSize = {sample_size}")
    arp_content.append("   SampleData= {")
    for sample in samples:
        sequence = sequence_dict.get(sample, "N" * 100)
        arp_content.append(f"       {sample} 1 {sequence}")
    arp_content.append("   }")
    arp_content.append("")

# Convert the list to a single string
arp_content_str = '\n'.join(arp_content)

# Previewing the beginning of the arp_content
arp_content_preview = '\n'.join(arp_content[:20])

# Save the .arp content to a new file
with open(arp_file_path, 'w', encoding='utf-8') as file:
    file.write(arp_content_str)
复制<br><br>这个版本会创建一个映射关系的字典，并把序列信息分开保存至.hap文件中。<br>import os
from collections import defaultdict

# 功能函数定义
def read_group_file(group_file_path):
    """读取分组文件并创建一个映射关系的字典"""
    group_map = {}
    with open(group_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            parts = line.strip().split('\t')
            if len(parts) == 2:
                sample_name, group_name = parts
                group_map[sample_name] = group_name
    return group_map

def read_fasta_file(fasta_file_path):
    """读取fasta文件并返回一个包含样本名称和序列的字典"""
    fasta_data = {}
    current_sample = None
    with open(fasta_file_path, 'r') as file:
        for line in file:
            if line.startswith('&gt;'):
                current_sample = line.strip()[1:]
                fasta_data[current_sample] = ''
            else:
                fasta_data[current_sample] += line.strip()
    return fasta_data

def replace_and_sort_fasta_samples(fasta_file_path, group_file_path, output_file_path):
    """替换fasta文件中的样本名称并排序，然后保存到新的文件中"""
    group_map = read_group_file(group_file_path)
    fasta_data = read_fasta_file(fasta_file_path)
    replaced_fasta_data = {}
    for name, seq in fasta_data.items():
        group_name = group_map.get(name, 'Unknown')
        replaced_fasta_data[f'{group_name} {name}'] = {'sequence': seq, 'province': group_name}
    with open(output_file_path, 'w') as output_file:
        for sample in sorted(replaced_fasta_data, key=lambda x: replaced_fasta_data[x]['province']):
            output_file.write(f'&gt;{sample}\n{replaced_fasta_data[sample]["sequence"]}\n')

# 主执行逻辑
if __name__ == "__main__":
    base_path = 'C:/Users/a/Desktop'
    fasta_file_path = os.path.join(base_path, 'Illumina_mtDNA_Filter_recode.fas')#改成你自己文件的名字
    group_file_path = os.path.join(base_path, '新建文本文档.txt')#改成你自己文件的名字
    output_file_path = os.path.join(base_path, 'New.fasta')#别改！！！Don't change it!

    replace_and_sort_fasta_samples(fasta_file_path, group_file_path, output_file_path)
########################################################################################
########################################################################################
########################################################################################

import os
from collections import defaultdict

# 设置基本路径
base_path = 'C:/Users/a/Desktop'#改成你的文件路径

# 文件路径
file_path = os.path.join(base_path, 'New.fasta') #别改！Don't change it!
new_file_path = os.path.join(base_path, 'New.arp')#别改！Don't change it!
new_file_txt_path = os.path.join(base_path, 'New.hap')#别改！Don't change it!
header_arp_path = os.path.join(base_path, '开头.arp')#别改！Don't change it!
final_arp_path = os.path.join(base_path, '最终.arp')#别改！Don't change it!

# 读取和解析 New.fasta 文件
sequences = {}
with open(file_path, 'r') as file:
    for line in file:
        line = line.strip()
        if line.startswith('&gt;'):
            parts = line[1:].split(' ')
            group = parts[0]
            seq_id = parts[1]
            sequences[seq_id] = {'group': group, 'sequence': ''}
        else:
            sequences[seq_id]['sequence'] += line

# 计算组别数量
group_set = set(details['group'] for details in sequences.values())
Group_Number = len(group_set)

# 初始化用于存储唯一序列及其对应hap类型的字典和统计
unique_sequences = {}
group_hap_counts = defaultdict(lambda: defaultdict(int))
hap_counter = 1

# 分配hap类型并统计
for seq_id, details in sequences.items():
    seq = details['sequence']
    group = details['group']
    if seq not in unique_sequences:
        hap_label = f'Hap_{hap_counter}'
        unique_sequences[seq] = hap_label
        hap_counter += 1
    else:
        hap_label = unique_sequences[seq]
    group_hap_counts[group][hap_label] += 1

# 写入New.arp文件
with open(new_file_path, 'w', encoding='utf-8') as new_file:
    for group, haps in group_hap_counts.items():
        new_file.write(f'[[Samples]]\nSampleName = "{group}"\nSampleSize = {sum(haps.values())}\nSampleData= {{\n')
        for hap, count in haps.items():
            new_file.write(f'    {hap} {count}\n')
        new_file.write('}}\n\n')

# 写入New.hap文件
with open(new_file_txt_path, 'w', encoding='utf-8') as txt_file:
    for seq, hap_label in unique_sequences.items():
        txt_file.write(f'{hap_label}\t{seq}\n')

# 创建并写入开头.arp文件
with open(header_arp_path, 'w', encoding='utf-8') as header_file:
    header_file.write("[Profile]\n   Title = \"Haplotype Data from I fuck DnaSP file\"\n")
    header_file.write(f"   NbSamples = {Group_Number}\n   DataType = DNA\n   GenotypicData = 0\n")#如果你的数据是二倍体，需要将GenotypicData = 1
    header_file.write("   LocusSeparator = NONE\n   MissingData = \"?\"\n   CompDistMatrix = 1\n\n[Data]\n\n")
    header_file.write("[[HaplotypeDefinition]]\n   HaplList = EXTERN \"New.hap\"\n\n")

# 将New.arp的内容追加到开头.arp
with open(header_arp_path, 'a', encoding='utf-8') as header_file, open(new_file_path, 'r', encoding='utf-8') as new_file:
    header_file.write(new_file.read())

# 重命名或移动文件以创建最终.arp
os.rename(header_arp_path, final_arp_path)

# 删除生成的New.arp文件
os.remove(new_file_path)

print(f"最终文件已保存至: {final_arp_path}")
# 删除位于C:/Users/a/Desktop的New.fasta文件
fasta_file_path = os.path.join(base_path, 'New.fasta')
os.remove(fasta_file_path)

print(f"文件 {fasta_file_path} 已被删除。")
print("给我点个赞如何？")


复制]]></description><link>软件\python\数据科学与格式转换\python：将fasta转换成arp.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：将FASTA转换成Arp.md</guid><pubDate>Tue, 23 Jul 2024 02:02:16 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402201357957.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402201357957.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[目的]]></title><description><![CDATA[ 
 <br><br>这个脚本只会把 VCF 文件转为 fasta，不依赖参考基因组。逻辑是只转化测序的位点。同时，这个脚本应该不会考虑 Indels 的情况。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301633073.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>def vcf_to_fasta_mtDNA_improved(vcf_path, output_path):
    """
    Convert a VCF file to FASTA format for mtDNA, considering it as haploid and minimizing 'N'.
    
    Args:
    vcf_path (str): Path to the input VCF file.
    output_path (str): Path to save the output FASTA file.
    """
    # 存储样本名的列表
    sample_names = []
    # 用字典存储每个样本的序列，键为样本名，值为序列的列表
    sample_sequences = {}
    
    # 打开并读取VCF文件
    with open(vcf_path, 'r') as file:
        for line in file:
            if line.startswith('##'):
                continue  # 忽略文件头的元信息行
            elif line.startswith('#CHROM'):
                # 提取样本名，这些名字位于文件的第一行，从第10列开始
                parts = line.strip().split('\t')
                sample_names = parts[9:]  # 获取所有样本名
                # 为每个样本初始化序列列表，包括描述行
                for name in sample_names:
                    sample_sequences[name] = [f"&gt;{name}\n"]
            else:
                # 处理VCF文件的数据行
                parts = line.strip().split('\t')
                ref_base = parts[3]  # 参考碱基
                alt_bases = parts[4].split(',')  # 所有替代碱基，以逗号分隔
                # 遍历每个样本的基因型信息
                for i, genotype_info in enumerate(parts[9:], start=9):
                    genotype = genotype_info.split(':')[0]  # 提取基因型
                    # 根据基因型确定使用的碱基
                    if '.' in genotype:
                        base = 'N'  # 未知碱基
                    elif '0/0' in genotype:
                        base = ref_base  # 使用参考碱基
                    else:
                        # 提取第一个非零碱基，考虑到mtDNA应为单倍体
                        alleles = [int(allele) for allele in genotype.replace('.', '0').split('/') if allele.isdigit()]
                        base = ref_base if alleles[0] == 0 else alt_bases[alleles[0]-1]
                    # 添加碱基到样本序列中
                    sample_sequences[sample_names[i-9]].append(base)

    # 将每个样本的序列写入到输出文件
    with open(output_path, 'w') as out_file:
        for sample, seq in sample_sequences.items():
            out_file.write(''.join(seq) + "\n")

# 指定输入VCF文件和输出FASTA文件的路径
vcf_path = 'C:/Users/a/Desktop/Illumina_mtDNA.vcf'  # 请替换为实际的VCF文件路径
output_path = 'C:/Users/a/Desktop/Illumina_mtDNA.fasta'  # 指定输出FASTA文件的路径

# 调用函数，生成FASTA文件
vcf_to_fasta_mtDNA_improved(vcf_path, output_path)

复制<br><br>

def vcf_to_fasta_with_sample_names(vcf_path, output_path):
    sample_names = []  # 用于存储样本名
    sample_sequences = {}
    
    with open(vcf_path, 'r') as file:
        for line in file:
            if line.startswith('##'):
                continue  # 跳过文件的元信息
            elif line.startswith('#CHROM'):
                parts = line.strip().split('\t')
                sample_names = parts[9:]  # 获取样本名
                for name in sample_names:
                    sample_sequences[name] = [f"&gt;{name}\n"]  # 为每个样本初始化序列列表，包括描述行
            else:
                parts = line.strip().split('\t')
                ref_base = parts[3]
                alt_bases = parts[4].split(',')
                for i, genotype_info in enumerate(parts[9:], start=9):
                    genotype = genotype_info.split(':')[0]
                    if '.' in genotype:
                        base = 'N'
                    else:
                        alleles = [int(allele) for allele in genotype.split('/') if allele.isdigit()]
                        base = ''.join([alt_bases[allele-1] if allele &gt; 0 else ref_base for allele in alleles])
                    sample_sequences[sample_names[i-9]].append(base)

    # 写入到输出文件
    with open(output_path, 'w') as out_file:
        for sample, seq in sample_sequences.items():
            out_file.write(''.join(seq) + "\n")  # 将序列和其描述行写入文件

# 指定VCF文件路径和输出文件路径
vcf_path = 'C:/Users/a/Desktop/Illumina_mtDNA.vcf'
output_path = 'C:/Users/a/Desktop/Illumina_mtDNA.fasta'

# 调用函数，生成包含样本名称ID的FASTA文件
vcf_to_fasta_with_sample_names(vcf_path, output_path)

复制]]></description><link>软件\python\数据科学与格式转换\python：将vcf文件伪二倍体转化为单倍体fatsa文件.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：将VCF文件伪二倍体转化为单倍体fatsa文件.md</guid><pubDate>Fri, 21 Jun 2024 07:31:21 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301633073.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301633073.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[更新时间]]></title><description><![CDATA[ 
 <br><br>2024年4月29日 ：加入了一个新功能，添加一个马金玥要求的 ;1<br>
并且可以根据 csv 文件的列表剔除多余的序列。<br><br>将 VCF 文件转为 rdf 进行network 软件分析。<br>
由于这个上古软件需要你的 ID 长度必须小于 15 个字符，所以我们需要对 ID 进行重命名来解决这个问题。<br><br><br>请注意，这个脚本读取的是一个伪二倍体的文件并将其转为单倍体。<br>
其原始格式应该类似如下：<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403292241495.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>请使用 csv 格式。标题请完全对应起来。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301626833.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import os
import pandas as pd
import re
import csv

def vcf_processing_combined(vcf_path, fasta_output_path, txt_output_path, csv_path, final_rdf_path, mapping_csv_path):
    # Load the CSV file for additional info mapping
    csv_df = pd.read_csv(csv_path)
    id_to_info = {row['Object_ID']: f";{row['Re_Population_Province']};{row['Haplogrouper']}" for index, row in csv_df.iterrows()}
    
    # Process the VCF file to create sequences
    sample_names = []
    sample_sequences = {}
    with open(vcf_path, 'r') as file:
        for line in file:
            if line.startswith('##'):
                continue
            elif line.startswith('#CHROM'):
                parts = line.strip().split('\t')
                all_sample_names = parts[9:]
                sample_names = [name for name in all_sample_names if name in id_to_info]
                for name in sample_names:
                    additional_info = id_to_info.get(name, "")
                    sample_sequences[name] = [f"&gt;{name}{additional_info};\n"]
            else:
                parts = line.strip().split('\t')
                ref_base = parts[3]
                alt_bases = parts[4].split(',')
                genotype_infos = parts[9:]  # Entire genotype info
                # Only process genotype info for filtered sample names
                for name, genotype_info in zip(sample_names, genotype_infos):
                    genotype = genotype_info.split(':')[0]
                    if '.' in genotype:
                        base = 'N'
                    elif '0/0' in genotype:
                        base = ref_base
                    else:
                        alleles = [int(allele) for allele in genotype.replace('.', '0').split('/') if allele.isdigit()]
                        base = ref_base if alleles[0] == 0 else alt_bases[alleles[0]-1]
                    sample_sequences[name].append(base)
    
    # Write the output FASTA file
    with open(fasta_output_path, 'w') as out_file:
        for sample, seq in sample_sequences.items():
            out_file.write(''.join(seq) + "\n")
    
    # Extract SNP positions and write to TXT file with FASTA appended
    numbers = []
    with open(vcf_path, 'r') as vcf_file:
        numbers = [line.split('\t')[1] for line in vcf_file if not line.startswith('#')]
    count = len(numbers)
    with open(txt_output_path, 'w') as output_file:
        output_file.write("  ;1.0\n")
        output_file.write(';'.join(numbers) + ';')
        output_file.write('\n')
        output_file.write(';'.join(['10'] * count) + ';')
    
    # Append FASTA content to the TXT file
    with open(fasta_output_path, 'r') as fasta_file, open(txt_output_path, 'a') as txt_file:
        fasta_content = fasta_file.read()
        txt_file.write("\n")  # Separator
        txt_file.write(fasta_content)
    
    os.remove(fasta_output_path)  # Remove the intermediate FASTA file
    
    # Rename sequences in the TXT file and create the final RDF file
    def extract_and_rename_handling_encoding(file_path):
        new_names_map = []
        seq_number = 1
        with open(file_path, 'r', encoding='latin-1') as file:
            for line in file:
                if line.startswith('&gt;'):
                    original_name = line[1:].split(';')[0]
                    if original_name not in id_to_info:  # Check if sequence should be included
                        continue
                    underscore_pos = original_name.find('_')
                    second_char = original_name[underscore_pos + 1] if underscore_pos != -1 and underscore_pos + 1 &lt; len(original_name) else ''
                    new_name = f"{original_name[0]}{second_char}{seq_number}"
                    seq_number += 1
                    new_names_map.append((original_name, new_name))
        return new_names_map
    
    new_names_map_handling_encoding = extract_and_rename_handling_encoding(txt_output_path)
    
    with open(mapping_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Original Name', 'New Name'])
        writer.writerows(new_names_map_handling_encoding)
    
    name_map = {old: new for old, new in new_names_map_handling_encoding}
    
    # Add a ';1' after the first semicolon in the line
    with open(txt_output_path, 'r', encoding='latin-1') as original_file, open(final_rdf_path, 'w', encoding='utf-8') as new_file:
        for line in original_file:
            if line.startswith('&gt;'):
                parts = line.split(';', 1)
                line = f"{parts[0]};1;{parts[1]}"
                original_name = parts[0][1:]
                if original_name in name_map:
                    new_name = name_map[original_name]
                    line = line.replace(original_name, new_name, 1)
            new_file.write(line)
    
    os.remove(txt_output_path)  # Optionally remove the intermediate TXT file

# Define your paths
vcf_path = 'C:/Users/a/Desktop/Illumina_mtDNA.vcf'
fasta_output_path = 'C:/Users/a/Desktop/一个悲惨的即将被删除的文件.fasta'
txt_output_path = 'C:/Users/a/Desktop/一个中间文件.txt'
csv_path = 'C:/Users/a/Desktop/Illumina芯片分析表.csv'
final_rdf_path = 'C:/Users/a/Desktop/Final.rdf'
mapping_csv_path = 'C:/Users/a/Desktop/映射关系.csv'

# Execute the combined function with the necessary paths
try:
    vcf_processing_combined(vcf_path, fasta_output_path, txt_output_path, csv_path, final_rdf_path, mapping_csv_path)
    print("所有的文件都成功处理,并且已删除中间文件\n今天是一个好结局")
except Exception as e:
    print(f"中间出现了一些问题,请检查你的VCF文件和CSV文件\n今天是一个普通结局")

复制<br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301625347.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403301625795.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这个代码功能差不多，但是不包含重命名的步骤，可能没什么用。<br>import os
import pandas as pd

def vcf_processing_combined(vcf_path, fasta_output_path, txt_output_path, csv_path):
    # Load the CSV file
    csv_df = pd.read_csv(csv_path)
    
    # Create a dictionary to map IDs to their group and haplogroup
    id_to_info = {row['Object_ID']: f";{row['Re_Population_Province']};{row['Haplogrouper']}" for index, row in csv_df.iterrows()}
    
    # Process the VCF file
    sample_names = []
    sample_sequences = {}
    with open(vcf_path, 'r') as file:
        for line in file:
            if line.startswith('##'):
                continue
            elif line.startswith('#CHROM'):
                parts = line.strip().split('\t')
                sample_names = parts[9:]
                for name in sample_names:
                    additional_info = id_to_info.get(name, "")
                    sample_sequences[name] = [f"&gt;{name}{additional_info};\n"]
            else:
                parts = line.strip().split('\t')
                ref_base = parts[3]
                alt_bases = parts[4].split(',')
                for i, genotype_info in enumerate(parts[9:], start=9):
                    genotype = genotype_info.split(':')[0]
                    if '.' in genotype:
                        base = 'N'
                    elif '0/0' in genotype:
                        base = ref_base
                    else:
                        alleles = [int(allele) for allele in genotype.replace('.', '0').split('/') if allele.isdigit()]
                        base = ref_base if alleles[0] == 0 else alt_bases[alleles[0]-1]
                    sample_sequences[sample_names[i-9]].append(base)
    
    # Write the output FASTA file
    with open(fasta_output_path, 'w') as out_file:
        for sample, seq in sample_sequences.items():
            out_file.write(''.join(seq) + "\n")
    
    print(f"FASTA file has been successfully written to {fasta_output_path}")

# Example usage



# Call the function with the necessary paths


    # 然后，从相同的VCF文件生成txt文件，并在最后追加FASTA内容
    numbers = []
    with open(vcf_path, 'r') as vcf_file:
        numbers = [line.split('\t')[1] for line in vcf_file if not line.startswith('#')]
    count = len(numbers)
    with open(txt_output_path, 'w') as output_file:
        output_file.write("  ;1.0\n")
        output_file.write(';'.join(numbers) + ';')
        output_file.write('\n')
        output_file.write(';'.join(['10'] * count) + ';')

    # 从FASTA文件读取内容并追加到txt文件中
    with open(fasta_output_path, 'r') as fasta_file, open(txt_output_path, 'a') as txt_file:
        fasta_content = fasta_file.read()
        txt_file.write("\n")  # 添加分隔
        txt_file.write(fasta_content)
    
    print(f"Data has been successfully written and FASTA content appended to {txt_output_path}")
    print(f"Extracted {count} numbers.")
    if count == 100:
        print(';'.join(['10'] * count) + ';')

    # 文件重命名
    rdf_output_path = txt_output_path.rsplit('.', 1)[0] + '.rdf'
    os.remove(fasta_output_path)
    print("一个不合时宜的fasta文件已经被删除了。")
    
# 指定文件路径
vcf_path = 'C:/Users/a/Desktop/Illumina_mtDNA.vcf' # 替换为你的VCF文件路径
fasta_output_path = 'C:/Users/a/Desktop/Illumina_mtDNA.fasta' # 指定你的fasta文件输出路径
txt_output_path = 'C:/Users/a/Desktop/extracted_numbers_with_count_optimized.rdf' # 指定你的rdf文件输出路径
csv_path = 'C:/Users/a/Desktop/Illumina芯片分析表.csv'  # 替换为你的分组名单

# 执行函数
vcf_processing_combined(vcf_path, fasta_output_path, txt_output_path, csv_path)


复制]]></description><link>软件\python\数据科学与格式转换\python：将vcf文件转为rdf并重新命名id.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：将VCF文件转为rdf并重新命名ID.md</guid><pubDate>Fri, 21 Jun 2024 07:31:22 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403292241495.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403292241495.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）]]></title><description><![CDATA[ 
 <br>更新内容：
2024年10月23日16:25:17：优化代码逻辑，提高速度
<br>在利用单倍群频率进行 PCA 分析的时候，需要将单倍群划分到合理的级别。我们可以参考之前的研究来将单倍群进行统一。例如，我们可以参考这篇文献 <a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>。附表中存在这样：<br>Haplogroup	Anhui	Beijing	Chongqing	Fujian	Gansu	Guangxi	Guangdong	Guizhou
A9.35 8.46 5.75 4.84 9.03 4.05 3.97 8.28 
B410.43 10.24 11.50 11.94 11.94 12.61 12.23 8.28 
B54.34 4.01 6.42 5.32 3.23 6.76 5.95 10.19 
B60.00 0.00 0.00 0.00 0.00 0.00 0.51 0.00 
C3.39 4.01 3.98 2.10 5.16 4.05 3.07 2.55 
D10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
D20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
D415.99 19.71 14.38 12.74 17.42 4.95 8.90 17.83 
D57.32 6.68 5.31 8.23 6.45 2.25 4.48 4.46 
D60.14 0.45 0.44 0.32 0.32 0.90 0.45 0.64 
E0.14 0.00 0.44 0.16 0.00 0.00 0.06 0.00 
F0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
F17.86 8.57 8.63 10.48 10.00 15.32 12.93 12.74 
F23.93 4.57 3.54 4.52 2.58 4.50 3.65 3.82 
F30.68 0.89 2.21 0.97 0.32 2.25 2.37 1.27 
F40.95 1.00 0.44 1.13 0.65 1.35 1.02 0.00 
G0.00 0.11 0.00 0.00 0.00 0.00 0.06 0.00 
G11.76 2.23 2.88 1.29 0.65 0.45 1.15 0.64 
G23.93 3.12 2.88 2.90 3.87 1.80 1.92 3.18 
G31.36 0.67 0.22 0.32 0.65 0.45 0.26 0.00 
G40.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 
H0.41 0.22 0.22 0.00 1.61 0.00 0.00 0.00 
I0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
J0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
K0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 
M0.00 0.11 0.00 0.00 0.00 0.45 0.00 0.00 
.....
复制<br>那么，我们可按照第一列的级别进行筛选。也就是说，将所有下游单倍群全部统一到我们制定的级别中去。<br>
将第一列的所有单倍群保存至一个文档，例如 目标单倍群.txt：<br>M
M7
M8
C
Z
M9
E
Q
D4
D5
D6
O
S
N1
N2
N9
A
X
Y
W
I
R0
JT
R9
B4
U
HV
P
N
R
F1
F2
F3
F4
B5
M1'20'51
M2
M3
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M5
M6
M10
M12'G
M1'20'51
M2
M3
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M4"67
M5
M6
M10
M12'G
M13'46'61
M13'46'61
M14
M15
M17
M19'53
M19'53
M21
M22
M23'75
M23'75
M24'41
M24'41
M25
M26
M27
M28
M29'Q
M31
M32'56
M32'56
M33
M34'57
M34'57
M35
M36
M39'70
M39'70
M40
M42'74
M42'74
M44
M47
M48
M49
M50
M52
M55'77
M55'77
M58
M59
M60
M62'68
M62'68
M69
M71
M72
M73'79
M73'79
M76
M81
M91
M80'D
N5
N3
N7
N8
N10
N11
N13
N14
N21
N22
O
S
R1
R2'JT
R5
R6
R7
R8
R12'21
R12'21
R14
R22
R23
R30
R31
R32
P
G1
G2
G3
G4
复制<br><br>只需要单倍群，例如准备一个名为 需要查询的单倍群.txt:<br>haplogroup
C5d
C5d1
C5d1
C7
C7
C7
C7
C7a2a
C7b
C7b
D2b
D2b2
D3
D3
D4a
D4a
D4a
D4a
D4a
......
复制<br><br>使用之前的脚本：<a data-href="Python：查找mtDNA上游单倍型（2024年10月23日更新）" href="软件\python\数据科学与格式转换\python：查找mtdna上游单倍型（2024年10月23日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Python：查找mtDNA上游单倍型（2024年10月23日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">。我们可以获得一些文件，最重要的是这个文件：逆序等级.txt。<br>Level_19	Level_18	Level_17	Level_16	Level_15	Level_14	Level_13	Level_12	Level_11	Level_10	Level_9	Level_8	Level_7	Level_6	Level_5	Level_4	Level_3	Level_2	Level_1	Level_0
C5dC5bC5CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C5d1C5dC5bC5CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C5d1C5dC5bC5CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7a2aC7a2C7aC7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7bC7aC7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
C7bC7aC7CCZM8ML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
D2bD2a'bD2D4e1D4e1'3D4eD4DM80'DML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
D2b2D2bD2a'bD2D4e1D4e1'3D4eD4DM80'DML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
D3D4b1cD4b1D4bD4DM80'DML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
D3D4b1cD4b1D4bD4DM80'DML3L3'4L3'4'6L2'3'4'6L2'3'4'5'6L1'2'3'4'5'6
复制<br>你需要认真检查每一步的总数量，因为这个脚本可能无法识别特殊字符*+-等！<br><br>为了把 M,N 和 R* 收上去，还需要一个 错误纠正.txt。<br>
点击这里： <a data-tooltip-position="top" aria-label="https://vip.123pan.cn/1835545223/%E6%96%87%E6%A1%A3%EF%BC%88%E5%85%B1%E4%BA%AB%EF%BC%89/%E9%94%99%E8%AF%AF%E7%BA%A0%E6%AD%A3.txt" rel="noopener nofollow" class="external-link" href="https://vip.123pan.cn/1835545223/%E6%96%87%E6%A1%A3%EF%BC%88%E5%85%B1%E4%BA%AB%EF%BC%89/%E9%94%99%E8%AF%AF%E7%BA%A0%E6%AD%A3.txt" target="_blank">下载地址</a><br><br>import pandas as pd
import re

# 定义预处理函数
def preprocess_haplogroup(haplogroup):
    # 删除特殊字符 * 和 -
    haplogroup = re.sub(r'[\*\-]', '', haplogroup)
    # 处理 + 及其后面的字符
    haplogroup = re.sub(r'\+.*', '', haplogroup)
    return haplogroup

# 读取文件内容
result_df = pd.read_csv(r'C:/Users/victo/Desktop/逆序等级.txt', delimiter='\t')
target_df = pd.read_csv(r'C:/Users/victo/Desktop/目标.txt', header=None, names=['Haplogroup'])

# 读取错误纠正文件
correction_df = pd.read_csv('C:/Users/victo/Desktop/错误纠正.txt', delimiter='\t', header=None, names=['Original', 'Correction'])

# 预处理目标文件
target_df['Haplogroup'] = target_df['Haplogroup'].apply(preprocess_haplogroup)

# 打开输出文件
output_file_path = r'C:/Users/victo/Desktop/最终.txt'
with open(output_file_path, 'w') as output_file:
    # 写入表头
    output_file.write('OriginalHaplogroup\tMatchedHaplogroup\n')
    
    # 遍历结果逆序文件的每一行
    for _, row in result_df.iterrows():
        original_haplogroup = row[0]
        matched_haplogroup = original_haplogroup  # 默认是其本身
        # 从最低等级到最高等级遍历
        for col in row:
            col = str(col).strip()  # 确保col是字符串类型并去除可能的空格
            if pd.notna(col) and target_df['Haplogroup'].str.fullmatch(col).any():
                matched_haplogroup = target_df[target_df['Haplogroup'].str.fullmatch(col)].iloc[0]['Haplogroup']
                break

        # 检查是否需要纠正
        if correction_df['Original'].str.fullmatch(matched_haplogroup).any():
            matched_haplogroup = correction_df[correction_df['Original'].str.fullmatch(matched_haplogroup)].iloc[0]['Correction']
        
        # 写入输出文件
        output_file.write(f"{original_haplogroup}\t{matched_haplogroup}\n")

print(f"结果已保存到 {output_file_path}")

复制<br><br>import pandas as pd
import re

# 定义预处理函数
def preprocess_haplogroup(haplogroup):
    # 删除特殊字符 * 和 -
    haplogroup = re.sub(r'[\*\-]', '', haplogroup)
    # 处理 + 及其后面的字符
    haplogroup = re.sub(r'\+.*', '', haplogroup)
    return haplogroup

# 读取文件内容
result_df = pd.read_csv(r'C:/Users/victo/Desktop/逆序等级.txt', delimiter='\t')

target_df = pd.read_csv(r'F:/OneDrive/文档（科研）/脚本/我的科研脚本/Python/母系专用/目标.txt', header=None, names=['Haplogroup'])
#target_df = pd.read_csv(r'F:/OneDrive/文档（科研）/脚本/我的科研脚本/Python/母系专用/目标粗糙.txt', header=None, names=['Haplogroup'])

# 读取错误纠正文件
correction_df = pd.read_csv('F:/OneDrive/文档（科研）/脚本/我的科研脚本/Python/母系专用/错误纠正.txt', delimiter='\t', header=None, names=['Original', 'Correction'])

# 预处理目标文件
target_df['Haplogroup'] = target_df['Haplogroup'].apply(preprocess_haplogroup)

# 将目标群体和纠正数据转为集合提高查找速度
target_set = set(target_df['Haplogroup'])
correction_dict = dict(zip(correction_df['Original'], correction_df['Correction']))

# 初始化结果列表
result_list = []

# 遍历结果逆序文件的每一行
for _, row in result_df.iterrows():
    original_haplogroup = row[0]
    matched_haplogroup = original_haplogroup  # 默认是其本身
    
    # 从最低等级到最高等级遍历，尽量减少字符串匹配操作
    for col in row:
        col = str(col).strip()  # 确保col是字符串类型并去除可能的空格
        if col in target_set:
            matched_haplogroup = col
            break

    # 检查是否需要纠正
    if matched_haplogroup in correction_dict:
        matched_haplogroup = correction_dict[matched_haplogroup]
    
    # 将结果加入结果列表
    result_list.append(f"{original_haplogroup}\t{matched_haplogroup}")

# 一次性写入文件
output_file_path = r'C:/Users/victo/Desktop/最终.txt'
with open(output_file_path, 'w') as output_file:
    output_file.write('OriginalHaplogroup\tMatchedHaplogroup\n')
    output_file.write('\n'.join(result_list))

print(f"结果已保存到 {output_file_path}")

复制<br>完成！<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240619223339.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>
<br>
<br>River Valleys Shaped the Maternal Genetic Landscape of Han Chinese<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\python\数据科学与格式转换\python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）.md</guid><pubDate>Wed, 23 Oct 2024 08:26:18 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[作用]]></title><description><![CDATA[ 
 <br><br>这个脚本是一个根据SPSS软件分析得出的主成分表，以及自己准备的分组文件转呈R语言的代码。<br>我真不知道第一个这样做的人是怎么想的。但是毫无疑问，他一定是牢牢记住了以下黄金准则：<br>
<br>不要注释你的代码：毕竟，你的代码是如此直观，任何人（包括六个月后的你）都能一眼看懂你的意图。谁需要那些烦人的注释呢？未来的你肯定会感激现在的自己，因为解读代码就像破解古代密码一样有趣。
<br>复制粘贴是最佳实践：为什么要浪费时间去理解代码的工作原理呢？当你在互联网上找到一个看起来可以工作的解决方案时，直接复制粘贴到你的项目中就行了。不需要修改，更不需要测试——它肯定能完美运行。
<br>忽略版本控制：版本控制系统，比如Git，实在是太复杂了。保存你所有代码的不同版本和备份，确保你能回溯到任何一个开发阶段——这听起来像是多余的工作。直接在一个文件上工作，用不同的文件名保存不同版本，比如final_project, final_project_really, final_project_really_this_time，这才是高效的方式。
<br>避免学习基础知识：谁需要了解数据结构和算法、计算机科学的基础概念呢？这些东西只会让编程变得更加复杂和难以理解。直接跳到最酷的框架和库，开始构建应用程序吧。当你遇到性能问题或不可解的错误时，一定是计算机的问题，不是你的。
<br>代码优化是浪费时间：如果你的程序可以运行，那么它就是完美的。不要浪费时间去优化代码或是使其运行得更快、更高效。用户完全不介意等待几分钟来加载你的应用程序，或是让他们的设备为了运行一个简单的任务而过热。
<br><br>
<br>分组文件
<br>====Central China====
Dong_Hunan
Gelao_Guizhou
Han_Henan
Tujia_Chongqing
Miao_Chongqing
Han_Chongqing
====East China====
Han_Fujian
Han_Shandong
She_Fujian
Tanka_Fujian
====Northwest====
Han_Shanxi
Baoan_Gansu
Dongxiang_Gansu
Han_Gansu
Han_Shaanxi
Hui_Gansu
Mongolian_InnerMongolia
Salar_Gansu
Yugu_Gansu
====South China====
Han_Guangdong
Han_Hainan
Hui_Hainan
Li_Hainan
====Southwest====
Zhuang_Guangxi
Bai_Guizhou
Bouyei_Guizhou
Dai_Yunnan
Dong_Guizhou
Gelao_Guizhou
Han_Guangxi
Han_Guizhou
Han_Sichuan
Hani_Yunnan
Hui_Guizhou
Hui_Sichuan
Manchu_Guizhou
Maonan_Guizhou
Miao_Guizhou
Miao_Sichuan
Mongolian_Guizhou
Nanjing_Guizhou
Qiang_Sichuan
Shui_Guizhou
Tibetan_Sichuan
Tibetan_Tibet
Tujia_Guizhou
Yao_Guizhou
Yi_Guizhou
Zhuang_Guangxi
复制<br>
<br>主成分分析结果
<br>,1,2,3,4,5,6,7
Han_Chongqing,0.966,,,,,,
Han_Guangdong,0.960,,-0.167,-0.128,,,
Hui_Hainan,0.946,,,,,,
Han_Guangxi,0.943,0.137,-0.212,-0.128,,,
Zhuang_Guangxi,0.936,0.162,-0.166,-0.158,,,
Han_Hainan,0.924,0.157,-0.214,,,,
Han_Shaanxi,0.904,-0.221,,0.107,-0.101,0.206,-0.133
Han_Henan,0.902,-0.249,,0.206,,,-0.126
Tujia_Guizhou,0.897,,,,,,
Dong_Guizhou,0.897,0.121,-0.209,-0.120,,,0.173
.....
.....
.....
复制<br>第一列是名称，第二三四列分别代表第几个主成分。<br><br># 此处填写主成分表格第一列为样本名的表格路径
poplist_path='C:/Users/a/Desktop/poplist.txt'
# 此处填写输出R脚本路径
out_path='C:/Users/a/Desktop/PCAplot.r'
poptext = open(poplist_path).readlines()
# 输出R脚本，编码为utf-8
pca = open(out_path,'w',encoding = 'utf-8')
pop_list=[]
# 写入R脚本头部
pca.writelines("""data=read.table("") # 替换为主成分表格第一列为样本名
pop=data[,1] 
ind=data[,1]
PC1=data[,2] # PC1所在列
PC2=data[,3] # PC2所在列
PC3=data[,4] # PC3所在列
#layout(matrix(c(1,2,3,4),2,2),widths=c(1,1),heights=c(1,1))
layout(matrix(c(1,2)),widths=c(6,6),heights=c(3,1))
par(mar=c(4,4,4,4))
plot(PC1,PC2,type="n",cex.axis=0.5,cex.lab=1,cex.main=1,mgp=c(1,0.1,0), tck=-0.005)
#par(mar = c(4, 4, 1, 0.5), bg = "yellow")   # 设置边距参数和背景色
par(pin=c(6,6))   #定义图形为6英寸宽，6英寸高
par(lwd=2,cex=1.5)   #线条为默认的2倍宽，符号为默认的1.5倍
#par(cex.axis=0.75,font.axis=3)   #坐标轴文字缩放为原来的75%，斜体
frame=data.frame(PC1=PC1,PC2=PC2,name=pop,region=pop)
# 请根据自己的需要替换颜色
to21=c("#EE0000","#0D6E6E","#BA55Dd9c","#87CEFA","#97FFFA",
               "#808000","#ADD8E6","#D2691E","#FFC0CB","#FF69B4","#EE82EE",
               "#BA55D3","#8A2BE2","#0000CD","#000080","#F4A460","#FFF5EE",
               "#FF7F50","#FF6347","#2F4F4F","#48D1CC","#7FFFAA","#8FBC8F",
               "#228B22","#7FFF00","#556B2F","#FFFFF0","#FDF5E6","#FFA500",
               "#FFDEAD","#DEB887","#FAF0E6","#8D5223","#6F96F2","#00CD66",
               "#ABF4DC","#F88BD5","#AB82FF","#FFC534","#3787B4","#FFDAB9",
               "#6B8E23","#FF8C69","#2B2B2C","#D02090","#FF1493","#20B2AA",
               "#708090","#FFFAFA","#CD5C5C","#B22222","#FFFFFF","#808080",
               "#D3D3D3","#F0FFFF","#C0C0C0","#F5FFFA","#F0FFF0","#E6E6FA")
#59 gray
########################
########################
""")
# 写入R脚本主体
j=-1
for i in poptext:
    i=i.strip()
    if i[0] == '=':
        j+=1
        pop_list.append([])
        pop_list[j].append(i.strip("="))
    else:
        pop_list[j].append(i)
for i in range(0,len(pop_list)):
    pca.writelines("".join("".join(pop_list[i][0].split(sep="-")).split())+'_col=to21['+str(i+1)+']\n')
for i in pop_list:
    pca.writelines('# '+i[0]+'\n')
    for j in range(1, len(i)):
        pca.writelines('reg="'+i[j]+'"\n')
        pca.writelines('points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch={},bg={}_col,col={}_col,cex=0.6)\n'.format(str(j-1),"".join("".join(i[0].split(sep="-")).split()),"".join("".join(i[0].split(sep="-")).split())))
pca.writelines("########################\n########################\n# legend\npar(mar=c(5,1,0,2))\nplot.new()\n")
# 写入R脚本尾部
pops=[]
for i in pop_list:
    pops.append('","'.join(i))
pops='"'+'","'.join(pops)+'"'
pca.writelines('pops=   c({})\n'.format(pops))
# cols,borders,symb,fonts
cols=[]
symb=[]
fonts=[]
for i in pop_list:
    cols.append('NA')
    symb.append('NA')
    fonts.append('2')
    for j in range(len(i)-1):
        cols.append("".join("".join(i[0].split(sep="-")).split())+"_col")
        symb.append(str(j))
        fonts.append(str(1))
cols = ",".join(cols)
symb = ",".join(symb)
fonts = ",".join(fonts)
# print(cols)
# print(symb)
# print(fonts)
pca.writelines('cols=   c({})\n'.format(cols))
pca.writelines('borders=c({})\n'.format(cols))
pca.writelines('symb=   c({})\n'.format(symb))
pca.writelines('fonts=  c({})\n'.format(fonts))
pca.writelines('''legend("top",pops,pch=symb,pt.bg=cols,col=borders,ncol=7,
               cex=0.3,pt.cex=0.4,bty="o",text.font=fonts,xpd=TRUE) 
               # ncol代表一行显示几个，cex代表字体大小，pt.cex代表点的大小
               # text.font代表字体，bty代表形状，xpd代表是否显示在页面外面''')
复制<br><br>你可以得到这样的R代码：<br>data=read.table("") # 替换为主成分表格第一列为样本名
pop=data[,1] 
ind=data[,1]
PC1=data[,2] # PC1所在列
PC2=data[,3] # PC2所在列
PC3=data[,4] # PC3所在列
#layout(matrix(c(1,2,3,4),2,2),widths=c(1,1),heights=c(1,1))
layout(matrix(c(1,2)),widths=c(6,6),heights=c(3,1))
par(mar=c(4,4,4,4))
plot(PC1,PC2,type="n",cex.axis=0.5,cex.lab=1,cex.main=1,mgp=c(1,0.1,0), tck=-0.005)
#par(mar = c(4, 4, 1, 0.5), bg = "yellow")   # 设置边距参数和背景色
par(pin=c(6,6))   #定义图形为6英寸宽，6英寸高
par(lwd=2,cex=1.5)   #线条为默认的2倍宽，符号为默认的1.5倍
#par(cex.axis=0.75,font.axis=3)   #坐标轴文字缩放为原来的75%，斜体
frame=data.frame(PC1=PC1,PC2=PC2,name=pop,region=pop)
# 请根据自己的需要替换颜色
to21=c("#EE0000","#0D6E6E","#BA55Dd9c","#87CEFA","#97FFFA",
               "#808000","#ADD8E6","#D2691E","#FFC0CB","#FF69B4","#EE82EE",
               "#BA55D3","#8A2BE2","#0000CD","#000080","#F4A460","#FFF5EE",
               "#FF7F50","#FF6347","#2F4F4F","#48D1CC","#7FFFAA","#8FBC8F",
               "#228B22","#7FFF00","#556B2F","#FFFFF0","#FDF5E6","#FFA500",
               "#FFDEAD","#DEB887","#FAF0E6","#8D5223","#6F96F2","#00CD66",
               "#ABF4DC","#F88BD5","#AB82FF","#FFC534","#3787B4","#FFDAB9",
               "#6B8E23","#FF8C69","#2B2B2C","#D02090","#FF1493","#20B2AA",
               "#708090","#FFFAFA","#CD5C5C","#B22222","#FFFFFF","#808080",
               "#D3D3D3","#F0FFFF","#C0C0C0","#F5FFFA","#F0FFF0","#E6E6FA")
#59 gray
########################
########################
CentralChina_col=to21[1]
EastChina_col=to21[2]
Northwest_col=to21[3]
SouthChina_col=to21[4]
Southwest_col=to21[5]
# Central China
reg="Dong_Hunan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=0,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
reg="Gelao_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=1,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
reg="Han_Henan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=2,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
reg="Tujia_Chongqing"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=3,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
reg="Miao_Chongqing"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=4,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
reg="Han_Chongqing"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=5,bg=CentralChina_col,col=CentralChina_col,cex=0.6)
# East China
reg="Han_Fujian"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=0,bg=EastChina_col,col=EastChina_col,cex=0.6)
reg="Han_Shandong"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=1,bg=EastChina_col,col=EastChina_col,cex=0.6)
reg="She_Fujian"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=2,bg=EastChina_col,col=EastChina_col,cex=0.6)
reg="Tanka_Fujian"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=3,bg=EastChina_col,col=EastChina_col,cex=0.6)
# Northwest
reg="Han_Shanxi"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=0,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Baoan_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=1,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Dongxiang_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=2,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Han_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=3,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Han_Shaanxi"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=4,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Hui_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=5,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Mongolian_InnerMongolia"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=6,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Salar_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=7,bg=Northwest_col,col=Northwest_col,cex=0.6)
reg="Yugu_Gansu"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=8,bg=Northwest_col,col=Northwest_col,cex=0.6)
# South China
reg="Han_Guangdong"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=0,bg=SouthChina_col,col=SouthChina_col,cex=0.6)
reg="Han_Hainan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=1,bg=SouthChina_col,col=SouthChina_col,cex=0.6)
reg="Hui_Hainan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=2,bg=SouthChina_col,col=SouthChina_col,cex=0.6)
reg="Li_Hainan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=3,bg=SouthChina_col,col=SouthChina_col,cex=0.6)
# Southwest
reg="Zhuang_Guangxi"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=0,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Bai_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=1,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Bouyei_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=2,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Dai_Yunnan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=3,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Dong_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=4,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Gelao_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=5,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Han_Guangxi"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=6,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Han_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=7,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Han_Sichuan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=8,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Hani_Yunnan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=9,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Hui_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=10,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Hui_Sichuan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=11,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Manchu_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=12,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Maonan_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=13,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Miao_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=14,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Miao_Sichuan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=15,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Mongolian_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=16,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Nanjing_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=17,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Qiang_Sichuan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=18,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Shui_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=19,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Tibetan_Sichuan"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=20,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Tibetan_Tibet"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=21,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Tujia_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=22,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Yao_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=23,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Yi_Guizhou"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=24,bg=Southwest_col,col=Southwest_col,cex=0.6)
reg="Zhuang_Guangxi"
points(subset(frame,region==reg)$PC1,subset(frame,region==reg)$PC2,pch=25,bg=Southwest_col,col=Southwest_col,cex=0.6)
########################
########################
# legend
par(mar=c(5,1,0,2))
plot.new()
pops=   c("Central China","Dong_Hunan","Gelao_Guizhou","Han_Henan","Tujia_Chongqing","Miao_Chongqing","Han_Chongqing","East China","Han_Fujian","Han_Shandong","She_Fujian","Tanka_Fujian","Northwest","Han_Shanxi","Baoan_Gansu","Dongxiang_Gansu","Han_Gansu","Han_Shaanxi","Hui_Gansu","Mongolian_InnerMongolia","Salar_Gansu","Yugu_Gansu","South China","Han_Guangdong","Han_Hainan","Hui_Hainan","Li_Hainan","Southwest","Zhuang_Guangxi","Bai_Guizhou","Bouyei_Guizhou","Dai_Yunnan","Dong_Guizhou","Gelao_Guizhou","Han_Guangxi","Han_Guizhou","Han_Sichuan","Hani_Yunnan","Hui_Guizhou","Hui_Sichuan","Manchu_Guizhou","Maonan_Guizhou","Miao_Guizhou","Miao_Sichuan","Mongolian_Guizhou","Nanjing_Guizhou","Qiang_Sichuan","Shui_Guizhou","Tibetan_Sichuan","Tibetan_Tibet","Tujia_Guizhou","Yao_Guizhou","Yi_Guizhou","Zhuang_Guangxi")
cols=   c(NA,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,NA,EastChina_col,EastChina_col,EastChina_col,EastChina_col,NA,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,NA,SouthChina_col,SouthChina_col,SouthChina_col,SouthChina_col,NA,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col)
borders=c(NA,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,CentralChina_col,NA,EastChina_col,EastChina_col,EastChina_col,EastChina_col,NA,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,Northwest_col,NA,SouthChina_col,SouthChina_col,SouthChina_col,SouthChina_col,NA,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col,Southwest_col)
symb=   c(NA,0,1,2,3,4,5,NA,0,1,2,3,NA,0,1,2,3,4,5,6,7,8,NA,0,1,2,3,NA,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)
fonts=  c(2,1,1,1,1,1,1,2,1,1,1,1,2,1,1,1,1,1,1,1,1,1,2,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
legend("top",pops,pch=symb,pt.bg=cols,col=borders,ncol=7,
               cex=0.3,pt.cex=0.4,bty="o",text.font=fonts,xpd=TRUE) 
               # ncol代表一行显示几个，cex代表字体大小，pt.cex代表点的大小
               # text.font代表字体，bty代表形状，xpd代表是否显示在页面外面
复制]]></description><link>软件\python\数据科学与格式转换\python：利用python将主成分表转换成r脚本.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：利用Python将主成分表转换成R脚本.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f975.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：利用Python快速填写频率表]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！
更好的办法建议使用 excel软件数据透视表功能！
<br>这个脚本是用来对单倍群的数量和频率进行计算的。<br>
需要准备2个文件：<br>
<br>
填表的模板：第一行是省份（或者你希望分组的依据）；第一列是每种单倍群的名称（不要重复）

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011810364.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br>
数据：第一列是单倍群，第二列是省份（或者你希望分组的依据）（不要去重）表包含标题。

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011808333.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br><br>import pandas as pd
from collections import Counter
import numpy as np

# 读取数据文件
data_txt_path = 'C:/Users/a/Desktop/数据.txt'  # 修改为您的 data.txt 文件路径
with open(data_txt_path, 'r', encoding='utf-8') as file:
    lines = file.readlines()

# 解析数据，获取每行的单倍群和地点
haplogroup_location_pairs = [line.strip().split('\t') for line in lines[1:]]  # 忽略标题行

# 初始化一个字典来存储每个地区的单倍群计数
location_haplogroup_counts = {}
for haplogroup, location in haplogroup_location_pairs:
    if location not in location_haplogroup_counts:
        location_haplogroup_counts[location] = Counter()
    location_haplogroup_counts[location][haplogroup] += 1

# 读取需要更新的 CSV 文件
csv_path = 'C:/Users/a/Desktop/填表.csv'  # 修改为您的 填表.csv 文件路径
df_csv = pd.read_csv(csv_path)

# 更新 DataFrame 的每个地区列
for location, counts in location_haplogroup_counts.items():
    for haplogroup, count in counts.items():
        if haplogroup in df_csv['Unnamed: 0'].values:
            df_csv.loc[df_csv['Unnamed: 0'] == haplogroup, location] = count

# 填充 NaN 为 0，方便计算频率
df_csv_filled = df_csv.fillna(0)

# 计算每个地区的单倍群频率
for location in df_csv_filled.columns[1:]:  # 跳过第一列（单倍群名称）
    total = df_csv_filled[location].sum()
    if total &gt; 0:  # 避免除以0
        df_csv_filled[location] = df_csv_filled[location] / total

# 保存计算频率后的 DataFrame 到新的 CSV 文件
frequency_csv_path = 'C:/Users/a/Desktop/frequency_填表.csv'  # 指定新的文件路径
df_csv_filled.to_csv(frequency_csv_path, index=False)

print(f"频率计算后的 CSV 文件已保存至: {frequency_csv_path}")

复制<br><br>最终得到这样一张表格：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403012108205.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>可以直接使用下列代码进行绘图：<br># 在开始之前请确保你已经安装了下列package
# 载入需要的package
library(ape)
library(igraph)
library(ggplot2)
library(pheatmap)
library(reshape2)
library(ggsci)
library(gridExtra)

# 设置工作路径，你需要把Fst文件和分组文件放到工作路径
setwd("C:/Users/a/Desktop")

# 清除内存
rm(list=ls())

# 读取数据
mydata&lt;-read.table("fst.csv",header=TRUE,sep=",", row.names = 1)
# group &lt;-read.table("group.csv",header=TRUE,sep=",", row.names = 1)

# 创建绘图PDF
pdf("FstMatrix1.pdf", width=11, height=8.5)

# 绘制图像
pheatmap(mydata, 
         cluster_cols=TRUE, # 建议换成FALSE
         cluster_rows=TRUE, # 建议换成FALSE
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group, 
         annotation_row = group,
         cellwidth =8, 
         cellheight = 8, 
         cutree_cols=4, # 建议换成1
         cutree_rows=4, # 建议换成1
         main = "FstMatrix",
         color = colorRampPalette(c("#00A087FF","#3C5488FF","#F39B7FFF"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))
# 结束绘图
dev.off()

# 换个样式
pheatmap(mydata, 
         cluster_cols=TRUE, 
         cluster_rows=TRUE, 
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group, 
         annotation_row = group,
         cellwidth =8, 
         cellheight = 8, 
         cutree_cols=4,
         cutree_rows=4, 
         main = "FstMatrix",
         color = colorRampPalette(c("#F8F8FF","#91D1C2FF","#3C5488FF"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))

# 换个样式
pheatmap(mydata,
         cluster_cols=TRUE,
         cluster_rows=TRUE,
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group,
         annotation_row = group,
         cellwidth =8,
         cellheight = 8,
         cutree_cols=4,
         cutree_rows=4,
         main = "FstMatrix",
         color = colorRampPalette(c("#20364F","#31646C","#4E9280","#96B89B","#DCDFD2","#ECD9CF","#D49C87","#B86265","#8B345E","#50184E"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))

# 换个样式
pheatmap(mydata,
         cluster_cols=TRUE,
         cluster_rows=TRUE,
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group,
         annotation_row = group,
         cellwidth =8,
         cellheight = 8,
         cutree_cols=4,
         cutree_rows=4,
         main = "FstMatrix",
         color = colorRampPalette(c("#023047","#126883","#279EBC","#90C9E6","#FC9E7F","#F75B41","#D52120"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))

复制]]></description><link>软件\python\数据科学与格式转换\python：利用python快速填写频率表.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：利用Python快速填写频率表.md</guid><pubDate>Fri, 23 Aug 2024 08:25:53 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011810364.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011810364.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[输入文件]]></title><description><![CDATA[ 
 <br>相关链接如下：
<a data-href="python：根据单倍群频率使用sklearn进行降维大礼包" href="软件\python\数据科学与格式转换\python：根据单倍群频率使用sklearn进行降维大礼包.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>python：根据单倍群频率使用sklearn进行降维大礼包</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;"><br>
<a data-href="Nat. Methods 科学家如何看待高维数据降维，关于PCA、t-SNE与UMAP的应用与挑战" href="文献及报道\报道\nat.-methods-科学家如何看待高维数据降维，关于pca、t-sne与umap的应用与挑战.html" class="internal-link" target="_self" rel="noopener nofollow">Nat. Methods 科学家如何看待高维数据降维，关于PCA、t-SNE与UMAP的应用与挑战</a>
<br><br>需要一个人群的 单倍群频率表格。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410231734929.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
请保存为 txt 文件，以制表符分割。<br><br>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import plotly.express as px

# 加载数据
file_path = 'C:/Users/victo/Desktop/新建 Text Document.txt'
data = pd.read_csv(file_path, sep='\t')

# 去掉第一列（假设是非数值数据，例如样本名称）
pca_data = data.drop(columns=data.columns[0])

# 执行PCA
dimension = int(input("请输入要保留的维度数(例如2)："))  # 确保输入的是整数
pca = PCA(n_components=dimension)
pca_results = pca.fit_transform(pca_data.T)  # T表示转置
explained_variance_ratio = pca.explained_variance_ratio_

# 将解释度写入文件
with open('C:/Users/victo/Desktop/PCA降维结果解释度.txt', 'w') as f:
    f.write('Explained Variance Ratio:\n')
    for i, ratio in enumerate(explained_variance_ratio):
        f.write(f'Component {i+1}: {ratio:.4f}\n')

# 生成PCA结果的DataFrame
# 动态生成主成分列名
columns = ['PC' + str(i+1) for i in range(dimension)]
pca_results_df = pd.DataFrame(pca_results, columns=columns)
pca_results_df['Sample Name'] = pca_data.columns

# 保存结果到txt文件
pca_results_df.to_csv('C:/Users/victo/Desktop/PCA结果.txt', sep='\t', index=False)

# 可视化PCA结果（只绘制前两个主成分）
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PC1', y='PC2', data=pca_results_df)
plt.title('PCA Visualization')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.savefig('C:/Users/victo/Desktop/PCA可视化.png')
plt.show()

# 使用Plotly进行可视化
fig = px.scatter(pca_results_df, x='PC1', y='PC2', text='Sample Name', title='PCA Visualization')
fig.update_traces(marker=dict(size=12, opacity=0.8), selector=dict(mode='markers'))
fig.show()

复制<br>运行代码，将提示 请输入要保留的维度数(例如2)：，直接输入数字按 回车 即可。]]></description><link>软件\python\数据科学与格式转换\python：利用sklearn降维pca.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/Python：利用sklearn降维PCA.md</guid><pubDate>Wed, 23 Oct 2024 09:35:45 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[什么是VCF]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！
更好的办法建议使用 <a data-href="python：将FASTA转换成Arp" href="软件\python\数据科学与格式转换\python：将fasta转换成arp.html" class="internal-link" target="_self" rel="noopener nofollow">python：将FASTA转换成Arp</a>
<br><br>VCF 是生物信息分析中非常重要的一种格式。主要用来描述基因组突变的信息，无论是检测出来的 SNP，indel，cnv，还是 SV，都可以存储格式都为 vcf 格式。从比对生成的 bam 文件中，将潜在变异信息筛选出来，就是 vcf 格式。vcf 是一种列表格式，里面包含很多的内容。需要掌握每一列的信息，并能使用相对应的软件对 vcf 进行处理。处理 VCF 格式软件主要包括 bcftools，vcftools，gatk，python pyvcf，plink 等。<br><br>Arlequin软件能够处理Arp格式的文件，从而进行<a data-href="分子方差分析（AMOVA ，Analysis of Molecular Variance)" href="术语\分子方差分析（amova-，analysis-of-molecular-variance).html" class="internal-link" target="_self" rel="noopener nofollow">分子方差分析（AMOVA ，Analysis of Molecular Variance)</a>和<a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a>。<br>但是需要经历从DnaSP软件对群体进行分类的操作，DnaSP软件手工标记十分费时，所以可以进行如下脚本对VCF文件进行批量分组。<br><br>下载地址：<a data-tooltip-position="top" aria-label="https://github.com/Sandman2127/VCF2ArlequinDiploid" rel="noopener nofollow" class="external-link" href="https://github.com/Sandman2127/VCF2ArlequinDiploid" target="_blank">Sandman2127/VCF2ArlequinDiploid: A script designed to convert GBS/RAD/DNASEQ vcf data to arlequin diploid .arp format files (github.com)</a><br>服务器。<br>前期工作，将VCF文件中的ID号码整理出来，然后用制表符分割写下每个ID对应的分组，这里以地区进行分类。得到分组文件。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401311442808.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
<br>将压缩包放在服务器内，然后解压。
<br>准备好上述的VCF文件和分组文件。
<br>进入终端。
<br>输入以下代码。
<br># 将软件路径、VCF路径和分群文本路径调整为===绝对路径===
python3 /home/luolintao/VCF2ArlequinDiploid/vcf2ArlequinDiploid.py --vcf /home/luolintao/VCF2ArlequinDiploid/Illumina_mtDNA_origin.vcf --popFile /home/luolintao/VCF2ArlequinDiploid/Group.txt --splitContigs  --debug      
复制<br>运行终端即可。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401311443027.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>]]></description><link>软件\python\数据科学与格式转换\python：批量修改vcf文件至arp格式.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：批量修改VCF文件至Arp格式.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44e.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[全矩阵转为半矩阵]]></title><description><![CDATA[ 
 <br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051449614.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import pandas as pd
import numpy as np

# 读取文件
file_path = '全矩阵.txt'
data = pd.read_csv(file_path, sep='\t', header=None)

# 删除对角线及右上角的内容
mask = np.tril(np.ones(data.shape)).astype(bool)
lower_triangle_data = data.where(mask)

# 将对角线上的0替换为空值
np.fill_diagonal(lower_triangle_data.values, np.nan)

# 保存结果到新的文件，删除第一行的索引
output_file_path = 'C:/Users/victo/Desktop/lower_triangle_matrix.txt'
lower_triangle_data.to_csv(output_file_path, sep="\t", index=False, header=False)

output_file_path
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051450539.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051450539.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import numpy as np
import pandas as pd

# 读取文件内容
file_path = 'lower_triangle_matrix.txt'
with open(file_path, 'r') as file:
    lines = file.readlines()

# 清理并处理文件中的每一行
processed_lines = []
for line in lines:
    # 移除末尾的制表符并将多个制表符替换为单个制表符
    cleaned_line = '\t'.join(line.strip().split())
    # 将数字字符串转换为浮点数列表，忽略空的条目
    float_numbers = [float(num) for num in cleaned_line.split('\t') if num]
    processed_lines.append(float_numbers)

# 确定矩阵的最大列数
max_length = max(len(line) for line in processed_lines)

# 填充行，使其具有相等的长度
for line in processed_lines:
    while len(line) &lt; max_length:
        line.append(0.0)

# 将处理后的数据转换为NumPy数组
matrix = np.array(processed_lines)

# 将左下部分镜像填充到右上部分
for i in range(matrix.shape[0]):
    for j in range(i+1, matrix.shape[1]):
        matrix[i, j] = matrix[j, i]

# 将矩阵转换为DataFrame以便于可视化
df = pd.DataFrame(matrix)
df.to_csv('全矩阵.txt', index=False, header=False)
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407051449614.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：全矩阵与半矩阵的相互转化.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：全矩阵与半矩阵的相互转化.md</guid><pubDate>Fri, 05 Jul 2024 06:52:35 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f0cf.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f0cf.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：删除fasta中的指定位点]]></title><description><![CDATA[ 
 <br>from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord

def remove_positions_from_fasta(input_fasta, output_fasta, positions):
    positions = sorted(positions, reverse=True)  # 逆序排序，先删除靠后的位点
    sequences = SeqIO.parse(input_fasta, "fasta")
    modified_sequences = []

    for seq_record in sequences:
        seq_str = str(seq_record.seq)
        seq_list = list(seq_str)  # 转换为列表，方便修改
        for pos in positions:
            if pos &lt;= len(seq_list):
                del seq_list[pos - 1]  # 删除指定位置的碱基
        modified_seq_str = ''.join(seq_list)
        modified_seq_record = SeqRecord(Seq(modified_seq_str), id=seq_record.id, description=seq_record.description)
        modified_sequences.append(modified_seq_record)
    
    SeqIO.write(modified_sequences, output_fasta, "fasta")

input_fasta = "C:/Users/victo/Desktop/最大似然树修位点.fasta"  # 输入文件名
output_fasta = "C:/Users/victo/Desktop/output.fasta"  # 输出文件名

# 指定要删除的位点
positions_to_remove = [254, 310, 311, 313, 314, 315, 316, 317, 318, 323, 530, 531, 538, 539, 540, 
                       541, 542, 543, 544, 547, 548, 592, 594, 595, 596, 597, 989, 5935, 5936, 
                       5937, 8324, 16238, 16239]

remove_positions_from_fasta(input_fasta, output_fasta, positions_to_remove)

复制]]></description><link>软件\python\数据科学与格式转换\python：删除fasta中的指定位点.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：删除fasta中的指定位点.md</guid><pubDate>Fri, 21 Jun 2024 07:31:26 GMT</pubDate></item><item><title><![CDATA[文件准备]]></title><description><![CDATA[ 
 <br>有时候需要根据一定的条件把 Illustrator 中的文字进行颜色变更或者字体设置等。这个代码可以批量生成。<br><br>一个 txt 文件：<br>Han_Jiangsu
Han_Zhejiang
Han_Shandong
Han_Guangdong
Han_Sichuan
Han_Hebei
Han_Shanghai
Han_Henan
Han_Beijing
Han_Hubei
Han_Hunan
Uyghur_Xinjiang
复制<br>里面是需要更改颜色的文字。<br><br># 读取文件内容
with open('新建 Text Document (2).txt', 'r', encoding='utf-8') as file:
    lines = file.readlines()

# 生成JavaScript代码
template = """
var doc = app.activeDocument;
var textFrames = doc.textFrames;
var targetText;
var changed = 0;
"""

# 添加每个targetText的JavaScript代码
for line in lines:
    target_text = line.strip()
    template += f"""
targetText = "{target_text}";
for (var i = 0; i &lt; textFrames.length; i++) {{
    if (textFrames[i].contents == targetText) {{
        textFrames[i].textRange.characterAttributes.fillColor = new RGBColor();
        textFrames[i].textRange.characterAttributes.fillColor.red = 255;
        textFrames[i].textRange.characterAttributes.fillColor.green = 0;
        textFrames[i].textRange.characterAttributes.fillColor.blue = 0;
        changed++;
    }}
}}
"""

# 添加alert语句
template += """
alert(changed + ' text items changed to red.');
"""

# 保存生成的JavaScript代码到文件
with open('1.jsx', 'w', encoding='utf-8') as file:
    file.write(template)

print("JavaScript代码生成完毕并保存为generated_script.js")

复制<br><br>生成 jsx 代码，然后用 Adobe Illustrator 进行：<br>
文件→脚本→其它脚本 。或者快捷键 ctrl+F12。<br>
选择 jsx 即可。]]></description><link>软件\python\数据科学与格式转换\python：生成变换字体颜色代码给jsx.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：生成变换字体颜色代码给jsx.md</guid><pubDate>Tue, 23 Jul 2024 09:12:44 GMT</pubDate></item><item><title><![CDATA[箱型图]]></title><description><![CDATA[ 
 <br><br>箱型图可以展示数据分布的情况，是否存在离群值等。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408231612741.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一列：个体或群体或 ID 等。<br>
第二列：第一个属性。<br>
第三列：第二个属性。<br>
第四列：第三个属性。<br>
第五列：第四个属性。<br><br>import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import f_oneway, kruskal

# 读取上传的文本文件内容
file_path = '新建 Text Document.txt'

# 读取数据为DataFrame
df = pd.read_csv(file_path, sep="\t")

# 根据经度和纬度分组（使用分位数进行分组）
df['Longitude_group'] = pd.qcut(df['Longitude'], q=4, labels=["Q1", "Q2", "Q3", "Q4"])
df['Latitude_group'] = pd.qcut(df['Latitude'], q=4, labels=["Q1", "Q2", "Q3", "Q4"])

# 设置字体和图形参数
plt.rcParams['pdf.fonttype'] = 42  # 使字体在矢量图形中可编辑
plt.rcParams['ps.fonttype'] = 42

# 绘制PC1和PC2相对于经度和纬度分组的箱线图
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 颜色设置，使用SCI配色方案（示例颜色）
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

# PC1 vs Longitude group
df.boxplot(column='PC1', by='Longitude_group', ax=axes[0, 0], patch_artist=True,
           boxprops=dict(facecolor=colors[0]))
axes[0, 0].set_title('PC1 vs Longitude Group')
axes[0, 0].set_xlabel('Longitude Group')
axes[0, 0].set_ylabel('PC1')

# PC2 vs Longitude group
df.boxplot(column='PC2', by='Longitude_group', ax=axes[0, 1], patch_artist=True,
           boxprops=dict(facecolor=colors[1]))
axes[0, 1].set_title('PC2 vs Longitude Group')
axes[0, 1].set_xlabel('Longitude Group')
axes[0, 1].set_ylabel('PC2')

# PC1 vs Latitude group
df.boxplot(column='PC1', by='Latitude_group', ax=axes[1, 0], patch_artist=True,
           boxprops=dict(facecolor=colors[2]))
axes[1, 0].set_title('PC1 vs Latitude Group')
axes[1, 0].set_xlabel('Latitude Group')
axes[1, 0].set_ylabel('PC1')

# PC2 vs Latitude group
df.boxplot(column='PC2', by='Latitude_group', ax=axes[1, 1], patch_artist=True,
           boxprops=dict(facecolor=colors[3]))
axes[1, 1].set_title('PC2 vs Latitude Group')
axes[1, 1].set_xlabel('Latitude Group')
axes[1, 1].set_ylabel('PC2')

# 去掉灰色背景线条
for ax in axes.flatten():
    ax.grid(False)  # 去掉网格线
    ax.set_facecolor('white')  # 背景设为白色

# 调整图像布局
plt.suptitle('PC1 and PC2 vs Longitude and Latitude Groups')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

plt.show()

# 分别对PC1和PC2在不同经度组和纬度组之间进行ANOVA和Kruskal-Wallis检验
results = []

# ANOVA和Kruskal-Wallis检验函数
def perform_tests(column, group, group_type):
    anova_result = f_oneway(*[group[column][group[group_type] == g] for g in group[group_type].unique()])
    kruskal_result = kruskal(*[group[column][group[group_type] == g] for g in group[group_type].unique()])
    return anova_result, kruskal_result

# 进行PC1和PC2在经度组之间的检验并保存结果
for col in ['PC1', 'PC2']:
    anova_res, kruskal_res = perform_tests(col, df, 'Longitude_group')
    results.append({
        'Test': f'{col} vs Longitude',
        'ANOVA F-value': anova_res.statistic,
        'ANOVA p-value': anova_res.pvalue,
        'Kruskal-Wallis H-value': kruskal_res.statistic,
        'Kruskal-Wallis p-value': kruskal_res.pvalue
    })

# 进行PC1和PC2在纬度组之间的检验并保存结果
for col in ['PC1', 'PC2']:
    anova_res, kruskal_res = perform_tests(col, df, 'Latitude_group')
    results.append({
        'Test': f'{col} vs Latitude',
        'ANOVA F-value': anova_res.statistic,
        'ANOVA p-value': anova_res.pvalue,
        'Kruskal-Wallis H-value': kruskal_res.statistic,
        'Kruskal-Wallis p-value': kruskal_res.pvalue
    })

# 将结果转换为DataFrame
results_df = pd.DataFrame(results)

# 保存结果为CSV文件
output_file_path = 'statistical_test_results.csv'
results_df.to_csv(output_file_path, index=False)

print(f"统计检验结果已保存到: {output_file_path}")

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408231615311.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>Test    ANOVA F-value   ANOVA p-value   Kruskal-Wallis H-value  Kruskal-Wallis p-value
PC1 vs Longitude    2.861671142 0.043636961 8.457800512 0.037439795
PC2 vs Longitude    19.4674561  4.41484E-09 34.94433579 1.25169E-07
PC1 vs Latitude 91.85140563 3.75754E-23 54.52219046 8.68241E-12
PC2 vs Latitude 5.416225929 0.002202674 17.29517075 0.000614511
复制<br><br>图：PC1 和 PC2 在不同经纬度分组下的分布情况：此图展示了PC1和PC2在按经度和纬度分组后的箱线图。各个组别分别根据样本的经度和纬度按分位数（Q1至Q4）分组，显示了不同地理位置对主成分（PC1和PC2）的影响。箱线图的中线表示各组的中位数，箱体代表四分位间距（IQR），而须线则延伸至不超过1.5倍IQR的范围。<br>
A. PC1与经度分组：显示了PC1在不同经度分组中的分布情况。可以观察到，PC1在经度上的分布存在统计显著性差异（ANOVA F = 2.8617, p = 0.0436; Kruskal-Wallis H = 8.4578, p = 0.0374）。<br>
B. PC2与经度分组：显示了PC2在不同经度分组中的分布情况。经度对PC2的影响显著（ANOVA F = 19.4675, p &lt; 0.0001; Kruskal-Wallis H =34.9443, p &lt; 0.0001）。<br>
C. PC1与纬度分组：显示了PC1在不同纬度分组中的分布情况。PC1在纬度上的分布同样表现出统计显著性（ANOVA F = 91.8514, p &lt; 0.0001; Kruskal-Wallis H = 54.5222, p &lt; 0.0001）。<br>
D. PC2与纬度分组：显示了PC2在不同纬度分组中的分布情况。PC2在纬度上的差异性极为显著（ANOVA F = 5.4162, p = 0.0022; Kruskal-Wallis H = 17.2951, p &lt; 0.0001）。<br>
此图表明，不同的地理位置（经度和纬度）对PC1和PC2有显著的影响，反映出地理因素对主成分分析结果的显著性贡献。]]></description><link>软件\python\数据科学与格式转换\python：四分位分组箱型图绘制.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：四分位分组箱型图绘制.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[准备文件]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！
更好的办法建议使用 TBtools 软件！
<br>这个脚本可以根据你的 csv 文件提到的 ID 名字提取对应的序列到新的 fasta 文件中：<br><br>一个普通的 fasta 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404052250359.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>一个 csv 文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404052250619.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>请注意，只需要第一列存在即可。<br>
另外，表没有包含标题。<br><br>import pandas as pd

# 从没有标题的CSV文件中读取ID列表
csv_file_path = 'C:/Users/a/Desktop/Illumina芯片分析表.csv'
df_no_header = pd.read_csv(csv_file_path, header=None)
# 创建一个集合，包含所有的ID
id_set = set(df_no_header[0])

# 定义一个函数，使用更严格的逻辑从FASTA文件中提取序列
def read_and_extract_fasta_strict(fasta_path, ids):
    with open(fasta_path, "r") as fasta_file:
        lines = fasta_file.readlines()
    
    extract = False
    extracted_sequences = []
    current_sequence = []
    
    # 遍历每一行
    for line in lines:
        if line.startswith("&gt;"):
            line_id = line.split()[0][1:]  # 提取行中的ID
            if line_id in ids:
                extract = True
                if current_sequence:
                    extracted_sequences.append("".join(current_sequence))
                    current_sequence = []
                # 保持原始ID不变
                updated_id = f"&gt;{line_id}\n"
                current_sequence.append(updated_id)
            else:
                if extract:
                    extracted_sequences.append("".join(current_sequence))
                    current_sequence = []
                extract = False
        else:
            if extract:
                current_sequence.append(line)
                
    if current_sequence:
        extracted_sequences.append("".join(current_sequence))
    
    return extracted_sequences

# FASTA文件路径和新FASTA文件保存路径
fasta_file_path = 'C:/Users/a/Desktop/Illumina_mtDNA_Filter_recode.fasta'
new_fasta_file_path = 'C:/Users/a/Desktop/extracted_sequences.fasta'

# 使用修正后的函数从FASTA文件中提取序列
extracted_sequences_strict = read_and_extract_fasta_strict(fasta_file_path, id_set)

# 将提取的序列保存到新的FASTA文件
with open(new_fasta_file_path, "w") as output_fasta:
    for sequence in extracted_sequences_strict:
        output_fasta.write(sequence)

# 输出新FASTA文件的路径，表示任务完成
print(f"提取的序列已保存至: {new_fasta_file_path}")

复制]]></description><link>软件\python\数据科学与格式转换\python：提取fasta序列至新的fasta.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：提取fasta序列至新的fasta.md</guid><pubDate>Fri, 23 Aug 2024 08:24:02 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404052250359.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404052250359.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[功能介绍]]></title><description><![CDATA[ 
 <br><br>以下脚本的功能是，针对某个特定的线粒体单倍群（例如，M9），分区域（地级市或者省）统计该单倍群以及该单倍群下游的所有单倍群的数量，然后把下游的单倍群累加到上游去，从而计算该地区的单倍群频率。<br>
计算公式如下，以 M9单倍群的某个地区为例：<br><br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612092651.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612092800.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>该脚本会自动检查你的原始数据中不存在但是在完整的系统发育树中存在的单倍群，并打印到终端，方便快速检查。<br>
例如，当我们想绘制 M9 单倍群的时候，针对我的数据，会打印：<br>The following haplogroups were not found in the dataset: E1, E2b1, E2a1, E2a2, E2a1a, (C16261T), E1a2a, E1a2a1, E1a2a2, E1a2a3, E1a2a4, E1a1b, E1a1c, E1a1b1, E1a1b2, E1a1b4, E1a1a1c, E1a1a1b1, E1a1a1b2, C150T, M9a1b1b, M9a1b1a1, M9a1a1c1b2, M9a1a1c1b1a1
复制<br><br>
<br>原始数据，格式如下所示：
<br>Province,City,Haplogroup
四川省,德阳市,B4h
......
复制<br>
<br>地图文件：包括 shp 和附带的数据文件，例如：
<br>市（等积投影）_修正.shx
市（等积投影）_修正.prj
市（等积投影）_修正.dbf
市（等积投影）_修正.cpg
复制<br>
<br>线粒体单倍群系统发育树，在文末给出。
<br><br>import pandas as pd
import matplotlib.pyplot as plt
import geopandas as gpd
from pypinyin import pinyin, Style

# 读取并解析单倍群列表文件
def parse_haplogroup_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        haplogroup_lines = f.readlines()

    haplogroup_tree = {}
    stack = []

    for line in haplogroup_lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
        indent_level = len(line) - len(stripped_line)
        while len(stack) &gt; 0 and stack[-1][1] &gt;= indent_level:
            stack.pop()
        if stack:
            parent = stack[-1][0]
            haplogroup_tree.setdefault(parent, []).append(stripped_line)
        stack.append((stripped_line, indent_level))

    return haplogroup_tree

# 查找特定单倍群的所有下游单倍群
def find_all_descendants(haplogroup, haplogroup_tree):
    descendants = []
    stack = [haplogroup]
    while stack:
        current = stack.pop()
        if current in haplogroup_tree:
            children = haplogroup_tree[current]
            descendants.extend(children)
            stack.extend(children)
    return descendants

# 替换下游单倍群为上游单倍群，并记录未找到的单倍群
def replace_haplogroups(data, haplogroup_name, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    found_haplogroups = set(data['Haplogroup'].unique())
    missing_haplogroups = [hap for hap in descendants if hap not in found_haplogroups]
    
    if missing_haplogroups:
        print(f"The following haplogroups were not found in the dataset: {', '.join(missing_haplogroups)}")
    
    data['Haplogroup'] = data['Haplogroup'].apply(lambda x: haplogroup_name if x in descendants else x)
    return data

# 加载数据
file_path = '数据清洗整理.csv'
data = pd.read_csv(file_path)

# 加载并解析单倍群列表文件
haplogroup_file_path = '线粒体单倍群phylotree(version17).txt'
haplogroup_tree = parse_haplogroup_file(haplogroup_file_path)

# 获取单倍群在各省的数量
haplogroup_name = 'M9'
data_replaced = replace_haplogroups(data, haplogroup_name, haplogroup_tree)

# 过滤指定单倍群并统计各省数量
def count_haplogroup_by_province(haplogroup_name, data, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    
    filtered_data = data[data['Haplogroup'] == haplogroup_name]
    province_counts = filtered_data['Province'].value_counts().reset_index()
    province_counts.columns = ['Province', 'Count']
    
    other_counts = data[~data['Haplogroup'].isin(descendants)].groupby('Province').size().reset_index(name='Total_Other')
    province_counts = province_counts.merge(other_counts, on='Province', how='left')
    return province_counts

province_counts = count_haplogroup_by_province(haplogroup_name, data_replaced, haplogroup_tree)

# 计算各省的频率
province_counts['Frequency'] = province_counts['Count'] / province_counts['Total_Other']

# 将中文省份名称转换为拼音
province_counts['Province_pinyin'] = province_counts['Province'].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 加载中国省份地理数据
china_map = gpd.read_file('省面/省面.shp')

# 确认地理数据中的省份名称列
province_column = '省全名'

# 将地理数据中的省份名称转换为拼音
china_map['Province_pinyin'] = china_map[province_column].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 合并统计数据和地理数据
china_map = china_map.merge(province_counts, left_on='Province_pinyin', right_on='Province_pinyin', how='left')
china_map = china_map.infer_objects()  # 确保数据类型正确
china_map = china_map.fillna(0)  # 填充缺失值

# 加载其他地理数据
buffer_map = gpd.read_file('省面/0-15KM缓冲国界.shp')
coastline_map = gpd.read_file('省面/海岸.shp')
south_china_sea_box_map = gpd.read_file('省面/南海附图框.shp')
#south_china_sea_box_small_map = gpd.read_file('省面/南海附图框南海诸岛小框.shp')
reef_map = gpd.read_file('省面/珊瑚礁.shp')

# 检查province_counts是否为空
if province_counts.empty:
    print("Error: The haplogroup data is empty. Please check the input data and haplogroup name.")
else:
    # 获取频率的最大值和最小值
    max_frequency = province_counts['Frequency'].max()
    min_frequency = province_counts['Frequency'].min()

    # 绘制热图
    fig, ax = plt.subplots(1, 1, figsize=(10, 18))  # 调整图形高度
    china_map.boundary.plot(ax=ax, linewidth=0.03, color='#8B8B8B')
    china_map.plot(column='Frequency', ax=ax, legend=True,
                   cmap='Spectral_r',  # 使用配色
                   legend_kwds={'label': f"Haplogroup {haplogroup_name} Frequency by Province",
                                'orientation': "vertical"},
                   vmin=min_frequency, vmax=max_frequency)  # 设置颜色条范围

    # 绘制其他shp文件
    buffer_map.plot(ax=ax, color='#8B8B7B', linewidth=0.5)
    coastline_map.plot(ax=ax, color='#8B8B6B', linewidth=0.5)
    south_china_sea_box_map.plot(ax=ax, color='#8B8B5B', linewidth=0.5)
    #south_china_sea_box_small_map.plot(ax=ax, color='#8B8B4B', linewidth=0.5)
    reef_map.plot(ax=ax, color='#8B8B3B', linewidth=0.5)

    # 隐藏地图外框的横轴、纵轴的数字标尺
    ax.set_xticks([])
    ax.set_yticks([])

    plt.title(f'Distribution of Haplogroup {haplogroup_name} in China by Province')

    # 添加表格
    table_data = province_counts[['Province_pinyin', 'Count', 'Frequency']].values
    column_labels = ['Province', 'Count', 'Frequency']
    table = plt.table(cellText=table_data,
                      colLabels=column_labels,
                      cellLoc='center',
                      loc='bottom',
                      bbox=[0, -1.4, 1, 1.2])  # 调整表格位置和大小

    table.auto_set_font_size(False)
    table.set_fontsize(8)  # 调整字体大小

    # 调整单元格的高度
    cell_dict = table.get_celld()
    for i in range(len(table_data) + 1):  # 包括标题行
        for j in range(len(column_labels)):
            cell_dict[(i, j)].set_height(0.15)  # 调整单元格高度

    plt.subplots_adjust(left=0.2, bottom=0.4)  # 调整图形布局

    plt.show()

复制<br><br>其实逻辑实现基本一致，只是把省替换为市罢了。<br>import pandas as pd
import matplotlib.pyplot as plt
import geopandas as gpd
from pypinyin import pinyin, Style

# 读取并解析单倍群列表文件
def parse_haplogroup_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        haplogroup_lines = f.readlines()

    haplogroup_tree = {}
    stack = []

    for line in haplogroup_lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
        indent_level = len(line) - len(stripped_line)
        while len(stack) &gt; 0 and stack[-1][1] &gt;= indent_level:
            stack.pop()
        if stack:
            parent = stack[-1][0]
            haplogroup_tree.setdefault(parent, []).append(stripped_line)
        stack.append((stripped_line, indent_level))

    return haplogroup_tree

# 查找特定单倍群的所有下游单倍群
def find_all_descendants(haplogroup, haplogroup_tree):
    descendants = []
    stack = [haplogroup]
    while stack:
        current = stack.pop()
        if current in haplogroup_tree:
            children = haplogroup_tree[current]
            descendants.extend(children)
            stack.extend(children)
    return descendants

# 替换下游单倍群为上游单倍群，并记录未找到的单倍群
def replace_haplogroups(data, haplogroup_name, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    found_haplogroups = set(data['Haplogroup'].unique())
    missing_haplogroups = [hap for hap in descendants if hap not in found_haplogroups]
    
    if missing_haplogroups:
        print(f"The following haplogroups were not found in the dataset: {', '.join(missing_haplogroups)}")
    
    data['Haplogroup'] = data['Haplogroup'].apply(lambda x: haplogroup_name if x in descendants else x)
    return data

# 加载数据
file_path = '数据清洗整理_最终修改.csv'
data = pd.read_csv(file_path)

# 加载并解析单倍群列表文件
haplogroup_file_path = '线粒体单倍群phylotree(version17).txt'
haplogroup_tree = parse_haplogroup_file(haplogroup_file_path)

# 获取单倍群在各市的数量
haplogroup_name = 'M7a1a1'
data_replaced = replace_haplogroups(data, haplogroup_name, haplogroup_tree)

# 过滤指定单倍群并统计各市数量
def count_haplogroup_by_city(haplogroup_name, data, haplogroup_tree):
    descendants = find_all_descendants(haplogroup_name, haplogroup_tree)
    
    filtered_data = data[data['Haplogroup'] == haplogroup_name]
    city_counts = filtered_data['City'].value_counts().reset_index()
    city_counts.columns = ['City', 'Count']
    
    other_counts = data[~data['Haplogroup'].isin(descendants)].groupby('City').size().reset_index(name='Total_Other')
    city_counts = city_counts.merge(other_counts, on='City', how='left')
    return city_counts

city_counts = count_haplogroup_by_city(haplogroup_name, data_replaced, haplogroup_tree)

# 计算各市的频率
city_counts['Frequency'] = city_counts['Count'] / city_counts['Total_Other']

# 将中文城市名称转换为拼音
city_counts['City_pinyin'] = city_counts['City'].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 加载中国地级市地理数据
china_map = gpd.read_file('雄鸡地级市/市（等积投影）_修正.shp')

# 确认地理数据中的城市名称列
city_column = '市'

# 将地理数据中的城市名称转换为拼音
china_map['City_pinyin'] = china_map[city_column].apply(lambda x: ''.join([item[0] for item in pinyin(x, style=Style.NORMAL)]))

# 合并统计数据和地理数据
china_map = china_map.merge(city_counts, left_on='City_pinyin', right_on='City_pinyin', how='left')
china_map = china_map.infer_objects()  # 确保数据类型正确
china_map = china_map.fillna(0)  # 填充缺失值

# 加载其他地理数据
multilinestring_map = gpd.read_file('省面/海岸.shp')
linestring_map = gpd.read_file('雄鸡地级市/中国轮廓线.shp')

# 检查city_counts是否为空
if city_counts.empty:
    print("Error: The haplogroup data is empty. Please check the input data and haplogroup name.")
else:
    # 获取频率的最大值和最小值
    max_frequency = city_counts['Frequency'].max()
    min_frequency = city_counts['Frequency'].min()

    # 绘制热图
    fig, ax = plt.subplots(1, 1, figsize=(10, 18))  # 调整图形高度
    china_map.boundary.plot(ax=ax, linewidth=0.03, color='#8B8B8B')
    china_map.plot(column='Frequency', ax=ax, legend=True,
                   cmap='Spectral_r',  # 使用配色
                   legend_kwds={'label': f"Haplogroup {haplogroup_name} Frequency by City",
                                'orientation': "vertical"})  # 删除vmin和vmax，使用默认值

    # 绘制其他shp文件
    #multilinestring_map.plot(ax=ax, color='#8B8B8B', linewidth=0.5)
    linestring_map.plot(ax=ax, color='#8B8B8B', linewidth=0.5)
    # 隐藏地图外框的横轴、纵轴的数字标尺
    ax.set_xticks([])
    ax.set_yticks([])

    plt.title(f'Distribution of Haplogroup {haplogroup_name} in China by City')

    # 添加表格
    table_data = city_counts[['City_pinyin', 'Count', 'Frequency']].values
    column_labels = ['City', 'Count', 'Frequency']
    table = plt.table(cellText=table_data,
                      colLabels=column_labels,
                      cellLoc='center',
                      loc='bottom',
                      bbox=[0, -5, 1, 5])  # 调整表格位置和大小

    table.auto_set_font_size(False)
    table.set_fontsize(5)  # 调整字体大小

    # 调整单元格的高度
    cell_dict = table.get_celld()
    for i in range(len(table_data) + 1):  # 包括标题行
        for j in range(len(column_labels)):
            cell_dict[(i, j)].set_height(0.15)  # 调整单元格高度

    plt.subplots_adjust(left=0.2, bottom=0.4)  # 调整图形布局

    plt.show()

复制<br><br>该发育树通过 phylotree2016年版本构建。<br>
下载地址(有效期至2025年6月)： <a rel="noopener nofollow" class="external-link" href="https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1749692016&amp;Signature=uUB7fAEOfW2cC1TdPlLfMEa2wlM%3D" target="_blank">https://scientific-attach.oss-cn-chengdu.aliyuncs.com/%E9%99%84%E4%BB%B6/%E7%BA%BF%E7%B2%92%E4%BD%93%E5%8D%95%E5%80%8D%E7%BE%A4phylotree(version17).txt?OSSAccessKeyId=LTAI5tELp2DoTHGGZeQkMQmK&amp;Expires=1749692016&amp;Signature=uUB7fAEOfW2cC1TdPlLfMEa2wlM%3D</a>]]></description><link>软件\python\数据科学与格式转换\python：一键绘制线粒体单倍群频率热图.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：一键绘制线粒体单倍群频率热图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:26 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612092651.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612092651.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[作用]]></title><description><![CDATA[ 
 <br><br>该脚本的作用是，根据原始的表格计算在某种没类状态下的单倍群频率，并按照字符长度自动生成所有级别的单倍群，然后利用累加原则（上游单倍群=下游单倍群之和）进行累加。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240531095706.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一列可以不要，但是需要有 Haplotype 列和 Classfication。<br><br>import pandas as pd
import os 
# 读取原始数据
file_path = 'C:/Users/victo/Desktop/南北方原始数据.csv'  # 请替换为你的文件路径
data = pd.read_csv(file_path)

# 计算每个区域内的每个Haplotype的数量
haplogroup_counts = data.groupby(['Haplotype', 'Classfication']).size().reset_index(name='count')

# 创建数据透视表，将Haplotype作为行，区域作为列
haplogroup_pivot = haplogroup_counts.pivot(index='Haplotype', columns='Classfication', values='count').fillna(0)

# 计算每个区域内每个Haplotype的频率
haplogroup_pivot = haplogroup_pivot.div(haplogroup_pivot.sum(axis=0), axis=1) * 100

# 提取Haplotype列表并处理
haplogroups = data['Haplotype'].tolist()
processed_haplogroups = []

# 按规则处理Haplotype
for haplogroup in haplogroups:
    processed_haplogroups.append(haplogroup)
    while len(haplogroup) &gt; 1:
        haplogroup = haplogroup[:-1]
        processed_haplogroups.append(haplogroup)

# 去重并排序
unique_processed_haplogroups = sorted(set(processed_haplogroups))

# 创建包含处理后Haplotype的新DataFrame
final_df = pd.DataFrame(unique_processed_haplogroups, columns=['Haplotype'])

# 合并最终DataFrame和Haplotype频率DataFrame
proce_df = final_df.merge(haplogroup_pivot, how='left', left_on='Haplotype', right_index=True).fillna(0)

# 将结果保存到CSV文件
proce_df.to_csv('./PROCE.csv', index=False)
df_cleaned = pd.read_csv('./PROCE.csv')

# 将频率列转换为数值类型
df_cleaned.iloc[:, 1:] = df_cleaned.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').fillna(0)

# 获取Haplotype列表
haplogroups = df_cleaned['Haplotype'].tolist()

# 初始化累加频率字典
cumulative_freq = {col: {hg: 0 for hg in haplogroups} for col in df_cleaned.columns[1:]}

# 定义检查Haplotype是否是另一个Haplotype下级的函数
def is_descendant(parent, child):
    return child.startswith(parent) and len(child) &gt; len(parent)

# 假设 is_descendant 函数和 haplogroups, df_cleaned, cumulative_freq 数据框已定义

# 缓存 is_descendant 结果
descendant_cache = {}

for parent in haplogroups:
    # 先处理 parent 自身的数据
    for col in df_cleaned.columns[1:]:
        if not df_cleaned.loc[df_cleaned['Haplotype'] == parent, col].empty:
            cumulative_freq[col][parent] += df_cleaned.loc[df_cleaned['Haplotype'] == parent, col].values[0]
    
    for child in haplogroups:
        if (parent, child) not in descendant_cache:
            descendant_cache[(parent, child)] = is_descendant(parent, child)
        if descendant_cache[(parent, child)]:
            for col in df_cleaned.columns[1:]:
                if not df_cleaned.loc[df_cleaned['Haplotype'] == child, col].empty:
                    cumulative_freq[col][parent] += df_cleaned.loc[df_cleaned['Haplotype'] == child, col].values[0]

# 创建存储累加频率的新DataFrame
cumulative_df = pd.DataFrame(cumulative_freq).reset_index()
cumulative_df.rename(columns={'index': 'Haplotype'}, inplace=True)

# 保存累加频率到新文件
cumulative_file_path = 'C:/Users/victo/Desktop/各级Haplotype频率总表累计.csv'
cumulative_df.to_csv(cumulative_file_path, index=False)
# 删除PROCE.csv
os.remove('./PROCE.csv')
print("一个不需要的文件已经被删除了;\n累加后的文件路径：", cumulative_file_path)

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240531095812.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：一键生成y染色体各级单倍群累加表格.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：一键生成Y染色体各级单倍群累加表格.md</guid><pubDate>Fri, 21 Jun 2024 07:31:26 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240531095706.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240531095706.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[python：AMOVA便捷分组]]></title><description><![CDATA[ 
 <br><a data-href="分子方差分析（AMOVA ，Analysis of Molecular Variance)" href="术语\分子方差分析（amova-，analysis-of-molecular-variance).html" class="internal-link" target="_self" rel="noopener nofollow">分子方差分析（AMOVA ，Analysis of Molecular Variance)</a>使用Arlequin软件进行。<br>但是在进行分组时需要一个个点击，很麻烦。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271156871.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>为了解决这个问题，可以使用如下代码。<br><br>需要一个txt文件，如下所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271156946.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br># Re-importing the necessary library as the code execution state has been reset.
import pandas as pd

# 替换这个txt文件
file_path = 'C:/Users/a/Desktop/分组.txt'

# Read the file into a pandas DataFrame
group_data = pd.read_csv(file_path, sep="\t", header=None, names=["ID", "Category"])

# Group the IDs by category
grouped = group_data.groupby('Category')['ID'].apply(list).to_dict()

# Start writing the content for the .arp file
arp_content = "[[Structure]]\n\n"
arp_content += 'StructureName="New Edited Structure"\n'
arp_content += f'NbGroups={len(grouped)}\n\n'

# Loop through each category and add its IDs to the content
for category, IDs in grouped.items():
    arp_content += f'Group={{\n'
    for ID in IDs:
        arp_content += f'\t"{ID}"\n'
    arp_content += "}\n.......\n"

# 替换输出的文件
arp_file_path = 'C:/Users/a/Desktop/分组.arp'

# Write the content to the new .arp file
with open(arp_file_path, 'w', encoding='utf-8') as arp_file:
    arp_file.write(arp_content)

arp_file_path

复制<br>输出的arp文件，直接把里面的内容贴到之前的arp文件中。具体参见：<a data-href="DnaSP：AMOVA及Fst分析软件操作" href="软件\其它生信软件\a-j\dnasp：amova及fst分析软件操作.html" class="internal-link" target="_self" rel="noopener nofollow">DnaSP：AMOVA及Fst分析软件操作</a>。]]></description><link>软件\python\数据科学与格式转换\python：amova便捷分组.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：AMOVA便捷分组.md</guid><pubDate>Fri, 23 Aug 2024 08:26:29 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271156871.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271156871.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>首先，利用 tracer 软件绘制一个<a data-href="贝叶斯天际线图（Bayesian Skyline Plot, BSP）" href="术语\贝叶斯天际线图（bayesian-skyline-plot,-bsp）.html" class="internal-link" target="_self" rel="noopener nofollow">贝叶斯天际线图（Bayesian Skyline Plot, BSP）</a> .<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409021536742.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>然后，导出原数据：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409021536046.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
点击图中的 Export Data Table.<br>检查导出的文件，是否存在异常，如果有，记得删去！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409021538356.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>运行代码，在终端输入时间节点即可。<br>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# 读取文件并保留整数
input_file_path = '输入文件'

# 读取文件
with open(input_file_path, 'r', encoding='utf-8') as file:
    lines = file.readlines()

# 提取标题和表头
title = lines[0].strip()
header = lines[1].strip().split('\t')

# 读取数据并保留整数
data_lines = lines[2:]

# 检查最后一行是否为非数字类型，如果是则删除
def is_numeric_list(line):
    try:
        [float(x) for x in line.strip().split('\t')]
        return True
    except ValueError:
        return False

while data_lines and not is_numeric_list(data_lines[-1]):
    data_lines.pop()

data = [list(map(lambda x: int(float(x)), line.strip().split('\t'))) for line in data_lines]

# 创建DataFrame
df = pd.DataFrame(data, columns=header)

# Ensuring all data are float64
df = df.astype(float)

# Set 'time' as the index
df.set_index('time', inplace=True)

# Rounding the data to remove all decimals
df_rounded = df.round()

# Asking user for time scale limits
time_lower_limit = int(input("请输入时间尺度的下限 (e.g., 0): "))
time_upper_limit = int(input("请输入时间尺度的上限 (e.g., 60000): "))

# Filtering the data based on the user input
data_filtered = df_rounded[(df_rounded.index &gt;= time_lower_limit) &amp; (df_rounded.index &lt;= time_upper_limit)]

# Converting columns to integer numpy arrays
time_values_int = data_filtered.index.to_numpy().astype(int)
upper_values_int = data_filtered['upper'].to_numpy().astype(int)
lower_values_int = data_filtered['lower'].to_numpy().astype(int)

# Determine the Y-axis range
y_min = 10**np.floor(np.log10(lower_values_int.min()))
y_max = 10**np.ceil(np.log10(upper_values_int.max()))

# 设置字体为Arial，并确保生成的图形中的文本是矢量可编辑的
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42

# Plotting the optimized data with specified adjustments
fig, ax = plt.subplots(figsize=(10, 6))

# Plot each line with different colors and increased linewidth for mean and median
ax.plot(time_values_int, data_filtered['mean'].round().astype(int), label='Mean', color='#C36B5F', linewidth=1, linestyle='--')
ax.plot(time_values_int, data_filtered['median'].round().astype(int), label='Median', color='#72B6AB', linewidth=1)
#ax.plot(time_values_int, upper_values_int, label='Upper', color='white')
#ax.plot(time_values_int, lower_values_int, label='Lower', color='white')

# Fill the area between upper and lower using the specified color
ax.fill_between(time_values_int, upper_values_int, lower_values_int, color='#B9DEDD', alpha=0.3)

# Removing background grid
ax.grid(False)

# Adding title and labels
ax.set_title(title)
ax.set_xlabel('Time')
ax.set_ylabel('Values')
ax.set_yscale('log')
ax.set_ylim(y_min, y_max)

# Adjusting y-axis ticks to standard notation
ax.set_yticks([10**i for i in range(int(np.log10(y_min)), int(np.log10(y_max)) + 1)])
ax.set_yticklabels([f'{10**i:.0f}' for i in range(int(np.log10(y_min)), int(np.log10(y_max)) + 1)])

# Set Y-axis crossing X-axis at 0
ax.spines['left'].set_position(('data', 0))

# Adding legend
ax.legend()

# Display the plot
plt.show()

复制]]></description><link>软件\python\数据科学与格式转换\python：beast：重新绘制绘制bsp.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：BEAST：重新绘制绘制BSP.md</guid><pubDate>Mon, 02 Sep 2024 07:38:50 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409021536742.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409021536742.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br>这个脚本已经弃用！
更好的办法建议使用 TBtools 软件！
<br><br>一个 fasta 文件，一个 csv 文件。csv 文件第一列是 ID，第二列是单倍群。表包含标题。<br><br>import pandas as pd
from Bio import SeqIO

def add_haplogroup_to_fasta(fasta_file, csv_file, output_file):
    # 读取CSV文件到DataFrame
    haplogroups = pd.read_csv(csv_file)
    # 创建一个字典来映射Sample_ID到Haplogroup
    haplogroup_dict = dict(zip(haplogroups['Sample_ID'], haplogroups['Haplogroup']))
    
    # 读取fasta文件
    with open(fasta_file, 'r') as fasta:
        records = list(SeqIO.parse(fasta, 'fasta'))
    
    # 更新记录的ID
    updated_records = []
    for record in records:
        id = record.id
        # 查找单倍群，如果存在则更新ID
        haplogroup = haplogroup_dict.get(id)
        if haplogroup:
            record.id = id + '_' + haplogroup  # 修改此处来调整ID的格式
            record.description = id + '_' + haplogroup  # 通常也需要更新description
        updated_records.append(record)
    
    # 将更新后的fasta记录写入新文件
    with open(output_file, 'w') as output:
        SeqIO.write(updated_records, output, 'fasta')

# 使用示例
add_haplogroup_to_fasta('path_to_your_fasta_file.fasta', 'path_to_your_csv_file.csv', 'output_fasta_file.fasta')

复制]]></description><link>软件\python\数据科学与格式转换\python：fasta文件加个单倍群label.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：fasta文件加个单倍群label.md</guid><pubDate>Fri, 23 Aug 2024 08:23:23 GMT</pubDate></item><item><title><![CDATA[2024年6月12日更新记录]]></title><description><![CDATA[ 
 <br><br>有些时候我们希望 FASTA 序列的 ID 存在单倍群，但是另一些时候则不希望。这真是一个很糟糕的步骤。<br>
所以我重新修改了，这下直接从终端输入 YES 或者 NO 来进行。<br>输入文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612212228.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>代码：<br>import os
from Bio import SeqIO

# Paths to the files
input_txt_path = r"C:/Users/victo/Desktop/新建 Text Document.txt"
input_fasta_path = r"C:/Users/victo/Desktop/10K_WMG_REFERENCE.ALN.fasta"
output_fasta_path = r"C:/Users/victo/Desktop/提取序列.fasta"

# Read the sample IDs and haplogroups from the input TXT file
sample_info = {}
with open(input_txt_path, 'r', encoding='utf-8') as txt_file:
    # Skip the header
    next(txt_file)
    for line in txt_file:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            sample_id, haplogroup = parts
            sample_info[sample_id] = haplogroup

# Ask the user if they want to include haplogroup information
include_haplogroup = input("是否需要添加单倍群信息至ID？请输入YES或者NO: ").strip().upper() == 'YES'

# Read the sequences from the input FASTA file and write the required sequences to the output FASTA file
with open(output_fasta_path, 'w') as output_fasta:
    for record in SeqIO.parse(input_fasta_path, "fasta"):
        original_id = record.id
        if original_id in sample_info:
            if include_haplogroup:
                new_id = f"{original_id}_{sample_info[original_id]}"
                record.id = new_id
                record.description = new_id
            SeqIO.write(record, output_fasta, "fasta")

print(f"提取已经完成. 序列已经保存至：{output_fasta_path}")

复制<br><br>2024年4月5日更新：为了只提取我们需要的序列并转化为 nex 文件，我将这个功能加入到了代码中。<br><br>nex文件用来绘制Network图。<br>
这里提供 Python 代码可以很迅速利用分组文件和 fasta 文件转化成 nex。<br><br>
<br>fasta文件
<br>分类文件
<br>FASTA文件格式没有特殊要求：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402231407451.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>分类文件要求csv格式：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402231407511.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>注意
nex文件对于格式的要求很高。所以注意，上面csv文件中不能出现多余的空格、+-符号等特殊符号。
<br><br><br>以下用法适合文件完全符合上述要求的。<br>import pandas as pd

# 从没有标题的CSV文件中读取ID列表
csv_file_path = 'C:/Users/a/Desktop/Illumina芯片分析表.csv' # 修改路径
nex_file_path = 'C:/Users/a/Desktop/final.nex' # 请修改！Change this!
df_no_header = pd.read_csv(csv_file_path, header=None)
# 创建一个集合，包含所有的ID
id_set = set(df_no_header[0])

# 定义一个函数，使用更严格的逻辑从FASTA文件中提取序列
def read_and_extract_fasta_strict(fasta_path, ids):
    with open(fasta_path, "r") as fasta_file:
        lines = fasta_file.readlines()
    
    extract = False
    extracted_sequences = []
    current_sequence = []
    
    # 遍历每一行
    for line in lines:
        if line.startswith("&gt;"):
            line_id = line.split()[0][1:]  # 提取行中的ID
            if line_id in ids:
                extract = True
                if current_sequence:
                    extracted_sequences.append("".join(current_sequence))
                    current_sequence = []
                # 保持原始ID不变
                updated_id = f"&gt;{line_id}\n"
                current_sequence.append(updated_id)
            else:
                if extract:
                    extracted_sequences.append("".join(current_sequence))
                    current_sequence = []
                extract = False
        else:
            if extract:
                current_sequence.append(line)
                
    if current_sequence:
        extracted_sequences.append("".join(current_sequence))
    
    return extracted_sequences

# FASTA文件路径和新FASTA文件保存路径
origin_fasta_file_path = 'C:/Users/a/Desktop/Illumina_mtDNA_Filter_recode.fasta' # 指定你的原始fasta路径
extracted_sequence_fasta_file_path = 'C:/Users/a/Desktop/extracted_sequences.fasta' # 指定你的提取fasta路径

# 使用修正后的函数从FASTA文件中提取序列
extracted_sequences_strict = read_and_extract_fasta_strict(origin_fasta_file_path, id_set)

# 将提取的序列保存到新的FASTA文件
with open(extracted_sequence_fasta_file_path, "w") as output_fasta:
    for sequence in extracted_sequences_strict:
        output_fasta.write(sequence)

# 输出新FASTA文件的路径，表示任务完成
print(f"提取的序列已保存至: {extracted_sequence_fasta_file_path}")


##############################################
###########现在开始进行fasta序列转化############
##############################################

# 这个代码的作用是根据分组文件，生成.nex文件
# 注意，使用前请将代码中出现的路径（C:/Users/a/Desktop/）全部替换成你的路径（建议设置为桌面）

import pandas as pd

# 定义 CSV 文件路径和 FASTA 文件路径
fasta_file_path = extracted_sequence_fasta_file_path
new_fasta_file_path = 'C:/Users/a/Desktop/ID_haplogroup.fasta' # 输出带有单倍型的fasta文件

try:
    # 加载单倍群信息，没有标题行，自定义列名
    haplogroup_df = pd.read_csv(csv_file_path, header=None, names=['SampleID', 'Region', 'Haplogroup'])
    
    # 创建 SampleID 到 Haplogroup 的映射字典
    haplogroup_dict = dict(zip(haplogroup_df['SampleID'], haplogroup_df['Haplogroup']))
    
    # 同时打开原始 FASTA 文件和新 FASTA 文件进行读写
    with open(fasta_file_path, 'r') as original_fasta, open(new_fasta_file_path, 'w') as modified_fasta:
        for line in original_fasta:
            if line.startswith('&gt;'):  # 这是一个 ID 行
                # 提取样本 ID（移除 '&gt;' 和换行符）
                sample_id = line[1:].strip()
                # 如果字典中存在对应的单倍群，则追加；否则，标记为 'Unknown'
                haplogroup = haplogroup_dict.get(sample_id, 'Unknown')
                # 用下划线连接样本 ID 和单倍群
                modified_line = f"&gt;{sample_id}_{haplogroup}\n"
            else:
                modified_line = line  # 序列行保持不变
            modified_fasta.write(modified_line)
    print("FASTA 文件处理完成。")
except Exception as e:
    print(f"处理过程中发生错误：{e}")

# 如果需要，可以在这里添加代码以检查和验证新文件的内容
import pandas as pd

# 定义原 CSV 文件路径和新 CSV 文件路径
new_csv_file_path = 'C:/Users/a/Desktop/do_not_change_myname.csv'  # 请不要改变文件名称

# 读取 CSV 文件，无标题行，指定列名
haplogroup_df = pd.read_csv(csv_file_path, header=None, names=['SampleID', 'Region', 'Haplogroup'])

# 将第一列内容与第三列内容用"_"连接，并替换原本第一列内容
haplogroup_df['SampleID'] = haplogroup_df.apply(lambda x: f"{x['SampleID']}_{x['Haplogroup']}", axis=1)

# 删除第三列（Haplogroup）
haplogroup_df.drop(columns=['Haplogroup'], inplace=True)

# 将修改后的 DataFrame 保存为新文件，保留原文件不变
haplogroup_df.to_csv(new_csv_file_path, header=False, index=False)

print(f"新的分类汇总文件已保存至：{new_csv_file_path}")

print("分组汇总.csv 文件已更新。")


# ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# 以下代码请勿更改文件名称，路径设置为刚才的路径（建议设置为桌面）

import pandas as pd
# 示例调用
fasta_file_path = new_fasta_file_path # 请勿修改！don't change this!
group_file_path = new_csv_file_path # 请勿修改！don't change this!

new_csv_path_final = './grouped_names_final.csv' # 请勿修改！don't change this!
def read_fasta_file(fasta_file_path):
    samples = {}
    with open(fasta_file_path, 'r') as fasta:
        for line in fasta:
            if line.startswith('&gt;'):
                current_sample = line.strip().lstrip('&gt;')
                samples[current_sample] = ''
            else:
                samples[current_sample] += line.strip()
    return samples

def create_nex_file(samples, nex_file_path):
    ntax = len(samples)
    nchar = len(next(iter(samples.values()))) if ntax &gt; 0 else 0
    nex_content = [
        "#NEXUS\n\n",
        "Begin Data;\n",
        f"\tDimensions ntax={ntax} NCHAR={nchar};\n",
        "\tFormat datatype=DNA missing=N GAP=-;\n",
        "\tMATRIX\n"
    ]
    for sample_id, sequence in samples.items():
        nex_content.append(f"\t{sample_id}\n{sequence}\n")
    nex_content.append(";\nEND;\n\n\n")
    with open(nex_file_path, 'w') as nex_file:
        nex_file.writelines(nex_content)

def process_group_file_to_csv(file_path, new_csv_path_final):
    data = pd.read_csv(file_path, sep=',', header=None, names=['Name', 'Group'], encoding='utf-8')
    unique_groups_sorted = sorted(data['Group'].unique())
    matrix_df = pd.DataFrame(0, index=data['Name'], columns=unique_groups_sorted)
    for index, row in data.iterrows():
        matrix_df.at[row['Name'], row['Group']] = 1
    matrix_df.reset_index(inplace=True)
    matrix_df.rename(columns={'index': ''}, inplace=True)
    matrix_df.to_csv(new_csv_path_final, index=False)

def append_traits_to_nex(input_csv, nex_file_path):
    df = pd.read_csv(input_csv)
    new_columns = [col.replace(',', ' ') for col in df.columns if col != 'Name']
    ntraits = len(new_columns)
    trait_labels = ' '.join(new_columns)
    nex_content = f"""Begin Traits;
Dimensions NTraits={ntraits};
Format labels=yes missing=? separator=Comma;
TraitLabels {trait_labels};
Matrix \n
"""
    for index, row in df.iterrows():
        row_data = ','.join([str(row[df.columns[1]])] + [str(row[col]) for col in df.columns[2:]])
        nex_content += f"{row['Name']} {row_data}\n"
    with open(nex_file_path, 'a') as file:
        file.write(nex_content)
        file.write("\t;\n\tend;\n")



samples = read_fasta_file(fasta_file_path)
create_nex_file(samples, nex_file_path)
process_group_file_to_csv(group_file_path, new_csv_path_final)
append_traits_to_nex(new_csv_path_final, nex_file_path)

# 输出文件路径以便下载
print(nex_file_path)

# 删除新分类汇总.csv
import os
os.remove(new_csv_path_final)
os.remove(group_file_path)
print("多余文件已删除。")


复制<br><br>如果你的fasta文件已经是自带单倍群信息的了，那就可以只用上述代码的第二部分。<br># ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# ================================================================== #
# 以下代码建议设置为桌面

import pandas as pd
# 示例调用
fasta_file_path = 'C:/Users/a/Desktop/ID_haplogroup.fasta' # 替换为自带单倍群的fasta文件，不能有特殊符号
group_file_path = 'C:/Users/a/Desktop/do_not_change_myname.csv'  # 替换为分组文件，第一列为ID_单倍群，第二列为分组条件，没有表头
nex_file_path = 'C:/Users/a/Desktop/final.nex'  # 设置为输出路径
new_csv_path_final = 'C:/Users/a/Desktop/grouped_names_final.csv'  
def read_fasta_file(fasta_file_path):
    samples = {}
    with open(fasta_file_path, 'r') as fasta:
        for line in fasta:
            if line.startswith('&gt;'):
                current_sample = line.strip().lstrip('&gt;')
                samples[current_sample] = ''
            else:
                samples[current_sample] += line.strip()
    return samples

def create_nex_file(samples, nex_file_path):
    ntax = len(samples)
    nchar = len(next(iter(samples.values()))) if ntax &gt; 0 else 0
    nex_content = [
        "#NEXUS\n\n",
        "Begin Data;\n",
        f"\tDimensions ntax={ntax} NCHAR={nchar};\n",
        "\tFormat datatype=DNA missing=N GAP=-;\n",
        "\tMATRIX\n"
    ]
    for sample_id, sequence in samples.items():
        nex_content.append(f"\t{sample_id}\n{sequence}\n")
    nex_content.append(";\nEND;\n\n\n")
    with open(nex_file_path, 'w') as nex_file:
        nex_file.writelines(nex_content)

def process_group_file_to_csv(file_path, new_csv_path_final):
    data = pd.read_csv(file_path, sep=',', header=None, names=['Name', 'Group'], encoding='utf-8')
    unique_groups_sorted = sorted(data['Group'].unique())
    matrix_df = pd.DataFrame(0, index=data['Name'], columns=unique_groups_sorted)
    for index, row in data.iterrows():
        matrix_df.at[row['Name'], row['Group']] = 1
    matrix_df.reset_index(inplace=True)
    matrix_df.rename(columns={'index': ''}, inplace=True)
    matrix_df.to_csv(new_csv_path_final, index=False)

def append_traits_to_nex(input_csv, nex_file_path):
    df = pd.read_csv(input_csv)
    new_columns = [col.replace(',', ' ') for col in df.columns if col != 'Name']
    ntraits = len(new_columns)
    trait_labels = ' '.join(new_columns)
    nex_content = f"""Begin Traits;
Dimensions NTraits={ntraits};
Format labels=yes missing=? separator=Comma;
TraitLabels {trait_labels};
Matrix \n
"""
    for index, row in df.iterrows():
        row_data = ','.join([str(row[df.columns[1]])] + [str(row[col]) for col in df.columns[2:]])
        nex_content += f"{row['Name']} {row_data}\n"
    with open(nex_file_path, 'a') as file:
        file.write(nex_content)
        file.write("\t;\n\tend;\n")



samples = read_fasta_file(fasta_file_path)
create_nex_file(samples, nex_file_path)
process_group_file_to_csv(group_file_path, new_csv_path_final)
append_traits_to_nex(new_csv_path_final, nex_file_path)

# 输出文件路径以便下载
print(nex_file_path)

# 删除新分类汇总.csv
import os
os.remove(new_csv_path_final)
os.remove(group_file_path)
print("多余文件已删除。")
复制]]></description><link>软件\python\数据科学与格式转换\python：fasta文件转nex（2024年6月12日更新）.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：FASTA文件转NEX（2024年6月12日更新）.md</guid><pubDate>Fri, 21 Jun 2024 07:31:28 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612212228.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240612212228.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[问题]]></title><description><![CDATA[ 
 <br><br>在 线粒体DNA系统发育树 的官方版本中，存在突变位点等信息。我们只需要单倍群上下游关系，从而可以便捷地实现如下功能：<br>
<br><a data-href="python：计算下游单倍群数量" href="软件\python\数据科学与格式转换\python：计算下游单倍群数量.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="🖥" aria-label="🖥" data-icon="🖥" aria-hidden="true" style="transform: translateY(0px);"></span>python：计算下游单倍群数量</a><img class="emoji" draggable="false" alt="🖥" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f5a5.svg" height="18px" style="max-width: 100%;">
<br><a data-href="Python：查找mtDNA上游单倍型（2024年10月23日更新）" href="软件\python\数据科学与格式转换\python：查找mtdna上游单倍型（2024年10月23日更新）.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>Python：查找mtDNA上游单倍型（2024年10月23日更新）</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">
<br><a data-href="python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）" href="软件\python\数据科学与格式转换\python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）.html" class="internal-link" target="_self" rel="noopener nofollow">python：快速整理线粒体单倍群至目标上游单倍群（2024年10月23日更新）</a><br>
等等。<br>
但是这需要我们将复杂的信息转为简单的信息。
<br><br>以下内容可以在<a data-tooltip-position="top" aria-label="http://www.phylotree.org/builds/mtDNA_tree_Build_17.zip" rel="noopener nofollow" class="external-link" href="http://www.phylotree.org/builds/mtDNA_tree_Build_17.zip" target="_blank">点击这里</a> 获得。<br>
直接全选放入 Excel，然后再复制粘贴为 txt 即可。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408191728788.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br># 这个脚本的作用是将excel全部线粒体单倍群发育树转化成可以被直接使用的txt文件
# 定义文件路径
file_path = '新建 Text Document.txt'

# 读取文件并逐行处理
processed_lines = []
with open(file_path, 'r', encoding='utf-8') as file:
    for line in file:
        # 去掉行首的制表符，以识别第一个非空字符
        stripped_line = line.lstrip('\t')
        
        # 检查行是否包含任何非空字符
        if stripped_line:
            # 找到第一个非空字符的位置
            non_whitespace_index = line.find(stripped_line[0])
            # 分割去掉空格后的行并获取第一个单词，同时处理可能的空分割
            first_word = stripped_line.split()[0] if stripped_line.split() else ""
            first_word_end = non_whitespace_index + len(first_word)
            processed_line = line[:first_word_end].rstrip()
            processed_lines.append(processed_line)

# 定义输出文件的路径
output_file_path = 'processed_text.txt'

# 将处理后的行写入输出文件
with open(output_file_path, 'w', encoding='utf-8') as output_file:
    output_file.write('\n'.join(processed_lines))

# 输出文件路径
output_file_path
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202408191730158.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\数据科学与格式转换\python：phylotree多余信息删除.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：phylotree多余信息删除.md</guid><pubDate>Wed, 23 Oct 2024 08:24:57 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[文件准备]]></title><description><![CDATA[ 
 <br><br>准备好需要被命名的 VCF 文件和名字更改的 txt 文件，例如：<br>New_ID	Old_ID
A-01	Kazak_Altay278
A-02	Kazak_Altay280
A-03	Kazak_Altay285
A-04	Kazak_Altay297
复制<br>New_ID	Old_ID 是必须的。<br><br>import pandas as pd

# 文件路径
replacement_csv_path = r'新建 Text Document.TXT'
vcf_file_path = r'原始的.vcf'
output_vcf_file_path = r'更改的.vcf'

# 读取ID替换表
replacement_data = pd.read_csv(replacement_csv_path, sep="\t")
replacement_dict = dict(zip(replacement_data["Old_ID"], replacement_data["New_ID"]))

# 打开VCF文件并替换样本ID
with open(vcf_file_path, 'r', encoding='utf-8') as vcf_file:
    with open(output_vcf_file_path, 'w', encoding='utf-8') as output_vcf:
        for line in vcf_file:
            # 替换头部行中的样本ID
            if line.startswith("#CHROM"):
                headers = line.strip().split("\t")
                # 替换样本ID为新ID
                headers = [replacement_dict.get(header, header) for header in headers]
                output_vcf.write("\t".join(headers) + "\n")
            else:
                # 其他行保持原样
                output_vcf.write(line)

print("ID替换已完成，修改后的VCF文件已保存至:", output_vcf_file_path)

复制]]></description><link>软件\python\数据科学与格式转换\python：vcf文件重命名.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/Python：VCF文件重命名.md</guid><pubDate>Tue, 29 Oct 2024 12:30:05 GMT</pubDate></item><item><title><![CDATA[python：VCF文件转换成NEX]]></title><description><![CDATA[ 
 <br>
#将vcf转化为ped map（注意要排除亲缘关系样本，但民族_地区少的样本不排除，因为是依据单倍群来划分的)
plink --vcf Illumina2636_Y.vcf --recode compound-genotypes --double-id  --out Illumina2636_Y_vcf_recode
#ped map转换为bed bim fam
plink --file Illumina2636_Y_vcf_recode --make-bed --out Illumina2636_Y_vcf_recode

#连接单倍群结果和样本名称，并将二倍体的ped文件转换为单倍体fasta
paste -d' '  &lt;(tail --lines=+2  China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2) &lt;(cat Illumina2636_Y_vcf_recode.ped |cut -d' ' -f3-) |perl -npe "s/\t/_/"|cut -d' ' -f1,6-|perl -npe "s/^/&gt;/;s/ ([ATCG]){2}/ \1/g;s/ 00/ N/g"|perl -npe  "s/ /\t/;s/ //g;"|perl -npe "s/.*\t\?+$//"|grep -v "^$"|perl -npe "s/\t/\n/"&gt;Illumina2636_Y_vcf_recode.fasta

#将fasta文件转换为NEXUS文件(含有序列信息)
python /home/biosoftware/vcf2phylip/fasta_nexus_converter/fasta_to_nexus/Main.py Illumina2636_Y_vcf_recode.fasta
mv example.nex Illumina2636_Y_IDnoN.nexus
      
#手动生成indpopnew，按照需求进行分组（语系，地理等）
# China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2是单倍群结果文件
paste -d' '  &lt;(tail --lines=+2  China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2) &lt;(cat indpopnew |cut -d' ' -f1-) &gt; China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2.Traits
cat  China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2.Traits | awk 'BEGIN{FS=" ";OFS="\t";}{print $1"_"$2, $4;}' &gt;  China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2_ind_Haplo_popOld
sed 's/[\t ]\+/ /g' China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2_ind_Haplo_popOld &gt; China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2_ind_Haplo_popALL
rm China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2_ind_Haplo_popOld

#生成trait文件（含有每个样本属于哪个分组特征的信息）
cat China1028_male_chrY_Filter.vcf_y_hg_ISOGG2019_hGrpr2top2_ind_Haplo_popALL |perl -npe "s/^(.*? .*?_.*?)_.*/\1/" &gt; Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1
cat Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1|perl -npe "s/ +/\t/g;"|cut -f2|sort |uniq|perl -npe "s/\n/,/"|perl -npe "s/$/\n/;s/^/,/" &gt;tmp_pop.txt; cat &lt;(cat tmp_pop.txt |perl -npe "s/^,//;s/,$//") &lt;(cat Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1|perl -npe "s/ +/\t/g"|cut -f1,2|while read sample pop;do cat tmp_pop.txt|perl -npe "s/,$pop,/,#,/"|perl -npe "s/[^#,\n]+/0/g;s/#/1/;s/^,/$sample\t/;s/,$//" ;done) &gt; Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits
trait_num=`head -n 1 Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits|perl -npe "s/,/\n/g"|wc -l `;trait_label=`head -n 1 Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits|perl -npe "s/,/, /g"`;matrix_value=`cat Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits|tail --lines=+2`;cat /home/biosoftware/ppgv1/popart/pop_Trait_template |perl -npe "s/trait_label/$trait_label/;s/matrix_value/$matrix_value/;s/trait_num/$trait_num/"  &gt; Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits.Matrix

#将生成的序列相关文件和trait文件合并成一个用于Popart画network的输入文件.nexus
cat Illumina2636_Y_IDnoN.nexus Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits.Matrix &gt; Illumina2636_Y_vcf_recode_ind_Haplo_pop_Leve1.Traits.Matrix.nexus

#现在把生成的.nexus文件导入Popart即可，可选择不同的算法进行network作图，形状有细微的差别
复制]]></description><link>软件\python\数据科学与格式转换\python：vcf文件转换成nex.html</link><guid isPermaLink="false">软件/Python/数据科学与格式转换/python：VCF文件转换成NEX.md</guid><pubDate>Fri, 21 Jun 2024 07:31:29 GMT</pubDate></item><item><title><![CDATA[八、Python函数]]></title><description><![CDATA[ 
 <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181723366.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181723834.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>为了减少重复，我们自己定义函数:<br>def  calculate_sector(central_angle, radius):
 # 接下来是函数的代码
  sector_area = central_angle / 360 * 3.14 * radius * 2
  print(f"此扇形的面积是:{sector_area}")

# 调用函数
calculate_sector(30,10)
复制<br>但是这个程序，我们没有存储计算结果。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181731863.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>def  calculate_sector(central_angle, radius):
 # 接下来是函数的代码
  sector_area = central_angle / 360 * 3.14 * radius * 2
  print(f"此扇形的面积是:{sector_area}")
  return sector_area
# 调用函数
calculate_sector(30,10)
复制<br><br>def calculate_BMI(weight,height):
    BMI=weight/(height**2)
    if BMI &lt; 18.5:
        category = "Underweight"
    elif BMI &gt;= 18.5 and BMI &lt; 25:
        category = "Normal"
    elif BMI &gt;= 25 and BMI &lt; 30:
        category = "Overweight"
    else:
        category = "Obese"
    print(f"Your BMI is {category}")
    return BMI
result = calculate_BMI(70,1.7)
print(result)
复制]]></description><link>软件\python\学习笔记\八、python函数.html</link><guid isPermaLink="false">软件/Python/学习笔记/八、Python函数.md</guid><pubDate>Fri, 21 Jun 2024 07:31:30 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181723366.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181723366.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[二、Python注释]]></title><description><![CDATA[ 
 <br>写好注释，不然就会出现下列情况：<br>
<img alt="978be1096e79b9275e9362d482e4afb.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181656746.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>使用ctrl+/为一整行打上#。<br><img alt="f954e3b96bb402445e7316eaef69147.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181656103.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\二、python注释.html</link><guid isPermaLink="false">软件/Python/学习笔记/二、Python注释.md</guid><pubDate>Fri, 21 Jun 2024 07:31:30 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181656746.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181656746.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[拼接]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202233254.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202234412.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202234301.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>区别在于是否改变原来的数组。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202235602.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202235552.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202236553.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>数组与数组运算，数组与数字运算。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202236211.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202237816.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202238479.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202238737.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202238959.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>
import numpy as np

复制<br>
arr1 = np.array([1,3,2,4,9])

arr2 = np.ones(5)

arr3 = np.zeros(3)

复制<br>
arr4 = np.concatenate([arr1,arr2,arr3])

print(arr4)

复制<br>&nbsp; &nbsp; [1. 3. 2. 4. 9. 1. 1. 1. 1. 1. 0. 0. 0.]<br>
# 排序

复制<br>
arr1.sort()

print(arr1)

复制<br>&nbsp; &nbsp; [1 2 3 4 9]<br>
print(arr1[2])

print(arr1[-3])

复制<br>&nbsp; &nbsp; 3<br>&nbsp; &nbsp; 3<br>
print(arr1[1:5])

复制<br>&nbsp; &nbsp; [2 3 4 9]<br>
print(arr1 + arr2)

复制<br>&nbsp; &nbsp; [ 2. &nbsp;3. &nbsp;4. &nbsp;5. 10.]<br>
print(arr1*(-5))

复制<br>&nbsp; &nbsp; [ -5 -10 -15 -20 -45]<br>
print(arr1.max())

print(arr1.min())

print(arr1.sum())

print(arr1.mean())

复制<br>&nbsp; &nbsp; 9<br>&nbsp; &nbsp; 1<br>&nbsp; &nbsp; 19<br>&nbsp; &nbsp; 3.8<br>
arr1[(arr1&lt;1)|(arr1&gt;3)]

复制<br>&nbsp; &nbsp; array([4, 9])]]></description><link>软件\python\学习笔记\二十、-numpy探索.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十、 Numpy探索.md</guid><pubDate>Fri, 21 Jun 2024 07:31:31 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202233254.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202233254.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[库安装]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271401126.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import matplotlib.pylot as plt
import seaborn as sns
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271402696.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271403210.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>sns.histplot(s1)
plt.show()
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271404664.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271404837.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271404974.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271405252.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>sns.histplot(df1,x= "列名")
plt.show()
复制<br><br><br>plt.title("标题")
复制<br><br>plt.xlabel("x轴")
plt.ylabel("y轴")
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271408335.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>sns.scatterplot(df1,x= "total_bill", y = "tip")
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271409193.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271409703.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>注意，这个图表与条形图不太一样。只需要一个变量。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271411704.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>由于sns库里没有绘制饼图的，所以需要使用matplolib库。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271413211.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271429608.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这是一个强大的绘图，可以一次性将所有的变量按照两两配对的关系绘制出来。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271433216.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271434024.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>autopct = "%.1%%<br>
<br>第一个%标识文字格式。
<br>.1代表保留1位小数。
<br>%%表示以%结尾并结束文字格式。
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271416914.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
或者使用十六进制。<br><br>sns.set_palette("pastel")
sns.set_palette("crest")
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271417430.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271423887.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>巧用hue参数和size参数。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271424973.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271425206.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271425249.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果图例挡住了图表，那么就将其移到外面去。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271426191.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里的bbox_to_anchor中的（1，1）分别代表右，上。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271429156.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
指定粗细。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271430034.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>plt.subplots(1,3,figsize=(15,5))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271432958.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\二十八、-python数据可视化.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十八、 Python数据可视化.md</guid><pubDate>Fri, 21 Jun 2024 07:31:32 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271401126.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271401126.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[二十二、Pandas操作]]></title><description><![CDATA[ 
 <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212043448.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>Series计算时会自动对索引。<br>描述性统计：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212044813.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212044675.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>有点像R语言中的summary()。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212044727.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>#!/usr/bin/env python

# coding: utf-8

  

# In[1]:

  
  

import pandas as pd

  
  

# In[6]:

  
  

s1 = pd.Series([92,67,70,88,76],index = ["xiaoming","xiaohong","xiaogang","xiaoliu","xiaoluo"])

s2 = pd.Series([95,70,80,60,90],index = ["xiaomin","xiaohon","xiaogan","xiaoli","xiaolu"])

  
  

# In[7]:

  
  

s1.add(s2,fill_value= 0 )

  
  

# In[8]:

  
  

s1.describe()

  
  

# In[9]:

  
  

s1 = s1+5

  
  

# In[14]:

  
  

def get_grade_from_score(score):

    if score &gt;=90:

        return "A"

    if score &gt;=80 and score &lt; 90:

        return "B"

    if score &gt;=70 and score &lt; 80:

        return "C"

    if score &gt;=60 and score &lt;70:

        return "D"

s1.apply(get_grade_from_score)

  
  

# In[ ]:
复制]]></description><link>软件\python\学习笔记\二十二、pandas操作.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十二、Pandas操作.md</guid><pubDate>Fri, 21 Jun 2024 07:31:33 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212043448.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212043448.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br>线性回归就是建立一个多元方程。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281400776.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>假如要对多种因素预测房价，那么可以建立多元线性回归。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281402255.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281401965.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br># 安装库
pip install statsmodels
# 导入
import pandas as pd
import statsmodels.api as sm
# 转换分类变量
data = pd.get_dummies(data,columns = ["城市"],dtype = int, drop_first = True ) # 删除第一个分类变量，保证分类变量数只有N-1个，避免共线性问题
y = data["价格"]
x = data.["自变量"]

# 检查自变量的相关性
# 方法一
X["面积"].corr(X["卧室数"])
# 方法二
X.corr()
# 绘制热图查看
X.corr().abs()
sns.heatmap(X.corr().abs(),annot = True)
plt.plot()
# 传入截距参数
X = sm.add_constant(X)
# 建立方程
model = sm.OLS(y,X)
# 建立模型
result = sm.OLS(y,X).fit()
# 结果展示
resulit.summary()
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281411131.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281414355.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果P值过大，则去除该自变量：<br>X =X.drop(... ,  axis = 1)
result = sm.OLS(y,X).fit()
resulit.summary()
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281415202.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果  则说明拟合效果越好。<br>可以在 SPSS 进行，<a data-href="四、线性回归分析" href="软件\spss\四、线性回归分析.html" class="internal-link" target="_self" rel="noopener nofollow">四、线性回归分析</a>。]]></description><link>软件\python\学习笔记\二十九、-python线性回归.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十九、 Python线性回归.md</guid><pubDate>Fri, 21 Jun 2024 07:31:33 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281400776.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402281400776.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[修改行名和列名]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251558907.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251558722.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>记得重新赋值。<br>具体操作可以参考： <a rel="noopener nofollow" class="external-link" href="https://pandas.pydata.org/docs/reference/" target="_blank">https://pandas.pydata.org/docs/reference/</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251603477.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251604869.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251604748.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251605205.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251605149.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251605425.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251608638.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>axis = 0 # 删除行
axis = 1 # 删除列
复制]]></description><link>软件\python\学习笔记\二十六、-清洗数据.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十六、 清洗数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:33 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251558907.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251558907.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[要素]]></title><description><![CDATA[ 
 <br><br>
<br>
API端点

<br>
请求方法<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251612729.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
查询参数、请求体数据<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251613445.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">

<br>
相应格式

<br><br>import requests
import json
response = requests.get("https://www.v2ex.com/api/topics/hot.json")
parsed_result = json.loads(response.text)
复制]]></description><link>软件\python\学习笔记\二十七、-通过api获取数据.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十七、 通过API获取数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:33 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251612729.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251612729.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[二十三、 Pandas的Dataframe]]></title><description><![CDATA[ 
 <br>Dataframe 是一个表格，类似于Series组成的字典。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212057223.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>创建Dataframe的方法有很多，例如将多个Series组合起来。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212100919.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212100071.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212101124.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212101405.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212102636.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212102338.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212102638.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212103041.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>除了提取特定的行和列，还可以提取某一个值。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212104722.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>永远记住
行在前，列在后。
<br>如果希望提取所有行和特定列，那么可以：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212106626.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>记住
每一行是一个样本，每一列是一个变量。
<br><br>通过.head看前几行的数据。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212124785.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>#!/usr/bin/env python
# coding: utf-8

# In[15]:


import pandas as pd


# In[17]:


name = pd.Series(["小陈","小李","小王","小张","小赵","小周"],index = ["001","002","003","004","005","006"])


# In[18]:


gender = pd.Series(["女","女","男","男","女","男"],index = ["006","005","004","003","002","001"])


# In[19]:


height = pd.Series(["172.5","168.0","178.2","181.3","161.7"],index = ["001","002","003","004","005",])


# In[22]:


students = pd.DataFrame({"姓名":name,"性别":gender,"身高":height})
students


# In[26]:


students.index


# In[27]:


students.columns


# In[28]:


students.T


# In[31]:


students["身高"]


# In[33]:


students[["身高","性别"]]


# In[35]:


students.loc["003"]


# In[36]:


students.loc["003":"005"]


# In[37]:


students.loc["005","身高"]


# In[38]:


students.loc[["003","005"],["姓名","身高"]]


# In[41]:


students.loc[["003","005"]]


# In[51]:


students["性别"]=="女"


# In[53]:


students.head(3)


# In[ ]:





复制]]></description><link>软件\python\学习笔记\二十三、-pandas的dataframe.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十三、 Pandas的Dataframe.md</guid><pubDate>Fri, 21 Jun 2024 07:31:34 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212057223.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402212057223.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[获取数据的几种方法]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251511402.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>飞奖（百度旗下深度学习平台）数据集： <a rel="noopener nofollow" class="external-link" href="https://aistudio.baidu.com/aistudio/datasetoverview" target="_blank">https://aistudio.baidu.com/aistudio/datasetoverview</a><br>
天池（阿里云旗下开发者竞赛平台）： <a rel="noopener nofollow" class="external-link" href="https://tianchi.aliyun.com/dataset/" target="_blank">https://tianchi.aliyun.com/dataset/</a><br>
和鲸社区（数据科学开源社区）数据集： <a rel="noopener nofollow" class="external-link" href="https://www.heywhale.com/home/dataset" target="_blank">https://www.heywhale.com/home/dataset</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251512508.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
获取网页源代码，解析源代码。<br>
但是有一些红线，一定要遵循法律法规。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251513026.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
省去了爬虫解析源代码的步骤。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251514376.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>后缀决定电脑用什么程序去打开。更改文件后缀并不改变文件本身的格式。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251516552.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251517061.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>就是Python字典。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251517720.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>就是Python列表。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251518382.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251522622.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用pd.read.json函数来读取。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251528871.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用pd.read.csv.函数来读取。]]></description><link>软件\python\学习笔记\二十四、-获取数据.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十四、 获取数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251511402.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251511402.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[结构]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251538761.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
整洁度的数据有助于代码分析数据。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251540154.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>丢失数据
<br>重复数据
<br>不一致数据
<br>无效或错误数据
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251543573.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251544206.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
另外还可以通过连续调用.isnull().sum()方法来计算空缺值的数量。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251546477.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251547012.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>使用duplicated函数。<br>如何发现多个变量同时重复？<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251549929.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251549055.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251552736.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>这个难度很大。针对某些特别离谱的数据，可以发现。例如可以使用describe()方法。]]></description><link>软件\python\学习笔记\二十五、-评估数据.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十五、 评估数据.md</guid><pubDate>Fri, 21 Jun 2024 07:31:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251538761.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402251538761.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[Series]]></title><description><![CDATA[ 
 <br><br>与Numpy的数组很相似，但是不同之处在于Series可以自己指定索引值。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211714113.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211716686.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
布尔值的运算逻辑与numpy类似。<br>#!/usr/bin/env python
# coding: utf-8

# In[7]:


import pandas as pd


# In[10]:


s1 = pd.Series([-1.2,3.7,2.5,-8.2,6.3])
s1


# In[13]:


print(s1.index)
print(s1.values)


# In[14]:


s2 = pd.Series([1,2,3,4,5],index=[5,4,3,2,1])
s2


# In[17]:


s2.iloc[1]


# In[18]:


s2.loc[5:3]


# In[20]:


s3 = pd.Series({"小李":82,"小张":90})


# In[21]:


s3


# In[22]:


s3.loc["小李"]


# In[24]:


(s3&gt;80) &amp; (s3&lt;90)


# In[ ]:





复制]]></description><link>软件\python\学习笔记\二十一、pandas入门.html</link><guid isPermaLink="false">软件/Python/学习笔记/二十一、Pandas入门.md</guid><pubDate>Fri, 21 Jun 2024 07:31:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211714113.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402211714113.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[九、Python模块]]></title><description><![CDATA[ 
 <br>你不可能每次都自己写代码，所以可以导入模块：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181741849.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
不建议使用第三种方法，因为会产生冲突：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181742015.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\九、python模块.html</link><guid isPermaLink="false">软件/Python/学习笔记/九、Python模块.md</guid><pubDate>Fri, 21 Jun 2024 07:31:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181741849.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181741849.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[什么是词云]]></title><description><![CDATA[ 
 <br><br>如图所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403291124705.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br>图像可以是 jpg，png 等。<br>
画面中只应该存在黑白 2 种颜色。<br>
例如：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403291127696.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
建议使用 jpg 格式。<br><br>建立一个 txt 文件，里面放入一些文字，一行一个文字。<br>Oxidative phosphorylation
ATP synthesis
mtDNA sequencing
Mitophagy
Mitochondrial biogenesis
Mitochondrial dysfunction
mtDNA repair
Heteroplasmy
Homoplasmy
Phylogenetics
Ancestry
Molecular clock
Evolutionary biology
复制<br><br># 导入必要的库
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import random

# 自定义颜色函数，这里选择了暖色
def warm_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    colors = ["#FF5733", "#FF8D33", "#FFC300", "#FF5733", "#C70039"]
    return random.choice(colors)

# Arial字体的路径（这里是一个示例路径，根据你的系统情况进行修改）
font_path = 'C:/Windows/Fonts/arial.ttf'
# 读取文本文件
text = open("C:/Users/a/Desktop/words.txt").read()

# 加载蒙版图像（如果你使用特定形状的词云），图形应该只有黑色和白色。黑色代表蒙版（形状）
mask_image = np.array(Image.open("C:/Users/a/Desktop/未标题-1.jpg"))

# 创建词云对象
wordcloud = WordCloud(font_path=font_path,
                      background_color='white',
                      mask=mask_image,
                      color_func=warm_color_func,
                      width= 2000,height= 2000) # 大小自行调整

# 生成词云
wordcloud.generate(text)

# 使用matplotlib显示词云， 大小自行调整
plt.figure(figsize=(2000, 2000))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403291129691.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\利用python生成词云.html</link><guid isPermaLink="false">软件/Python/学习笔记/利用Python生成词云.md</guid><pubDate>Fri, 21 Jun 2024 07:31:35 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403291124705.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403291124705.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[六、Python while循环]]></title><description><![CDATA[ 
 <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181706598.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181707199.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>当我们不知道循环什么时候结束的时候，while比for更合适。有时候可以互换：<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181708965.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181709328.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>对用户输入数据求出平均值：<br>print("hello , I am a programm")
user_input = input("请输入数字，并输入q终止程序")

total = 0
count = 0 

while user_input != "q" :
	num = float(user_input)
	total = total + num
	count = count + 1 
	user_input = input("请输入数字，并输入q终止程序")

if count == 0:
	result =0
else : result = total / count 
print("结果就是：" + str(result))
复制]]></description><link>软件\python\学习笔记\六、python-while循环.html</link><guid isPermaLink="false">软件/Python/学习笔记/六、Python while循环.md</guid><pubDate>Fri, 21 Jun 2024 07:31:36 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181706598.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181706598.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[七、Python 格式化字符串]]></title><description><![CDATA[ 
 <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181719776.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181719163.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181720435.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181721377.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\七、python-格式化字符串.html</link><guid isPermaLink="false">软件/Python/学习笔记/七、Python 格式化字符串.md</guid><pubDate>Fri, 21 Jun 2024 07:31:36 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181719776.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181719776.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[多重if]]></title><description><![CDATA[ 
 <br>if是强大的语句，必须掌握。<br>
<img alt="772877b0050c8509d860cf7536ac36c.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181658040.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
写代码时，注意缩进。不同层级之间的错误缩进会导致报错。<br>
<img alt="0bac11a452e2b224d28f2030fe262aa.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181658157.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="c169ec927571c2e0a1129393d6815f9.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181659560.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="7e6b491de4b6eae220442e97c1a9b25.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181659184.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="1f17f34207a337fe8cb59573b233888.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181659854.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\三、python-if语句.html</link><guid isPermaLink="false">软件/Python/学习笔记/三、Python if语句.md</guid><pubDate>Fri, 21 Jun 2024 07:31:37 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181658040.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181658040.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br>逻辑回归是用于求算二分类数据的，得到一个概率。概率位于  之间。<br> <img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011357736.png" referrerpolicy="no-referrer"><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011402417.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>import statsmodels.apias sm # 引入模块
pd.get_dummies(data,columns=["gender", "smoking"],dtype=int,drop_first=True) # drop是为了将完全相关的两个自变量删除一个，不然会导致算法无法收敛
logistics_data.corr().abs()&gt;0.8 # 检查已有变量之间的相关性
y = logitics_data['cancer']
X = logitics_data.drop('cancer',axis = 1)
X = sm.add_constant(X) # 手动添加截距
result = sm.Logit(y,X).fit() # 建立回归模型
result.summary() # 查看结果
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011407318.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>注意，这里的表示的是以为底的次方。$$<br>
e^{coef}<br>]]></description><link>软件\python\学习笔记\三十、-python逻辑回归.html</link><guid isPermaLink="false">软件/Python/学习笔记/三十、 Python逻辑回归.md</guid><pubDate>Fri, 21 Jun 2024 07:31:37 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011357736.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011357736.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[HTTP]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011417935.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011417197.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>大多数时候用get方法。一个完整的http请求分为如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011418183.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>请求行
<br>请求头
<br>请求体<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011419636.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011420134.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403021457131.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>import requests as rq
# 伪装成浏览器，如何做呢？
# 可以直接用浏览器打开该网页，然后点击右键→检查→Network→刷新网页→
# 点击任何一个Name,查看Headers，找到Usr-Agent，复制粘贴进去就可以啦！
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}
# 将Headers传入get方法中
response = rq.get('https://movie.douban.com/top250',headers=headers)
print(response.status_code)
print(response.text) # 现在得到了html格式的网页内容

# 所有以4开头的状态码都是从错误信息，418表示服务器不想理你

复制<br><br>输入pip install bs4。<br>
为了提取所有的价格和名称，我们需要查看原始网页：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403021522360.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403021531355.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>import requests as rq
from bs4 import BeautifulSoup
# 伪装成浏览器，如何做呢？
# 可以直接用浏览器打开该网页，然后点击右键→检查→Network→刷新网页→
# 点击任何一个Name,查看Headers，找到Usr-Agent，复制粘贴进去就可以啦！
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}
# 将Headers传入get方法中


# 所有以4开头的状态码都是从错误信息，418表示服务器不想理你
for start_num in range(0,250,25):
    response = rq.get(f'https://movie.douban.com/top250?start={start_num}',headers=headers)
    print(start_num)
    html = response.text
    soup = BeautifulSoup(html,"html.parser") # 解析html，检查
    all_titles = soup.find_all("span",attrs={"class":"title"}) # 查找所有span标签，class属性为title的标签
    for title in all_titles:
        title_string = title.string
        if "/" not in title_string: # 所有斜杠不存在时才打印
            print(title_string)
        # print(title) # 找出所有的标题包括html标签
        # print(title.string) # 找出所有的标题不包括html标签

# 但是这个页面只是第一页，为了拿到后面几页，怎么办？
# 打开浏览器的网页，看到页面的网址 start=0,25,50,75,100.....
        
复制]]></description><link>软件\python\学习笔记\三十一、-python爬虫.html</link><guid isPermaLink="false">软件/Python/学习笔记/三十一、 Python爬虫.md</guid><pubDate>Fri, 21 Jun 2024 07:31:37 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011417935.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403011417935.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十、Python创建类]]></title><description><![CDATA[ 
 <br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181753210.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意，定义类别需要使用首字母大写来定义：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181753815.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>class CuteCat:
	def _init_(self,cat_name):
		self.name =cat.name
cat1 = CuteCat("Jojo)
print(cat1.name)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181757823.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>class CuteCat:
	def _init_(self, cat_name, cat_age, cat_color):
	self.name = cat.name
	self.age = cat_age
	self.coler = cat_color
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181801522.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181802891.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br># 定义类Student
class Student:
# 从零开始，自身，名字，编号
    def __init__(self, name, student_id):

        self.name = name

        self.student_id = student_id

        self.grades = {"语文":0,"数学":0,"英语":0}

    def set_grade(self,course,grade):

        if course in self.grades:

            self.grades[course] = grade

        else:

            print("课程不存在")

    def print_grade(self):

        print(self.name)

        print(self.student_id)

        print(self.grades)

        for course in self.grades:

            print(course,self.grades[course])

  

chen = Student("小陈","100618")

chen.set_grade("语文",90)

chen.set_grade("数学",90)

chen.print_grade()

zeng = Student("小曾","100622")

print(chen.name)

zeng.set_grade("数学",90)

print(zeng.grades)
复制]]></description><link>软件\python\学习笔记\十、python创建类.html</link><guid isPermaLink="false">软件/Python/学习笔记/十、Python创建类.md</guid><pubDate>Fri, 21 Jun 2024 07:31:37 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181753210.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181753210.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十八、 Jupyter使用]]></title><description><![CDATA[ 
 <br>略]]></description><link>软件\python\学习笔记\十八、-jupyter使用.html</link><guid isPermaLink="false">软件/Python/学习笔记/十八、 Jupyter使用.md</guid><pubDate>Fri, 21 Jun 2024 07:31:37 GMT</pubDate></item><item><title><![CDATA[十二、 Python文件操作]]></title><description><![CDATA[ 
 <br>使用open函数打开文件：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192211056.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192211240.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192212942.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>在文件比较大的情况下，不要直接用read，不然可能会爆内存。<br>
在这种情况下，可以使用：<br># 读第1-10 字节
print(f.read(10))
复制<br>或者使用<br>print(f.readline()) # 读一行
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192215666.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192215963.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>读完文件之后，需要close关闭文件。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192215610.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果经常忘记关闭文件，可以用这种方法读取文件：<br>with open(...) as f:
	print(f.read())
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192220783.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192220485.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十二、-python文件操作.html</link><guid isPermaLink="false">软件/Python/学习笔记/十二、 Python文件操作.md</guid><pubDate>Fri, 21 Jun 2024 07:31:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192211056.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192211056.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十九、 Numpy入门]]></title><description><![CDATA[ 
 <br>数组更适合用于分析，Numpy的矩阵默认只能使用一种type。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202222804.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202223706.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>在python中的range方法也有类似的，在numpy中是np.arange(5,10,2)。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202224501.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>#!/usr/bin/env python

# coding: utf-8

  

# In[1]:

  
  

import numpy as np

  
  

# In[5]:

  
  

arr1 = np.array([6,2,-7,2,8,-2,1])

arr1

  
  

# # 创建了一个数组

  

# In[6]:

  
  

arr2 = np.array([[1,3,5],[2,4,6]])

arr2

  
  

# # 创建了另一个数组

  

# In[7]:

  
  

print(arr1.ndim)

  
  

# In[8]:

  
  

print(arr2.ndim)

  
  

# In[9]:

  
  

print(arr1.shape)

  
  

# In[10]:

  
  

print(arr2.shape)

  
  

# In[14]:

  
  

arr_all_0 = np.zeros(3)

arr_all_0

  
  

# In[16]:

  
  

arr_all_1 = np.ones(3)

arr_all_1

  
  

# In[20]:

  
  

arr_range = np.arange(5,10,2)

arr_range

  
  

# In[ ]:
复制]]></description><link>软件\python\学习笔记\十九、-numpy入门.html</link><guid isPermaLink="false">软件/Python/学习笔记/十九、 Numpy入门.md</guid><pubDate>Fri, 21 Jun 2024 07:31:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202222804.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402202222804.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十六、 Python高阶函数]]></title><description><![CDATA[ 
 <br>当我们想在一个程序里执行多种函数操作，可能需要多种函数。我们可以使用def进行定义，然后用函数来执行函数，类似于数学中的：<br><br>例如，我def了一个计算平方的函数，那么我就可以：<br>calculate_and_print(3,calculate_square)
复制<br>请注意，括号中的函数不要再加括号。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192250272.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>一次性函数。可以当成只用一次函数，减少代码行数，不需要起名字。只使用于比较简单的场景。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192252917.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十六、-python高阶函数.html</link><guid isPermaLink="false">软件/Python/学习笔记/十六、 Python高阶函数.md</guid><pubDate>Fri, 21 Jun 2024 07:31:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192250272.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192250272.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[如何安装？]]></title><description><![CDATA[ 
 <br>为什么要用Jupyter？<br>
<br>能够分行执行代码，一次只运行一次代码。
<br>支持丰富的信息格式，有二级、三级标题等。

<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192256634.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">


<br>交互模式环境：想查看输出的时候，不要打印。有利于快速探索数据。
<br><br><br>
<br>windows: 命令提示符
<br>pip install notebook
<br>jupyter notebook
<br><br>安装ANACONDA导航<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192300937.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十七、jupyter数据分析安装.html</link><guid isPermaLink="false">软件/Python/学习笔记/十七、Jupyter数据分析安装.md</guid><pubDate>Fri, 21 Jun 2024 07:31:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192256634.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192256634.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十三、Python 文件操作]]></title><description><![CDATA[ 
 <br>当我们需要写文件时，需要传入模式"w"。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192223391.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
需要注意，这会清除原文件的内容。<br>如果不想清除原本的内容，需要附加，那么就不能够使用w，而需要"a"。<br>如果我们需要既要读，又要写，那么可以使用"r+"。<br><br>with open("C:/Users/a/Desktop/poem.txt","w",encoding="utf-8") as f:

    f.write("我欲乘风归去\n")

    f.write("又恐琼楼玉宇\n")

    f.write("高处不胜寒\n")

  

with open("C:/Users/a/Desktop/poem.txt","a",encoding="utf-8") as f:

    f.write("谁家TRS80\n")

    f.write("夜半来客\n")
复制]]></description><link>软件\python\学习笔记\十三、python-文件操作.html</link><guid isPermaLink="false">软件/Python/学习笔记/十三、Python 文件操作.md</guid><pubDate>Fri, 21 Jun 2024 07:31:38 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192223391.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192223391.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十四、Python预判]]></title><description><![CDATA[ 
 <br>来一波预判！<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192235730.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果你不知道会发生什么错误，可以这样：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192236369.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192236058.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十四、python预判.html</link><guid isPermaLink="false">软件/Python/学习笔记/十四、Python预判.md</guid><pubDate>Fri, 21 Jun 2024 07:31:39 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192235730.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192235730.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十五、 Python测试]]></title><description><![CDATA[ 
 <br>为了提前发现代码，可以使用assert语句。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192239074.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
Python自带的语句可以测试布尔值是否True。<br>或者可以使用unittest模块：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192240142.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192241918.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十五、-python测试.html</link><guid isPermaLink="false">软件/Python/学习笔记/十五、 Python测试.md</guid><pubDate>Fri, 21 Jun 2024 07:31:39 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192239074.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192239074.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[十一、 Python文件路径]]></title><description><![CDATA[ 
 <br>代码如何找到文件？<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192207239.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192207538.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>绝对路径
<br>相对路径
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192208004.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192208032.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192209244.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
很多编辑器都能够复制路径。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192209917.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\十一、-python文件路径.html</link><guid isPermaLink="false">软件/Python/学习笔记/十一、 Python文件路径.md</guid><pubDate>Fri, 21 Jun 2024 07:31:40 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192207239.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402192207239.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[列表]]></title><description><![CDATA[ 
 <br><br>shopping_list = ["电脑", "手机", "平板"]
# 增加元素
shopping_list.append("女朋友")
复制<br><img alt="c4611368932cfc3525c6f9237ebd3ae.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181701443.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="b50f710bf834a757f5d13ca5788008d.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181701032.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="9a5bc1f6249e931d53bad486ff1c7ac.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181704748.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="a0aaa3b4119b15432f33c027ebd2d5c.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181704252.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="50f9aae60078870a5b9c0a35acd42ab.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181704806.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="cca2d29a66bc43c09fdfcbd7c04f454.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181704305.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\四、python列表和字典.html</link><guid isPermaLink="false">软件/Python/学习笔记/四、Python列表和字典.md</guid><pubDate>Fri, 21 Jun 2024 07:31:40 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181701443.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181701443.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[五、Python for循环]]></title><description><![CDATA[ 
 <br>列表是Python中可以用来迭代的对象。函数格式如下所示：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181639312.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181646307.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以配合range一起使用：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181647432.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以用来打印偶数奇数等：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181649469.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>用来解决高斯小学题目：<br>total = 0

for i in range(1,101):

    total = total + i

    print(total)
复制]]></description><link>软件\python\学习笔记\五、python-for循环.html</link><guid isPermaLink="false">软件/Python/学习笔记/五、Python for循环.md</guid><pubDate>Fri, 21 Jun 2024 07:31:40 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181639312.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181639312.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[逻辑运算符]]></title><description><![CDATA[ 
 <br><img alt="40d5dda63729c1302d82bad33c1951e.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654758.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="2f8561eec3b52957c9daa836b15cc89.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654945.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="1180d92bfe24369fce8d055dc376ecc.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654266.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="ba9f3d4584adb3cb80a8213f4176466.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654340.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="5cbd2817f23e0a10d316cce6cacda46.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181655256.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="800d24884967c0e17a190fa7b862f6a.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181655512.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="cd36aaa9958c49d6038bfcc49496ead.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181655421.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="ec83b36affbdae3d8cfc67f504ba117.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181655927.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="a70b7f391b92b77baa37aee9b45ccec.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181700888.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="6da69c249f3c157be6294ed6bbf90cf.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181700252.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="55943abefc415555f2924f15abc6eac.jpg" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181655086.jpg" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\一、python数据类型.html</link><guid isPermaLink="false">软件/Python/学习笔记/一、Python数据类型.md</guid><pubDate>Fri, 21 Jun 2024 07:31:41 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654758.jpg" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402181654758.jpg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[前提]]></title><description><![CDATA[ 
 <br>警告！禁止商用，违者自负责任！<br><br>首先，你需要创建一个微信读书的账号，其次，你的微信读书的账号必须已经购买了你想要导出的书籍，否则会导出不完整！<br><br>地址： <a data-tooltip-position="top" aria-label="https://1drv.ms/u/s!AnGqDjyiZ5t_hotFKoYKLpC_DfgMLw?e=nabvhC" rel="noopener nofollow" class="external-link" href="https://1drv.ms/u/s!AnGqDjyiZ5t_hotFKoYKLpC_DfgMLw?e=nabvhC" target="_blank">weread-exporter-main.zip</a><br>
下载之后，解压，进入该文件夹，然后输入：<br>pip3 install -e .
复制<br><br>默认使用 谷歌chrome 浏览器，你可以下载安装之后进行配置：浏览器默认安装在：C:\Program Files\Google\Chrome\Application\。<br>安装完成之后，你需要修改代码如下的：weread-exporter-main\weread_exporter\webpage.py，进入这个文件，然后在 def _check_chrome(self): 的下一行添加如下代码: return "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"<br>于是，这段代码应该变成了：<br>    def _check_chrome(self):
        return "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
        path_list = os.environ["PATH"].split(";" if sys.platform == "win32" else ":")
        for chrome in ("chrome", "google-chrome"):
            if sys.platform == "win32":
                chrome += ".exe"
            for path in path_list:
                if os.path.isfile(os.path.join(path, chrome)):
                    return chrome
        if sys.platform == "darwin":
            chrome = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
            if os.path.isfile(chrome):
                return chrome
        if sys.platform == "win32":
            command = "where chrome"
        else:
            command = "which chrome"
        raise utils.ChromeNotInstalledError(
            "Please make sure `chrome` is installed, and the install path is added to PATH environment. \nYou can test that with `%s` command."
            % command
        )
复制<br><br>python -m weread_exporter -b $book_id -o epub -o pdf --force-login
复制<br>-o参数用于指定要保存的文件格式，目前支持的格式有：epub、pdf、mobi，生成的文件在当前目录下的output目录中。<br>
epub 格式适合手机端访问，pdf 格式适合电脑端访问，mobi 格式适合kindle访问。<br>导出时间随书本的长度而增长，因为需要一页页渲染，没办法。]]></description><link>软件\python\学习笔记\python：导出微信读书中的书籍.html</link><guid isPermaLink="false">软件/Python/学习笔记/python：导出微信读书中的书籍.md</guid><pubDate>Fri, 27 Sep 2024 06:47:03 GMT</pubDate></item><item><title><![CDATA[什么是 Manim]]></title><description><![CDATA[ 
 <br><br>Manim 是一个开源的动画引擎，主要用于创建数学可视化和动画。它是由 Grant Sanderson 开发的，Grant Sanderson 是斯坦福大学的数学系学生，同时也是知名的数学和科学教育网站 3Blue1Brown 的创始人。Manim 最初是为了帮助 Grant 在其 YouTube 频道 3Blue1Brown 中创建数学教学视频中的动画效果而开发的。Manim 的核心编程语言是 Python，它利用 Python 的强大功能，结合其他 Python 库如 Numpy（用于数值计算）、Matplotlib（用于绘制图形）、PIL（用于处理图像）和 pycairo（用于矢量图形绘制）等，使得开发者可以轻松创建数学动画和可视化效果。<br><br>直接使用 pip 安装即可。<br>pip3 install manim
复制<br><br><br>Manim 中有三个类：<br>
<br>Scene：播放的场景。
<br>Mobject：物体。
<br>Animation：动画。
<br><br>有4个常用的方法：<br>
<br>add
<br>remove
<br>play
<br>wait
<br>self.add(物体)
self.remove(物体)
self.play(FadeIn(物体))
self.play(FadeOut(物体))
self.wait(1)
复制<br><br>self.play(FadeIn(物体))
self.play(FadeOut(物体))
self.play(ReplacementTransform(物体1,物体2),run_time=3)
复制<br><br>很多。<br><br>manim 你的文件.py -p
# -p表示渲染后直接打开
复制]]></description><link>软件\python\学习笔记\python：manim入门.html</link><guid isPermaLink="false">软件/Python/学习笔记/python：Manim入门.md</guid><pubDate>Sun, 06 Oct 2024 08:06:39 GMT</pubDate></item><item><title><![CDATA[PandasGUI]]></title><description><![CDATA[ 
 <br><br>Pandas 是处理和分析数据的利器，但对于编程和算法基础薄弱，注意力不易集中的朋友来说，纯代码的方式可能显得有些枯燥乏味。<br><a data-tooltip-position="top" aria-label="https://github.com/adamerose/PandasGUI" rel="noopener nofollow" class="external-link" href="https://github.com/adamerose/PandasGUI" target="_blank">原文链接</a><br>PandasGUI 是一个基于 Pandas 的图形用户界面工具，旨在通过可视化的方式简化数据分析流程。<br>它将数据的导入、清洗、探索和可视化等步骤变得直观易操作，即使是编程新手也能快速上手，深入数据的世界。<br>主要特点<br>
<br>交互式界面：PandasGUI 提供了一个直观的图形界面，让用户可以直接看到数据和操作的即时反馈。<br>

<br>数据探索：支持直接在界面上对数据进行筛选、排序和编辑，简化了数据预处理的复杂度。<br>

<br>可视化支持：内置多种数据可视化工具，一键生成图表，帮助用户直观理解数据。<br>

<br>代码生成：虽然是图形界面操作，但 PandasGUI 能自动生成对应的 Pandas 代码，便于学习和进一步定制化处理。<br>

<br>功能特性<br>
<br>查看 DataFrame 和 Series（支持多索引）<br>

<br>交互式绘图<br>

<br>筛选<br>

<br>统计摘要<br>

<br>数据编辑和复制/粘贴<br>

<br>通过拖放导入 CSV 文件<br>

<br>搜索工具栏
<br>pip install pandasgui
复制<br>from pandasgui import show  
import pandas as pd

# 加载数据集  
df = pd.read_csv('你的数据文件路径/data.csv')

# 使用 PandasGUI 展示数据集  
show(df)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181002743.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>PyGWalker 是一个在 Jupyter Notebook 环境中运行的可视化分析工具，使用一行代码即可将数据集转化为一个可视化分析工具，生成一个可拖拽、可交互的图形界面，以类似 Tableau/PowerBI 的方式，通过拖拽字段来生成各种图表进行数据分析，从而大幅减少数据分析师在数据可视化上的时间成本，能够快速出图、快速分析。<br><a data-tooltip-position="top" aria-label="https://github.com/Kanaries/pygwalker" rel="noopener nofollow" class="external-link" href="https://github.com/Kanaries/pygwalker" target="_blank">Kanaries/pygwalker: PyGWalker: Turn your pandas dataframe into an interactive UI for visual analysis (github.com)</a><br>这里还有为 R 打造的GWalkR：<a data-tooltip-position="top" aria-label="https://github.com/Kanaries/GWalkR/blob/main/docs/README.zh.md" rel="noopener nofollow" class="external-link" href="https://github.com/Kanaries/GWalkR/blob/main/docs/README.zh.md" target="_blank">GWalkR/docs/README.zh.md at main · Kanaries/GWalkR (github.com)</a><br><br><a rel="noopener nofollow" class="external-link" href="https://www.kaggle.com/asmdef/pygwalker-test" target="_blank">https://www.kaggle.com/asmdef/pygwalker-test</a><br>
<a rel="noopener nofollow" class="external-link" href="https://colab.research.google.com/drive/171QUQeq-uTLgSj1u-P9DQig7Md1kpXQ2?usp=sharing" target="_blank">https://colab.research.google.com/drive/171QUQeq-uTLgSj1u-P9DQig7Md1kpXQ2?usp=sharing</a><br>
<a rel="noopener nofollow" class="external-link" href="https://graphic-walker.kanaries.net/" target="_blank">https://graphic-walker.kanaries.net/</a><br>pip install pygwalker
复制<br>在您的 Jupyter Notebook 中导入 pygwalker 和 pandas 来开始使用。<br>import pandas as pd
import pygwalker as pyg

df = pd.read_csv('./bike_sharing_dc.csv')
walker = pyg.walk(df)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181006891.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\python\学习笔记\python交互式库.html</link><guid isPermaLink="false">软件/Python/学习笔记/Python交互式库.md</guid><pubDate>Fri, 21 Jun 2024 07:31:41 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181002743.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181002743.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[前言]]></title><description><![CDATA[ 
 <br>
<a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&amp;mid=2247614398&amp;idx=1&amp;sn=68848b771fc627acc5ddf0e7ceb3cc13&amp;chksm=e9e03635de97bf238af998fd7e00fd24a62c6e92b89e9df4bdd72ccab4ffce5fb4c8bd33f014&amp;mpshare=1&amp;scene=1&amp;srcid=0408McLgGUaclsKBZRMR5g1z&amp;sharer_shareinfo=24d5bfcda3f93a51759a82e3234ff212&amp;sharer_shareinfo_first=24d5bfcda3f93a51759a82e3234ff212#rd" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&amp;mid=2247614398&amp;idx=1&amp;sn=68848b771fc627acc5ddf0e7ceb3cc13&amp;chksm=e9e03635de97bf238af998fd7e00fd24a62c6e92b89e9df4bdd72ccab4ffce5fb4c8bd33f014&amp;mpshare=1&amp;scene=1&amp;srcid=0408McLgGUaclsKBZRMR5g1z&amp;sharer_shareinfo=24d5bfcda3f93a51759a82e3234ff212&amp;sharer_shareinfo_first=24d5bfcda3f93a51759a82e3234ff212#rd" target="_blank">Scikit-Learn 的建模万能模板！ (qq.com)</a>
<br><br>你只需要两步就能构建起自己的机器学习模型：<br>
<br>明确你需要解决的问题是什么类型，以及知道解决该类型问题所对应的算法。
<br>从skicit-learn中调用相应的算法构建模型即可。是的！在机器学习领域，如果你只是抱着体验机器学习的心态，实现起来就是这么简单。<br>
<img alt="ml_map.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404221610810.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><br>常见的问题类型只有三种：分类、回归、聚类。<br>比如，如果你需要通过输入数据得到一个类别变量，那就是分类问题。分成两类就是二分类问题，分成两类以上就是多分类问题。常见的有：<br>
<br>判别一个邮件是否是垃圾邮件、根据图片分辩图片里的是猫还是狗等等。
<br>如果你需要通过输入数据得到一个具体的连续数值，那就是回归问题。比如：预测某个区域的房价等。常用的分类和回归算法算法有：SVM (支持向量机) 、xgboost、, KNN、LR算法、SGD (随机梯度下降算法)、Bayes (贝叶斯估计)以及随机森林等。这些算法大多都既可以解分类问题，又可以解回归问题。
<br>如果你的数据集并没有对应的属性标签，你要做的，是发掘这组样本在空间的分布, 比如分析哪些样本靠的更近，哪些样本之间离得很远, 这就是属于聚类问题。常用的聚类算法有k-means算法。
<br><br>在介绍万能模板之前，为了能够更深刻地理解这三个模板，我们加载一个Iris（鸢尾花）数据集来作为应用万能模板的小例子，Iris数据集在前边的文章中已经提到过多次了，这里不再赘述。它是一个典型的多分类问题。加载步骤如下：<br><br>因为原始的数据集中包含很多空值，而且类别特征用英文名表示各个花的名字，也需要我们转换成数字。<br>在 scikit-learn 下的 datasets 子包里，也自带了一个 Iris 数据集，这个数据集和原始数据集的区别就是 scikit-learn 已经帮我们提前处理好了空值等问题，可以直接输入模型用来训练。所以为了方便起见，我们直接使用 scikit-learn 的数据集。加载方法如下：<br>from sklearn.datasets import load_iris
data = load_iris()
x = data.data
y = data.target
复制<br>x 值如下，可以看到 scikit-learn 把数据集经过去除空值处理放在了 array 里，所以 x 是一个（150,4）的数组，保存了150个数据的4个特征：<br>array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4], [4.6, 3.4, 1.4, 0.3], [5. , 3.4, 1.5, 0.2], [4.4, 2.9, 1.4, 0.2], [4.9, 3.1, 1.5, 0.1], [5.4, 3.7, 1.5, 0.2], [4.8, 3.4, 1.6, 0.2], [4.8, 3. , 1.4, 0.1], [4.3, 3. , 1.1, 0.1], …………
复制<br>y 值如下，共有150行，其中0、1、2分别代表三类花：<br>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
复制<br>为了方便查看数据，我们可以将数据集转换为 DataFrame 来进行查看：<br>from sklearn.datasets import load_iris
import pandas as pd

# 加载数据集
iris = load_iris()

# 将数据集转换为DataFrame以便更容易查看
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# 查看数据的前五行
print(iris_df.head())

# 查看目标变量（标签）
print("Target:", iris.target)

# 查看数据集描述
print(iris.DESCR)

复制<br><br>数据集拆分是为了验证模型在训练集和测试集是否过拟合，使用 train_test_split 的目的是保证从数据集中均匀拆分出测试集。这里，简单把10%的数据集拿出来用作测试集。<br># 拆分数据集
from sklearn.model_selection import train_test_split  
train_x,test_x,train_y,test_y = \
train_test_split(x,y,test_size=0.1,random_state=0)
复制<br><br>from sklearn.算法位置 import 算法名字
from sklearn.metrics import accuracy_score # 用精确度评估模型

# 生成一个模型对象
模型名 = 算法名字(模型参数选填)

# 训练
模型名.fit(train_x,train_y)

# 在训练集评估模型
pred1 = 模型名.predict(train_x)
accuracy1 = accuracy_score(train_y,pred1)
print("在训练集的精确度为:{}".format(accuracy1))

# 在测试集评估模型
pred2 = 模型名.predict(text_x)
accuracy2 = accuracy_score(text_y,pred2)
print("在训练集的精确度为:{}".format(accuracy2))
复制<br>不同的算法只是改变了名字，以及模型的参数不同而已。<br>
有了这个万能模板，接下来就是简单的复制粘贴改名字了：<br>
而且在 scikit-learn 中，每个包的位置都是有规律的。<br><br><br>通过查阅资料，我们知道svm算法在scikit-learn.svm.SVC下，所以：<br>
<br>算法位置填入：svm
<br>算法名填入：SVC()
<br>模型名自己起，这里我们就叫svm_model
<br># svm分类器

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

svm_model = SVC()

svm_model.fit(train_x,train_y)

pred1 = svm_model.predict(train_x)
accuracy1 = accuracy_score(train_y,pred1)
print('在训练集上的精确度: %.4f'%accuracy1)

pred2 = svm_model.predict(test_x)
accuracy2 = accuracy_score(test_y,pred2)
print('在测试集上的精确度: %.4f'%accuracy2)
复制<br><br>同理，找到LR算法在sklearn.linear_model.LogisticRegression下，所以：<br>
<br>算法位置填入：linear_model
<br>算法名填入：LogisticRegression
<br>模型名叫做：lr_model。<br>
程序如下：
<br># LogisticRegression分类器

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score #评分函数用精确度评估

lr_model = LogisticRegression()

lr_model.fit(train_x,train_y)

pred1 = lr_model.predict(train_x)
accuracy1 = accuracy_score(train_y,pred1)
print('在训练集上的精确度: %.4f'%accuracy1)

pred2 = lr_model.predict(test_x)
accuracy2 = accuracy_score(test_y,pred2)
print('在测试集上的精确度: %.4f'%accuracy2)
复制<br><br>随机森林算法在 sklearn.ensemble.RandomForestClassifier 下，好了，现在你应该可以自己写了。<br><br><br>在1.0版的模板中，当你多次运行同一个程序就会发现：每次运行得到的精确度并不相同，而是在一定范围内浮动，这是因为数据输入模型之前会进行选择，每次训练时数据输入模型的顺序都不一样。所以即使是同一个程序，模型最后的表现也会有好有坏。<br>更糟糕的是，有些情况下，在训练集上，通过调整参数设置使模型的性能达到了最佳状态，但在测试集上却可能出现过拟合的情况。这个时候，我们在训练集上得到的评分不能有效反映出模型的泛化性能。<br>为了解决上述两个问题，还应该在训练集上划分出验证集(validation set)并结合交叉验证来解决。首先，在训练集中划分出不参与训练的验证集，只是在模型训练完成以后对模型进行评估，接着再在测试集上进行最后的评估。<br>但这样大大减少了可用于模型学习的样本数量，所以还需要采用交叉验证的方式多训练几次。比如说最常用的 k-折交叉验证如下图所示，它主要是将训练集划分为 k 个较小的集合。然后将 k-1份训练子集作为训练集训练模型，将剩余的 1 份训练集子集作为验证集用于模型验证。这样需要训练 k 次，最后在训练集上的评估得分取所有训练结果评估得分的平均值。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404231404268.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这样一方面可以让训练集的所有数据都参与训练，另一方面也通过多次计算得到了一个比较有代表性的得分。唯一的缺点就是计算代价很高，增加了k倍的计算量。<br>
原理就是这样，但理想很丰满，现实很骨干。在自己实现的时候却有一个很大的难题摆在面前：怎么能够把训练集均匀地划分为K份？<br>这个问题不用思考太多，既然别忘了，我们现在是站在巨人的肩膀上，scikit-learn 已经将优秀的数学家所想到的均匀拆分方法和程序员的智慧融合在了cross_val_score()&nbsp;这个函数里了，只需要调用该函数即可，不需要自己想什么拆分算法，也不用写 for 循环进行循环训练。<br>from sklearn.算法位置 import 算法名字
from sklearn.metrics import accuracy_score # 用精确度评估模型

# 生成一个模型对象
模型名 = 算法名字(模型参数选填)

# 训练
模型名.fit(train_x,train_y)

scores1=cross_val_score(模型名,train_x,train_y,cv=n,scoring='accuracy')
# 输出精确度的平均值
print("在训练集的精确度为:{}".format(scores1.mean()))

scores2=cross_val_score(模型名,train_x,train_y,cv=n,scoring='accuracy')
# 输出精确度的平均值
print("在训练集的精确度为:{}".format(scores2.mean()))

# 输出精确度的平均值  
# print("训练集上的精确度: %0.2f " % scores1.mean())

# 输出精确度的平均值和置信度区间  
print("训练集上的平均精确度: %0.2f (+/- %0.2f)" % (scores2.mean(), scores2.std() * 2))
复制<br><br><br>### svm分类器

from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

svm_model = SVC()
svm_model.fit(train_x,train_y)

scores1 = cross_val_score(svm_model,train_x,train_y,cv=5, scoring='accuracy')
# 输出精确度的平均值和置信度区间
print("训练集上的精确度: %0.2f (+/- %0.2f)" % (scores1.mean(), scores1.std() * 2))

scores2 = cross_val_score(svm_model,test_x,test_y,cv=5, scoring='accuracy')
# 输出精确度的平均值和置信度区间
print("测试集上的平均精确度: %0.2f (+/- %0.2f)" % (scores2.mean(), scores2.std() * 2))


print(scores1)
print(scores2)
复制<br><br># LogisticRegression分类器

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression


lr_model = LogisticRegression()
lr_model.fit(train_x,train_y)

scores1 = cross_val_score(lr_model,train_x,train_y,cv=5, scoring='accuracy')
# 输出精确度的平均值和置信度区间
print("训练集上的精确度: %0.2f (+/- %0.2f)" % (scores1.mean(), scores1.std() * 2))

scores2 = cross_val_score(lr_model,test_x,test_y,cv=5, scoring='accuracy')
# 输出精确度的平均值和置信度区间
print("测试集上的平均精确度: %0.2f (+/- %0.2f)" % (scores2.mean(), scores2.std() * 2))

print(scores1)
print(scores2)
复制<br><br>以上都是通过算法的默认参数来训练模型的，不同的数据集适用的参数难免会不一样，自己设计算法是设计不来的，只能调调参这样子，调参，是广大算法工程师最后的尊严。再说，若是做算法不调参，岂不是辱没了算法工程师在江湖上大名鼎鼎的“炼丹工程师”的名声？<br>scikit-learn对于不同的算法也提供了不同的参数可以自己调节。如果细说起来，又能写好几篇文章，本文目的是构建一个万能算法框架构建模板，所以，这里只介绍一下一个通用的自动化调参方法，至于更细节的每个算法对应参数的含义以及手动调参方法，会在以后的文章中结合实例具体慢慢介绍。<br>首先要明确的是，scikit-learn 提供了算法().get_params()方法来查看每个算法可以调整的参数，比如说，我们想查看 SVM 分类器算法可以调整的参数，可以：<br>SVC().get_params()
复制<br>接着，就可以引出我们的 V3.0版万能模板了。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404231410415.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
程序就会按照顺序测试这几个参数的组合效果，根本不需要自己辛辛苦苦实现。写到这里，感谢各为大佬编写了scikit-learn这么方便的机器学习包。忽然就想到了一句话：哪有什么岁月静好，只是因为有人替你负重前行。<br>看到这里，可能有人会有疑惑：为什么要采用列表、字典、列表三层嵌套的方式呢？params直接是字典的形式不行吗？答案是：行，但是不好。<br>让我们先算一个小的数学题：假如我们要调节n个参数，每个参数有4个备选值。那么程序就会训练&nbsp;&nbsp;。当n为10的时候，&nbsp;，这是一个对于计算机来说庞大的计算量。而当我们将这10个参数拆分成5组，每次只调节两个参数，其他参数采用默认值，那么计算量就是&nbsp;&nbsp;，计算量会大大减少。列表的作用这是如此，保证了每次只调节列表中的一个字典中的参数。<br>
运行之后，best_model就是我们得到的最优模型，可以利用这个模型进行预测。<br>
当然，best_model 还有好多好用的属性：<br>
<br>bestmodel.cv_results：可以查看不同参数情况下的评价结果。<br>

<br>bestmodel.param&nbsp;:得到该模型的最优参数<br>

<br>bestmodel.best_score:&nbsp;得到该模型的最后评分结果
<br><br><br>###1、svm分类器
from sklearn.model_selection import cross_val_score,GridSearchCV
from sklearn.svm import SVC

svm_model = SVC()

params = [
        {'kernel': ['linear'], 'C': [1, 10, 100, 100]},
        {'kernel': ['poly'], 'C': [1], 'degree': [2, 3]},
        {'kernel': ['rbf'], 'C': [1, 10, 100, 100], 'gamma':[1, 0.1, 0.01, 0.001]}
        ]


best_model = GridSearchCV(svm_model, param_grid=params,cv = 5,scoring = 'accuracy')
best_model.fit(train_x,train_y)

# 查看最优秀
best_model.best_score_

# 查看最优参数
best_model.best_params_

# 查看最优模型的所有参数
best_model.best_estimator_

# 查看每个参数的交叉验证结果
best_model.cv_results_
复制]]></description><link>软件\python\学习笔记\scikit-learn-的建模万能模板！.html</link><guid isPermaLink="false">软件/Python/学习笔记/Scikit-Learn 的建模万能模板！.md</guid><pubDate>Fri, 21 Jun 2024 07:31:42 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404221610810.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404221610810.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>将<a data-href="R：Fst绘制频率热图矩阵" href="软件\r语言语法\r：fst绘制频率热图矩阵.html" class="internal-link" target="_self" rel="noopener nofollow">R：Fst绘制频率热图矩阵</a>整理成矩形。<br>
这一步不知道怎么完成的可以看<a data-href="DnaSP：AMOVA及Fst分析软件操作" href="软件\其它生信软件\a-j\dnasp：amova及fst分析软件操作.html" class="internal-link" target="_self" rel="noopener nofollow">DnaSP：AMOVA及Fst分析软件操作</a>、<a data-href="python：将FASTA转换成Arp" href="软件\python\数据科学与格式转换\python：将fasta转换成arp.html" class="internal-link" target="_self" rel="noopener nofollow">python：将FASTA转换成Arp</a>.<br><br>然后运行下列代码<br>library(ggplot2)
library(ggrepel)
library(reshape2)
library(plyr)
library(zoo)
library(grid)
library(gridExtra)
library(reshape2)
library("ggsci")
library("ggplot2")
library("gridExtra")

#######E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops
setwd("C:/Users/HGL/Desktop/Wangjie/") # 这里替换成Fst矩阵
mydata&lt;-read.table("C:/Users/HGL/Desktop/Wangjie/37POPSRst_Fst.csv",header=TRUE,sep=",", row.names = 1)
library(ggplot2)
mds=cmdscale(mydata,k=4,eig=T)
write.csv(mds$points,file="mdsFst_SNP_genotypes_Global.csv") # 这里替换为输出文件
复制<br><br>你得到了一个表格，类似下列。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402271925512.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
将V1,V2,V3,V4当做PC1,PC2,PC3,PC4来绘制散点图即可。<br>
例如，为了探究前两个成分，则将V1作为X轴，V2作为Y轴。
<br># 这个代码我不知道有什么用，反正不是我写的
mydata&lt;-read.table("C:/Users/HGL/Desktop/Wangjie/MDS.csv",header=TRUE,sep=",")
p1 &lt;- ggplot(mydata, aes(x = PC1, y = PC2,fame = TRUE))
p1  + geom_point(size = 3, aes(shape = Group, color = Group)) + 
  theme_bw() + 
  scale_shape_manual(values = c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,23,0,1,2,15,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,23,0,1,17,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,22,23))+
  scale_colour_manual(values=c("#3B4992FF", "#EE0000FF", "#008B45FF", "#631879FF", "#008280FF", "#BB0021FF","#5F559BFF","#A20056FF", "#4DBBD5FF", "#00A087FF", "#3C5488FF", "#F39B7FFF", "#8491B4FF", "#91D1C2FF", "#DC0000FF", "#7E6148FF", "#5050FFFF","#CE3D32FF","#749B58FF","#F0E685FF","#466983FF","#BA6338FF","#5DB1DDFF","#802268FF","#6BD76BFF", "#D595A7FF","#924822FF","#837B8DFF","#C75127FF","#D58F5CFF","#7A65A5FF","#E4AF69FF","#3B1B53FF","#CDDEB7FF","#612A79FF","#AE1F63FF","#E7C76FFF","#5A655EFF","#A9A9A9FF","#CC9900FF","#99CC00FF","#33CC00FF","#00CC33FF","#00CC99FF","#0099CCFF","#0A47FFFF","#4775FFFF","#FFC20AFF","#FFD147FF","#990033FF","#991A00FF","#996600FF","#809900FF","#339900FF","#00991AFF","#009966FF","#008099FF","#003399FF","#1A0099FF","#660099FF","#990080FF","#D60047FF","#FF1463FF","#00D68FFF","#14FFB1FF","#E64B35FF"))+
  geom_hline(yintercept=0,linetype="dashed", size=0.4,colour="black") + 
  geom_vline(xintercept=0,linetype="dashed", size=0.4,colour="black")+
  theme(legend.position = "left",legend.key.size = unit(0.1, "cm"),legend.key.width = unit(0.05, "cm"),legend.background = element_rect(fill="white",  size=0.5, linetype="solid", colour ="black"))+
  geom_text_repel(data = mydata, aes(x = PC1, y = PC2, label = Text, color = Group,size = 5))








#######E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops

setwd("E:/1Projects/21CQMiaoShe/2f3/")

mydata&lt;-read.table("E:/1Projects/21CQMiaoShe/2f3/mds_f3.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=4,eig=T)

write.csv(mds$points,file="1_Outgroup_f3_mds_f3_MDS.csv")





















#######E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops

setwd("E:/1Projects/21CQMiaoShe/2f3/")

mydata&lt;-read.table("E:/1Projects/21CQMiaoShe/2f3/mds_f3.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=4,eig=T)

write.csv(mds$points,file="1_divided_Outgroup_f3EastAsian112popsAsainMDS.csv")










#######E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops

setwd("E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops/")

mydata&lt;-read.table("E:/1Projects/5Mogolian/3f3/1outgroupf3/400pops/1_divided_Outgroup_f3_list_X1_X2_Yoruba400pop.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=4,eig=T)

write.csv(mds$points,file="358nonAfricans1_f3MDS.csv")











#######E:/1Projects/5Mogolian/4fst/2_400pops/FstMatrix

setwd("E:/1Projects/5Mogolian/4fst/2_400pops/")

mydata&lt;-read.table("E:/1Projects/5Mogolian/4fst/2_400pops/FstMatrix1.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=4,eig=T)

write.csv(mds$points,file="mds_168pops_fst.csv")










#######C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/2f3

setwd("C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/2f3/MDS")

mydata&lt;-read.table("E:/1Projects/4Shaanxi_Han/7Rawdata/FstHeatmap.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=4,eig=T)

write.csv(mds$points,file="mds_152pops_1_divided_f31234.csv")




#C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/1fst/2Fstheatmap_MDS/FstMatrix133.csv

mydata&lt;-read.table("C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/1fst/2Fstheatmap_MDS/FstMatrix133.csv",header=TRUE,sep=",", row.names = 1)

library(ggplot2)

mds=cmdscale(mydata,k=3,eig=T)











###########C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/2f3/Outgroup_f3_list_X1_X2_Yoruba_1_f3.csv

#####C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/1fst/2Fstheatmap_MDS/FstMatrix308.csv
mydata&lt;-read.table("C:/Users/HGL/Desktop/3AncientDNA_Xinancheng/2Modern/1fst/2Fstheatmap_MDS/FstMatrix308.csv",header=TRUE,sep=",", row.names = 1)
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)








####C:/Users/HGL\Desktop/1PrecisionIDAncestry/4Kidd_55AISNP/2Cavallli_Scorza/Cavalli_Sforza166pops.csv

mydata&lt;-read.table("C:/Users/HGL/Desktop/1PrecisionIDAncestry/5Seldins list of 128 AISNPs/2Cavalli_Sforza/Cavalli_Sforza_44pops.csv",header=TRUE,sep=",", row.names = 1)
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)




write.table(mds,file="MDSdata_treeresults", sep = ",", col.names = NA,qmethod = "double")

mydata&lt;-read.table("C:/Users/Guanglin He/Desktop/Hainan_HlaiHan/3HanHlaiPanasia/2Outgroupf3/MDS1.F3.csv",header=TRUE,sep=",",row.names="pop")
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)


#f3 in HGDP and HapMap3

mydata&lt;-read.table("C:/Users/Guanglin He/Desktop/Hainan_HlaiHan/5Hgdphapmap32048/Outgroupf3/1_f3.csv",header=TRUE,sep=",",row.names="pop")
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)
mydata&lt;-read.table("C:/Users/Guanglin He/Desktop/Hainan_HlaiHan/5Hgdphapmap32048/Outgroupf3/1.f3.csv",header=TRUE,sep=",",row.names="pop")
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)





mydata&lt;-read.table("C:/Users/Guanglin He/Desktop/Guizhou-Sui/2population_comparison/heatmap.csv",header=TRUE,sep=",",row.names="pop")
library(ggplot2)
mds=cmdscale(mydata,k=3,eig=T)


write.table(mds,file="MDSdata_treeresults", sep = ",", col.names = NA,qmethod = "double")


复制]]></description><link>软件\r语言语法\r：根据fst算mds.html</link><guid isPermaLink="false">软件/R语言语法/R：根据Fst算MDS.md</guid><pubDate>Sun, 29 Sep 2024 09:00:37 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据结构]]></title><description><![CDATA[ 
 <br><br>向量(vector)是用于存储数值型、字符型、逻辑型数据的一维数组。标量可以看作是只含有一个元素的向量。函数 c)可用来创建向量，例如:<br>Code
x1&lt;- c(2，4，1，-2，5)<br>
x2&lt;-c("one"，"two"，"three")<br>
x3 &lt;- C(TRUE，FALSE，TRUE， FALSE
<br>BMI的计算
weight &lt;-c(68，72，57，90，65，81)<br>
height &lt;- c(1.75，1.80，1.65，1.90，1.72，1.87)<br>
bmi &lt;- weight/height^2<br>
bmi
<br><br><br>一般来说，变量有数值型、名义型和有序型之分。名义型变量是没有顺序关系的分类变量，例如人的性别、血型、民族等。而有序型变量是有层级和顺序关系的分类变量如患者的病情(较差、好转、很好)。名义型变量和有序型变量在R中称为因子(factor)因子在R中非常重要，它决定了数据的展示和分析方式。数据存储时因子经常以整数向量形式存储。<br>资料分类
计数资料：数值型<br>
计量资料：名义型、有序型
<br>性别
sex &lt;- c(1，2，1，1，2，1，2)<br>
sex,f &lt;- factor(sex，levels = c(1，2)，labels =c("Male"，Female"))<br>
sex.f
<br>上面的命令先定义了一个变量 sex 表示性别，假设其取值1表示男性2表示女性。接着用函数factor()将变量sex 转换成了因子并存为对象sexf其中参数levels 表示原变量的分类标签值，参数 labels 表示因子取值的标签。注意，这两个参数在赋值时需要一一对应R会将它们相关联。因子型变量与一般的字符型变量的区别就是它有一个水平(level)属性。<br>要表示有序因子，需要在函数factor()里指定参数ordered=TRUE。例如:<br>有序因子
status&lt;-c(1，2，2，3，1，2，2)<br>
status.f &lt;- factor(status，<br>
levels=c(1，2，3),<br>
labels=c("Poor""Improved""Excellent")<br>
ordered=TRUE)<br>
status.f
]]></description><link>软件\r语言语法\r：基本数据集.html</link><guid isPermaLink="false">软件/R语言语法/R：基本数据集.md</guid><pubDate>Fri, 21 Jun 2024 07:31:42 GMT</pubDate></item><item><title><![CDATA[R：基本运算符]]></title><description><![CDATA[ 
 <br><img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210536.png" referrerpolicy="no-referrer"><br>
<img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210537.png" referrerpolicy="no-referrer"><br>
当输人正确的命令时，R 将执行相应的计算。如果左括号的数目多于右括号的数目时按下了 Enter 键，则新的一行将会显示“+”号，提示等待这个命令的完成。当左右括号数相等后，R就会执行计算并显示结果。<br>如何赋值：在R中有2种赋值的方法：<br>
<br>使用&lt;-；推荐；&lt;或者&gt;均可以，方向代表赋值给谁。

<br>例如，3-&gt;a、2&lt;-x。


<br>使用=
<br><img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210539.png" referrerpolicy="no-referrer"><br>注意
=代表赋值，但是==代表检查逻辑性；<img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210540.png" referrerpolicy="no-referrer">
<br><img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210541.png" referrerpolicy="no-referrer">]]></description><link>软件\r语言语法\r：基本运算符.html</link><guid isPermaLink="false">软件/R语言语法/R：基本运算符.md</guid><pubDate>Fri, 21 Jun 2024 07:31:42 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210536.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312011210536.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br><a data-href="主成分判别分析 (DAPC)" href="主成分判别分析 (DAPC)" class="internal-link" target="_self" rel="noopener nofollow">主成分判别分析 (DAPC)</a> 旨在研究生物群体的遗传结构。这种多变量方法包括两个步骤。首先，遗传数据被转换（居中，可能缩放）并提交给主成分分析（PCA）。<br><br><br>文件要求是序列对齐的版本。对齐可以使用 <a data-href="MAFFT：使用教程" href="软件\其它生信软件\k-s\mafft：使用教程.html" class="internal-link" target="_self" rel="noopener nofollow">MAFFT：使用教程</a>。<br>
获得大概如下：<br>&gt;1053180816471W_B4a5
GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCT-CCATGCATTTGGTATTTTCGTCTGGGGGGTGTGCACGCGATAGCATTGCGAGACGCTGGAGCCGGAGCACCCTATGTCGCAGTATCTGTCTTTGATTCCTGCCCCATCCTATTATTTATCGCACCTACGTTCAATATTACAGGCGAACATACTTACTAAAGTGTGTTAATTAATTAATGCTTGTAGGAC---
复制<br><br>注意用制表符分隔，同时表包含标题，第一列是 ID（必须与 FASTA 文件中完全对应）<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407171158370.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br># 安装和加载所需的包，如果包未安装则进行安装
required_packages &lt;- c("adegenet", "ggplot2", "RColorBrewer")
new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

library(adegenet)  # 加载 adegenet 包，用于遗传数据分析
library(ggplot2)   # 加载 ggplot2 包，用于数据可视化
library(RColorBrewer)  # 加载 RColorBrewer 包，用于补充颜色

# 定义文件路径
dfpath &lt;- "C:/Users/victo/Desktop/UPDATED_MSA_unique.fasta"
group_info_path &lt;- "C:/Users/victo/Desktop/新建 Text Document.txt" # 注意大小写，请一定保证这个数据第第一列是"ID"，第二列是"Group"

# 从 TXT 文件中读取分组信息
group_info &lt;- read.table(group_info_path, header = TRUE, sep = "\t")

# 检查 group_info 的列名，确保存在 'ID' 列
if (!"ID" %in% colnames(group_info)) {
  stop("The 'group_info' data frame must contain a column named 'ID'.")
}

# 读取 fasta 文件并转换为 genlight 对象
flu &lt;- fasta2genlight(dfpath, chunkSize = 100, parallel = FALSE)

# 绘制 SNP 位置图
snpposi.plot(position(flu), genome.size = 16594, codon = FALSE) + theme_bw()
snpposi.plot(position(flu), genome.size = 16594, codon = TRUE) + theme_bw()

# PCA 分析
df.pca &lt;- glPca(flu, nf = 3) #这一步耗费很多时间

df.pca.scores &lt;- as.data.frame(df.pca$scores)
df.pca.scores$ID &lt;- rownames(df.pca.scores)

# 计算每个主成分的标准差和方差解释率
sdev &lt;- apply(df.pca.scores[, -ncol(df.pca.scores)], 2, sd)
variance_explained &lt;- sdev^2 / sum(sdev^2) * 100

# 将分组信息与 PCA 结果合并
df.pca.scores &lt;- merge(df.pca.scores, group_info, by = "ID")
df.pca.scores
# 创建一个形状向量，为每个组别分配不同的形状
unique_groups &lt;- unique(df.pca.scores$Group)
shapes &lt;- seq(1, length(unique_groups))

# 指定颜色
custom_colors &lt;- c('#EA1F1F', '#E88421', '#E5C923', '#a5ce4c', '#35ca32', '#41fcb1', 
                   '#449657', '#176555', '#369BA8', '#2B7EBC', '#3626D1', '#A128CE', 
                   '#999999')

# 如果指定颜色不够，用 RColorBrewer 中的色盘补充
if (length(unique_groups) &gt; length(custom_colors)) {
  additional_colors &lt;- brewer.pal(length(unique_groups) - length(custom_colors), "Set3")
  custom_colors &lt;- c(custom_colors, additional_colors)
}

# 定义绘图函数
plot_pca &lt;- function(data, show_id = FALSE) {
  p &lt;- ggplot(data, aes(x = PC1, y = PC2, color = Group, shape = Group)) +  
    geom_point(size = 2) +  
    theme_bw() +  
    stat_ellipse(aes(fill = Group, color = Group), geom = "polygon", alpha = 0.2, level = 0.95) +  
    scale_shape_manual(values = shapes) +  
    scale_color_manual(values = custom_colors) +  
    scale_fill_manual(values = custom_colors)
  if (show_id) {
    p &lt;- p + geom_text(aes(label = ID), vjust = -0.5, size = 3, show.legend = FALSE)
  }
  return(p)
}

# 绘制 PCA 结果，带 ID 和不带 ID
plot_pca(df.pca.scores, show_id = TRUE)
plot_pca(df.pca.scores, show_id = FALSE)

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202407171159465.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\r语言语法\r：利用dapc进行mtdna降维.html</link><guid isPermaLink="false">软件/R语言语法/R：利用DAPC进行mtDNA降维.md</guid><pubDate>Sun, 08 Sep 2024 13:16:04 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f601.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f601.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>首先，你需要一个 树文件，例如 .nwk。<br>((OTU_47:0.101892516,OTU_46:0.205918075)0.999:0.12637554249999997,((OTU_7:0.079452354,(OTU_9:0.03389913,OTU_35:0.011411273)0.857:0.016187755)0.936:0.055036757,(((OTU_10:0.178702248,(OTU_27:0.105807393,(OTU_21:0.021742283,OTU_23:0.036247052)1.000:0.119621254)0.934:0.046734483)0.847:0.024928304,(OTU_359:0.115728596,((OTU_6:0.017083581,OTU_13:0.043230986)0.991:0.04686027,(OTU_5:0.048441113,OTU_17:0.040966445)0.874:0.016500727)0.998:0.076649272)0.884:0.025403911)0.896:0.031188452,((((OTU_4:0.016277276,OTU_1049:0.018764271)1.000:0.10486842,(OTU_61:0.033971226,OTU_66:0.014225781)0.949:0.029591816)0.938:0.030200498,(OTU_50:0.039428411,((OTU_14:0.027067745,OTU_24:0.006239384)0.713:0.003838445,(OTU_28:0.035593371,(OTU_12:0.019817974,OTU_1263:0.010126282)1.000:0.051942928)0.889:0.016154455)1.000:0.114060029)0.353:0.019135966)0.931:0.027767722,((OTU_26:0.011320021,(OTU_2:0.013893208,OTU_509:0.013184741)0.420:0.004818858)0.869:0.012841976,(((OTU_20:0.014868571,OTU_16:0.015213271)0.912:0.014485178,(OTU_19:0.042730937,OTU_31:0.03306436)0.071:0.011603998)0.445:0.011922646,(OTU_107:0.057678909,(OTU_8:5e-09,(OTU_3:0.01725298,((OTU_18:0.014018692,(OTU_11:0.009416427,OTU_756:0.019954543)0.815:0.007464513)0.616:0.003595574,(OTU_41:0.01321098,OTU_601:0.020022049)0.096:0.008198644)0.883:0.015142165)0.979:0.01496564)0.999:0.044655374)0.971:0.028869096)0.864:0.008905959)0.995:0.05488604)0.835:0.02976759)0.883:0.039831577):0.013815376500000032)root;
复制<br>原数据：<br>ID  NS  OS  Phylum  Class  Genus  Dissimilarity
OTU_4  7.635971271  4.770148464   Proteobacteria   Gammaproteobacteria   Pseudomonas  5.128352562
OTU_3  3.470455042  0.322522661   Proteobacteria   Betaproteobacteria   Variovorax  2.842534081
OTU_11  1.629405372  1.568607796   Proteobacteria   Betaproteobacteria   Alicycliphilus  1.563969309
OTU_9  1.882164078  0.799191737   Proteobacteria   Alphaproteobacteria   Rhizobium  1.458080931
OTU_26  0.297432905  1.251457384   Proteobacteria   Betaproteobacteria   Candidatus_Accumulibacter  0.99979998
OTU_20  0.213755428  1.003571157   Proteobacteria   Betaproteobacteria   Dechloromonas  0.806473806
OTU_31  0.243519198  0.74243736   Proteobacteria   Betaproteobacteria   Zoogloea  0.577408001
OTU_5  0.687363441  2.879332984   Actinobacteria   Actinobacteria   Sinomonas  2.326370564
OTU_21  0.128554203  2.884393027   Firmicutes   Bacilli   Bacillus  2.370021097
OTU_2  0.63593789  2.693549848   Proteobacteria   Betaproteobacteria   Candidatus_Accumulibacter  2.152672757
OTU_6  0.359578156  2.332422107   Actinobacteria   Actinobacteria   Rhodococcus  1.895257238
OTU_7  2.352963337  0.139433436   Proteobacteria   Alphaproteobacteria   Brevundimonas  1.932097306
OTU_23  1.89887647  0.04409441   Firmicutes   Bacilli   Bacillus  1.56172981
OTU_756  0  1.791122404   Proteobacteria   Betaproteobacteria  Unassigned  1.473091986
OTU_1049  0.929126346  1.506002396   Proteobacteria   Gammaproteobacteria   Pseudomonas  1.31072499
OTU_19  0.496918471  1.449424765   Proteobacteria   Betaproteobacteria   Thauera  1.119821414
OTU_107  1.472052603  0   Proteobacteria   Betaproteobacteria   Ralstonia  1.210784869
OTU_12  0.155895638  1.453109046   Proteobacteria   Gammaproteobacteria   Candidatus_Competibacter  1.188276062
OTU_10  0.266397228  1.34180309   Nitrospirae   Nitrospira   Nitrospira  1.081665383
OTU_35  1.320256516  0.031171574   Proteobacteria   Alphaproteobacteria   Ochrobactrum  1.085817664
OTU_16  0.379313613  1.180708996   Proteobacteria   Betaproteobacteria   Ferribacterium  0.919619487
OTU_8  1.220387302  0.132279503   Proteobacteria   Betaproteobacteria  Unassigned  0.997797575
OTU_50  0.748306533  0.96756762   Proteobacteria   Gammaproteobacteria   Dyella  0.93284511
OTU_14  0.192202973  1.171201989   Proteobacteria   Gammaproteobacteria   Candidatus_Competibacter  0.950210503
OTU_509  0.226988694  1.019429383   Proteobacteria   Betaproteobacteria   Candidatus_Accumulibacter  0.817373843
OTU_13  0.054004324  0.98844231   Actinobacteria   Actinobacteria   Mycobacterium  0.811726555
OTU_17  0.292485697  0.861252634   Actinobacteria   Actinobacteria   Microbacterium  0.681542368
OTU_41  0.220470821  0.850461698   Proteobacteria   Betaproteobacteria   Variovorax  0.680220552
OTU_28  0.139433378  0.843575268   Proteobacteria   Gammaproteobacteria   Candidatus_Competibacter  0.684251416
OTU_61  0.847025482  0.108006481   Proteobacteria   Gammaproteobacteria   Stenotrophomonas  0.696634768
OTU_24  0.116658549  0.800407229   Proteobacteria   Gammaproteobacteria   Candidatus_Competibacter  0.65130638
OTU_27  0  0.806457252   Proteobacteria  Others   Bdellovibrio  0.663249576
OTU_1263  0.069715852  0.70551017   Proteobacteria   Gammaproteobacteria   Candidatus_Competibacter  0.577408001
OTU_47  0.053999843  0.706198803   Bacteroidetes  Others  Unassigned  0.579137289
OTU_18  0.57997066  0.273598459   Proteobacteria   Betaproteobacteria   Ottowia  0.440454311
OTU_46  0  0.639748609   Bacteroidetes  Others  Unassigned  0.526212885
OTU_66  0.585808487  0   Proteobacteria   Gammaproteobacteria   Thermomonas  0.481767579
OTU_601  0.530049982  0.108001931   Proteobacteria   Betaproteobacteria  Unassigned  0.434741302
OTU_359  0.41715247  0   Firmicutes  Others   Eubacterium_rectale_group  0.343074336
复制<br><br>如上述，数据文件中，列名很重要。接下来我们都需要时刻注意。<br>
注意分类数据和数值数据的区别。<br><br># 加载必要的R包
library(dplyr)  # 加载dplyr包，用于数据处理和管道操作
library(itol.toolkit)  # 加载itol.toolkit包，用于制作和管理iTOL树图的数据单元
library(data.table)  # 加载data.table包，提供高效的数据读取和操作功能

# 设置工作目录
setwd("C:/Users/victo/Desktop")  # 将当前R会话的工作目录设置为指定路径

# 读取和处理数据
tree_1 &lt;- "abunt-tree.nwk"  # 指定新克文件的路径，这个文件包含树的信息
hub_1 &lt;- create_hub(tree_1)  # 创建一个以此树为中心的hub，用于添加不同的数据单元
data_file_1 &lt;- "abunt-metadata.txt"  # 指定元数据文件的路径
data_1 &lt;- data.table::fread(data_file_1)  # 使用data.table的fread函数读取元数据文件

#############################功能1################################################
# 为树的节点添加标签，按属分类
unit_1 &lt;- create_unit(data = data_1 %&gt;% select(ID, Genus),  # 从data_1中选取ID和Genus列
                      key = "itol_3al_1_labels",  # 为这个单元设置一个键名
                      type = "LABELS",  # 设置数据单元的类型为标签
                      tree = tree_1)  # 指定这个单元关联的树文件
write_unit(unit_1, paste0(getwd(), "/itol_3al_1_labels_Genus.txt"))  # 将单元写入文件

#############################功能2################################################
# 为树的分支添加颜色，按门分类
unit_2 &lt;- create_unit(data = data_1 %&gt;% select(ID, Phylum),
                      key = "itol_3al_2_range",
                      type = "TREE_COLORS",  # 设置类型为树颜色
                      subtype = "range",  # 子类型为范围，表示颜色将根据指定的范围变化
                      tree = tree_1)
write_unit(unit_2, paste0(getwd(), "/itol_3al_2_range_Phylum.txt"))

#############################功能3################################################
# 为树添加颜色条带，按綱分类
set.seed(123)  # 设置随机数种子，确保颜色选择的可重复性
unit_3 &lt;- create_unit(data = data_1 %&gt;% select(ID, Class),
                      key = "itol_3al_3_strip",
                      type = "DATASET_COLORSTRIP",  # 设置类型为颜色条带
                      color = "wesanderson",  # 使用Wes Anderson调色板
                      tree = tree_1)
unit_3@common_themes$basic_theme$margin &lt;- 50  # 设置条带的边缘空白
write_unit(unit_3, paste0(getwd(), "/itol_3al_3_strip_Class.txt"))

#############################功能4################################################
# 添加柱状图，表示某个数值特征
unit_4 &lt;- create_unit(data = data_1 %&gt;% select(ID, NS),
                      key = "itol_3al_4_simplebar",
                      type = "DATASET_SIMPLEBAR",  # 类型为简单柱状图
                      tree = tree_1)
unit_4@specific_themes$basic_plot$size_max &lt;- 100  # 设置柱状图的最大宽度
write_unit(unit_4, paste0(getwd(), "/itol_3al_4_simplebar.txt"))

#############################功能5################################################
# 添加多数据柱状图，同时表示多个数值特征
unit_5 &lt;- create_unit(data = data_1 %&gt;% select(ID, NS, OS),
                      key = "itol_3al_5_multibar",
                      type = "DATASET_MULTIBAR",  # 类型为多数据柱状图
                      tree = tree_1)
unit_5@specific_themes$basic_plot$size_max &lt;- 100  # 设置柱状图的最大宽度
write_unit(unit_5, paste0(getwd(), "/itol_3al_5_multibar.txt"))

#############################功能6################################################
# 添加梯度色柱状图，用于展示数据的变化
unit_6 &lt;- create_unit(data = data_1 %&gt;% select(ID, Dissimilarity),
                      key = "itol_3al_6_gradient",
                      type = "DATASET_GRADIENT",  # 类型为渐变数据集
                      tree = tree_1)
unit_6@specific_themes$heatmap$color$min &lt;- "#0000ff"  # 设置渐变的最小颜色
unit_6@specific_themes$heatmap$color$max &lt;- "#ff0000"  # 设置渐变的最大颜色
write_unit(unit_6, paste0(getwd(), "/itol_3al_6_gradient.txt"))

#############################功能7################################################
# 绘制热图，用于展示多个变量的组合数据
unit_7 &lt;- create_unit(data = data_1 %&gt;% select(ID, NS, OS),
                      key = "itol_7_heatmap",
                      type = "DATASET_HEATMAP",  # 类型为热图
                      tree = tree_1)
write_unit(unit_7, paste0(getwd(), "/itol_7_heatmap.txt"))

#############################功能8################################################
# 生成每个唯一Class的随机颜色
class_unique &lt;- unique(data_1$Class)  # 提取所有唯一的Class值
set.seed(123)  # 设置随机种子以保证颜色可以复现
colors &lt;- sample(colors(), length(class_unique), replace = FALSE)  # 为每个Class随机分配颜色
class_colors &lt;- setNames(colors, class_unique)  # 创建一个以Class为名称、颜色为值的向量
data_1$Color &lt;- class_colors[data_1$Class]  # 将颜色值分配给相应的Class
unit_colors &lt;- create_unit(data = select(data_1, ID, Color),
                           key = "itol_random_class_colors",
                           type = "TREE_COLORS",
                           subtype = "branch",  # 指定subtype为branch，可以改为range或clade
                           tree = tree_1)  # 请替换为您的树文件名称
write_unit(unit_colors, paste0(getwd(), "/itol_random_class_colors.txt"))

#############################功能9################################################
# 形状：1 矩形，2 圆形，3星形，4右边尖的三角形，5左边尖的三角形，6勾
unit_9 &lt;- create_unit(data = data_1 %&gt;%  select(ID, Symbol),
                      key = "Sample_Symbols", 
                      type = "DATASET_SYMBOL",
                      position = 1,
                      size = 2,
                      subtype = "Symbol",
                      tree = tree_1,
                      fill = 1)
write_unit(unit_9,paste0(getwd(),"/itol_Symbol.txt"))
复制]]></description><link>软件\r语言语法\r：利用r进行itol注释文件生成.html</link><guid isPermaLink="false">软件/R语言语法/R：利用R进行iTol注释文件生成.md</guid><pubDate>Fri, 13 Sep 2024 07:27:00 GMT</pubDate></item><item><title><![CDATA[双色配]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311400965.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311401147.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311401605.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312021522975.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>三色折线图  配色：
#EA8379 #7DAEE0 #B395BD
三色柱状图配色：
#299D8F&nbsp;#E9C46A #D87659
三色柱状图配色：&nbsp;
#EF76TA&nbsp;#456990&nbsp;#48C0AA
复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312021524407.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>四色柱状图配色：
#55B7E6&nbsp;#193E8F&nbsp;#E53528&nbsp;#F09739
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311401949.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311401314.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311402132.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311402237.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312021524546.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>六色分面组图配色：

#427AB2&nbsp;#F09148&nbsp;#FF9896&nbsp;

#DBDB8D&nbsp;#C59D94&nbsp;#AFC7E8
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312021525601.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>六色密度图配色：

#EEA599 #FAC795 #FFE9BE&nbsp;

#E3EDE0&nbsp;#ABD3E1 #92B4C8
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311402609.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311402989.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312021525771.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>多色机制图配色：

#43978F&nbsp;#9EC4BE&nbsp;#ABD0F1&nbsp;

#DCE9F4&nbsp;#E56F5E&nbsp;#F19685&nbsp;

#F6C957&nbsp;#FFB77F&nbsp;#FBE8D5 
复制]]></description><link>软件\r语言语法\r：配色图.html</link><guid isPermaLink="false">软件/R语言语法/R：配色图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:44 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311400965.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403311400965.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[COLLAPSE&nbsp;包]]></title><description><![CDATA[ 
 <br><br><br><br>COLLAPSE&nbsp;template are used to collapse branches.&nbsp;COLLAPSE 模板用于折叠分支。<br>Typically, The user merges branches A and B by folding them at their most recent common ancestor (MRCA). However, it is normally hard for users to declare the MRCA node directly. The node name is usually generated automatically by software rather than specified by the user when building the phylogenetic tree. Some programs do not even generate the node name.  <br>通常，用户通过在最近的共同祖先 (MRCA) 处折叠分支 A 和 B 来合并它们。然而，用户通常很难直接声明MRCA节点。节点名称通常由软件自动生成，而不是由用户在构建系统发育树时指定。有些程序甚至不生成节点名称。<br>This section has taken these complicated cases into account, and provide the corresponding solution, so that users can easily prepare the input data.  <br>本节考虑了这些复杂的情况，并提供了相应的解决方案，以便用户可以轻松准备输入数据。<br><br>This basic grammar for range id is written as:&nbsp;branch_A|branch_B.<br>
范围 id 的基本语法写为：branch_A|branch_B。<br>This section provides an example of how to collapse branches using&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/TongZhou2017/itol.toolkit/tree/master/inst/extdata/dataset1" rel="noopener nofollow" class="external-link" href="https://github.com/TongZhou2017/itol.toolkit/tree/master/inst/extdata/dataset1" target="_blank">dataset 1</a>&nbsp;(refer to the&nbsp;<a data-tooltip-position="top" aria-label="https://tongzhou2017.github.io/itol.toolkit/articles/Datasets.html" rel="noopener nofollow" class="external-link" href="https://tongzhou2017.github.io/itol.toolkit/articles/Datasets.html" target="_blank">Dataset</a>&nbsp;for detail information) document for detailed information). Users can download the data locally from the provided link above. The files are read using a relative path in this document, so please adjust the path based on actual situation.  <br>本节提供了如何使用数据集 1 折叠分支的示例（详细信息请参阅数据集文档）。用户可以从上面提供的链接本地下载数据。本文档采用相对路径读取文件，请根据实际情况调整路径。<br>The first step is to load the&nbsp;newick&nbsp;format tree file&nbsp;tree_of_itol_templates.tree&nbsp;and its corresponding metadata&nbsp;template_groups.  <br>第一步是加载newick格式树文件tree_of_itol_templates.tree及其对应的元数据template_groups。<br>library(itol.toolkit)
tree &lt;- system.file("extdata",
                    "tree_of_itol_templates.tree",
                    package = "itol.toolkit")
data("template_groups")
复制<br>Suppose that the user wants to collapse&nbsp;DATASET_SIMPLEBAR&nbsp;and&nbsp;DATASET_MULTIBAR&nbsp;into one branch;&nbsp;DATASET_TEXT&nbsp;and&nbsp;LABELS&nbsp;into another branch;DATASET_ALIGNMENT,&nbsp;DATASET_CONNECTIONS,&nbsp;DATASET_IMAGE&nbsp;and&nbsp;POPUP_INFO&nbsp;which belong to advanced visualization into one branch:<br>
假设用户想要将 DATASET_SIMPLEBAR 和 DATASET_MULTIBAR 折叠成一个分支； DATASET_TEXT 和 LABELS 放到另一个分支；DATASET_ALIGNMENT、DATASET_CONNECTIONS、DATASET_IMAGE 和 POPUP_INFO 属于高级可视化到一个分支：<br>range_ids &lt;- c("DATASET_SIMPLEBAR|DATASET_MULTIBAR",
               "DATASET_TEXT|LABELS",
               "DATASET_CONNECTIONS|POPUP_INFO")
unit_1 &lt;- create_unit(data = range_ids, 
                      key = "E001_collapse_1", 
                      type = "COLLAPSE", 
                      tree = tree)
write_unit(unit_1,
           paste0(getwd(),"/E001_collapse_1.txt"))
复制<br><img src="https://tongzhou2017.github.io/itol.toolkit/articles/images/COLLAPSE_1.png" referrerpolicy="no-referrer"><br>While the first two sets of folded branches are relatively simple to define because no other branches exist between them, the third set of folded branches is slightly complex, requiring the user to select the two branches with the longest spacing among the four to accurately describe the extent to be folded. Writing the code as&nbsp;DATASET_ALIGNMENT | POPUP_INFO&nbsp;may result in the omission of the&nbsp;DATASET_CONNECTIONS&nbsp;branch, because the branch does not belong to MCRA of&nbsp;DATASET_ALIGNMENT&nbsp;and&nbsp;POPUP_INFO.  <br>虽然前两组折叠分支定义相对简单，因为它们之间不存在其他分支，但第三组折叠分支稍微复杂，需要用户选择四个分支中间距最长的两个分支来准确描述折叠的程度。将代码编写为 DATASET_ALIGNMENT | POPUP_INFO 可能会导致省略 DATASET_CONNECTIONS 分支，因为该分支不属于 DATASET_ALIGNMENT 和 POPUP_INFO 的 MCRA。<br>Because the process of defining branches is very cumbersome and requires manual judgment, this function is not used frequently in the 23 templates in iTOL v6.  <br>由于定义分支的过程非常繁琐，并且需要手动判断，因此该功能在iTOL v6的23个模板中使用频率并不高。<br><br>Although we used a tip label in the above example, using node names directly is more convenient if it is available. Thus, the&nbsp;itol.toolkit&nbsp;also enable you to define the collapsed branch using the node name directly .<br>
尽管我们在上面的示例中使用了提示标签，但如果可以的话，直接使用节点名称会更方便。因此，itol.toolkit 还允许您直接使用节点名称定义折叠分支。<br>The tree provided by&nbsp;itol.toolkit&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/TongZhou2017/itol.toolkit/blob/master/inst/extdata/dataset1/itol_templates.tree" rel="noopener nofollow" class="external-link" href="https://github.com/TongZhou2017/itol.toolkit/blob/master/inst/extdata/dataset1/itol_templates.tree" target="_blank">dataset 1</a>&nbsp;including the node labels:<br>
itol.toolkit 数据集 1 提供的树包括节点标签：<br>ape::read.tree(tree)
# Phylogenetic tree with 23 tips and 22 internal nodes.
#
# Tip labels:
#   SPACING, COLLAPSE, PRUNE, TREE_COLORS, DATASET_STYLE, DATASET_CONNECTIONS, ...
# Node labels:
#   I1, I2, tree_sturcture, I4, theme_style, I6, ...
# 
# Rooted; includes branch lengths.
复制<br>Therefore, you could specify the group you want to merge directly.<br>
因此，您可以直接指定要合并的组。<br>group_names &lt;- unique(c("tree_sturcture", "theme_style", "text", "basic_plot"))
unit_2 &lt;- create_unit(data = group_names, 
                      key = "E002_collapse_2", 
                      type = "COLLAPSE", 
                      tree = tree)
write_unit(unit_2,
           paste0(getwd(),"/E002_collapse_2.txt"))
复制<br><img src="https://tongzhou2017.github.io/itol.toolkit/articles/images/COLLAPSE_2.png" referrerpolicy="no-referrer"><br>If you have questions about how to upload results files from iTOL toolkit for interactive visualization on iTOL, please refer to the&nbsp;<a data-tooltip-position="top" aria-label="https://tongzhou2017.github.io/itol.toolkit/articles/Operation.html" rel="noopener nofollow" class="external-link" href="https://tongzhou2017.github.io/itol.toolkit/articles/Operation.html" target="_blank">Operation</a>&nbsp;document for step-by-step guidance.<br>
如果您对如何从 iTOL 工具包上传结果文件以在 iTOL 上进行交互式可视化有疑问，请参阅操作文档以获取分步指导。<br><br>
<br>IOCAS,&nbsp;<a data-tooltip-position="top" aria-label="mailto:weiyLiu@outlook.com" rel="noopener nofollow" class="external-link" href="mailto:weiyLiu@outlook.com" target="_blank">weiyLiu@outlook.com</a><a data-tooltip-position="top" aria-label="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref1" rel="noopener nofollow" class="external-link" href="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref1" target="_blank"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">&nbsp;IOCAS, <a data-tooltip-position="top" aria-label="mailto:weiyLiu@outlook.com" rel="noopener nofollow" class="external-link" href="mailto:weiyLiu@outlook.com" target="_blank">weiyLiu@outlook.com</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">︎
<br>CACMS,&nbsp;<a data-tooltip-position="top" aria-label="mailto:njbxhzy@hotmail.com" rel="noopener nofollow" class="external-link" href="mailto:njbxhzy@hotmail.com" target="_blank">njbxhzy@hotmail.com</a><a data-tooltip-position="top" aria-label="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref2" rel="noopener nofollow" class="external-link" href="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref2" target="_blank"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">&nbsp;CACMS，<a data-tooltip-position="top" aria-label="mailto:njbxhzy@hotmail.com" rel="noopener nofollow" class="external-link" href="mailto:njbxhzy@hotmail.com" target="_blank">njbxhzy@hotmail.com</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">︎
<br>IOCAS,&nbsp;<a data-tooltip-position="top" aria-label="mailto:tongzhou2017@gmail.com" rel="noopener nofollow" class="external-link" href="mailto:tongzhou2017@gmail.com" target="_blank">tongzhou2017@gmail.com</a><a data-tooltip-position="top" aria-label="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref3" rel="noopener nofollow" class="external-link" href="https://tongzhou2017.github.io/itol.toolkit/articles/COLLAPSE.html#fnref3" target="_blank"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">&nbsp;IOCAS, <a data-tooltip-position="top" aria-label="mailto:tongzhou2017@gmail.com" rel="noopener nofollow" class="external-link" href="mailto:tongzhou2017@gmail.com" target="_blank">tongzhou2017@gmail.com</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">︎
<br><br><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1kc411R73e/?spm_id_from=333.999.0.0&amp;vd_source=1ee3ed0a9cabee330429fe4af1d78acd" rel="noopener nofollow" class="external-link" href="https://www.bilibili.com/video/BV1kc411R73e/?spm_id_from=333.999.0.0&amp;vd_source=1ee3ed0a9cabee330429fe4af1d78acd" target="_blank">点我</a>，这个视频长度较长。]]></description><link>软件\r语言语法\r：谱系树修饰.html</link><guid isPermaLink="false">软件/R语言语法/R：谱系树修饰.md</guid><pubDate>Fri, 21 Jun 2024 07:31:47 GMT</pubDate><enclosure url="https://tongzhou2017.github.io/itol.toolkit/articles/images/COLLAPSE_1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://tongzhou2017.github.io/itol.toolkit/articles/images/COLLAPSE_1.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240516204424.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一列为地区（例如省份、城市等），第三列是第四列单倍群在该地区的频率，计算公式应该是：<br><br><br>library(ggplot2)  # 加载ggplot2库，用于数据可视化
library(ggpubr)  # 加载ggpubr库，用于增强的ggplot2图表
# library(ggExtra)  # 注释掉ggExtra库的加载，因为不再使用边际图

# 加载数据
data &lt;- read.csv("C:/Users/victo/Desktop/相关分析.csv", header = TRUE, row.names = 1)

# 检查数据是否包含NULL值或其他异常情况
if (any(is.null(data)) || any(is.na(data))) {
  stop("数据中包含NULL或NA值，请检查数据文件。")
}

# 获取index列的第二行的值
index_second_row &lt;- data$index[2]

# 自定义形状和颜色
custom_shapes &lt;- c(17)  # 设置点的形状为三角形
custom_colors &lt;- c("#81B3A9")  # 设置点的颜色
ci_color &lt;- "#C1DDDB"  # 置信区间的颜色

# 定义一个函数，用于创建图表
create_plot &lt;- function(group_data, x_var, y_var, x_label, y_label) {
  # 创建基础散点图，添加线性拟合线和相关系数
  p &lt;- ggplot(group_data, aes(x = .data[[x_var]], y = .data[[y_var]], colour = index, shape = index)) +
    geom_point() +  # 添加散点
    geom_smooth(method = "lm", se = TRUE, fill = ci_color, color = ci_color) +  # 添加线性拟合线，显示95%置信区间，置信区间颜色为ci_color
    scale_shape_manual(values = custom_shapes) +  # 应用自定义点形状
    scale_colour_manual(values = custom_colors) +  # 应用自定义点颜色
    theme_bw() +  # 使用白色背景主题
    theme(legend.position = "none",  # 隐藏图例
          aspect.ratio = 1) +  # 设置图表为正方形
    stat_cor(color= '#113A34',method = 'pearson', aes(x = .data[[x_var]], y = .data[[y_var]])) +  # 计算并显示Pearson相关系数
    xlab(paste(x_label, index_second_row, sep = " - ")) + ylab(y_label)  # 设置坐标轴标签
  
  return(p)
}

# 对每个组应用图表，针对Lat变量
plots_lat &lt;- lapply(unique(data$index), function(group) {
  group_data &lt;- subset(data, index == group)  # 按组过滤数据
  create_plot(group_data, "Fre", "Lat", "Fre", "Lat")  # 创建并返回图表
})

# 对每个组应用图表，针对Long变量
plots_long &lt;- lapply(unique(data$index), function(group) {
  group_data &lt;- subset(data, index == group)  # 按组过滤数据
  create_plot(group_data, "Fre", "Long", "Fre", "Long")  # 创建并返回图表
})

# 在网格布局中显示所有图表
final_plot &lt;- ggarrange(plotlist = c(plots_lat, plots_long), ncol = 2, nrow = ceiling((length(plots_lat) + length(plots_long)) / 2))  # 使用2列布局，行数根据图表总数动态计算
print(final_plot)


复制<br><img alt="ef473d5e47bbb1b69fb39fc541ab79e.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/ef473d5e47bbb1b69fb39fc541ab79e.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\r语言语法\r：特定某个单倍群与经纬度相关性回归.html</link><guid isPermaLink="false">软件/R语言语法/R：特定某个单倍群与经纬度相关性回归.md</guid><pubDate>Fri, 23 Aug 2024 08:38:44 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[R：相关性热图]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=MzkxNjMzOTYxOA==&amp;mid=2247484017&amp;idx=1&amp;sn=c6a5a0bbc2c55f97961f0d228e2f8403&amp;chksm=c1502cbcf627a5aab9e66b93a388d03a6fc8cb3fb3068c419d01564c25171b57cab9d6ae0f0c&amp;mpshare=1&amp;scene=1&amp;srcid=1215i3JoMOQl7qk6yAGAwEMi&amp;sharer_shareinfo=b49ef7f466d4818bdc46a684296d85fc&amp;sharer_shareinfo_first=b49ef7f466d4818bdc46a684296d85fc#rd" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=MzkxNjMzOTYxOA==&amp;mid=2247484017&amp;idx=1&amp;sn=c6a5a0bbc2c55f97961f0d228e2f8403&amp;chksm=c1502cbcf627a5aab9e66b93a388d03a6fc8cb3fb3068c419d01564c25171b57cab9d6ae0f0c&amp;mpshare=1&amp;scene=1&amp;srcid=1215i3JoMOQl7qk6yAGAwEMi&amp;sharer_shareinfo=b49ef7f466d4818bdc46a684296d85fc&amp;sharer_shareinfo_first=b49ef7f466d4818bdc46a684296d85fc#rd" target="_blank">原文链接</a><br>相关性是指两个或多个变量之间的关系程度。在统计学中，常用相关系数来衡量变量之间的相关性，其中最常见的是皮尔逊相关系数。相关性系数的取值范围在-1到1之间，-1表示完全的负相关，1表示完全的正相关，0表示没有线性相关性。<br>相关性图是一种可视化手段，用于展示变量之间的相关关系。这种图形通常以矩阵的形式呈现，其中每个单元格表示两个变量之间的相关系数。图形的颜色和形状可以表示相关性的强弱，从而直观地帮助我们理解数据中的关联模式。<br>相关性详见：<a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/s?__biz=MzkxNjMzOTYxOA==&amp;mid=2247483846&amp;idx=1&amp;sn=1e2c402b0863e3b59961aeae43ad535f&amp;chksm=c1502f0bf627a61d8d9b11ef5eee57d3f56c220d040f6302cf89aae615ac42b7577c6d8e2300&amp;scene=21#wechat_redirect" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/s?__biz=MzkxNjMzOTYxOA==&amp;mid=2247483846&amp;idx=1&amp;sn=1e2c402b0863e3b59961aeae43ad535f&amp;chksm=c1502f0bf627a61d8d9b11ef5eee57d3f56c220d040f6302cf89aae615ac42b7577c6d8e2300&amp;scene=21#wechat_redirect" target="_blank">链接</a>。<br><br>R 包 corrplot 提供了很好的相关矩阵的可视化方法，不仅在图形布局、颜色、图例、文本标签等方面提供了丰富的绘图选项,还提供&nbsp;p 值和<a data-href="置信区间" href="术语\置信区间.html" class="internal-link" target="_self" rel="noopener nofollow">置信区间</a>来确定相关性的统计显着性。corrplot方法有大约 50 个参数，最常用的参数有method、type、order、diag等。<br># 安装R包
复制<br>使用自带的mtcars数据集<br>
# ?mtcars
# A data frame with 32 observations on 11 (numeric) variables.
# &gt; mtcars
# mpg cyl  disp  hp drat    wt  qsec vs am gear carb
# Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
# Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
# Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
# Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
复制<br>计算相关性矩阵<br>cordata = cor(mtcars)
复制<br>使用默认参数绘图<br>corrplot(cordata, ) # method = 'circle'
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312260838225.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>使用method、order、type、diag 参数调整图形！<br># 从左到右 第一排 color-&gt;shade-&gt;square
corrplot(cordata, method = 'color', order = 'alphabet')
corrplot(cordata, method = 'shade', order = 'AOE', 
         diag = FALSE)
corrplot(cordata, method = 'square', order = 'FPC', 
         type = 'lower', diag = FALSE)
# 从左到右 第二排
corrplot(cordata, method = 'ellipse', order = 'AOE', 
         type = 'upper')
corrplot.mixed(cordata, order = 'AOE')
corrplot.mixed(cordata, lower = 'shade', upper = 'pie', 
               order = 'hclust')
复制<br><br># 安装
if(!require(devtools)) install.packages("devtools")
devtools::install_github("kassambara/ggcorrplot")

library(ggcorrplot)

p1 &lt;- ggcorrplot(cordata, method = "circle")
p2 &lt;- ggcorrplot(cordata, hc.order = TRUE, outline.col = "white")
p3 &lt;- ggcorrplot(cordata, hc.order = TRUE, type = "upper",
           outline.col = "white")
p4 &lt;- ggcorrplot(cordata, hc.order = TRUE, type = "lower",
           lab = TRUE)
复制<br><br># install remotes package if necessary
install.packages("remotes")
# install corrmorant from the github repository
remotes::install_github("r-link/corrmorant")

# correlation plot of the drosera data using style = 'light'
corrmorant(drosera, style = "light")
# the "dark" style has a dark background in the diagonal facets
corrmorant(drosera, style = "dark")
# in the "blue_red" style, colors and correlation labels are colored by the strength of
# correlation
corrmorant(drosera, style = "blue_red")
复制<br>library(tidyverse)
select(mtcars, mpg, disp:qsec) %&gt;% 
  ggcorrm(rescale = "by_sd") +
  utri_heatmap(alpha = 0.5) +
  lotri_heatcircle(alpha = 0.5, col = 1) +
  utri_corrtext() +
  dia_names(y_pos = 0.15, size = 3) +
  dia_density(lower = 0.3, fill = "lightgrey", color = 1) +
  scale_fill_corr()
复制<br><br>install.packages('pheatmap')

# 模拟数据
library(pheatmap)
# Data 
set.seed(8)
m &lt;- matrix(rnorm(200), 10, 10)
colnames(m) &lt;- paste("Col", 1:10)
rownames(m) &lt;- paste("Row", 1:10)

# 绘制 Heat map， 并按照列归一化
pheatmap(m, scale = "column")

# 参数调整
# display_numbers
pheatmap(m,
         display_numbers = TRUE,
         number_color = "black", 
         fontsize_number = 8)
# kmeans聚类
pheatmap(m, kmeans_k = 3, cellheight = 50)
# border_color
pheatmap(m, border_color = "black")
# Color palette
pheatmap(m, color = hcl.colors(50, "BluYl"))
复制<br><br>
install.packages("devtools")
devtools::install_github("hannet91/ggcor")

library(ggplot2)
library(ggcor)
library(vegan)
library(dplyr)
data("varechem")
data("varespec")

fortify_mantel(varespec, varechem, 
                           spec.select = list(1:10, 5:14, 7:22, 9:32)) %&gt;% 
  mutate(r = cut(r, breaks = c(-Inf, 0.25, 0.5, Inf), 
                 labels = c("&lt;0.25", "0.25-0.5", "&gt;=0.5"),
                 right = FALSE),
         p.value = cut(p.value, breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
                       labels = c("&lt;0.001", "0.001-0.01", "0.01-0.05", "&gt;=0.05"),
                       right = FALSE))

quickcor(varechem, type = "upper") + geom_square() + 
  add_link(mantel02, mapping = aes(colour = p.value, size = r),
           diag.label = TRUE) +
  scale_size_manual(values = c(0.5, 1.5, 3)) +
  add_diag_label() + remove_axis("x")
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312260842218.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br># 安装PerformanceAnalytics
install.packages("PerformanceAnalytics")
# 绘制散点矩阵
chart.Correlation(mtcars[,1:8], histogram=TRUE)
复制<br><br># 安装
install.packages('GGally')

library(GGally)
library(ggplot2)
# 自带的iris数据集
ggpairs(iris, columns=1:5, aes(color=Species)) + 
  ggtitle("Iris (GGally_ggpairs) ")+
  theme_bw()
复制<br><br>panel.hist &lt;- function(x, ...)
{
  usr &lt;- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h &lt;- hist(x, plot = FALSE)
  breaks &lt;- h$breaks; nB &lt;- length(breaks)
  y &lt;- h$counts; y &lt;- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)}

pairs(iris[1:5], main = "Iris(pairs)", pch = 21,
      panel = panel.smooth,
      diag.panel = panel.hist,
      bg = c("#1b9e77", "#d95f02", "#7570b3")[unclass(iris$Species)]
复制<br><br>
# 安装
install.packages("psych")
library(psych)
# 绘图
pairs.panels(iris,
             smooth = TRUE,      # If TRUE, draws loess smooths
             scale = FALSE,      # If TRUE, scales the correlation text font
             density = TRUE,     # If TRUE, adds density plots and histograms
             ellipses = TRUE,    # If TRUE, draws ellipses
             method = "pearson", # Correlation method (also "spearman" or "kendall")
             pch = 21,           # pch symbol
             lm = FALSE,         # If TRUE, plots linear fit rather than the LOESS (smoothed) fit
             cor = TRUE,         # If TRUE, reports correlations
             jiggle = FALSE,     # If TRUE, data points are jittered
             factor = 2,         # Jittering factor
             hist.col = 4,       # Histograms color
             stars = TRUE,       # If TRUE, adds significance level with stars
             ci = TRUE)          # If TRUE, adds confidence intervals
复制]]></description><link>软件\r语言语法\r：相关性热图.html</link><guid isPermaLink="false">软件/R语言语法/R：相关性热图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:48 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312260838225.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312260838225.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520094458.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
热图的可操作性很高，因此可以把很多内容放进去。<br>
图中包括了几个重要的部分：<br>
<br>每个采样地点的经纬度。
<br>每个群体在 PCA 图形上的位置关系，即 PC1,PC2。
<br>群体与群体之间的 Fst 矩阵关系。
<br>单倍群在特定群体中的频率。
<br>关于第四点，计算方法应该是：<br><br><br><br># 加载必要的库
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)
library(GGally)
library(Rmisc) 
library(ggcorrplot)
library(RColorBrewer)
library(grDevices)

# 读取数据
dd = read.csv("C:/Users/victo/Desktop/eth_pop相关性矩阵.csv", header=T, row.names=1)

# 计算相关系数
cor(dd)

# 转换为矩阵
dd = as.matrix(dd)

# 计算完整观测相关系数
re = cor(dd, use="complete.obs")

# 计算p值
p = round(cor_pmat(dd, method="pearson"), 10)

# 设定PDF的尺寸
pdf("C:/Users/victo/Desktop/output_fixed.pdf", width=20, height=14)  # 设置PDF宽度和高度

# 调整绘图边距
par(mar=c(1, 1, 1, 1))  # 设置绘图边距，确保内容不会被截掉

# 指定颜色渐变
color_palette &lt;- colorRampPalette(c("#20364F","#31646C","#4E9280","#96B89B",
                                    "#DCDFD2","#ECD9CF","#D49C87","#B86265",
                                    "#8B345E","#50184E"))(100)

# 绘制上三角区域的相关系数矩阵
corrplot(re, p.mat=p,
         order="original", # 保持原有的排序顺序
         type="upper", # 将部分放在上边
         tl.col="black", # 标签颜色
         tl.cex=0.8, # 标签大小
         tl.pos="tp",
         insig="label_sig",
         sig.level=c(.001, .01, .05),
         pch.cex=0.8,  # 调整不显著关系的符号大小
         col=color_palette)  # 使用指定的颜色

# 在上一步绘制的基础上，继续绘制下三角区域
corrplot(re, add=TRUE,
         type="lower",
         mar=c(0, 0, 0, 0),  # 设置边距，确保内容不会被截掉
         method="number", # 用数字表示
         order="original", # 保持原有排序
         diag=FALSE,
         tl.col="black",
         tl.cex=0.8,
         tl.pos="n",
         cl.pos="n",
         number.digits=2,
         number.cex=0.6,  # 调整数字大小
         number.font=1,  # 指定字体
         col=color_palette,
         addCoef.col=NA)  # 不设置addCoef.col

# 自定义绘制数字背景
for(i in 1:nrow(re)) {
  for(j in 1:i) {
    value_color &lt;- color_palette[round((re[i, j] + 1) / 2 * 99) + 1]
    rect(xleft=j-0.5, ybottom=nrow(re)-i+0.5, xright=j+0.5, ytop=nrow(re)-i+1.5,
         col=value_color, border=NA)
    
    # 计算背景颜色的亮度
    bg_col &lt;- col2rgb(value_color) / 255
    brightness &lt;- sqrt(0.299 * bg_col[1]^2 + 0.587 * bg_col[2]^2 + 0.114 * bg_col[3]^2)
    
    # 根据亮度选择字体颜色
    text_color &lt;- ifelse(brightness &gt; 0.5, "black", "white")
    
    text(j, nrow(re)-i+1, round(re[i, j], 2), col=text_color, cex=0.6)
  }
}

# 关闭PDF设备
dev.off()
复制<br>如何描述绘制的图形？请参考下列：<br>变量相关矩阵的可视化表示：本图展示了变量间的皮尔逊相关系数矩阵。矩阵中每个单元格代表两个变量间的相关性。

上三角部分使用颜色渐变和符号标记来表示相关系数及其显著性，而下三角部分则以背景色和数值展示相关系数。上三角区域：使用颜色填充和符号标记展示相关系数的大小及其显著性。
相关系数通过颜色渐变进行视觉呈现，颜色从深蓝到深红，表示相关性从负到正的变化。显著性水平以符号标记表示，显著性水平的阈值设定为0.001、0.01和0.05。在不显著的相关性上，符号标记如 * 用于显示。

下三角区域：数值和背景色一起表示相关系数。背景色通过颜色渐变填充，颜色范围与上三角一致，视觉上强调了相关系数的大小和方向。相关系数以数值形式显示在矩形的中心，数值保留两位小数。
使用了 R 语言的 corrplot 包进行绘图。

复制<br><br>下面的代码改变了除了经纬度、PC1、PC2之外的排序方式。<br># 加载必要的库
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)
library(GGally)
library(Rmisc)
library(ggcorrplot)
library(RColorBrewer)
library(grDevices)

# 读取数据
dd = read.csv("C:/Users/victo/Desktop/新建 Text Document.txt", sep = "\t", header=T, row.names=1)

# 提取PC1, PC2, Latitude, Longtitude列
fixed_cols = c("PC1", "PC2", "Latitude", "Longitude")
fixed_data = dd[, fixed_cols]

# 将列名分为群组和单倍型组
group_cols = colnames(dd)[grepl("_", colnames(dd))]
haplotype_cols = colnames(dd)[!grepl("_", colnames(dd)) &amp; !colnames(dd) %in% fixed_cols]

# 提取群组和单倍型数据
group_data = dd[, group_cols]
haplotype_data = dd[, haplotype_cols]

# 计算群组和单倍型的相关系数矩阵
re_group = cor(group_data, use="complete.obs")
zero_variance_cols = sapply(haplotype_data, function(col) sd(col, na.rm = TRUE) == 0)
# 打印常数列名称
if (any(zero_variance_cols)) {
  cat("具有零标准差的列：", names(haplotype_data)[zero_variance_cols], "\n")
}
# 从数据中移除这些列
haplotype_data_filtered = haplotype_data[, !zero_variance_cols]
# 计算群组和单倍型的相关系数矩阵
re_group = cor(group_data, use="complete.obs")
re_haplotype = cor(haplotype_data_filtered, use="complete.obs")

# 层次聚类等操作
hc_group = hclust(as.dist(1 - re_group))
hc_haplotype = hclust(as.dist(1 - re_haplotype))

# 重新排序列名
group_order = group_cols[hc_group$order]
haplotype_order = haplotype_cols[hc_haplotype$order]

# 重新排列数据框
dd_reordered = dd[, c(fixed_cols, group_order, haplotype_order)]

# 计算完整观测相关系数
re = cor(dd_reordered, use="complete.obs")

# 计算p值
p = round(cor_pmat(dd_reordered, method="pearson"), 10)

# 设定PDF的尺寸
# pdf("C:/Users/victo/Desktop/output_fixed.pdf", width=20, height=14)  # 设置PDF宽度和高度

# 调整绘图边距
par(mar=c(1, 1, 1, 1))  # 设置绘图边距，确保内容不会被截掉

# 指定颜色渐变
color_palette &lt;- colorRampPalette(c("#20364F","#31646C","#4E9280","#96B89B",
                                    "#DCDFD2","#ECD9CF","#D49C87","#B86265",
                                    "#8B345E","#50184E"))(100)

# 绘制上三角区域的相关系数矩阵
corrplot(re, p.mat=p,
         order="original", # 保持原有的排序顺序
         type="upper", # 将部分放在上边
         tl.col="black", # 标签颜色
         tl.cex=0.8, # 标签大小
         tl.pos="tp",
         insig="label_sig",
         sig.level=c(.001, .01, .05),
         pch.cex=0.8,  # 调整不显著关系的符号大小
         col=color_palette, # 使用指定的颜色
         #outline=TRUE, # 添加边框
         #addgrid.col="#BDF7F6"
         )  # 设置边框颜色

# 在上一步绘制的基础上，继续绘制下三角区域
corrplot(re, add=TRUE,
         type="lower",
         mar=c(0, 0, 0, 0),  # 设置边距，确保内容不会被截掉
         method="number", # 用数字表示
         order="original", # 保持原有排序
         diag=FALSE,
         tl.col="black",
         tl.cex=0.8,
         tl.pos="n",
         cl.pos="n",
         number.digits=2,
         number.cex=0.6,  # 调整数字大小
         number.font=1,  # 指定字体
         col=color_palette,
         addCoef.col=NA, # 不设置addCoef.col
         #outline=TRUE, # 添加边框
         #addgrid.col="#BDF7F6"
         )  # 设置边框颜色

# 自定义绘制数字背景
for(i in 1:nrow(re)) {
  for(j in 1:i) {
    value_color &lt;- color_palette[round((re[i, j] + 1) / 2 * 99) + 1]
    rect(xleft=j-0.5, ybottom=nrow(re)-i+0.5, xright=j+0.5, ytop=nrow(re)-i+1.5,
         col=value_color, border=NA)
    
    # 计算背景颜色的亮度
    bg_col &lt;- col2rgb(value_color) / 255
    brightness &lt;- sqrt(0.299 * bg_col[1]^2 + 0.587 * bg_col[2]^2 + 0.114 * bg_col[3]^2)
    
    # 根据亮度选择字体颜色
    text_color &lt;- ifelse(brightness &gt; 0.5, "black", "white")
    
    text(j, nrow(re)-i+1, round(re[i, j], 2), col=text_color, cex=0.6)
  }
}

# 关闭PDF设备
# dev.off()

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240520100109.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>以下代码可以绘制将固定的名称作为行，所有列作为列，并且使用相关系数矩阵的一部分显示相关性。<br># 加载必要的库
library(PerformanceAnalytics)
library(Hmisc)
library(corrplot)
library(RColorBrewer)
library(grDevices)

# 读取数据
dd = read.csv("新建 Text Document.txt", sep = "\t", header=T, row.names=1)

# 提取PC1, PC2, Latitude, Longtitude列
fixed_cols = c("Afanasievo", "Mongolia_N_North", "Haojiatai_LBIA", "Lubrak","Taiwan_Hanben","Hmong")
fixed_data = dd[, fixed_cols]

# 计算相关系数矩阵
re = cor(dd, use="complete.obs")

# 计算p值矩阵
p = cor_pmat(dd, method="pearson")

# 提取固定行和所有列的相关系数矩阵
re_fixed = re[fixed_cols, ]
p_fixed = p[fixed_cols, ]

# 设置显著性符号
stars &lt;- ifelse(p_fixed &lt; 0.001, "***", ifelse(p_fixed &lt; 0.01, "**", ifelse(p_fixed &lt; 0.05, "*", "")))

# 绘制相关性矩阵，并且只绘制固定的行和所有列
corrplot(re_fixed, method="color",
         type="full", # 绘制整个矩阵
         order="original", # 保持原有顺序
         tl.col="black", # 标签颜色
         tl.cex=0.8, # 标签大小
         col=colorRampPalette(c("#20364F","#31646C","#4E9280","#96B89B",
                                "#DCDFD2","#ECD9CF","#D49C87","#B86265",
                                "#8B345E","#50184E"))(100),
         p.mat = p_fixed, # 提供 p 值矩阵
         insig = "label_sig", # 用显著性符号显示
         sig.level = c(0.001, 0.01, 0.05),
         pch = "*", # 使用 '*' 符号表示显著性
         pch.cex=0.5, # 显著性符号的大小
         pch.col = "#DCDFD2",
         #addCoef.col="black", # 添加相关系数值
         #number.cex = 0.7, # 相关系数数值大小
         #number.font = 2,   # 字体加粗
         cl.ratio = 1,
         cl.pos = "b") # 设置颜色条在右边

复制<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202409131752222.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\r语言语法\r：相关性热图绘制.html</link><guid isPermaLink="false">软件/R语言语法/R：相关性热图绘制.md</guid><pubDate>Fri, 13 Sep 2024 09:52:22 GMT</pubDate><enclosure url="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" length="0" type="image/svg+xml"/><content:encoded>&lt;figure&gt;&lt;img src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[散点图]]></title><description><![CDATA[ 
 <br>原文链接：<a data-tooltip-position="top" aria-label="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzU0OTQ0MzU5Mw==&amp;action=getalbum&amp;album_id=3102753971792019458&amp;scene=173&amp;subscene=&amp;sessionid=svr_f8337a77aef&amp;enterid=1703506757&amp;from_msgid=2247483713&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" rel="noopener nofollow" class="external-link" href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzU0OTQ0MzU5Mw==&amp;action=getalbum&amp;album_id=3102753971792019458&amp;scene=173&amp;subscene=&amp;sessionid=svr_f8337a77aef&amp;enterid=1703506757&amp;from_msgid=2247483713&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" target="_blank">R语言高级可视化100图 (qq.com)</a><br><br>散点图是最常用的图形。每当你想了解两个变量之间关系的本质时，首选总是散点图。在ggplot2中使用 geom_point() 可以绘制散点图。此外，geom_smooth 默认绘制平滑线（基于 loess），可以通过设置 method='lm' 调整为绘制最佳拟合线。<br>library('tidyverse')

pal = c('#025259', '#007172', '#f4e2de', '#f29325', '#d94f04')

ggplot(data = midwest %&gt;% filter(poptotal&lt;=200000),
       aes(x=area, y=poptotal)) +
  geom_point(aes(fill=state), shape=21, color="black") +
  geom_smooth(se=FALSE, color=pal[1]) +
  scale_fill_manual(values = pal) +
  scale_y_continuous(breaks = seq(50000, 200000, 50000)) +
  labs(title = 'ScatterPlot',
       subtitle = 'Area vs Poptotal',
       caption = 'source: midwest') +
  theme_minimal() +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252025172.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>散点图可以让您比较两个连续变量之间的关系，而气泡图则可以很好地帮助您了解基本组内的关系：一个分类变量（通过改变颜色）和另一个连续变量（通过改变点的大小）。如果您有四维数据，其中两个是数值变量（X 和 Y），另一个是分类变量（颜色）和另一个数值变量（大小），那么气泡图就比较适合。<br>我们在之前的散点图中展现了美国不同县的面积（X轴）与总人口（Y轴）之间的关系，并且用县所属的州来映射颜色。我们在此基础之上，使用各个县中亚洲人的数量来映射点的大小，就可以得到气泡图了。<br>
library('tidyverse')

pal = c('#025259', '#007172', '#f4e2de', '#f29325', '#d94f04')

ggplot(data = midwest %&gt;% filter(poptotal&lt;=200000),
       aes(x=area, y=poptotal)) +
  geom_point(aes(fill=state, size=popasian), 
             shape=21, alpha=0.85) +
  scale_fill_manual(values = pal) +
  scale_y_continuous(breaks = seq(50000, 200000, 50000),
                     expand = c(0, 0),
                     limits = c(0, 210000)) +
  scale_size(range = c(1, 12)) +
  labs(title = 'Bubble Plot',
       subtitle = 'Area vs Poptotal',
       caption = 'source: midwest') +
  theme_minimal() +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252026916.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>ggplot(data=midwest %&gt;% filter(poptotal&lt;=200000),
       aes(x=area, y=poptotal)) +
  geom_point(aes(fill=state, size=popasian),
             alpha=0.8,
             shape=21,
             color="black") +
  geom_polygon(data=midwest %&gt;%
                 filter(poptotal&lt;=200000 &amp; state=="OH") %&gt;%
                 slice(chull(.$area, .$poptotal)),
               color="grey30",
               fill="gold",
               alpha=0.1) +
  scale_size(range = c(1, 12)) +
  scale_fill_manual(values = pal) +
  labs(subtitle="Area Vs Population", 
       y="Population", 
       x="Area", 
       title="Scatterplot + Encircle", 
       caption="Source: midwest") +
  theme_minimal(base_size = 18) +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252027272.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>
mpg %&gt;% 
  filter(cyl %in% c(4, 8)) %&gt;% 
  mutate(cyl = as.factor(cyl)) %&gt;% 
  ggplot(mapping = aes(x=displ, y=hwy, fill=cyl, color=cyl)) +
  geom_point(color='black', shape=21, size=3) +
  geom_smooth(method = 'lm',
              fullrange=TRUE,
              alpha=0.3) +
  scale_x_continuous(limits = c(0.5, 7.5), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 50), expand = c(0, 0)) +
  scale_fill_manual(values = c('#025259', '#f29325'), guide = guide_legend(ncol=1))+
  scale_color_manual(values = c('#025259', '#f29325')) +
  labs(subtitle="Cyl Vs Displ", 
       y="Cyl", 
       x="Displ", 
       title="Scatterplot with Fitted Lines", 
       caption="Source: mpg") +
  theme_minimal(base_size = 18) +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252027053.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br><br>mpg %&gt;% 
  ggplot(aes(x=cyl, y=hwy, fill=as.factor(cyl))) +
  geom_point(shape=21,  size=3.5, color='black') +
  scale_fill_manual(name ='CYL', values = pal) +
  labs(title = 'Scatterplot with overlapping points',
       subtitle = 'mpg: cyl vs highway mileage',
       caption = 'source:mpg',
       x= 'Cyl',
       y= 'Hwy') +
  theme_minimal(base_size = 18) +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252028248.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
值得注意的是，数据存在重叠的问题， 很多散点被隐藏了。由于cty和hvy两个变量都是整数，所以数据重叠的现象更加严重。对于这类数据集的散点图，展示过程中应该格外小心。那么应该如何解决一个问题呢？我们可以使用jitter_geom()对数据增加抖动，通过设置wigth，使得重叠的点在原始位置随机抖动。<br>mpg %&gt;% 
  ggplot(aes(x=cyl, y=hwy)) +
  geom_jitter(aes(fill=as.factor(cyl)), shape=21, color='black', width = 0.35, size=3.5) +
  scale_fill_manual(name='CYL', values = pal) +
  labs(title = 'Jittered Plot Points',
       subtitle = 'map: city vs highway mileage',
       caption = 'source:mpg',
       x= 'City',
       y= 'Highway')  +
  theme_minimal(base_size = 18) +
  theme(plot.title =  element_text(color = "grey20", size = 20, face = 'bold', hjust = 0),
        plot.subtitle =  element_text(color = "grey20", size = 14, hjust = 0),
        plot.caption  =  element_text(color = "grey20", size = 12, face = 'italic', hjust = 1),
        plot.background = element_rect(fill = 'white', color='white'),
        axis.text = element_text(color = "grey20", size = 12),  
        axis.title = element_text(color = "grey20", size = 16, face = 'bold'))
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252029038.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">]]></description><link>软件\r语言语法\r：相关性图.html</link><guid isPermaLink="false">软件/R语言语法/R：相关性图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:49 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252025172.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312252025172.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[介绍]]></title><description><![CDATA[ 
 <br><br>小提琴图结合了箱线图和密度图的特点，可以提供关于数据分布的丰富信息。<br>
<br>形状：小提琴图通常由一对镜像的「小提琴」形状组成，它们代表了数据的分布情况。
<br>宽度：小提琴的宽度表示在该位置上的数据密度，即数据点的数量。
<br>高度：小提琴图的高度没有特定意义，通常用于美观和区分。
<br>中间线：小提琴图中间的线通常表示中位数，有时也会表示均值。
<br>端点：小提琴的端点表示数据的最大值和最小值。<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414557.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">
<br><br>
<br>与箱线图的比较：小提琴图能够同时显示数据的分布形状和密度，而箱线图只能提供分位数信息。小提琴图可以发现数据的双峰分布，而箱线图无法捕捉这种情况。
<br>与直方图的比较：小提琴图在可视化数据分布的同时，避免了直方图的分箱误差。小提琴图可以清晰地展示数据的峰值和形状，而直方图可能会模糊这些信息。
<br>与散点图的比较：小提琴图适用于大量数据，可以更好地展示密度分布，而散点图可能会因为数据点的重叠而失去一部分信息。散点图通常用于显示个体数据点的分布，而小提琴图更适合于总体分布的描述。
<br>与密度图的比较：小提琴图可以同时显示多个分组的分布情况，而密度图通常适用于单一分布的情况。
<br>library(ggplot2)<br>
ggplot(warpbreaks, aes(x = tension, y = breaks)) +<br>
geom_violin()<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414558.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
ggplot(warpbreaks, aes(x = tension, y = breaks)) +<br>
geom_violin(trim = FALSE)<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414559.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
ggplot(warpbreaks, aes(x = tension, y = breaks)) +<br>
geom_violin(trim = FALSE)+<br>
geom_boxplot(width = 0.07)<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414560.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
ggplot(warpbreaks, aes(x = tension, y = breaks, fill = tension)) +<br>
geom_violin(trim = FALSE,bw = 5) +<br>
geom_boxplot(width = 0.07)<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414561.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
ggplot(warpbreaks, aes(x = tension, y = breaks, fill = wool)) +<br>
geom_violin(trim = FALSE) +<br>
geom_boxplot(width = 0.07, position = position_dodge(width = 0.9))<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414562.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
library(ggplot2)<br>
library(ggpubr)<br>
ggplot(warpbreaks, aes(x = tension, y = breaks, fill = wool)) +<br>
geom_violin(trim = FALSE) +<br>
geom_boxplot(width = 0.07, position = position_dodge(width = 0.9))+<br>
stat_compare_means(aes(label = ..p.signif..),method = "wilcox.test",hide.ns = F)<br>
<img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414563.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
ggplot(warpbreaks, aes(x = tension, y = breaks, fill = tension)) +<br>
geom_violin(trim = FALSE,bw = 5) +<br>
geom_boxplot(width = 0.07) +<br>
guides(fill = guide_legend(title = "Title"))+<br>
scale_fill_hue(labels = c("G1", "G2", "G3"))<br><img alt="" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414564.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;">]]></description><link>软件\r语言语法\r：小提琴图.html</link><guid isPermaLink="false">软件/R语言语法/R：小提琴图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:49 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414557.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312171414557.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[实战]]></title><description><![CDATA[ 
 <br>tg<br>
<a data-href="种群配对Fst值（Population pairwise Fst values）" href="术语\种群配对fst值（population-pairwise-fst-values）.html" class="internal-link" target="_self" rel="noopener nofollow">种群配对Fst值（Population pairwise Fst values）</a>是一种衡量种群间遗传差异的指标。它基于遗传变异的分布，通过比较种群内和种群间的遗传差异来度量种群结构。Fst 值的范围从0到1，表示种群间遗传差异的程度。值接近0表示种群之间的遗传差异较小，可能存在较高的基因流和遗传交流。而值接近1表示种群之间的遗传差异较大，可能存在较低的基因流和遗传交流。<br><br><br>需要准备2个文件：<br>
<br>Fst计算结果矩阵。
<br>Group分组信息。
<br>格式如下所示：<br>
<br>
<br><br>
<br>
<br><br>我在下面贴了很多个代码，你只需要用到其中一个就行了。<br>
第一个我设置了输入输出文件路径，其余的请自己设置。<br># 在开始之前请确保你已经安装了下列package
# 载入需要的package
library(ape)
library(igraph)
library(ggplot2)
library(pheatmap)
library(reshape2)
library(ggsci)
library(gridExtra)

# 设置工作路径，你需要把Fst文件和分组文件放到工作路径
setwd("C:/Users/a/Desktop")

# 清除内存
rm(list=ls())

# 读取数据
mydata&lt;-read.table("fst.csv",header=TRUE,sep=",", row.names = 1)
group &lt;-read.table("group.csv",header=TRUE,sep=",", row.names = 1)

# 创建绘图PDF
pdf("FstMatrix1.pdf", width=11, height=8.5)

# 绘制图像
pheatmap(mydata, 
         cluster_cols=TRUE, 
         cluster_rows=TRUE, 
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group, 
         annotation_row = group,
         cellwidth =8, 
         cellheight = 8, 
         cutree_cols=4, # 这个数字决定最终的矩形被分成几个部分
         cutree_rows=4, # 这个数字决定最终的矩形被分成几个部分
         main = "FstMatrix",
         color = colorRampPalette(c("#00A087FF","#3C5488FF","#F39B7FFF"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))
# 结束绘图
dev.off()

# 换个样式
pheatmap(mydata, 
         cluster_cols=TRUE, 
         cluster_rows=TRUE, 
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group, 
         annotation_row = group,
         cellwidth =8, 
         cellheight = 8, 
         cutree_cols=4,
         cutree_rows=4, 
         main = "FstMatrix",
         color = colorRampPalette(c("#F8F8FF","#91D1C2FF","#3C5488FF"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))

# 换个样式
pheatmap(mydata,
         cluster_cols=TRUE,
         cluster_rows=TRUE,
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group,
         annotation_row = group,
         cellwidth =8,
         cellheight = 8,
         cutree_cols=4,
         cutree_rows=4,
         main = "FstMatrix",
         color = colorRampPalette(c("#20364F","#31646C","#4E9280","#96B89B","#DCDFD2","#ECD9CF","#D49C87","#B86265","#8B345E","#50184E"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))

# 换个样式
pheatmap(mydata,
         cluster_cols=TRUE,
         cluster_rows=TRUE,
         angle_col = c("45"),
         fontsize = 8,
         fontsize_row =8,
         fontsize_col =6,
         annotation_col = group,
         annotation_row = group,
         cellwidth =8,
         cellheight = 8,
         cutree_cols=4,
         cutree_rows=4,
         main = "FstMatrix",
         color = colorRampPalette(c("#023047","#126883","#279EBC","#90C9E6","#FC9E7F","#F75B41","#D52120"))(10000),
         display_numbers = matrix(ifelse(abs(mydata)&gt; 50, "++", ifelse(abs(mydata)&gt;=40,"+"," ")), nrow(mydata)))


复制]]></description><link>软件\r语言语法\r：fst绘制频率热图矩阵.html</link><guid isPermaLink="false">软件/R语言语法/R：Fst绘制频率热图矩阵.md</guid><pubDate>Sun, 06 Oct 2024 03:31:34 GMT</pubDate></item><item><title><![CDATA[基础箱线图的绘制]]></title><description><![CDATA[ 
 <br><br>箱型图（Box Plot），也称为盒须图，是一种用于显示一组数据分布情况的图表。它主要展示数据的中位数、四分位数、极值等统计量。以下是解读箱型图的主要步骤：<br>
<br>
中位数（Median）：箱型图中间的线表示数据的中位数，即将数据集分成两半的数值。

<br>
四分位数（Quartiles）：

<br>第一四分位数（Q1）：箱子的底部线，表示所有数据中排在25%位置的数值。
<br>第三四分位数（Q3）：箱子的顶部线，表示所有数据中排在75%位置的数值。
<br>这个箱子（即Q1到Q3的范围）包含了中间50%的数据。


<br>
四分位距（Interquartile Range, IQR）：Q3与Q1的差值，表示数据的“散布”程度。

<br>
异常值（Outliers）：箱型图中可能会有一些单独的点，这些通常表示异常值。它们是距离箱子较远的数据点，表明这些值异常高或低。

<br>
“胡须”（Whiskers）：

<br>这些是从箱子外伸出的线，通常表示数据的范围。
<br>胡须的长度通常是1.5倍IQR（四分位距），超出这个范围的点被视为异常值。


<br>
最大值和最小值：

<br>最大值通常是胡须的上端（不包括异常值）。
<br>最小值通常是胡须的下端（不包括异常值）。


<br>
数据（随机编写，无实际意义）

<br># 数据(随机编写，无实际意义)  
df&lt;-data.frame(   sample=paste0("S",1:20),   
group1=rep(c("A","B","C","D"),each=5),   
group2=rep(c("group1","group2"),times=2),   
group3=rep(c("CG","EG"),each=10),   
value=sample(20:100,20,replace=FALSE)   )   
df$sample&lt;-factor(df$sample,levels=df$sample)
复制<br>或者也可以用Excel 导入数据。实例数据：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072231258.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>基础箱线图的绘制：ggplot2中用于绘制箱线图的函数为geom_boxplot()
<br>library(ggplot2)
ggplot(df,aes(group1,value))+
geom_boxplot()
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072241074.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>颜色修改：包括填充色、边框颜色等
<br>ggplot(df, aes(group1,value))+
geom_boxplot(fill="orange")
#填充不同颜色
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1))
#修改边框颜色为统一颜色
ggplot(df,aes(group1,value))+
geom_boxplot(color="orange",linewidth=0.8,fill=NA)
#修改边框颜色为不同颜色
ggplot(df,aes(group1,value))+
geom_boxplot(aes(color=group1),linewidth=0.8,fill=NA)
#同时修改填充色与边框色
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1),color="blue",linewidth=0.8)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242479.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242561.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242899.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242192.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242832.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>异常值处理
<br>#细节设置
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1),
outlier.color="blue",#异常点边框颜色
outlier.fill="red",#异常点填充颜色
outlier.shape=21,#异常形状
outlier.size=2.5,##异常点大小
outlier.stroke=0.5,##异常点边框线条粗细
outlier.alpha=0.7)#异常点透明度
#去除
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1),
outlier.color=NA)#异常点去除
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242678.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242811.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>箱线图添加散点（下列两种方式都可以实现）：
<br>#geom_point()
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1))+
geom_point(color="black",size=2.5,position="jitter")
复制<br>#geom_jitter()
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1))+
geom_jitter(color="black",size=2.5,width=0.2,alpha=0.9)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242341.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>添加最大最小值线横线：
<br>ggplot(df,aes(group1,value))+
stat_boxplot(geom="errorbar",width=0.1,size=0.8)+
geom_boxplot(aes(fill=group1),outlier.color=NA)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242353.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>添加均值点:
<br>ggplot(df,aes(group1,value))+
stat_boxplot(geom="errorbar",width=0.1,size=0.8)+
geom_boxplot(fill="white",outlier.color=NA)+
stat_summary(fun=mean,geom="point",size=3,color="blue")
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242370.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>修改误差棒线条类型:
<br>ggplot(df,aes(group1,value))+
stat_boxplot(aes(color=group1),geom="errorbar",width=0.1,size=0.8,linetype=2)+
geom_boxplot(aes(fill=group1),outlier.color=NA,color=NA)+
stat_summary(fun=median,geom="point",size=3,shape=21,color="black",fill="white")
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072242902.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>有缺口箱线图：
<br>ggplot(df,aes(group3,value))+   stat_boxplot(geom="errorbar",width=0.1,size=0.8)+   geom_boxplot(aes(fill=group3),outlier.color=NA,   notch=T) 
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243831.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>分组箱线图：
<br>ggplot(df,aes(group1,value))+   geom_boxplot(aes(fill=group3))+   scale_fill_manual(values=c("#037ef3","#f85a40"))  
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243704.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>水平箱线图：
<br>ggplot(df,aes(group1,value))+   geom_boxplot(aes(fill=group1))+   scale_fill_manual(values=c("#037ef3","#f85a40","#00c16e","#7552cc"))+#自定义颜色   coord_flip() 
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243347.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>并列箱线图：
<br>ggplot(df,aes(group3,value))+   geom_boxplot(aes(fill=group2),position=position_dodge(width=0.9))+   scale_fill_manual(values=c("#037ef3","#f85a40"))#自定义颜色
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243075.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>分面箱线图：
<br>ggplot(df,aes(group1,value))+   geom_boxplot(aes(fill=group1))+   scale_fill_manual(values=c("#037ef3","#f85a40","#00c16e","#7552cc"))+#自定义颜色   facet_grid(~group2)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243112.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>分组分面箱线图：
<br>ggplot(df,aes(group1,value))+   geom_boxplot(aes(fill=group3))+   scale_fill_manual(values=c("#037ef3","#f85a40","#00c16e","#7552cc"))+#自定义颜色   facet_grid(~group2) 
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243061.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>显著性比较：
<br>library(ggsignif)#SignificanceBracketsfor'ggplot2'   ggplot(df,aes(group1,value))+   geom_boxplot(aes(fill=group1))+   geom_signif(comparisons=list(c("A","C"),   c("C","D")),   map_signif_level=F,##是否使用*显示显著性   tip_length=c(0,0,0,0),   y_position=c(100,110),   size=0.8,textsize=6,   test="t.test")+   scale_y_continuous(limits=c(20,130))   
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243661.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>字母标记：  
<br>library(dplyr)
# AGrammarofDataManipulation
library(agricolae)
# 实现多重比较的R包
# 数据的方差检验
variance&lt;-aov(value~group1,data=df)
MC&lt;-LSD.test(variance,"group1",p.adj="none")
GB&lt;-group_by(df,group1)
error&lt;-summarise(GB,sd(value,na.rm=T))
# 整理数据
error2&lt;-merge(error,MC$group,by.x="group1",by.y="row.names",all=F)
# 绘图
ggplot(df,aes(group1,value))+
geom_boxplot(aes(fill=group1))+
geom_text(data=error2,aes(group1,value+30,label=groups,color=group1),size=5)
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243355.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>半箱线图：  
<br>library(gghalves)   ggplot(df,aes(group1,value))+   geom_half_boxplot(aes(fill=group1),outlier.color=NA,side="r")   
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243518.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>半箱线图与散点图组合：
<br>library(gghalves)   ggplot(df,aes(group1,value))+   geom_half_boxplot(aes(fill=group1),outlier.color=NA,side="r")+   geom_half_point(aes(color=group1),side='l',size=2.5)  
复制<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243276.png" referrerpolicy="no-referrer" style="width: 550px; max-width: 100%;"><br>
<br>箱线图与半小提琴和散点组合图：
<br>ggplot(df,aes(group3,value))+   geom_half_violin(aes(fill=group3),position=position_nudge(x=0.26),side="r",width=0.5,color=NA)+   geom_boxplot(aes(fill=group3),width=0.1,size=1.2,outlier.color=NA,position=position_nudge(x=0.2))+   geom_jitter(aes(fill=group3),shape=21,size=3,width=0.12,alpha=0.5)
复制<br><img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072243278.png" referrerpolicy="no-referrer">]]></description><link>软件\r语言语法\r：ggplot2个性化绘制散点图.html</link><guid isPermaLink="false">软件/R语言语法/R：ggplot2个性化绘制散点图.md</guid><pubDate>Fri, 21 Jun 2024 07:31:49 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072231258.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202312072231258.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[介绍]]></title><description><![CDATA[ 
 <br><br>ggplot2的几个要素：<br>
Plot(图)= data(数据集)+ Aesthetics(美学映射)+ Geometry(几何对象)
<br>
<br>data: 数据集，主要是data frame
<br>Aesthetics: 美学映射，比如将变量映射给x,y坐标轴，或者映射给颜色、大小、形状等图形属性
<br>Geometry: 几何对象，比如条形图、直方图、散点图、箱线图、小提琴图等
<br>其它参数：<br>
<br>标尺(scale)：在对图形属性进行映射之后，使用标尺可以控制这些属性的显示方式，比如坐标刻度、颜色属性等
<br>坐标系统 (coordinate)：数据如何被映射到图中，提供作图所需的坐标轴和网格线，如coord_polar:极坐标、coord_map:地理投影、coord_flip:坐标翻转
<br>统计变换(statistic)：对数据进行汇总，如箱线图：stat_boxplot、线图：stat_abline、直方图：stat_bin
<br>刻面(facet)：用来描述数据如何被拆分为子集，以及对不同子集是如何绘制的
<br>图层(layer)：直接使用+号即可实现叠加图层
<br>主题(theme)：设置与数据无关的图形参数实现对图形的精细控制，比如设置字体、颜色、背景色、网格线等
<br>用“+”连接不同的图层：<br>
<br>ggplot():初始化图形并且指定数据来源(mtcars)和绘图用的变量(wt和mpg)
<br>geom_point():几何函数，绘制散点图
<br>labs():添加注释，包括轴标签和标题
<br>theme(): 主题设置，与数据无关的美化图形的设计，都可以用主题来设置。
<br>举例说明：<br>library(ggplot2)
ggplot(data=mtcars,aes(x=wt,y=mpg)) +
   geom_point(pch=17,color="blue",size=2) +
   geom_smooth(method="lm",color="red",linetype=2) +
   labs(title="Automobile Data", x="Weight", y="Miles Per Gallon") +
   theme(plot.title = element_text(hjust = 0.5))
复制<br>geom_point():几何函数，绘制散点图。pch=17设置点的性状为三角形，color=“blue”设置颜色为蓝色,size=2设置点的大小加倍<br>
geom_smooth():添加一条“平滑”曲线。采用线性拟合(method=“lm”)，拟合曲线为红色(color=“red”)的虚线(linetype=2) 。默认绘制95%的<a data-href="置信区间" href="术语\置信区间.html" class="internal-link" target="_self" rel="noopener nofollow">置信区间</a>(深灰色暗带) 。<br><br><br><br><br>图形的呈现多种多样，可以自己多加尝试。]]></description><link>软件\r语言语法\r：ggplot2简介.html</link><guid isPermaLink="false">软件/R语言语法/R：ggplot2简介.md</guid><pubDate>Fri, 21 Jun 2024 07:31:49 GMT</pubDate></item><item><title><![CDATA[R：GWalkR——一行代码将数据集转化为交互式可视化分析工具]]></title><description><![CDATA[ 
 <br>一行代码，开启您在R中的数据探索之旅！<br><a data-tooltip-position="top" aria-label="https://github.com/Kanaries/GWalkR" rel="noopener nofollow" class="external-link" href="https://github.com/Kanaries/GWalkR" target="_blank">GWalkR</a>&nbsp;是 R 中的交互式探索性数据分析（EDA）工具。它整合了 htmlwidgets 和&nbsp;<a data-tooltip-position="top" aria-label="https://github.com/Kanaries/graphic-walker" rel="noopener nofollow" class="external-link" href="https://github.com/Kanaries/graphic-walker" target="_blank">Graphic Walker</a>。通过将您的数据框转换为 Tableau 风格的用户界面进行可视化探索，它可以简化您的 R 数据分析和数据可视化工作流程。<br><br><br>install.packages("GWalkR")
复制<br><br>如果您已在R中安装了devtools，您可以在脚本中运行以下R代码来下载。<br>devtools::install_url("https://kanaries-app.s3.ap-northeast-1.amazonaws.com/oss/gwalkr/GWalkR_latest.tar.gz")
复制<br><br>或者，从<a data-tooltip-position="top" aria-label="https://kanaries-app.s3.ap-northeast-1.amazonaws.com/oss/gwalkr/GWalkR_latest.tar.gz" rel="noopener nofollow" class="external-link" href="https://kanaries-app.s3.ap-northeast-1.amazonaws.com/oss/gwalkr/GWalkR_latest.tar.gz" target="_blank">这个链接</a>中下载包 GWalkR_latest.tar.gz。 打开 R Studio，点击 "Packages" 窗口中的 "Install"，然后在 "Install from" 中选择 "Package Archive File (.tgz; .tar.gz)"。然后，选择您的文件系统中的下载好的包，最后点击"Install"。<br><br>library(GWalkR)
data(iris)
gwalkr(iris)
复制<br><img alt="image" src="https://private-user-images.githubusercontent.com/33870780/252876378-718d8ff6-4ad5-492d-9afb-c4ed67573f51.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5OTExNTgsIm5iZiI6MTcxMDk5MDg1OCwicGF0aCI6Ii8zMzg3MDc4MC8yNTI4NzYzNzgtNzE4ZDhmZjYtNGFkNS00OTJkLTlhZmItYzRlZDY3NTczZjUxLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIxVDAzMTQxOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzZTMzOWQ1OGU4MWJjNWVmNTI3NDZjOGVhMTBkYmY1ODBjNjI3ZjkzMTQ5Y2YyY2E3YTUwNGRmMjE5ZmY0NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2rYnZN1-MUNSEmPGfvHaxuKN_usywBti1FWfabZf0ws" referrerpolicy="no-referrer">]]></description><link>软件\r语言语法\r：gwalkr——一行代码将数据集转化为交互式可视化分析工具.html</link><guid isPermaLink="false">软件/R语言语法/R：GWalkR——一行代码将数据集转化为交互式可视化分析工具.md</guid><pubDate>Fri, 21 Jun 2024 07:31:50 GMT</pubDate><enclosure url="https://private-user-images.githubusercontent.com/33870780/252876378-718d8ff6-4ad5-492d-9afb-c4ed67573f51.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5OTExNTgsIm5iZiI6MTcxMDk5MDg1OCwicGF0aCI6Ii8zMzg3MDc4MC8yNTI4NzYzNzgtNzE4ZDhmZjYtNGFkNS00OTJkLTlhZmItYzRlZDY3NTczZjUxLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIxVDAzMTQxOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzZTMzOWQ1OGU4MWJjNWVmNTI3NDZjOGVhMTBkYmY1ODBjNjI3ZjkzMTQ5Y2YyY2E3YTUwNGRmMjE5ZmY0NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2rYnZN1-MUNSEmPGfvHaxuKN_usywBti1FWfabZf0ws" length="0" type="false"/><content:encoded>&lt;figure&gt;&lt;img src="https://private-user-images.githubusercontent.com/33870780/252876378-718d8ff6-4ad5-492d-9afb-c4ed67573f51.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA5OTExNTgsIm5iZiI6MTcxMDk5MDg1OCwicGF0aCI6Ii8zMzg3MDc4MC8yNTI4NzYzNzgtNzE4ZDhmZjYtNGFkNS00OTJkLTlhZmItYzRlZDY3NTczZjUxLmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzIxVDAzMTQxOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTEzZTMzOWQ1OGU4MWJjNWVmNTI3NDZjOGVhMTBkYmY1ODBjNjI3ZjkzMTQ5Y2YyY2E3YTUwNGRmMjE5ZmY0NmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.2rYnZN1-MUNSEmPGfvHaxuKN_usywBti1FWfabZf0ws"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[数据准备]]></title><description><![CDATA[ 
 <br><br>
<br>单倍群在特定的地区的频率。
<br>,F,F3,F3a,F3a1,F3a1a,F3a1a1,F3a1a1a,F3a1a1b,F3a1a1b1,F3a1a1b1a,F3a1a1b2,F3a1a1b3,F3a1a2,F3a1a2a,F3a1a2b,F3a1a2b1,F3a1a2b1b,F3a1a2b1c,F3a2,F3a2a,F3a2a1,F3a2a3,F3c,F3c1,F3c2,F3c2a
Anhui,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Beijing,0.00030172,0.00030172,0.000201147,0.000201147,0.000100573,0,0,0,0,0,0,0,0.000100573,0,0.000100573,0,0,0,0,0,0,0,0,0,0,0
Chongqing,0.001104728,0.001104728,0.001104728,0.001104728,0.000220946,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Fujian,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Gansu,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Guangdong,0.001227137,0.001227137,5.84E-05,5.84E-05,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.001110267,0.00011687,5.84E-05,5.84E-05
Guangxi,0.001223242,0.001223242,0.00030581,0.00030581,0.00030581,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.000917431,0,0,0
Guizhou,0.004180214,0.004180214,0.004180214,0.004180214,0.003715745,0.001857873,0,0.001857873,0,0,0.000464468,0.001393405,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Hainan,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Hebei,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Heilongjiang,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Henan,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Hubei,0.000346661,0.000346661,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.000346661,0,0,0
Hunan,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Inner_Mongolia,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Jiangsu,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Jiangxi,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Jilin,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Liaoning,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Ningxia,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Qinghai,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Shaanxi,0.000163425,0.000163425,0.000163425,0.000163425,0.000163425,0,0,0,0,0,0,0,0.000163425,0.000163425,0,0,0,0,0,0,0,0,0,0,0,0
Shandong,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Shanghai,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Shanxi,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Sichuan,0.000605373,0.000605373,0.000605373,0.000529701,0.00045403,0,0,0,0,0,0,0,0,0,0,0,0,0,7.57E-05,7.57E-05,0,7.57E-05,0,0,0,0
Taiwan,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Tianjin,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Tibet,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Xinjiang,0.001020408,0.001020408,0.001020408,0.001020408,0.001020408,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Yunnan,0.029693783,0.029693783,0.029693783,0.028765852,0.024435509,0.002165172,0.00030931,0.001855861,0.00030931,0.00030931,0.001546551,0,0.002165172,0,0.002165172,0.001237241,0.00030931,0.00061862,0.00061862,0.00030931,0.00030931,0,0,0,0,0
Zhejiang,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0

复制<br>
<br>不同地区的经纬度以及这些地区的不同的遗传组成成分的比例。
<br>,Longitude,Latitude,Lubrak,Shenxian,DevilsCave_N,Japan_Jomon,Russia_Afanasievo,Russia_Shamanka_EBA,Hmong,Mongolia_N_North,Erdaojingzi_LN,Taiwan_Hanben
Anhui,117.28,31.86,0.02,0.215,0.027,0.003,0.003,0.005,0.018,0.009,0.672,0.029
Beijing,116.41,39.92,0.03,0.21,0.035,0.004,0.007,0.006,0.022,0.01,0.655,0.022
Chongqing,106.45,29.57,0.021,0.283,0.012,0.003,0.002,0.002,0.04,0.004,0.593,0.039
Fujian,118.34,26.09,0.043,0.299,0.006,0.005,0,0.002,0.034,0.007,0.561,0.044
Gansu,103.73,36.07,0.065,0.164,0.047,0.003,0.021,0.009,0.012,0.016,0.648,0.014
Guangdong,113.25,23.19,0.001,0.427,0.008,0.003,0,0.001,0.037,0,0.451,0.072
Guangxi,108.34,22.83,0.007,0.563,0.004,0.002,0,0.001,0.065,0.003,0.298,0.057
Guizhou,106.76,26.57,0.031,0.304,0.006,0.004,0.001,0.003,0.1,0.007,0.515,0.028
Hainan,110.23,20.06,0.01,0.398,0.01,0.004,0.002,0.002,0.034,0.003,0.471,0.066
Hebei,115.48,38.03,0.034,0.146,0.053,0.004,0.013,0.009,0.011,0.011,0.705,0.014
Heilongjiang,127.63,47.75,0.024,0.159,0.051,0.006,0.006,0.007,0.013,0.013,0.707,0.013
Henan,113.65,34.78,0.027,0.181,0.032,0.002,0.01,0.002,0.019,0.011,0.689,0.027
Hubei,114.32,30.57,0.013,0.265,0.017,0.003,0.001,0.004,0.023,0.005,0.631,0.038
Hunan,113.12,28.26,0.014,0.334,0.01,0.003,0,0.002,0.065,0.004,0.526,0.042
Inner_Mongolia,111.67,41.81,0.03,0.102,0.233,0.006,0.026,0.026,0.008,0.025,0.534,0.01
Jiangsu,119.78,32.05,0.013,0.228,0.021,0.003,0.001,0.002,0.014,0.004,0.678,0.035
Jiangxi,115.94,28.67,0.004,0.287,0.014,0.003,0,0.003,0.025,0.004,0.61,0.048
Jilin,125.35,43.88,0.031,0.156,0.055,0.005,0.005,0.008,0.011,0.015,0.696,0.02
Liaoning,123.38,41.8,0.033,0.164,0.054,0.005,0.007,0.005,0.014,0.01,0.689,0.018
Ningxia,106.23,38.52,0.057,0.163,0.051,0.004,0.037,0.011,0.012,0.014,0.637,0.016
Qinghai,101.78646,36.62716,0.156,0.115,0.058,0.004,0.046,0.013,0.016,0.022,0.56,0.01
Shaanxi,108.96,34.26,0.039,0.169,0.048,0.002,0.016,0.007,0.012,0.014,0.674,0.018
Shandong,117,36.67,0.029,0.153,0.046,0.005,0.003,0.005,0.011,0.01,0.721,0.018
Shanghai,121.55,31.23,0.011,0.24,0.014,0.003,0.001,0.003,0.016,0.005,0.67,0.038
Shanxi,112.53,37.86,0.035,0.158,0.045,0.002,0.013,0.01,0.012,0.009,0.699,0.018
Sichuan,104.12,30.67,0.221,0.154,0.01,0.003,0.001,0.007,0.034,0.026,0.531,0.015
Taiwan,121.57463,25.04094,0.003,0.276,0.008,0.002,0.001,0.001,0.011,0.001,0.307,0.39
Tianjin,117.2,39.13,0.027,0.158,0.051,0.004,0.009,0.006,0.016,0.013,0.703,0.013
Tibet,79.54848,42.71073,0.442,0.029,0.016,0.004,0.01,0.011,0.009,0.032,0.442,0.005
Xinjiang,87.63,43.85,0.043,0.166,0.079,0.006,0.083,0.015,0.019,0.015,0.558,0.017
Yunnan,102.73,25.05,0.095,0.244,0.02,0.004,0.004,0.005,0.032,0.016,0.558,0.022
Zhejiang,120.2,30.28,0,0.245,0.021,0.003,0.001,0,0.018,0,0.678,0.035

复制<br><br># 清空工作空间
rm(list = ls())

# 加载所需的库
library(httpgd)
library(linkET)
library(ggplot2)
library(dplyr)
library(cols4all)

# 读取数据
auto_data &lt;- read.csv("C:/Users/victo/Desktop/auto.csv", row.names = 1, header = TRUE, sep = ",", check.names = FALSE)
haplo_data &lt;- read.csv("C:/Users/victo/Desktop/haplo.csv", row.names = 1, header = TRUE, sep = ",", check.names = FALSE)

# 计算auto数据的相关性矩阵
cor_auto &lt;- correlate(haplo_data, method = "pearson")
corr_auto &lt;- as_md_tbl(cor_auto)
write.csv(corr_auto, file = "C:/Users/victo/Desktop/pearson_correlate(env&amp;env).csv", row.names = TRUE)

# 计算auto数据和haplo数据之间的相关性矩阵
cor_auto_haplo &lt;- correlate(auto_data, haplo_data, method = "pearson")
corr_auto_haplo &lt;- as_md_tbl(cor_auto_haplo)
write.csv(corr_auto_haplo, file = "C:/Users/victo/Desktop/pearson_result(bio&amp;env).csv", row.names = TRUE)

# 数据准备
r_p_data_plot &lt;- corr_auto_haplo %&gt;% 
  mutate(
    r_sign = cut(r, breaks = c(-Inf, 0, Inf), labels = c("Negative", "Positive")),  # 根据相关系数的正负分组
    p_sign = cut(p, breaks = c(0, 0.05, Inf), labels = c("P&lt;0.05", "P&gt;=0.05"), include.lowest = TRUE, right = FALSE),  # 根据p值分组
    r_abs = cut(abs(r), breaks = c(0, 0.25, 0.5, 1), labels = c("&lt;0.25", "0.25-0.5", "0.5-1"), include.lowest = TRUE, right = FALSE)  # 根据相关系数的绝对值分组
  )

# 绘制上半部分为圆形相关性图
p4 &lt;- qcorrplot(cor_auto,
                grid_col = "#fbfbfb",  # 网格颜色
                grid_size = 0.1,  # 网格大小
                type = "upper",  # 显示矩阵的上半部分
                diag = FALSE) +  # 不显示对角线
  geom_shaping(marker = marker("circle"), color = "#fbfbfb", size = 0.05) +  # 设置圆形轮廓颜色和大小
  scale_fill_gradientn(colours = c("#00668c", "#71c4ef",
                                   "#96B89B",
                                   "#B86265", "#8B345E"), limits = c(-1, 1))  # 设置颜色梯度

# 添加显著性连线
p5 &lt;- p4 +
  geom_mark(size = 3,  # 标记大小
            only_mark = TRUE,  # 仅显示标记
            sig_level = c(0.10,0.05,0.01),  # 显著性水平
            sig_thres = 0.05,  # 显著性阈值
            colour = '#fbfbfb')  # 连线颜色

# 添加连线和标签
p6 &lt;- p5 +
  geom_couple(data = r_p_data_plot,
              aes(colour = r_sign,  # 根据相关性符号设置颜色
                  size = r_abs,  # 根据相关性绝对值设置大小
                  linetype = p_sign),  # 根据p值设置线型
              nudge_x = 0.15,  # 调整标签位置
              curvature = 0.1,  # 设置曲率
              label.fontface = 1,  # 标签字体样式
              label.family = "arial",  # 标签字体
              label.size = 2.5)  # 标签大小

# 最终美化
p7 &lt;- p6 +
  scale_size_manual(values = c("&lt;0.25" = 0.05, "0.25-0.5" = 0.15, "0.5-1" = 0.5)) +  # 手动设置大小比例
  scale_colour_manual(values = c("Negative" = "#4E9280", "Positive" = "#B86265")) +  # 手动设置颜色
  scale_linetype_manual(values = c("P&lt;0.05" = "solid", "P&gt;=0.05" = "dotted")) +   # 手动设置线型
  guides(
    fill = guide_colorbar(title = "Pearson's r", barwidth = 0.8, barheight = 5, order = 1),  # 设置颜色条指南
    linetype = guide_legend(title = NULL, override.aes = list(size = 4, linewidth = 0.5, order = 3)),  # 设置线型指南
    colour = guide_legend(title = NULL, override.aes = list(size = 4, linewidth = 0.5, order = 4)),  # 设置颜色指南
    size = guide_legend(title = "|Pearson's r|", override.aes = list(colour = "black", size = 4), order = 2)  # 设置大小指南
  ) +
  theme(
    axis.text = element_text(color = "black", size = 4, family = "arial", face = "plain"),  # 设置坐标轴文本
    axis.text.x.top = element_text(color = "black", size = 4, family = "arial", face = "plain", angle = 45, hjust = 0, vjust = 0),  # 设置上方坐标轴文本
    legend.key = element_blank(),  # 移除图例键
    legend.key.size = unit(0.1, "cm"),  # 设置图例键大小
    legend.spacing.y = unit(0.2, "cm"),  # 设置图例间距
    legend.key.spacing.y = unit(0.2, "cm"),  # 设置图例键间距
    legend.text = element_text(color = "black", size = 4, family = "arial", face = "plain"),  # 设置图例文本
    legend.title = element_text(color = "black", size = 4, family = "arial", face = "plain", margin = margin(b = 12))  # 设置图例标题
  )

print(p7)

复制]]></description><link>软件\r语言语法\r：linket相关性分析.html</link><guid isPermaLink="false">软件/R语言语法/R：linkET相关性分析.md</guid><pubDate>Fri, 21 Jun 2024 07:31:07 GMT</pubDate></item><item><title><![CDATA[什么是 treestats 包？]]></title><description><![CDATA[ 
 <br><br>这是一个可以用于评价系统发育树的 各项统计指标 的 R 包，包括树的稳健性、分支长度等。<br><br>需要输入一个树文件，推荐使用 nwk 格式。<br>
这里我给一个示例数据： <a data-tooltip-position="top" aria-label="https://1drv.ms/t/s!AnGqDjyiZ5t_hfwXJk7juw80aD-f7A?e=luRurh" rel="noopener nofollow" class="external-link" href="https://1drv.ms/t/s!AnGqDjyiZ5t_hfwXJk7juw80aD-f7A?e=luRurh" target="_blank">生命之树.txt</a><br><br># 安装并加载必要的包（如果没有安装）
if (!require("treestats")) install.packages("treestats")
if (!require("RSpectra")) install.packages("RSpectra")
if (!require("ape")) install.packages("ape")
if (!require("treestats")) install.packages("treestats")
# 加载包
library(treestats)
library(ape)
library(RSpectra)
library(treestats)
# 读取树文件
phylo_tree &lt;- read.tree("生命之树.txt")

# 打印树的概述
print(phylo_tree)

# 计算所有统计量并打印结果
results &lt;- calc_all_stats(phylo_tree)
print(results)


# 假设你只对某些物种或支系感兴趣，可以列出不需要的叶节点
tips_to_remove &lt;- c("Tip1", "Tip2", "Tip3")  # 替换为你想删除的叶节点名称
# 使用 drop.tip 函数来剪枝
sub_tree &lt;- drop.tip(phylo_tree, tips_to_remove)
# 打印子树概述
print(sub_tree)
# 计算子树的统计量
sub_tree_results &lt;- calc_all_stats(sub_tree)
print(sub_tree_results)


# 假设你知道感兴趣支系的共同祖先节点ID
ancestor_node &lt;- 50  # 这是假设的节点编号，替换为你感兴趣的支系节点ID
# 使用 extract.clade 提取从该节点开始的子树
sub_tree &lt;- extract.clade(phylo_tree, ancestor_node)
# 打印子树概述
print(sub_tree)
# 计算子树的统计量
sub_tree_results &lt;- calc_all_stats(sub_tree)
print(sub_tree_results)

复制<br><br>我们发现终端出现了这些：<br>&gt; print(results)
         area_per_pair     average_leaf_depth             avg_ladder
          1.669176e+01           1.030366e+01           2.315789e+00
        avg_vert_depth                     b1                     b2
          9.333333e+00           9.792910e+01           5.825504e+00
                  beta                   blum               cherries
         -5.383826e-01           2.351575e+02           6.000000e+01
               colless           colless_corr           colless_quad
          1.018000e+03           5.669730e-02           3.672600e+04
             crown_age               diameter        double_cherries
          2.656710e+00           3.400000e+01           4.000000e+00
      eigen_centrality      eigen_centralityW             ew_colless
          2.748954e-01           6.751077e-01           5.317563e-01
            four_prong                  gamma                 i_stat
          1.600000e+01                     NA           6.028266e-01
             il_number        imbalance_steps                  j_one
          7.100000e+01           1.700000e+02           7.354110e-01
                j_stat     laplace_spectrum_a     laplace_spectrum_e
          1.181485e-02           1.251379e+00           7.321651e+00 
    laplace_spectrum_g     laplace_spectrum_p                max_adj
o             pitchforks
          2.656710e+00           1.304070e+00           3.500000e+01
                   psv                 rogers         root_imbalance
          1.128318e+00           1.240000e+02           7.853403e-01
              rquartet                 sackin                 stairs
          4.679263e+07           1.968000e+03           6.526316e-01
               stairs2         symmetry_nodes               tot_coph
          5.909060e-01           1.250000e+02           3.552400e+04
     tot_internal_path               tot_path            tree_height
          1.588000e+03           3.556000e+03           2.656710e+00
              treeness      var_branch_length  var_branch_length_ext
          4.607367e-01           2.495831e-02           2.542879e-02
 var_branch_length_int              var_depth                    vpd
          2.445976e-02           1.399151e+01           1.985848e+00
                wiener
          1.471838e+05
复制<br><br>
<br>area_per_pair：两个叶子节点之间的边数总和。衡量树的拓扑复杂度。
<br>average_leaf_depth：平均叶子深度，指树中从根到叶子节点的平均路径长度。用于衡量树的平衡性。
<br>avg_ladder：平均阶梯数，表示在树中存在的“阶梯”状结构的平均长度。较高的值表示树的形态更不平衡。
<br>avg_vert_depth：平均顶点深度，表示从叶子节点到树根的平均路径长度。
<br>b1：B1 平衡指标，衡量树的平衡性，较高的值表示树不平衡。
<br>b2：B2 平衡指标，基于信息熵来衡量树的平衡性，数值越高树越平衡。
<br>beta：Aldous’ beta 统计量，用于拟合节点的子代分裂情况，负值表示树的形态更不平衡。
<br>blum：Blum 指数，衡量树的不平衡性，较高的值表示树更加不平衡。
<br>cherries：樱桃对数，指树中相连的叶子节点对数。
<br>colless：Colless 指数，衡量树的不平衡性，数值越高表示树越不平衡。
<br>colless_corr：修正的 Colless 指数，进行树大小修正后的 Colless 指数。
<br>colless_quad：Colless 指数的平方版本，提供更大的差异。
<br>crown_age：树的冠年龄，表示树的最早分支到现在的时间长度。
<br>diameter：树的直径，树中最长路径的长度。
<br>double_cherries：双樱桃对，指一对樱桃节点同时连接到同一个节点。
<br>eigen_centrality：特征向量中心性，衡量节点在整个树中的影响力。
<br>eigen_centralityW：加权特征向量中心性，使用加权分支长度计算。
<br>ew_colless：平衡的 Colless 指数，考虑到不同节点的大小。
<br>four_prong：四分叉节点的数量，表示树中有四个叶子节点连接到同一个父节点的数量。
<br>gamma：Gamma 统计量，用于衡量内部节点相对于树根的位置（NA表示无法计算）。
<br>i_stat：I 统计量，衡量节点连接子代的大小不平衡性。
<br>il_number：IL 数量，树中只有一个子叶节点的内部节点数。
<br>imbalance_steps：不平衡步数，指将树变为完全不平衡树所需的步骤数。
<br>j_one：J^1 指数，基于 Shannon 信息熵计算的树平衡性指数。
<br>j_stat：J 统计量，用于衡量树的分化程度。
<br>laplace_spectrum_a：拉普拉斯光谱的不对称性，衡量树的形状不对称性。
<br>laplace_spectrum_e：拉普拉斯光谱的最大特征值。
<br>laplace_spectrum_g：拉普拉斯光谱的特征间隙，表示特征值之间的差异。
<br>laplace_spectrum_p：拉普拉斯光谱的峰值高度。
<br>max_adj：最大邻接矩阵特征值。
<br>max_betweenness：最大中介中心性，衡量节点在最短路径中的重要性。
<br>max_closeness：最大接近中心性，表示节点与其他节点的距离接近程度。
<br>max_closenessW：加权接近中心性，考虑分支长度。
<br>max_del_width：最大宽度差，衡量树的不平衡性。
<br>max_depth：最大深度，树中从根到叶子节点的最长路径。
<br>max_ladder：最长的阶梯状结构。
<br>max_laplace：最大拉普拉斯矩阵特征值。
<br>max_width：树的最大宽度，表示在特定深度的最大节点数。
<br>mean_branch_length：平均分支长度，表示树中分支的平均长度。
<br>mean_branch_length_ext：平均外部分支长度，指连接叶子节点的分支的平均长度。
<br>mean_branch_length_int：平均内部分支长度，指树内部节点之间的分支长度。
<br>min_adj：最小邻接矩阵特征值。
<br>min_laplace：最小拉普拉斯矩阵特征值。
<br>mntd：最近种系距离，指每个叶子节点到最近的叶子节点的距离。
<br>mpd：平均成对距离，所有叶子节点之间的平均距离。
<br>mw_over_md：最大宽度与最大深度的比值。
<br>nltt_base：标准化谱系通过时间的统计量。
<br>number_of_lineages：谱系数量，表示树中的谱系数量。
<br>phylogenetic_div：系统发育多样性，基于树的分支长度来衡量物种多样性。
<br>pigot_rho：Pigot的Rho统计量，表示树在不同时间段内的分化速率变化。
<br>pitchforks：三分支叉，表示树中有三个叶子节点连接到同一个父节点的数量。
<br>psv：系统发育物种变异性，用于衡量物种间的系统发育相关性。
<br>rogers：Rogers不平衡指数，基于内部节点的平衡性。
<br>root_imbalance：树根节点的不平衡性。
<br>rquartet：随机四分支统计量，表示树中随机四个节点之间的拓扑结构。
<br>sackin：Sackin指数，衡量树的不平衡性。
<br>stairs：阶梯结构的数量，衡量树中的阶梯状模式。
<br>stairs2：另一种阶梯统计量。
<br>symmetry_nodes：对称节点数，衡量树中对称节点的数量。
<br>tot_coph：总共系距离，表示树中所有叶子节点之间的成对距离总和。
<br>tot_internal_path：内部路径长度总和。
<br>tot_path：路径总长度，表示树中所有路径的总长度。
<br>tree_height：树的高度，表示从根到叶子的最大距离。
<br>treeness：树的树状结构指数，表示树的分化程度。
<br>var_branch_length：分支长度的方差。
<br>var_branch_length_ext：外部分支长度的方差。
<br>var_branch_length_int：内部分支长度的方差。
<br>var_depth：树的深度方差。
<br>vpd：成对距离的方差。
<br>wiener：Wiener指数，衡量树的传递性，即所有节点之间距离的总和。
<br><br>人类 Y染色体 或 线粒体DNA（mtDNA）构建的系统发育树时，有几个特定的统计量和指标可以帮助我们更好地理解这些树的拓扑结构、进化过程以及遗传多样性。这些统计量能反映树的平衡性、分化速率以及不同族群的遗传结构。以下是特别需要关注的几个指标及其意义：<br><br>
<br>意义：这些指数衡量系统发育树的不平衡性，即不同分支在树上的分化程度是否一致。Y染色体和线粒体DNA由于单亲遗传，往往呈现明显的不平衡现象。高Sackin或Colless值表示某些谱系可能经历了快速扩张或特定事件（如人口迅速扩张等）。
<br>重要性：对于研究人类遗传谱系中的 瓶颈效应、快速扩张事件 或 祖先遗传谱系的集中性，这些指数非常重要。
<br><br>
<br>意义：表示树的最早分支到现在的时间长度。在Y染色体和线粒体DNA的研究中，冠年龄能告诉我们最早的共同祖先出现的时间。
<br>重要性：帮助确定 最近的共同祖先（MRCA） 的时间，尤其是在推测人类起源、迁徙路线以及不同族群的分化时间时具有重要作用。
<br><br>
<br>意义：树高度反映从根部（即共同祖先）到最远叶子节点的距离。对于mtDNA或Y染色体，这个值往往和谱系的 时间深度 相关。
<br>重要性：树的高度可以用于估计不同族群的遗传分化的时间深度，帮助了解 古老谱系与近代谱系的分化。
<br><br>
<br>意义：分支长度代表在树中的遗传差异。平均分支长度反映了不同谱系之间的平均遗传差异，而方差则显示遗传差异的波动性。
<br>重要性：可以用于评估 遗传多样性 和 谱系内的突变速率。特别是在研究mtDNA或Y染色体时，分支长度可以揭示 不同族群的多样性和分化速率。
<br><br>
<br>意义：每个叶子节点（即样本）的最近邻叶子节点的平均距离。它反映了谱系之间的紧密程度。
<br>重要性：在Y染色体或线粒体DNA的研究中，MNTD可以揭示 不同族群的亲缘关系，帮助识别哪些谱系在遗传上最接近。
<br><br>
<br>意义：基于系统发育树的总分支长度来衡量遗传多样性。较高的PD意味着较高的谱系多样性。
<br>重要性：PD对研究 人口历史事件（如迁徙或隔离） 的影响特别有用，因为它可以反映 群体间的遗传差异 和 多样性丧失。
<br><br>
<br>意义：衡量树在不同时间段内的分化速率变化。正值表示分化速率在后期增加，负值表示早期有更高的分化速率。
<br>重要性：适用于研究 谱系扩展模式，例如某些时期的快速人口扩展或迁徙事件。在人类Y染色体或线粒体DNA的研究中，rho值可以帮助我们理解特定人群的 历史人口动态。
<br><br>
<br>意义：这些指标衡量系统发育树中有多个叶子节点从同一个祖先节点派生的情况，反映了分支的不均匀性。
<br>重要性：Y染色体和线粒体DNA由于单亲遗传和特定族群的快速扩张，常出现多分支节点。这些指标可以帮助识别 特定时期的大规模扩张事件，如 “Y染色体亚当” 或 线粒体夏娃 相关的族群扩展。
<br><br>
<br>意义：衡量内部节点在树中的位置，相对于根的位置。正值表示大多数分支发生在树的晚期，负值表示更多分支发生在树的早期。
<br>重要性：在人类谱系的研究中，Gamma值可以帮助理解 谱系分化模式 和 不同时间点的多样性变化，例如某些族群是否经历了早期或晚期的快速分化。
]]></description><link>软件\r语言语法\r：treestats：简易使用.html</link><guid isPermaLink="false">软件/R语言语法/R：treestats：简易使用.md</guid><pubDate>Thu, 12 Sep 2024 01:58:07 GMT</pubDate></item><item><title><![CDATA[八、单因素方差分析及多重比较]]></title><description><![CDATA[ 
 <br>独立样本 T 检验：比较两组均值的差异。<br>单因素方差分析：比较三组或三组以上均值的差异。<br>只要是比较均值差异的统计方法，都要求数据服从正态分布。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181352867.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
首先依然是检查是否符合正态分布。<br>如果服从，则进行下一步：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181354933.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181354223.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181355744.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181355456.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
可以看到 。证明多组之间的平均值存在差异。<br>那么，如果我想知道两两比较，知道它们是否存在差异，就应该进行事后多重比较。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181358011.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181358301.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181358278.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
由上图可知，ANOVA 结果显示：两两比较之间依然存在显著性差异，平均值和标准差叶贝算出来了。<br>为什么要进行校正？
我们进行 4 组之间比较需要 6 次，那么 6 次如果都定在 0.05，那么每一次结论正确的结论是 95%，6 次都准确的概率=(95%)^6. 最终就只有 73%的准确性，为了保证 6 次的准确率依然达到 95%，就需要校正。<br>
校正的步骤其实就是保证 N 次之后依然达到 95%，那么每次就需要（0.05/6）=0.0083333, 按照这个概率进行每一次的运算就可以了。<br>
也可以把每一个的 P 值×6，如果任意一个＞0.05，则终止运算，缺乏统计学差异。
]]></description><link>软件\spss\八、单因素方差分析及多重比较.html</link><guid isPermaLink="false">软件/SPSS/八、单因素方差分析及多重比较.md</guid><pubDate>Fri, 21 Jun 2024 07:31:50 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181352867.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403181352867.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[变量分类]]></title><description><![CDATA[ 
 <br><br><img alt="IMG_2758(20240214-000510).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140950596.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="IMG_2757(20240214-000510).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140950743.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
在SPSS中，变量被分为两大类，三小类，分别是：<br>
<br>计数资料

<br>有序
<br>无序


<br>计量资料
<br>注意，SPSS无法对字符进行运算，例如男，好，差等。因此，需要首先对其进行赋值，例如，将男赋值为1，女赋值为0。<br>赋值可以通过如下修改：<br>
<img alt="50effcbc509c3f8d570dd595f50b923f.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402141005178.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
赋值完成之后，可以点击如下进行切换视图：<br>
<img alt="IMG_2754(20240213-161901).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402141006142.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>在SPSS中，可以像Excel一样进行复杂的运算，可以点击计算变量按钮：<br>
<img alt="IMG_2753(20240213-161901).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140952428.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="IMG_2752(20240214-000510).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402141006474.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\spss\二、认识变量.html</link><guid isPermaLink="false">软件/SPSS/二、认识变量.md</guid><pubDate>Fri, 21 Jun 2024 07:31:51 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140950596.JPG" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140950596.JPG"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[案例]]></title><description><![CDATA[ 
 <br>刚才讲到的 T 检验和单因素方差检验都是参数类检验。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201327358.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果变量不服从正态分布，那么就需要使用非参数检验。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201353819.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
如果要比较病例组和对照组之间的血红蛋白含量是否有差异，首先进行正态分布检验。结果显示，两组（或者任一一组）的糖化血红蛋白含量都不服从。所以我们需要进行非参数检验。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201356598.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意：<br>
如果我们有 2 个组别，使用 2 个独立样本；<br>
如果我们有 2 个组别以上，使用 K 个独立样本。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201357112.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201358916.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>我们可以看到显著性水平&lt;0.05，因此具有差异。<br>
那么，究竟哪个高，哪个低？看看中位数。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201359654.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201359592.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201400207.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我们可以看到两者中位数的差异，病例组的要高一些。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201402237.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这里的  没特殊含义，其存在是为了计算 。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201403207.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>如果我们要比较两组的疼痛等级的差异，也要选择非参数检验。因为疼痛等级是等级变量。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201405430.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>需要注意，等级变量也有中位数和四分位数，所以仍需要计算：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201407099.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201408213.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>如果我们要比较一批人在治疗前后的疼痛状况有没有差异，那么就可以进行配对样本的秩和检验。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201410378.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201411473.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201411234.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我们可以看到显著性&lt;0.05，因此下一步就是计算治疗前后的中位数。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201411178.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\spss\九、非参数检验（秩和检验）.html</link><guid isPermaLink="false">软件/SPSS/九、非参数检验（秩和检验）.md</guid><pubDate>Fri, 21 Jun 2024 07:31:52 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201327358.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403201327358.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br>独立样本 T 检验：比较两组的均值的差异。更进一步，只有连续且服从正态分布的变量才有均值（因为连续但非正态分布的变量对应的统描述统计量是中位数和四分位数）。<br>举例：A 班 50 名学生的成绩平均分与 B 班 55 名学生的成绩平均分有店无显著差异。<br>既然都有了平均分了，为什么要进行检验？
为了检验两个班级的差异是否具有统计学差异。
<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151537301.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我想研究病例组和对照组的年龄是否有统计学差异。那么第一步就是检验年龄是否服从正态分布？<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151538164.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151538657.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151538286.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151539887.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151540372.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
独立样本 T 检验首先检查假定方差大小。<br>如果  ，那么说明假定方差相等成立；那么以第一行为最终结果；<br>
如果  ，那么说明假定方差不相等成立；那么以第二行为最终结果。<br>有图可知，该例应该以第一行为最终结果：显著性&gt;0.05，说明病例组年龄平均值和对照组年龄平均值不具有统计学差异。<br>如果进行可视化，可以如此进行（在 Graph pad 绘制）：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151553510.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\spss\六、独立样本t检验.html</link><guid isPermaLink="false">软件/SPSS/六、独立样本T检验.md</guid><pubDate>Fri, 21 Jun 2024 07:31:52 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151537301.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151537301.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[概念]]></title><description><![CDATA[ 
 <br><br>配对样本 T 检验：比较同一批样本前后两个均值的差异；或者，比较同一批样本两个状态下的均值的差异配对样本 T 检验，也要求两列变量均服从正态分布。<br>举例：<br>
<br>同一批人减肥前后的差异；
<br>同一批样本疫情前后的销量。
<br>配对 T 永远只涉及同一批人。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151558897.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我想知道同一批人四月考试成绩与六月考试成绩之间的差异。<br>首先对其进行<a data-href="三、正态分布" href="软件\spss\三、正态分布.html" class="internal-link" target="_self" rel="noopener nofollow">三、正态分布</a>；<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151601757.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151601181.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
因为配对 T 是对同一批人进行的，因此 2 种状态下的数据应当具备很强的相关性，如果没有，那么就说明数据问题。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151604454.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
假如，算出来的相关性显著性&gt;0.05，那么就不能使用配对样本，而必须使用<a data-href="六、独立样本T检验" href="软件\spss\六、独立样本t检验.html" class="internal-link" target="_self" rel="noopener nofollow">六、独立样本T检验</a>。但是很少遇到这种情况。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151607656.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>结果显著性&gt;0.05，那么再看平均值，发现 <br>
证明某毕业班的学生的六月的平均成绩相对四月的平均成绩出现了显著提升，成绩均值提升了 6.21 分。<br>利用 graph pad 进行可视化如下图：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151619907.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\spss\七、配对样本t检验.html</link><guid isPermaLink="false">软件/SPSS/七、配对样本T检验.md</guid><pubDate>Fri, 21 Jun 2024 07:31:52 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151558897.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403151558897.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[检验方法]]></title><description><![CDATA[ 
 <br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041333770.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041333361.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041335922.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041335204.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041337730.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041338612.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意，如果显著性（P值）＞0.05则服从正态分布；反之不符合正态分布。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041338123.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041338612.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意，如果显著性（P值）＞0.05则服从正态分布；反之不符合正态分布。]]></description><link>软件\spss\三、正态分布.html</link><guid isPermaLink="false">软件/SPSS/三、正态分布.md</guid><pubDate>Fri, 21 Jun 2024 07:31:54 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041333770.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403041333770.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[举例]]></title><description><![CDATA[ 
 <br>线性回归: 因变量为连续型变量，自变量类型不限。<br>
二元 Logistic 回归: 因变量为二分类变量，自变量类型不限。<br>
除此之外，还有泊松回归，负二项回归等……<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221400207.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
我们希望查看年龄、BMI 和 HDL 对代谢综合征（有、无）的影响。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221403887.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221403874.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221403395.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221404436.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>显著性：就是 ，一般认为  认为显著。
<br>B 值：这里的  就是系数，正数代表正相关，负数代表负相关。
<br>Exp（B）：也称为 ，<a data-href="风险比（Odds Ratio, OR）" href="术语\风险比（odds-ratio,-or）.html" class="internal-link" target="_self" rel="noopener nofollow">风险比（Odds Ratio, OR）</a>。如图，表示，年龄每增加 1 岁，患病风险为原来的 1.096 倍。
<br>为什么这里 HDL 的 Exp（B）为 0.000？因为人的生理范围内 HDL 是不可能变化超过 1 的，所以为了使得该结果更准确，我们可以采取：将 HDL 原始数值乘 1000.<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221412190.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
这样就可以更准确查看 HDL 与代谢征的影响。<br><br>我们可以通过森林图进行可视化：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221414348.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">]]></description><link>软件\spss\十、-二元-logistic回归分析.html</link><guid isPermaLink="false">软件/SPSS/十、 二元 Logistic回归分析.md</guid><pubDate>Fri, 21 Jun 2024 07:31:54 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221400207.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403221400207.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[四、线性回归分析]]></title><description><![CDATA[ 
 <br>SPSS 线性回归分析：<br>
<br>因变量只能有 1 个，且必须是连续数值型变量。
<br>自变量可以有多个，既可以是连续型变量，也可以是分类型变量。如果自变量中有分类变量，此时要进行虚拟线性回归。
<br>举例，要研究  之间的因果关系。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121352304.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121353908.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121354990.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
注意，如果属于 ，还要勾选德宾沃森。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121354513.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121356826.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
从上表的结果可以看到，R 2=0.568，意味着模型拟合程度为 56.8%，即拟合状况较好，表示，价格满意度，环境满意度，服务满意度可以比较好的解释消费者的再次购买意愿。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121358587.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>上表的方差分析结果显示，P&lt;0.05，意味着回归模型的存在有显著意义。即，价格满意度，环境满意度，服务满意度这三个自变量中至少有一个自变量可以显著影响因变量。<br>注意，是至少有一个，而不是全都有因果关系。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121406792.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>服务满意度可以显著影响再次购买意愿，P=0.026&lt;0.05，更进步，回归系数为0.127&gt;0，意味着服务满意度可以显著正向影响再次购买意愿，二者之间具体的定量关系是：服务满意度提升 1 分，再次购买意愿则随之提升 0.127 分。
<br>环境满意度不能显著影响再次购买意愿，P=0.153&gt;0.053。
<br>价格满意度可以显著影响再次购买意愿，P&lt;0.05，更进一步，回归系数为 0.120，意味着服务满意度可以显著正向影响再次购买意愿，二者之间具体的定量关系是：服务满意度提升 1 分，再次购买意愿则随之提升 0.127 分。
<br>需要注意，三个变量之间不能存在共线性关系，也就是说自变量之间不能存在重叠。如何考察是否存在这个问题？查看 。<br>
<br>三个自变量的 VIF 值均小于 5，意味着三个自变量之间不存在多重共线性，即回归模型的运算结果稳定可靠。
<br>回归模型的残差基本服从正态分布，即回归模型的运算结果稳定可靠。
<br>打个比方，只要残差随机散乱分布，那么就说明运算结果稳定可靠。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121413915.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>可以用 python 实现操作，具体<a data-href="二十九、 Python线性回归" href="软件\python\学习笔记\二十九、-python线性回归.html" class="internal-link" target="_self" rel="noopener nofollow">二十九、 Python线性回归</a>。]]></description><link>软件\spss\四、线性回归分析.html</link><guid isPermaLink="false">软件/SPSS/四、线性回归分析.md</guid><pubDate>Fri, 21 Jun 2024 07:31:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121352304.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403121352304.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[案例]]></title><description><![CDATA[ 
 <br>
<br>考察构成比的差异。
<br>适用于分类变量的统计学方法。
<br>被检验的分类变量一定是无序变量。
<br>分组变量无所谓。
<br>例如 A 班男女构成和 B 班男女构成之间的差异。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141352757.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br>案例：考察三个年龄组（分组变量）的感染（分组变量）率是否存在显著差异？<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141355458.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141356252.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141356097.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141358590.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>经卡方检验，P=0.009&lt;0.05，意味着三个年龄组的感染率存在显著差异，60-69 感染率最低，70-79 感染率居中，80~感染率最高。<br>注意：这样检验说明三组之间存在差异，不能说明两两比较存在差异，如果需要比较，那么就应该删掉其中一组。]]></description><link>软件\spss\五、卡方检验.html</link><guid isPermaLink="false">软件/SPSS/五、卡方检验.md</guid><pubDate>Fri, 21 Jun 2024 07:31:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141352757.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202403141352757.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[一、认识SPSS]]></title><description><![CDATA[ 
 <br>PSS（Statistical Package for the Social Sciences）是一款强大的统计分析软件，最初由斯坦福大学的一群社会科学家在1968年开发，后来由IBM收购。它主要用于社会科学领域的统计数据分析，但现在已经扩展到各个领域，包括健康科学、市场研究、数据挖掘、政府、教育等。<br>SPSS软件提供了一系列的统计功能，包括描述性统计分析、复杂的样本分析、线性模型分析、回归分析等。它支持各种数据格式，可以轻松地导入和导出数据，进行数据清洗、转换和分析。SPSS具有直观的图形用户界面（GUI），使得即使是没有编程背景的用户也能够使用它进行复杂的统计分析。此外，SPSS也支持编程和自动化，通过SPSS Syntax或Python插件，用户可以编写脚本来自动化重复性的分析任务。SPSS的主要特点包括：<br>
<br>用户友好的界面：提供了丰富的菜单和对话框，用户可以通过点击而不是编写代码来完成大部分的统计分析任务。
<br>强大的数据处理能力：支持大量的数据格式，提供数据转换、排序、筛选等处理功能。
<br>丰富的统计分析功能：涵盖从基本的描述性统计到复杂的多变量分析等广泛的统计方法。
<br>可扩展性：通过SPSS Syntax和Python或R的集成，用户可以扩展其功能，实现更复杂的数据分析。
<br>输出和报告：生成的统计输出和图形可以直接用于报告，支持多种格式的输出，包括HTML、PDF、Word等。
<br>SPSS是一个适合各种水平用户的统计软件，从初学者到高级用户都能从中受益。尽管SPSS是商业软件，需要购买许可证才能使用，但它的功能强大、操作简便，是进行统计分析的重要工具之一。<br><img alt="IMG_2753(20240213-161901).JPG" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140952428.JPG" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
上图显示了软件的工具栏，常用的工具栏是：<br>
<br>编辑→首选项
<br>数据
<br>转换
<br>分析
<br>图形<br>
其他功能基本用不上。
]]></description><link>软件\spss\一、认识spss.html</link><guid isPermaLink="false">软件/SPSS/一、认识SPSS.md</guid><pubDate>Fri, 21 Jun 2024 07:31:55 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140952428.JPG" length="0" type="image/jpeg"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202402140952428.JPG"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[一、步骤]]></title><description><![CDATA[ 
 <br>
<br>原文链接：<a data-tooltip-position="top" aria-label="https://zhuanlan.zhihu.com/p/613163023?utm_medium=social&amp;utm_oi=1318097956147269632&amp;utm_psn=1735664651126435840&amp;utm_source=wechat_session" rel="noopener nofollow" class="external-link" href="https://zhuanlan.zhihu.com/p/613163023?utm_medium=social&amp;utm_oi=1318097956147269632&amp;utm_psn=1735664651126435840&amp;utm_source=wechat_session" target="_blank">SPSS主成分分析——操作步骤及结果解读——超详细版 - 知乎 (zhihu.com)</a>
<br>参考资料<a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a>
<br><br>
<br>分析<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301411384.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301411379.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301412659.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301412502.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>抽取——点击“提取”——勾选“主成分”（此时不能更改其他方法，否则就不叫主成分分析了）——勾选“碎石图”（用于从图视角度判定提取几个主成分较为合适）——“基于特征值“这里特征值大于1是系统默认的，如果当抽取的因子只有一个或者不符合预期时，可以在序号5这里输入你想要的因子个数——点击继续（不过一般主成分分子是基于特征值大于1 实现的，只有因子分析时才进行此操作）。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301413526.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>“得分”——勾选“保存为变量”（计算因子得分的，勾选后因子得分会保存在数据当）——勾选“显示因子得分系数矩阵”——点击“继续”<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301413984.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>“选项”——勾选“禁止显示小系数（勾选后，禁止显示小系数设置成0.5，是为了方便分析成分矩阵和旋转成分矩阵，观察因子归属）——点击“继续”——最后点击“确定”（序号4）得出结果。<br>
<br>得分<br>
主成分得分计算：利用SPSS“转换——计算变量”输入文中对应公式即可计算得到。<br>
综合得分计算：利用SPSS“转换——计算变量”输入文中对应公式即可计算得到。<br>
T分数转换：同样利用SPSS“转换——计算变量”，计算新的排名K1=60+10×K，让spss软件利用K产生一组0-100分的数据，然后对其划分区间分析即可。这里60位新数据的均值，10位标准差的近似值。
<br><br><br><br>对于主成分分析而言，变量间相关性越高，越适合主成分提取，此处大致看看就可，不是主要判定结果。<br><br><br>由表格2可以看出KMO=0.897，巴特利特球形度检验P=0.000&lt;0.05，因此本列适合进行进行主成分分析。<br>此处判断标准是KMO&gt;0.5,P&lt;0.05,则适合做主成分分析。<br><br><br>如果每个变量包含信息为1的话，此处进行主成分的提取，每个变量到底有多少啊信息能够被提取出来，提取的越多，说明信息浓缩的越好，由表可知每个公因子的提取均大于0.5。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301419324.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
使用最大方差法，成分1的特征值为4.77，成分2的特征值为1.72，这两个的方差贡献率为64.93%，总的来看，原有指标的信息丢失较少，主成分分析效果比较理想，具有研究意义。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301420425.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
由碎石图可以看出，越高的点，其势能越大，对于研究而言就越重要，第一个因子的特征值最高，对解释原有的题项的贡献最大，第三个以后的特征值都较小，对解释原有的题项的贡献小，可以忽略，因此提取三个因子是比较适合。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301421605.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
第一成分有组成认知1、认知2、认知3、认知4和认知5，且位于第一个因子，有较高的载荷，将其解释为认知。<br>第二成分由情感1、情感2、情感3、情感4和情感五组成，第二个因子主要解释这几个题项，解释为情感。<br><br><br><br><br><br>因为主成分分析是对数据进行标准化后分析，出现部分负值，不美观，考虑将其进行T分数转化。<br><br>T分数转化后，得到的综合得分位于0-100之间，将其划分为四个等级优秀、良好、及格和不及格。对应的分数为：优秀：大于等于80分；良好：大于等于70分小于80分；及格：大于等于60分小于70分；不及格：为小于60分。<br><br><br><br>
<br>
<br>武松，SPSS实战与统计思维，清华大学出版社<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>软件\spss\spss主成分分析.html</link><guid isPermaLink="false">软件/SPSS/SPSS主成分分析.md</guid><pubDate>Fri, 21 Jun 2024 07:31:56 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301411384.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401301411384.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[O 单倍群进入东亚]]></title><description><![CDATA[ 
 <br><a data-tooltip-position="top" aria-label="https://www.bilibili.com/video/BV1ma411o7nB/?spm_id_from=333.788&amp;vd_source=4410d713724fce7981d19f087cc50ee6" rel="noopener nofollow" class="external-link" href="https://www.bilibili.com/video/BV1ma411o7nB/?spm_id_from=333.788&amp;vd_source=4410d713724fce7981d19f087cc50ee6" target="_blank">【史前中国】一万年前的危机！全球气候剧变 与 农业革命的爆发_哔哩哔哩_bilibili</a><br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061655914.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
M122：<a data-href="汉藏语系" href="术语\汉藏语系.html" class="internal-link" target="_self" rel="noopener nofollow">汉藏语系</a> <a data-href="苗瑶语（Hmong–Mien languages）" href="术语\苗瑶语（hmong–mien-languages）.html" class="internal-link" target="_self" rel="noopener nofollow">苗瑶语（Hmong–Mien languages）</a>、奠基者。也被认为是<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>的标志。<br>
继续分化出 M324，进入 <a data-href="末次盛冰期 (Last Glacial Maximum,LGM)" href="术语\末次盛冰期-(last-glacial-maximum,lgm).html" class="internal-link" target="_self" rel="noopener nofollow">末次盛冰期 (Last Glacial Maximum,LGM)</a>。之后继续分化：<br>
M7: <a data-href="大溪文化" href="术语\大溪文化.html" class="internal-link" target="_self" rel="noopener nofollow">大溪文化</a><br>
M117 : <a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a><br>
00261: <a data-href="大汶口文化" href="术语\大汶口文化.html" class="internal-link" target="_self" rel="noopener nofollow">大汶口文化</a>。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061700411.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>M119：是<a data-href="壮侗语（Kra–Dai languages）" href="术语\壮侗语（kra–dai-languages）.html" class="internal-link" target="_self" rel="noopener nofollow">壮侗语（Kra–Dai languages）</a>，傣，高山等先民。形成<a data-href="百越族群" href="术语\百越族群.html" class="internal-link" target="_self" rel="noopener nofollow">百越族群</a>。是<a data-href="良渚文化" href="术语\良渚文化.html" class="internal-link" target="_self" rel="noopener nofollow">良渚文化</a>奠基者。<br>M268（曹操）：目前主要分布在华南、<a data-href="日本" href="术语\日本.html" class="internal-link" target="_self" rel="noopener nofollow">日本</a>等地。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061705498.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<br>最早的农业是西亚，<a data-href="大麦" href="大麦" class="internal-link" target="_self" rel="noopener nofollow">大麦</a>和<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>。
<br>中国的<a data-href="百越族群" href="术语\百越族群.html" class="internal-link" target="_self" rel="noopener nofollow">百越族群</a>是最早驯化<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061707918.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061710310.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>有学者认为，文字、城市、冶金是形成文明的要素。<br>
但是，现在更多学者认为：<br>
<br>生产发展，人口增加，出现城市
<br>社会分工，阶层分化，出现阶级
<br>出现王权和国家。
<br><br>彭头山文化被认为是苗族文化的起源。之后出现了<br>
<br>皂市下层文化
<br>城背溪文化
<br>新文化（可能是仰韶文化和苗族共同形成的新群体）
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061738502.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
统称为<a data-href="大溪文化" href="术语\大溪文化.html" class="internal-link" target="_self" rel="noopener nofollow">大溪文化</a>。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061739967.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
应该就是三苗的起源。汉东部落向西扩张，与峡江部落发生冲突。最终，大溪文化被<a data-href="屈家岭文化" href="术语\屈家岭文化.html" class="internal-link" target="_self" rel="noopener nofollow">屈家岭文化</a>替代。实现了苗族文化的一统。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061743643.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061744490.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>后来尧舜禹相继对苗蛮进行了反击。遇到了4.2千年降温事件，当时还出现日食，应该是在公元前2072年4月29日。可能东夷参与了南征的战争。禹征三苗正式结束。<br>苗族的 Y 染色体 M7在如今的中国人非常罕见，但是在<a data-href="苗瑶语（Hmong–Mien languages）" href="术语\苗瑶语（hmong–mien-languages）.html" class="internal-link" target="_self" rel="noopener nofollow">苗瑶语（Hmong–Mien languages）</a>有分布。]]></description><link>书籍\青格历史.html</link><guid isPermaLink="false">书籍/青格历史.md</guid><pubDate>Tue, 20 Aug 2024 12:44:13 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061655914.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202404061655914.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[公式]]></title><description><![CDATA[ 
 <br><br><br><a data-href="遗传力（heritability）" href="术语\遗传力（heritability）.html" class="internal-link" target="_self" rel="noopener nofollow">遗传力（heritability）</a>：在总表型方差中，遗传方差（VG）所占的比例。一个表型的方差（VP）是遗传方差（VG）和环境方差（VE）之和。<br><br>这个公式较为理性地阐述了遗传与环境的关系。<br><br><br>连续平均值计算：<br><br>离散平均值计算：<br><br>其中fi为频率。<br><br><br>由于统计原因，自由度为数量减去一。即：<br><br><br>]]></description><link>书籍\群体演化.html</link><guid isPermaLink="false">书籍/群体演化.md</guid><pubDate>Mon, 26 Aug 2024 00:53:36 GMT</pubDate></item><item><title><![CDATA[第一节]]></title><description><![CDATA[ 
 <br><br>这是我整理的思维导图啦！欢迎大家指正或提出建议。<br><img src="https://pic4.zhimg.com/80/v2-6e718018632d259603f553f05a31cf13_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-09a5c41f559229bb85a75b04ad980564_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-41c04d64e3fe76b3e0e8bcfd05b4a3d6_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-d579b10affc698f0c8e7fff31170054c_720w.webp" referrerpolicy="no-referrer"><br><br>这是我整理的思维导图，欢迎大家指正或提出建议！<br><img src="https://pic1.zhimg.com/80/v2-5e55b8f4618a57115f28f4c4a1489b0c_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-b79c453818b050d267f29fdfe4bfd312_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-2f4c924c2c7ecb89dcea5e29e8ece2a2_720w.webp" referrerpolicy="no-referrer"><br><br>这是我整理的思维导图，欢迎大家指正或提出建议！<br><img src="https://pic3.zhimg.com/80/v2-131f18f646a6606ad1866508733f5bb6_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic2.zhimg.com/80/v2-f3235946de70705373acc964fe94f841_720w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-949a40251d03622632c1f0e5ed8da64c_720w.webp" referrerpolicy="no-referrer"><br><br>这是我整理的思维导图，欢迎大家指正或提出建议！<br><img src="https://pic2.zhimg.com/80/v2-34e1b490d4aa345f1eb4df222a4688b5_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-9e742a07911a67a613f806e703b9f56a_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-c58e8946f29a6d93c5b8ab8a76b3c758_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-0e839cb83dc7c7a818757e320345cc38_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-6374dcca043dd66563a60ee4568e7bd8_1440w.webp" referrerpolicy="no-referrer"><br><br>这是我整理的思维导图，欢迎大家指正或提出建议！<br><img src="https://pic1.zhimg.com/80/v2-adb268b74560afff2f862ce5bec3de84_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-f417d807bb31736daea1adfc267caa40_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-fd5f4d593e418d0944942c9d0e58dd2a_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic3.zhimg.com/80/v2-1f2b75757eecf6cd7ec327a1559a5016_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-3979373a69f28a418feabd3e83bf58ec_1440w.webp" referrerpolicy="no-referrer"><br><br>这是我整理的思维导图，欢迎大家指正或提出建议！<br><img src="https://pic2.zhimg.com/80/v2-895984d3bc39d73d49906195f9bfdeb5_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic4.zhimg.com/80/v2-b6af0a2661d6aed6acf66465c5835a3b_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic2.zhimg.com/80/v2-cf343ac793583f8eb34d4b0e76bd9f51_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic4.zhimg.com/80/v2-d726a4fa372d6a6515b3027b6e57130f_1440w.webp" referrerpolicy="no-referrer"><br><img src="https://pic1.zhimg.com/80/v2-928ec582855b6568f633741360fee2d8_1440w.webp" referrerpolicy="no-referrer">]]></description><link>书籍\群体遗传学.html</link><guid isPermaLink="false">书籍/群体遗传学.md</guid><pubDate>Thu, 20 Jun 2024 12:44:21 GMT</pubDate><enclosure url="https://pic4.zhimg.com/80/v2-6e718018632d259603f553f05a31cf13_720w.webp" length="0" type="image/webp"/><content:encoded>&lt;figure&gt;&lt;img src="https://pic4.zhimg.com/80/v2-6e718018632d259603f553f05a31cf13_720w.webp"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[线粒体单倍群]]></title><description><![CDATA[<a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> <a class="tag" href="?query=tag:单倍群" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#单倍群</a> 
 <br><br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> 非洲：L=L1，L2，L3<br>
近东：J，N（N 包括在 A，B，F，H，I，J，K，P，R，S，T，U，V，W，X）欧洲：J，K，H，V，T，U，X<br>
亚洲：A，B，C，D，E，F，G，M（M 包括在 C，D，E，G，Q，Z）<br>
美洲：A，B，C，D 和少量的 X<br>
现在，L0频率最高的群体是非洲中部的俾格米人（Pygmies）和非洲南部的科伊桑人（Khoisan，常称桑人）​。<br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> M 单倍群离开了非洲，可能通过红海——亚丁湾（Gulf of Aden）一带渡过狭窄的海峡，从东非海岸来到阿拉伯半岛。这是一场长距离大迁徙的开始，M 沿着中东——欧亚大陆南部——澳大利亚，最后到达波利尼西亚。M 是出现在约6万年前的第一批走出非洲的人类。<br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> C 起源于里海和贝加尔湖之间的中亚大平原，属于西伯利亚血统，占西伯利亚地区的20%。由于年代久远，在欧亚大陆北部频率较高，被认为是最早定居在这一带的第一批人类。<br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> 5万年前，从 M 分支的另一批人群 D 单倍群进入中亚干草原——亚洲东部，他们的第一批成员继续向东，最终进入北美洲和南美洲。与 C 单倍群一样，D 单倍群也居住在里海和贝加尔湖之间的中亚大平原，属于欧亚大陆东部血统，D 的后裔向四周扩散并迅速南下，现在是亚洲东部的重要血统，约占20%。D 的频率向西的方向逐步减少，在亚洲中部为15%——20%。<br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> 约3万年前，Z 单倍群（L1/LO→L2→L3→M→Z）的第一个成员北上进入西伯利亚，开始向亚洲东部的旅程。Z 属于西伯利亚血统，居住在里海和贝加尔湖之间，现在约占这一地区的3%。Z 单倍群向四周扩散并南下进入亚洲的北部和中部，现在约占亚洲东部的2%。但是，Z 向其他方向的迁移似乎都失败了。当 C 和 D 的后裔进入美洲时，Z 的后裔没有前往美洲。由于 C-D-Z 的居住区域相同，也许 Z 也进入了美洲，但是这支血统在美洲绝嗣了。<br><a data-footref="1" href="about:blank#fn-1-3d0ee807868908f1" class="footnote-link" target="_self" rel="noopener nofollow">[1]</a><img alt="线粒体单倍群地图" src="https://upload.wikimedia.org/wikipedia/commons/a/a4/Migration_route_of_Human_mtDNA_haplogroups.png" referrerpolicy="no-referrer"><br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> M 来自第一波走出非洲的大迁移，N 来自第二波走出非洲的大迁移。M 的路线是通过红海，N 的路线是沿着尼罗河，通过西奈半岛走出非洲，因为沿着尼罗河谷地迁移可以找到足够的食物与饮水。这些 L3的后裔最终构成了 N 单倍群。N 被认为是欧亚大陆西部的单倍群，因为在中东和欧洲的几乎所有线粒体血统中都发现了 N。<br><a href=".?query=tag:单倍群" class="tag" target="_blank" rel="noopener nofollow">#单倍群</a> A单倍群的第一个成员越过西伯利亚，最终来到北美洲和南美洲。A 可能起源于中亚高原，然后扩散到亚洲东部几个地区。在美洲土著中第一次发现 A，使得遗传学家开始用这个突变研究史前人类的迁移。除了极少的例外，A 是爱斯基摩人的唯一血统，A 也是西伯利亚——阿拉斯加——加拿大的美洲土著的血统，可靠的起源时间约为1.1万年前。<br>B 单倍群（L1/LO→L2→L3→N→B）的第一个成员进入亚洲东部，最终来到北美洲和南美洲，以及波利尼西亚的大部分地区。这个 B 单倍群可能起源于里海和贝加尔湖之间的中亚高原，成为亚洲东部的创始血统之一，B、F、M 构成了现在亚洲东部所有线粒体血统的大约四分之三。B 单倍群向四周扩散并迅速南下进入亚洲东部，现在约占东南亚的17%，约占中国全部基因池的20%，并且广泛分布在太平洋沿岸，从越南到日本，少量存在于西伯利亚土著（约3%）​。由于历史久远，频率较高，B 被广泛承认是欧亚大陆最早的人群之一，也是南北美洲的5个线粒体血统之一。B4，从东南亚向波利尼西亚扩张。B4积累了在欧亚大陆的突变，最近不到5，000年内扩散到波利尼西亚，其中的一些中间血统出现在越南——马来西亚——婆罗洲，支持 B4起源于东南亚的可能性。<br><br><br>
<br>
<br>崎谷満『DNA・考古・言語の学際研究が示す新・日本列島史 日本人集団・日本語の成立史』<a href="about:blank#fnref-1-3d0ee807868908f1" class="footnote-backref footnote-link" target="_self" rel="noopener nofollow"><span></span>︎</a><img class="emoji" draggable="false" alt="↩" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/21a9.svg" height="18px" style="max-width: 100%;">
]]></description><link>书籍\人类六万年：基因中的人类历史.html</link><guid isPermaLink="false">书籍/人类六万年：基因中的人类历史.md</guid><pubDate>Wed, 02 Oct 2024 13:19:09 GMT</pubDate><enclosure url="https://upload.wikimedia.org/wikipedia/commons/a/a4/Migration_route_of_Human_mtDNA_haplogroups.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://upload.wikimedia.org/wikipedia/commons/a/a4/Migration_route_of_Human_mtDNA_haplogroups.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[当语言遇上基因：东亚的人类起源与族群演化]]></title><description><![CDATA[ 
 <br><br><br>我们已经很明确地告诉大家，现在全世界的人类都属于智人（Homo sapiens）物种的现代人（Homo sapiens sapiens）亚种。这一亚种大约20万年前起源于非洲东部，在6万多年前绕过红海走出非洲，渐渐散布到世界各地。由于旧石器时代的地理隔离，为适应不同的气候环境，演化出了8个地理种。在冰川期结束以后，地理隔离渐渐打破，人群再次迁徙，使得地理种的分布渐渐交错。其中4个地理种在东亚有分布，按到来顺序依次为：远东沿海环境的澳大利亚人种、东南亚雨林与青藏高原的尼格利陀人种、东亚季风平原的蒙古利亚人种、西亚和中亚草原的高加索人种。这4个地理种在东亚渐渐融合起来，又在新石器时代以来的文明演化过程中形成了多个语系群体。东亚有汉藏、侗傣、苗瑶、南亚、南岛、阿尔泰和印欧等7个语系的200多种语言。这使得东亚成为世界上研究人类进化、遗传多样性和基因与文化相互作用的最重要区域之一[1]。<br><br>在过去数年中，分子人类学的研究者们使用常染色体和X染色体、父系Y染色体、母系线粒体等遗传标记体系来解析东亚人群的遗传多样性。常染色体和X染色体遗传自父母双方，会被重组所打乱，而Y染色体上主干的非重组区呈严格父系遗传，并且理论上Y染色体的“有效群体大小”至多为常染色体的四分之一。不过实际上，由于人类社会长期的男性生育权不平等，使得这个数值接近于四百分之一，所以Y染色体对漂变非常敏感，容易形成群体特异性多态标记，在群体之间差异最大，从而包含更多的关于群体历史的信息。Y染色体的这些特点使其成为研究人类进化和迁徙最强有力的工具之一[2,3]。<br>Y染色体进入人们的视野，始于其在追溯现代人起源上的应用。自20世纪90年代以来，人类学界争论最激烈的话题，就是东亚地区现代人的起源问题。由于东亚出土了大量的古人类化石，一些人类学家认为，东亚地区的人类是本土连续进化的，从而支持全球现代人的多地区起源。直到现在，还有部分学者和大量民众认为，中国早期的各种古人类化石都是现代中国人的祖先留下的。特别是一些变异非常丰富的早期智人的化石，经常会表现出与现代人非常相似的特征。但是，外形相似并不等于亲缘关系相近，亲缘关系是必须通过DNA比对来确定的。就像现代社会的亲子鉴定原则，不可能通过长相分析亲子关系，必须经DNA分析以后才能真正确定。中国人是不是与世界其他人群不一样，其他人群都是非洲早期智人进化成的现代人，而中国人来自东亚的直立人？这必须经DNA鉴定才能判断。因此，我们有必要简单回顾一下东亚人群起源的DNA研究历史。<br>其实1978年“线粒体夏娃学说”发表的时候，世界线粒体谱系树中就包含了中国样本，很明确位于非洲起源的进化树上。不仅母系的“夏娃”如此，父系的“亚当”也是同样的。1999年，宿兵等人[4]采用Y染色体非重组区的19个SNP来研究东亚人群，得出结论为，东亚地区现代人起源于非洲，并由南方进入东亚，而后向北方迁。但是，有没有可能在中国保存了少量东亚直立人的后裔，只是采样太少而被遗漏了？于是，复旦大学的团队扩大了采样范围，2000年，柯越海等人[5]对东亚地区12 127份男性随机样本的Y染色体进行SNP分型研究。这些样本包括所有的语系、大部分民族，甚至极其偏远的隔离群体，从东南亚的热带雨林一直到北极冰天雪地中的楚科奇村落。如果这样大规模的采样调查都没有发现东亚直立人后裔，那么被遗漏的概率就只有六百亿分之一了，也就是基本不可能存在了。非洲起源的最典型标记是Y染色体突变M168，这个标记被认为是约6.4万年前现代人走出非洲时所产生的突变，其原始型仅出现在非洲的撒哈拉以南人群中，除非洲以外的人群都是突变型。柯越海等人的研究虽然没有直接检测M168这个突变，但他们检测了M89、M130和YAP这三个M168下游的突变，有这三个突变之一的个体，必然有M168突变。结果显示，这一万多份样品无一例外都带有M89、M130和YAP三种突变之一，也就是说都是M168突变型。M89突变在东亚形成的Y染色体类型（单倍群）主要是O型和N型，还有一些旁支类型。M130形成的单倍群是C型，YAP形成的是D型。尽管现在来看，东亚现代人或许与一些古人种有少许基因交流[6-8]，但从父系角度看，现存的东亚人群都是现代人走出非洲的后裔，这是支持现代人非洲单一起源的强有力的遗传学证据。<br>解决现代人从哪里来这个问题之后，接下来就要回答早期现代人是如何迁徙来到东亚的。<br>人群的迁徙和分布与气候的变迁有着密切的关系，为便于从不同角度探索和认识人群演变规律，这里介绍一些近10万年来的气象学材料。在距今约11万～1万年，也就是考古学上的旧石器时代到中石器时代，地球处于末次冰期[9]，那段时间，海平面远低于现在，现在的许多岛屿与大陆相连，成为人类迁徙的重要通道。始于距今2.65万年终于距今2万～1.9万年间的末次冰盛期，是末次冰期中气候最寒冷、冰川规模最大的时期，亚洲的绝大部分、北欧和北美都被冰雪覆盖，人类的生存空间也随冰川蔓延而逐渐缩小。大约1.5万年前，气温开始转暖，冰川开始退却，到1.2万年前气候基本回升到现代的水平，现代人才迎来了人口扩张的黄金时期[10-11]。<br><br>从地理上看，东亚与欧亚大陆其他部分隔着高耸的喜马拉雅山脉和青藏高原，早期人类的大规模迁徙不太可能选择横穿青藏高原，更可能通过高原的南北两侧，较容易地进入东亚。所以，人类进入东亚只能有两个入口：南方的横断山区，北方的阿尔泰山区。不同的人种不一定选择同样的道路。现阶段比较一致的看法是，东亚的四个人种中，东亚的高加索人种类型来自西北[10,12]，澳大利亚人和尼格利陀人来自东南[4,10]。最具争议的还是蒙古利亚人来自哪里。早期的人类学家提出三种可能的模式：（1）蒙古利亚人由北向南迁徙，与东南亚和中国南方的尼格利陀和澳大利亚人种混合；（2）蒙古利亚人来自南方；（3）北方人群来自北方，南方人群来自南方，自一万多年前的晚更新世以来，蒙古利亚人在南北方共同进化[13]。要解决这一争议，Y染色体是关键、有力的工具。<br>Y染色体可以分为20种主干单倍群，编号从A到T（P非常罕见，偶见于古代遗骸中），其中M89之下的O-M175和N-M231、M130定义的C-M130、YAP下的D-M174是东亚四个主要单倍群，约占东亚全部男性的93%（图4.1）。其他单倍群，例如M89下的G-M201、H-M69、I-M170、J-P209、L-M20、Q-M242、R-M207和T-M70，以及YAP下的E-SRY4064，仅占东亚男性的7%[12]。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211547630.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图4.1　Y染色体主干单倍群C、D、N和O在欧亚地区的地理分布。颜色越深，表示在人群中所占比例越大。
<br>O-M175是东亚最大的单倍群，约75%的中国人以及超过50%的日本人都可归到这一类型下，因此有理由认为它代表着蒙古利亚人，是蒙古利亚地理种演化过程中漂变形成的单倍群。O-M175分出三个主要的下游单倍群O1a-M119、O2-M268以及O3-M122，这三个单倍群约占东亚男性的60%[14,15]。国际Y染色体命名委员会规定，单倍群的一级编号是固定的，次级编号需要随着谱系树结构的细化而调整，因为发现O1和O2有共同的特有突变而关系更近，所以新的系统中把它们定义为O1a和O1b，把O3改为O2。但是因为长期以来都用O1、O2、O3来分析讨论东亚族群演化历史，所以本书中就沿用原有名称以方便阅读。O1a-M119在中国东南沿海、侗傣族群、台湾原住民、南岛语人群中集中分布[16]。O2-M268在汉族中约占5%以上[14], O2a1-M95是O2下的主要支系，在华南、南方少数民族、中南半岛及印度门哒人群中分布较多[16,17]。O2b-M176是O2下的另一支系，主要集中于朝鲜半岛、朝鲜族和日本弥生系人群，越南人和汉族中也有极少量分布[18,19]。O3-M122是中国最常见的单倍群，遍及整个东亚和东南亚，占汉族50%～60%。O3a1c-002611、O3a2c1-M134和O3a2c1a-M117是O3下的三个主要支系，各占到汉族的12%～17%。O3a2c1a-M117在藏缅族群中也有较多分布。O3下的另一支系O3a2b-M7在苗瑶人群和孟高棉人群中高频出现，但在汉族中不足5%[14,15]。所以，通过观察Y染色体O单倍群各个亚型的分布，可以发现，Y染色体与语系人群明显相关，而且各个语系从Y染色体多样性角度体现出不同的亲疏关系。<br>宿兵等[4]在亚洲大范围群体样本中对包括M119、M95和M122在内的19个Y染色体SNP位点以及3个STR位点进行了检测。在随后的主成分分析中，北方人群紧密聚在一起，且被包含在南方人群的聚类簇之内，南方人群比北方人群多样性高。他们认为，北方人群来自旧石器时代定居南方的南方人群。他们还使用STR位点的一步突变模式和0.18%突变率估算O3-M122这一单倍群的时间为6.0万～1.8万年前，这一时间可能反映的是最初定居东亚的瓶颈时期。2005年，石宏等[15]对东亚多个群体的2000多个O3样本进行了更系统的研究，也发现南方群体中O3-M122的多样性高于北方，支持O3-M122的南方起源。他们进一步使用均方差（ASD）方法和STR的进化突变率（每位点每25年0.000 69）[20,21]估算O3支系北迁的时间为3.0万～2.5万年前。2011年，蔡晓云等[22]对东南亚的孟高棉族群、苗瑶族群中的O3a2b-M7和O3a2c1a-M117进行了系统研究，揭示其在约1.9万年前末次盛冰期经由东南亚进入东亚的单向瓶颈扩散[22]。O3下的另一主要支系O3a1c-002611的STR位点多样性也与其他兄弟支系一样，有着大体上自南向北递减的趋势[23]。总体来看，绝大多数证据都支持Y染色体单倍群O3-M122经由南方路线进入东亚并逐渐向北扩散的观点（图4.2）。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211547200.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图4.2　各类Y染色体单倍群在东亚的迁徙。虚线表示另外可能的迁徙路线。
<br><br>东亚的特征单倍群O-M175的产生时间，由足够多的STR的位点估算下来，很可能不超过3万年，因此单倍群O人群很可能根本不是东亚最早的定居者。单倍群C-M130人群却极可能是最早到达东亚的人群。单倍群C从阿拉伯半岛南部、巴基斯坦、印度、斯里兰卡、东南亚、东亚、大洋洲到美洲都有分布，尤其在远东和大洋洲高频分布，但在撒哈拉以南的非洲没有被发现（图4.1）。C下游的分支，例如C1-M8、C2-M38、C3-M217、C4-M347、C5-M356和C6-P55，都有着区域特异性分布[24]。C3-M217是分布最广的支系，在蒙古和西伯利亚群体中最高频出现。单倍群C1仅在日本人和琉球人中出现，但频率很低，不足5%。单倍群C2出现在从印度尼西亚东部到波利尼西亚的太平洋岛屿人群，尤其是在波利尼西亚的一些群体中，且由于连续的奠基者效应和遗传漂变而成为上述地方的特征单倍群[19,25]。C4几乎仅局限在大洋洲的澳大利亚原住民中。C5在印度及其周边的巴基斯坦和尼泊尔等地低频出现[26,27]。C6则仅出现在新几内亚高地上[28]。单倍群C的分布模式说明了这个单倍群很可能是在亚洲大陆起源，且那时还没到达东南亚。<br>为更清楚地说明单倍群C的源流，钟华等[24]对取自东亚和东南亚140多个群体的465个单倍群C的样本，检测了C内部的12个SNP和8个STR位点。他们发现，C3的STR多样性最高出现在东南亚，且呈自南向北、自东向西递减的趋势，ASD方法估算时间落在距今4.2万～3.2万年间，这表明旧石器时代C3是沿海岸线逐渐向北扩张的（图4.2）。单倍群C很可能在6万年前就已到达东南亚和澳大利亚，比其向北扩散的时间要早得多，这也就是说，单倍群C在蒙古利亚人（单倍群O）到来之前就已在东亚生活了数万年。经过如此长的时间，单倍群C的人群或已与蒙古利亚人有着不同的体质特征。因为现在单倍群C的人群多有着澳大利亚人的体质特征，例如澳大利亚原住民、巴布亚人和一些达罗毗荼人的体质特征，所以我们认为，单倍群C是由具澳大利亚人体质特征的人带来的，他们达到远东的时间要早于其他现代人。北京周口店出土的一万年前的人骨就有着澳大利亚人的体质特点，或也支持澳大利亚人是东亚最早定居者的观点。<br><br>最具神秘色彩的是Y染色体单倍群D的迁徙历史，迄今为止我们仍对此知之甚少。单倍群D是从非洲的DE-M1（YAP插入）单倍群衍生出来的，很可能与矮黑体质的尼格利陀人相关联。单倍群E是D的兄弟支系，E随着大黑人西迁非洲，D则可能由小黑人东迁带到东亚。<br>单倍群D-M174在安达曼尼格利陀人、北部藏缅群体和日本的阿伊努人中高频分布，在其他东亚、东南亚和中亚群体中也有低频分布（图5.1）[17,19,29,30]。D下分D1-M15、D2-M55和D3-P99三个主要支系，还有许多未明确定位的小支系。D1在藏族、羌语支和彝语支人群中广泛分布，在东亚其他群体中也有低频分布[31,32]。D2仅分布于日本，占日本40%以上，是上古绳文人的主要成分。D3在青藏高原东部（康区）、白马人及纳西族等群体中高频分布[31]。D  多在安达曼群岛被发现[30]，且已被隔离了至少2万年。其他一些被包含在D  中的小支系也多分布于西藏周边藏缅语人群、东南亚人群，阿尔泰人中也有少量来源不明的D 。这些D  的内部谱系需要详细调查分析。单倍群D高频人群的肤色大多较深，包括安达曼人、一些藏缅人和孟高棉人等。阿伊努人肤色变白可能是为了吸收更多紫外线以适应高纬度地区生存。<br>对于单倍群D的起源，钱德拉塞卡（A. Chandrasekar）等认为CT-M168在南亚分出了YAP插入和D-M174突变，因为他们在印度东北一些族群中发现有YAP插入，而在安达曼群岛上检测到了M174突变[33]。这样来看，同样带有YAP插入的E单倍群也很可能是亚洲起源，但没有证据进一步支持。如果单倍群D诞生于非洲，那非常有趣的是它是如何随着总单倍群C和F的群体来到东亚的？<br>另一不可思议的是单倍群D是如何由东亚的西南角一路到日本的。它可能通过东亚大陆北上，也可能经由巽他大陆，但穿过东亚大陆似乎更近。石宏等人推论单倍群D北上扩张到中国西部的时间约在6万年前（ASD方法），要早于东亚其他主要支系的迁徙。随后，这一先头部队可能通过北向路线经由朝鲜半岛到达日本列岛，或者通过南向路线经由台湾岛和琉球群岛所形成的大陆桥到达日本列岛，这一过程中他们可能与澳大利亚人相遇过。后来，单倍群O的北上以及新石器时代汉族扩张，单倍群D的主体人群可能就被挤出了中国东部[31]。但是无论是遗传学上还是考古学上，都没有任何证据表明D2或尼格利陀人曾到过中国大陆东部。相反，从马来半岛到波利尼西亚的巽他大陆至今仍有大量的尼格利陀人。尼格利陀人或许在旧石器晚期占据了整个巽他大陆。那么，这些人群可能直接从菲律宾群岛到台湾岛和琉球群岛。唯一难以解释的是在菲律宾群岛的尼格利陀人中从未发现过D的存在，他们的父系或许已在约1.8万年前（BATWING方法）被来自巴布亚岛的C2和K的扩张所取代[34]，当然也可能被非常晚近时期来自东亚大陆的单倍群O所替换[35]。因为相关数据不足，东亚的黑人遗存——单倍群D的源流还远未揭开。<br><br>单倍群O的兄弟支系是单倍群N-M231，单倍群N在欧亚大陆北部，尤其是包括芬兰、乌戈尔、萨摩耶德和尤卡吉尔等分支的乌拉尔语人群，以及阿尔泰语人群和因纽特人中高频分布，它还低频出现在东亚内陆（图4.1）[29,36]。对于单倍群N的详细分析显示，N在东欧的高频分布是缘于很晚近的迁徙，这次迁徙从1.4万～1.2万年前（ASD方法）开始，由内亚/南西伯利亚出发，走一条逆时针的北部路线[36]。N的下游分支N1a-M128低频分布于中国北部一些群体，例如满族、锡伯族、鄂温克族和朝鲜族，以及中亚的一些突厥语族群中。另一分支N1b-P43在北部的萨摩耶德人中广泛分布，也在一些乌拉尔人群和阿尔泰人群中呈低频或中频分布，N1b在1.8万～1.6万年前诞生于西伯利亚[37,38]。频率最高的下游单倍群N1c-Tat，可能在1.4万年前起源于中国西部地区，然后在西伯利亚经历多次瓶颈效应，最后扩散到东欧和北欧[36]。这些研究把单倍群N的起源追溯到中国西南或东南亚。实际上，我们的研究数据显示，N的大量最原始类群存在于汉族群体中，东南亚和北欧的类型分别是从汉族的类型中衍生出来的。单倍群N的人群艰苦跋涉由东亚穿越大陆一直到北欧，谱写了壮丽的迁徙史诗。<br>基因上汉族与乌拉尔语系人群有这么紧密的联系，说明了两个族群历史上一定有亲密接触。目前分析东亚民族起源时期的各个考古遗址中Y染色体分布情况，发现N的扩张源头应该在辽宁西部。辽西在近3万年前就成为细石器文化的一个源头。大约8200年前开始出现精细的玉器，进入最早的新石器时代文化——兴隆洼文化。大约7200年前进入了赵宝沟文化，飞鹿纹等文化特征呈现出非常浓郁的乌拉尔民族特色。大约6400年前，赵宝沟文化被红山文化取代，汉文化的基本要素（龙凤、冠冕、岐黄）都出现在红山文化中。这一系列考古文化中发现的人类遗骸，经过基因检测发现，大多数Y染色体类型是N。说明这里最有可能是乌拉尔族群的发源地。而从红山文化开始，高等级墓地中的人骨检出的都是O3类型，是汉族的主体类型。这说明红山的上层阶级来自华北的磁山文化人群，进入辽西与赵宝沟人群混合以后，开始孕育形成最初的汉族。所以汉族中有乌拉尔族群特色的N单倍群，汉语中也有大量乌拉尔语系同源的词汇。汉语一直被认为是一种混合语，虽然历史上汉语曾经吸收了苗瑶语、侗傣语、阿尔泰语甚至印欧外来语的大量词汇，但是混合语所指的并非这些晚期的混入词汇。汉语与藏缅语分开，其起源上可能就是原始汉藏语与原始乌拉尔语的混合。研究乌拉尔语的学者高晶一发现，汉语中的很多词汇都是双套的，有俗言和雅言两种说法，俗言来自乌拉尔语，雅言来自汉藏语。例如“爷娘”来自乌拉尔语，“父母”来自汉藏语，“家”来自乌拉尔语，“宫”来自汉藏语。这是一个非常重大的发现，很出乎意料。因为乌拉尔语系地理分布上与汉语太远，以往的语言学家研究汉语混合起源时从未想过，直到现在很多语言学家还不敢相信。但是，远古人类迁徙的距离本就是很惊人的，不然人类也不可能从非洲到达南美。2019年，复旦人类学系的张梦翰在《自然》上发表了汉藏语系语言谱系树构建的文章，认为汉语和藏缅语有两次分化，分别大约是距今6000多年和5300年。6000多年前，正是红山文化形成的年代。而5300年前，是红山文化南下、中原的仰韶文化西迁的年代。<br>单倍群N的迁徙史为东亚人群南方起源提供了又一项强有力的证据。然而仍有一些研究在质疑南方起源。卡拉费特（T. M. Karafet）等对来自东亚和中亚地区的25个群体的1300多份样本进行Y染色体分型研究，他们发现各单倍群间的两两差异在东亚南部是非常小的，且东亚南北群体之间并未发现遗传分化[29]。薛雅丽等[39]使用贝叶斯全似然法，分析取自中国、蒙古、韩国和日本的27个群体近1000份样本的Y染色体45个SNP和16个STR位点，发现东亚北方群体的Y染色体的STR多样性要高于南方，北方群体的扩散要早于南方群体[39]。但随后石宏指出，卡拉费特所观察到的北方群体的高多样性应是由近期的人群混合造成的，薛雅丽等的分析结果也存在这一问题，即蒙古族、维吾尔族和满族的基因多样性高，应是他们与西方人群及汉族大规模混合的结果。历史上汉族的南迁，使得中国南方的人群被替换，也降低了中国南方的多样性[40]。另外，薛雅丽等所选取的南方群体代表性不够，长期地理隔离所造成的群体内部的瓶颈效应或对基因多样性的估算有较大影响[31]。<br>后续的争论就集中在如何辨析中亚和欧亚西部人群对东亚的基因贡献。钟华等[13]对117个群体的近4000份样本的Y染色体进行高分辨率的分型判断，以试图阐明这一问题。在钟华等的研究中，单倍群O-M175、C-M130、D-M174和N-M231仍显出了南方路线基因贡献较大。然而，与中亚和欧亚西部相关的单倍群，例如单倍群R-M207和Q-M242，多在东亚西北地区出现，且它们的频率自西向东有递减的趋势。另外，单倍群R-M207和Q-M242的Y染色体STR多样性也提示了北方路线存在的可能性，即可能存在1.8万年前人群由中亚到北亚进行迁徙，以及沿丝绸之路的人群3000年前开始的频繁流动和混合。<br><br>进入东亚的数支现代人约5万年前以来在各地散布开来，在冰期过着原始的狩猎采集生活。约1.2万年前冰期结束，温带动植物开始繁盛，人类食物来源增加，人口大幅增长。增长的人口积累了技术与文化，各地很快发明了农业。一万多年前，中国南方沅江流域、钱塘江流域的人们驯化了水稻，北方桑干河流域的人们驯化了小米。农业给人们带来相对稳定的食物来源，分散的人群向农业核心聚合，在各个区域形成了人群、语言、文化的稳定共同体。所以，早期的民族、语系与考古区系是对应的。<br>汉藏语系起源于桑干河流域的磁山文化及上游大同境内的前体人群，之后南北分化为仰韶文化与红山文化。苗瑶语系起源于湖广地区的高庙文化。南亚语系可能起源于四川盆地。南岛语系起源于江浙早期的马家浜文化。侗傣语系起源于江浙稍晚的良渚文化。芬兰-乌拉尔语系起源于辽西的赵宝沟文化。由于气候变动，人群迁徙，族群之间发生了竞争，造成了融合与外迁。早期外迁的南岛、南亚、古亚、芬乌等族群的文化渐行渐远。而长期在中国内地的汉藏、苗瑶、侗傣、匈羯等族群语言发生大量交融，从周代至汉代，演化出了声调，变成了无需时态语态的分析语<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211549521.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图4.3　第十棵进化树——东亚族群演化。图中数字表示距今万年数。
<br><br>东亚语言间的演化关系，虽然比非洲和西亚要复杂一点，但是比美洲和大洋洲的情况要简单得多，而且东亚的语言学调查和研究做得非常细致。不过，东亚语系划分问题，至今国内外争议颇多，这可能是因为在具体语系划分时没有执行相对统一的标准。两个语言类群分化超过多久可以算作不同语系，这应该有一个相对统一的标准。目前较为一致的看法是：两个语言类群分化超过8000年，其间的相似性就难以判别了，如果超过一万年，可能就完全看不出关系了。8000年这个标准年代，应该是一个很有意义的年代，因为这是新石器时代和农业全面开始的年代。以此为语系划分的标准，也就是说语系是新石器时代人群文化区系集中的结果。那么，不同的语系势必追溯到不同的新石器时代文化区系。反过来说，如果没有独特的新石器文化，语系就没有时空来源。所以语系必定有着承载它的人类群体以及造就它的新石器文化区系。遗传学的人群区分和年代计算、考古学的区系文化比较，在语言学的语系演化的研究中应该起到重要的作用。<br>从遗传学的分析结果，特别是Y染色体精细分型数据来看，东亚的各个语言类群的人群之间的分化年代已经比较清楚了。汉语族和藏缅语族的分化年代大约是5000～6000年，也就是在8000年之内，所以支持汉藏语系的概念。汉语族与乌拉尔语系分化略超过8000年，毫无疑问是不同的语系，即便还有些许同源词。汉藏与苗瑶之间的分化超过1.2万年，就支持把汉藏语系与苗瑶语系分开。而侗傣与前二者的分化年代超过了1.5万年，更加支持其独立的语系地位。后期的语言接触造成的语言相似性应该是被排除在语系划分所考虑的因素之外的。所以历史时期汉藏南迁造成的苗瑶、侗傣与汉藏之间语言的诸多相似性，不能用以支持三者的合并。侗傣与南岛的人群之间仅有大约6000年的分化，其语言间的同源词也比较清晰，按此标准可以作为一个语系，不过两者在类型学上差异过大，是分是合可以进一步讨论。<br>考古文化区系与语系起源的对应关系，也可通过遗传学的古DNA分析来确认。对良渚文化区系的人骨进行DNA检测，检出高频的Y染色体O1单倍群，与侗傣和南岛语的族群高度一致。从5900年前的崧泽文化开始，长江下游的人群开始受到中华文明的影响，这体现在八角星纹等符号的传播上，侗傣族群和南岛族群的祖先可能就此开始分化。南岛族群的祖先或许就是分布于闽粤台的大坌坑文化人群。长江中游的大溪文化区系中检测出了高频的Y染色体O3-M7单倍群，这又与现代的苗瑶族群高度一致。黄河中下游地区有两个文化区系：西边的仰韶和东边的大汶口-龙山。这两个区系的边界在不断地往西推，从最早的河南-山东边界，渐渐到达陕西-河南，最后到达甘肃青海-陕西。仰韶文化退到了藏缅族群的分布区，龙山文化彻底占据中原，融合部分仰韶文化的因素。这可能体现了汉语族和藏缅语族先民早期的冲突和互动。龙山文化是一种大融合的文化，可能是成熟的汉语族先民的文化。从4600年前开始，龙山文化人群在西进的过程中，人口上吸纳了仰韶的居民，文化上与之同化。而拒绝同化的部分仰韶先民只能向西退到甘青地区，成为藏缅语族的人群。造成的效应是，现代的藏缅语族人群和汉族，在语言的多样性上是西高东低，在遗传的多样性上是东高西低。这种相反的结构，只能用前述的过程解释。龙山文化的遗骸检测出高频的Y染色体O3-M122（F11）和O3-M134，与现代汉族的遗传结构吻合。<br>单一学科的研究，必然是片面的。语系起源的问题，也是语系使用者起源的问题，区域文化凝聚和发展的问题，需要语言学、遗传学、考古学共同发力，多角度解析，才能最终看清其全貌。<br><br>对于东亚现存的民族，可以很容易地研究他们的语言和遗传，分析他们的起源和归属。对于古代族群，也可以结合历史学、考古学，从语言学及遗传学的角度进行探索。有些古代族群的记录比较详细，所以容易研究，例如，吐蕃是藏族的祖先，东胡、鲜卑、契丹都属于阿尔泰语族群，与蒙古族关系密切。而对有些族群的研究，困难较大，例如，古代东胡的死敌、汉族曾经最大的威胁匈奴到底是一个什么民族，是谁的祖先，在学术界就争议不休。2019年，在蒙古召开了世界匈奴后裔大会，与会者表示，阿尔泰语系、乌拉尔语系甚至印欧语系的很多民族都有人自称是匈奴后裔。这到底有没有根据？<br><br>匈奴是一个曾在我国北方生活的游牧民族，其统一政权大约兴起于公元前3世纪（战国时期），衰落于公元1世纪（东汉初）。由于匈奴自战国时期至东汉初与中原时有往来，我国古代史料中保留了部分关于匈奴的史料[41]。荤粥、猃狁等名称很可能是匈奴一词的不同音译。根据史料记载，匈奴民族的起源、变迁、灭亡等过程从史前一直持续到南北朝。<br>匈奴的原始祖先很可能是来自漠北的某支北亚蒙古利亚人种的居民，迄今为止发现的是，石板墓文化与匈奴主体民族有着最为接近的血缘关系[41]。在蒙古草原，目前所知规模最大、时代最早的匈奴墓是在呼尼河谷发现的[41]。呼尼河（Khunui-göl）（或译为呼奴伊河），即为“匈奴河”，疑即《汉书·西域传》提到的“匈河水”。呼尼河畔是早期匈奴统治集团的中心所在，这里最有可能是匈奴人的原始故乡[42]。阴山南北农牧交错地带是“匈奴原始人群”向“匈奴民族”过渡的重要转折点[42]。匈奴王朝的发祥地在内蒙古河套及大青山一带，匈奴第一个单于头曼单于的驻牧中心及以他为首的匈奴部落联盟的政治统治中心在五原郡稒阳县（今内蒙古包头市东）[41]。至头曼单于之子冒顿单于时期，匈奴灭东胡，征西嗕，西击月氏，南并楼烦、白羊河南王，北服浑庾、丁令、坚昆、薪犁，西北平定楼兰、乌孙、呼揭，南西伯利亚至阿尔泰山的乌兰固木、塔加尔、巴泽雷克地区均在这一时期为匈奴征服，最后形成了冒顿单于时期的匈奴疆域[41,42]。这是匈奴最强盛的时期，其势力东至辽河，西及葱岭，北抵贝加尔湖，南达长城[42]。<br>之后，匈奴与中原时战时和。西汉武帝时期，匈奴因战败远撤王廷，迁至漠北[41]。时至东汉，匈奴因内乱分化为南北匈奴两部分，南匈奴内附于汉，北匈奴则自公元91年后，向西逃遁，开始西迁[43]。<br>北单于逃亡后，漠北出现了混乱局面[42]：北单于弟左谷蠡王於除鞬退至蒲类海（巴里坤湖），归附汉朝；北单于远走乌孙，后至康居（今中亚哈萨克斯坦东南部）；残留漠北的群体后来加入鲜卑，鲜卑中的宇文部，就由加入鲜卑的匈奴部落中的宇文部落演变而来；还有一部分始终留在漠北西北角，至公元4世纪末5世纪初，力量还相当强大，直至柔然兴起才被吞并。<br>北单于一支远走乌孙、康居，灭阿兰聊国（奄蔡），后不见于我国史料记载。目前发现的匈奴遗存从西汉时期开始，自东向西，沿巴里坤—吐鲁番—和静—哈萨克斯坦分布，越往西，年代越晚，因此可以将这条路线推测为匈奴人西迁的路线[41]。之后西方史料中出现一支匈人，在阿提拉时代（约公元5世纪中叶）于多瑙河东平原（今匈牙利境内）建立王廷，称为匈人王国，但尚未确定匈人与匈奴的关系。<br>在汉代内迁后的南匈奴，在魏晋南北朝时期建立了多个政权。公元304～329年，南匈奴与屠各胡在山西和陕西建立了汉（前赵）政权。公元401～460年，临松卢水胡在今甘肃河西走廊建立北凉政权。还有一支铁弗匈奴，是北匈奴残部与拓跋鲜卑的混合，匈奴父鲜卑母。五胡十六国时期，铁弗改称赫连氏，在统万城（今陕西榆林市西）建立了大夏政权（407～431年）。<br><br>从语言学方面分析，根据文献资料，《史记》《汉书》中有西汉时期大约190个可能的匈奴语词，《后汉书》中有57个，《晋书》中有31个。通过以上材料，各家学者都提出了各自不同的看法。早期人们认为，匈奴语同斯拉夫语或芬兰-乌戈尔语接近[44]。目前在西方占统治地位的观点是，匈奴语可能与阿尔泰语系突厥语有关。还有学者认为，匈奴语与其他阿尔泰语、伊朗语或者叶尼塞语有关[45,46]。也有学者从匈奴的考古学与人种学角度探索匈奴语言的属性，认为外贝加尔的匈奴主体在语言上更加接近蒙古语，而中亚的匈奴语虽以蒙古语为主，但是夹杂着突厥语的混合语言[47]。白鸟库吉从语源学和音韵学研究出发，考察了17个文献记录中的匈奴词汇，并与阿尔泰语系比较，发现其中存于蒙古语者二，突厥语者二，通古斯语者三，突厥语和蒙古语共通者一，蒙古语和通古斯语共通者四，蒙古语、突厥语和通古斯语共通者五[48]。方壮猷考释了21个匈奴名号，与今土耳其语近似的有11个，与今通古斯语近似的有12个，与今蒙古语相似的有20个[49]。<br>通过将匈奴语的音韵特点、语词与其他邻近语言进行比较，可以得出以下结论[50]：<br>
<br>
匈奴语中声母r和l以及复辅音声母出现的证据，证明匈奴语最不可能是阿尔泰语。

<br>
匈奴语与任何一种已知的突厥语或蒙古语都不像。虽然某些匈奴名词（以及表“天”“酸奶”“马乳酒”意义的词）能在后来的蒙古语、突厥语或这两种语言中找到痕迹，但这是因为，继匈奴人之后，蒙古人和突厥人主宰中亚草原东部，从而继承了匈奴文化和政治组织形式的一些要素，相应的名称也就继承下来了。

<br>
一些意思已知或可推测的词语，可以同叶尼塞语中在意义上接近甚至相同的词语密切对应，如“儿子”“乳”“石”，它们不大可能是叶尼塞语中的借词。

<br>
叶尼塞语系的人群可能是匈奴的后裔，是匈奴帝国解体后迁到西伯利亚的，匈奴人一部先迁入北阿富汗和西土耳其斯坦，后又进入叶尼塞河流域[51]。

<br>综上所述，匈奴语与叶尼塞语之间关系似乎更密切。<br><br>在遗传学上，种族之间有着明显的差异性遗传标记，而民族之间也可以用一些遗传标记进行辨认区别。目前用于民族和种族分析的最佳遗传标记是Y染色体分型。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211551455.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
表4.1　匈奴遗骸的古DNA分析
<br>从目前的4项研究中看，匈奴人的Y染色体出现了4个类群Q、C、N、R, Q型是自始至终出现最多的类型，其他类型只是偶发。在现代人群中，R广泛分布于中亚、东欧乃至南欧的各个民族中。N集中分布于乌拉尔语系人群中，在北亚和东亚各民族中也有零星发现。C集中分布在蒙古和通古斯民族中。Q是美洲印第安人的主流类型，在北亚也零星出现，但是邻近的叶尼塞语系凯特人的Q比例占到94%[52]。可以推断，北亚地区各语系的人群中，只有叶尼塞语系人群是以Q为主流的，与匈奴数据一致。<br><br>如前文所述，匈奴人与叶尼塞语系人群在语言学及遗传学上都有一定联系。考古发掘也支持类似观点，在南西伯利亚叶尼塞河流域的考古发掘中，发现了3处公元前2世纪至公元1世纪的遗存（表4.2）[53,54]：<br><br>
表4.2　公元前2世纪至公元1世纪的遗存
<br>可以推测，叶尼塞语系人群很有可能是匈奴人向北迁徙的后代。但具体来自哪一支匈奴人，还需要深入研究。两个存在Q型的匈奴墓葬在对应时间的文献中，都没有发现匈奴人向叶尼塞河流域迁徙的记录。有较大可能的是迁往巴里坤湖的一支，他们属于Q型，并且地理位置与叶尼塞河流域比较接近，但需要确定样本年代后才能有较为可靠的结论。遗留漠北的一支匈奴人，在地理位置上也与叶尼塞河流域较为接近，并且最后不知所终，他们也有可能是凯特人的起源，但目前没有遗传学上的证据。迁往乌孙一支的精壮部分都向康居迁徙，后继续向西灭阿兰聊，留在乌孙的皆是羸弱人员，要继续向叶尼塞河流域迁徙的可能性较小，并且也没有遗传学的证据。因此要获得确切的结论，还有待更多的遗传学证据及考古遗址的佐证。无论如何，把蒙古语或突厥语的族群当作匈奴的后裔，是完全没有根据的。<br>匈牙利的国名与匈奴接近，与历史上的匈人有关，但学术界从不认为匈牙利人与匈奴有关。现代匈牙利语属于乌拉尔语系，匈牙利人中也很难找到匈奴的Q型Y染色体。但是，最近的古匈牙利人DNA研究让问题峰回路转[55,56]：关键的古匈牙利人的Y染色体居然是匈奴典型的Q型的。相信随着新发现不断出现，终有一天能解开匈奴后裔之谜。<br><br>Y染色体在解析东亚现代人源流史中起到了重要作用。尽管许多问题仍有待探索，但史前迁徙过程的基本框架已经明晰了。占东亚男性90%以上的C、D、N和O这四个单倍群很可能起源于东南亚，随澳大利亚人、尼格利陀人和蒙古利亚人这三种不同体质特征的现代人经历了三次大的迁徙浪潮。欧亚中西部特征Y染色体单倍群E、G、H、I、J、L、Q、R和T在中国西北的分布模式反映出来自西方的近期基因交流和可能的北部路线的影响，这些单倍群自西向东递减的趋势也可以被清晰地观察到。<br>然而，现阶段东亚的Y染色体研究遇到了两个瓶颈。一是东亚特异单倍群O-M175的解析度太低。虽然，单倍群O人口众多，但O下的位点却比R和E都少。例如，002611、M134和M117这三个位点代表了东亚近2.6亿人，但没有更下游的位点可以用来更精细地解析这些群体的遗传结构。另一个瓶颈是支系和群体分化时间的估算。现在绝大部分的时间估算用的是Y染色体的STR位点，尽管这在理论上说得通，但对于以STR估算时间哪种方式最恰当还一直有争议。尤其值得提出的是，这里有两种经常用到的Y染色体STR突变率，即进化突变率[20,21]和家系突变率[57]，如何选用这两种突变率争议很大，因为两者估算出的时间甚至可相差3倍。而且STR位点的相似性及多变性也使得时间估算的准确度大打折扣。因此，上文提到一些时间点也仅仅是作为某些单倍群或人群分化的粗略参考。<br>随着DNA二代测序技术的不断发展，全测序大样本量和深度测序家系的Y染色体成为可能。例如，千人基因组计划在其低覆盖项目中，已经以1.83的平均深度测序了77个男性的Y染色体，以15.23的深度测序了两个连续三代的男性家系[58]。更进一步的深度测序将既可以细化Y染色体谱系树，又可以为进化研究提供较精确的生物钟校准。<br><br>
<br>
Cavalli-Sforza L L. The Chinese human genome diversity project[J]. Proceedings of the National Academy of Sciences of the United States of America,1998,95（20）:11501-11503.

<br>
Jobling M A, Tyler-Smith C. Father and sons: the Y chromosome and human evolution. Trends Genet,1995,11（11）:449-456.

<br>
Underhill P A, Shen P, Lin A A, et al. Y chromosome sequence variation and the history of human populations[J]. Nature Genetics,2000,26（3）:358-361.

<br>
Su B, Xiao J, Underhill P. Y-chromosome evidence for a northward migration of modern human into Eastern Asia during the last ice age. American Journal of Human Genetics,1999,65（6）:1718-1724.

<br>
Ke Y, Su B, Song X. rican origin of modern humans in East Asia: a tale of 12 000 Y chromosomes[J]. Science,2001,292（5519）:1151-1153.

<br>
Green R E, Krause J, Briggs A W, et al. A draft sequence of the Neandertal genome[J]. Science,2010,328（5979）:710-722.

<br>
Reich D, Green R E, Kircher M, et al. Genetic history of an archaic hominin group from Denisova Cave in Siberia[J]. Nature,2010,468（7327）:1053-1060.

<br>
Wang C C, Farina S E, Li H, et al. Neanderthal DNA and modern human origins [J]. Quaternary International,2013:126-129.

<br>
Shi Y F, Cui Z J, Li J J. Quaternary Glacier in Eastern China and the Climate Fluctuation[M]. Beijing: Science Press,1989.

<br>
Jobling M A, Hurles M, Tyler-Smith C. Human Evolutionary Genetics:Origins, Peoples and Disease[M]. New York: Garland Science,2004.

<br>
Clark P U, Dyke A S, Shakun J D, et al. The Last Glacial Maximum[J]. Science,2009,325（2941）:710-714.

<br>
Zhong H, Shi H, Qi X B, et al. Extended Y Chromosome investigation suggests postglacial migrations of modern humans into East Asia via the Northern Route[J]. Molecular Biology and Evolution,2011,28（1）:717-727.

<br>
Piazza A. Towards a genetic history of China[J]. Nature, 1998, 395（6703）:636-637,639.

<br>
Yan S, Wang C C, Li H, et al. An updated tree of Y-chromosome Haplogroup O and revised phylogenetic positions of mutations P164 and PK4[J]. European Journal of Human Genetics,19（9）:1013-1015.

<br>
Shi H, Dong Y, Wen B, et al. Y-Chromosome evidence of southern origin of the East Asian: specific haplogroup O3-M122[J]. American Journal of Human Genetics, 2005,77（3）:408-419.

<br>
Kayser M, Choi Y, Van Oven M, et al. The impact of the Austronesian expansion: evidence from mtDNA and Y-chromosome diversity in the Admiralty Islands of Melanesia[J]. Molecular Biology and Evolution,2008,25（7）:1362-1374.

<br>
Su B, Jin L, Underhill P A, et al. Polynesian origins: insights from the Y chromosome[J]. Proceedings of the National Academy of Sciences of the United States of America,2000,97（15）:8225-8228.

<br>
Ding Q L, Wang C C, Farina S E, et al. Mapping human genetic diversity on the Japanese Archipelago[J]. Advances in Anthropology,2011,01（2）:19-25.

<br>
Hammer M F, Karafet T M, Park H, et al. Dual origins of the Japanese: common ground for hunter-gatherer and farmer Y chromosomes[J]. Journal of Human Genetics, 2006,51（1）:47-58.

<br>
Zhivotovsky L A. Estimating divergence time with theuse of microsatellite genetic distances: impacts of populationgrowth and gene flow[J]. Molecular Biology and Evolution,2001,18（5）:700-709.

<br>
Zhivotovsky L A, Underhill P A, Cinnioglu C, et al. The effective mutationrate at˘Y chromosome short tandem repeats, with applicationto human population-divergence time[J]. American Journal of Human Genetics,2004,74:50-61.

<br>
Cai X, Qin Z, Wen B, et al. Human migration through bottlenecks from Southeast Asia into East Asia during Last Glacial Maximum revealed by Y Chromosomes[J]. PLOS ONE,2011,6（8）: e24282.

<br>
Wang C C, Yan S, Qin Z D, et al. Late Neolithic expansion of ancient Chinese revealed by Y chromosome haplogroup O3a1c-002611[J]. Journal of Systematics and Evolution,2013,51（3）:280-286.

<br>
Zhong H, Shi H, Qi X B, et al. Global distribution of Y-chromosome haplogroup C reveals the prehistoric migration routes of African exodus and early settlement in East Asia[J]. Journal of Human Genetics,2010,55（7）:428-435.

<br>
Kayser M, Brauer S, Cordaux R, et al. Melanesian and Asian origins of Polynesians: mtDNA and Y chromosome gradients across the Pacific[J]. Molecular Biology and Evolution,2006,23（11）:2234-2244.

<br>
Sengupta S, Zhivotovsky L A, King R, et al. Polarity and temporality of high-resolution Y-chromosome distributions in India identify both indigenous and exogenous expansions and reveal minor genetic influence of Central Asian pastoralists[J]. American Journal of Human Genetics,2006,78（2）:202-221.

<br>
Gayden T, Cadenas A M, Regueiro M, et al. The Himalayas as a directional barrier to gene flow[J]. American Journal of Human Genetics,2007,80（5）:884-894.

<br>
Karafet T M, Mendez F L, Meilerman M B, et al. New binary polymorphisms reshape and increase resolution of the human Y chromosomal haplogroup tree[J]. Genome Research,2008,18（5）:830-838.

<br>
Karafet T M, Xu L, Du R, et al. Paternal population history of East Asia: sources, patterns, and microevolutionary processes[J]. American Journal of Human Genetics, 2001,69（3）:615-628.

<br>
Thangaraj K, Singh L, Reddy A G, et al. Genetic affinities of the Andaman Islanders, a vanishing human population[J]. Current Biology,2003,13（2）:86-93.

<br>
Shi H, Zhong H, Peng Y, et al. Y chromosome evidence of earliest modern human settlement in East Asia and multiple origins of Tibetan and Japanese populations [J]. BMC Biology,2008,6（1）:45.

<br>
Wen B, Xie X, Gao S, et al. Analyses of genetic structure of Tibeto-Burman populations reveals sex-biased admixture in southern Tibeto-Burmans[J]. American Journal of Human Genetics,2004,74（5）:856-865.

<br>
Chandrasekar A, Saheb S Y, Gangopadyaya P, et al. YAP insertion signature in South Asia[J]. Annals of Human Biology,2007,34（5）:582-586.

<br>
Delfin F C, Salvador J M, Calacal G C, et al. The Y-chromosome landscape of the Philippines: extensive heterogeneity and varying genetic affinities of Negrito and non-Negrito groups[J]. European Journal of Human Genetics,2011,19（2）:224-230.

<br>
Scholes C, Siddle K, Ducourneau A, et al. Genetic diversity and evidence for population admixture in Batak Negritos from Palawan[J]. American Journal of Physical Anthropology,2011,146（1）:62-72.

<br>
Rootsi S, Zhivotovsky L A, Baldovic M, et al. A counter-clockwise northern route of the Y-chromosome haplogroup N from Southeast Asia towards Europe[J]. European Journal of Human Genetics,2007,15（2）:204-211.

<br>
Derenko M V, Malyarchuk B A, Denisova G A, et al. Y-chromosome haplogroup N dispersals from south Siberia to Europe[J]. Journal of Human Genetics,2007,52（9）:763-770.

<br>
Mirabal S, Regueiro M, Cadenas A M, et al. Y-Chromosome distribution within the geo-linguistic landscape of northwestern Russia[J]. European Journal of Human Genetics,2009,17（10）:1260-1273.

<br>
Xue Y, Zerjal T, Bao W, et al. Male Demography in East Asia: a North-South contrast in human population expansion times[J]. Genetics,2006,172（4）:2431-2439.

<br>
Wen B, Li H, Lu D, et al. Genetic evidence supports demic diffusion of Han culture[J]. Nature,2004,431（7006）:302-305.

<br>
马利清．原匈奴、匈奴历史与文化的考古学探索[M]．呼和浩特：内蒙古大学出版社，2005.

<br>
林幹．匈奴史[M]．呼和浩特：内蒙古人民出版社，2007.

<br>
泽田动．匈奴：古代游牧国家的兴亡[M]．王庆宪，丛晓明，译．呼和浩特：内蒙古人民出版社，2010:174-175,190.

<br>
陈立柱．三十年间国内匈奴族源研究评议[J]．学术界，2001, （9）:53-71.

<br>
Bailey H W（1985）Indo-Scythian Studies: Being Khotanese Texts, Ⅶ[M]. Cambridge: Cambridge University Press,1985:25-41.

<br>
Vovin A. Did the Xiong-nu speak a Yeniseian language[J]? Central Asiatic Journal,2000,44（1）:87-104.

<br>
李法军．匈奴的语言属性：来自考古学与人种学的线索[J]．青海民族研究，2007, （4）:20-25.

<br>
白鸟库吉．匈奴民族考[M]．何健民，译//林幹．匈奴史论文选．北京：中华书局，1983:184-216.

<br>
方壮猷．匈奴语言考[J]．国学季刊，1930,2（4）:693-740.

<br>
蒲立本．上古汉语的辅音系统·附录·匈奴语[M]．北京：中华书局，1999:163-167.

<br>
王士元．汉语的祖先[M]．李葆嘉，主译．北京：中华书局，2005:508-509.

<br>
Zhao Y B, Li H J, Cai D W, et al. Ancient DNA from nomads in 2500-year-old archeological sites of Pengyang, China[J]. Journal of Human Genetics, 2010, 55（4）:215-218.

<br>
艾尔迪 M．摩尼教、景教和禽鸟服人与叶尼塞河谷岩画中匈人铜釜的关系[J]．欧亚研究，1996, （68）:45-95.

<br>
艾尔迪M．从北方蛮人（公元前8世纪）和匈奴墓葬看古代匈牙利人的丧葬习俗[M]．贾衣肯，译．西北民族研究，2002,34:29-47.

<br>
Neparáczki E, Maróti Z, Kalmár T, et al. Mitogenomic data indicate admixture components of Central-Inner Asian and Srubnaya origin in the conquering Hungarians [J]. PLoS ONE,2018,13（10）: e0205920.

<br>
Neparáczki E, Maróti Z, Kalmár T, et al. Y-chromosome haplogroups from Hun, Avar and conquering Hungarian period nomadic people of the Carpathian Basin[J]. Scientific Reports,2019,9:16569.

<br>
Gusmão L, Sánchez-Diz P, Calafell F, et al. Mutation rates at Y chromosome specific microsatellites[J]. Human Mutation,2005,26（6）:520-528.

<br>
Altshuler D, Durbin R, Abecasis G R, et al. A map of human genome variation from population-scale sequencing[J]. Nature,2010,467（7319）:1061-1073.

]]></description><link>书籍\人类起源与迁徙之谜（第四章）.html</link><guid isPermaLink="false">书籍/人类起源与迁徙之谜（第四章）.md</guid><pubDate>Mon, 21 Oct 2024 08:08:21 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211547630.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211547630.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[三皇五帝：用基因拨开早期历史的迷雾]]></title><description><![CDATA[ 
 <br><br>DNA 可以研究人类的起源，可以分析族群的演化，那都是数千年甚至数百万年尺度的历史。实际上有文字记载之前的历史叫作自然史、史前史，不是严格意义上的历史。但是，如将 DNA 工具的精度提升到足够高的程度，比如提升到百年级的精度，就可以分析更近的历史时期的群体关系或家族历史。把两个时空相距遥远的群体或者个体，用 DNA 的纽带连接在一起，就可以揭秘一段尘封的历史。所以今天研究历史，必须用学科融合的手段。从遗传学、历史学、社会文化的角度去发现证据。历史人类学就是通过基因考古获得实证，以验证文本的方式讲述历史故事。研究历史，有的时候关键是家族史、血缘史。我们能通过染色体谱系树揭示人类起源过程，能从基因看人类走出非洲的足迹，并进而探究农业起源与民族的聚合形成过程，以及中华民族融合统一的历史。<br><br>我们一直说“实践是检验真理的唯一标准”，但是在某些学术领域，有人依然不从实践出发，用理论否定实践，用权威观点否定证据。判断观点正确与否，看的是证据，在客观证据面前，主观的观点又有什么用？只要与证据不符合的，就不应相信，不管是谁讲的。大师讲的就是真理吗？不一定。“吾爱吾师，吾更爱真理。”在证据和权威之间选择什么，这在文科和理科之间往往存在思维差异。这些差异，实际上是由学科本身的方法论造成的，而我们现在提倡的学科交叉融合的理念，就是要通过思维互补解决这种方法论上存在的根本性缺陷。复旦大学提倡通识教育，文科的学生要多学点理科的东西，理科的学生要多学点文科的东西。这样，文理科学生都会视野更加宽广，积淀更加深厚。这就是复旦大学通识教育的目标：有一颗理科的机芯，但是有文科的情怀；达到博学，方能笃志，能切问，方能近思。<br>我们今天研究历史，不是单纯从文本资料中去复原历史，而是用学科交叉的手段，从各种各样的材料中去寻找证据。这些证据至少来自三个领域。复旦大学文科资深教授姚大力先生提出，研究历史学要有三个窗口（即三个领域）：遗传学的从基因角度研究的窗口；考古学的从化石、文物角度研究的窗口；从社会学、语言文化的角度研究的窗口。历史是复杂的，有多个维度的问题需要解决。<br>要解决两个个体之间有什么关系、群体之间有什么关系，可以通过基因来分析。父子之间是不是亲子关系，做一下基因检测就知道了。如果去查他们的族谱，或者是用社会学的方法，在村子里发问卷，问多少人同意他们是父子，那就是笑话了。也不可能根据孩子长得像不像父亲来判断是不是亲生父子，这种做法也是笑话。亲子鉴定，肯定是要以基因检测的结果作为证据。<br>研究历史要落到问题的实处，解决时空的问题。遗传关系的问题是时间问题，两个人之间距离多少年、多少代。此外还有空间问题，即这个家族从哪里迁到哪里，在哪里落过脚，迁徙过程中走过哪些路径。这是要通过考古学解决的。但是知道了这些，还没解决所有问题，我们还要问一问为什么。这些家庭、这些族群为什么迁徙？这个为什么的问题有的时候就很复杂了，只从基因角度研究不能解决。基因研究只是告诉你，不同群体间有没有关系。全世界的人，当然也包括中国人，都是从非洲走出来的，这从基因就能看清楚了。但要知道从非洲什么地方走出来的，就只能看化石了。考古结果发现现代人走出非洲后，在西班牙没有留下化石痕迹，只有以色列有。所以现代人最可能是通过埃及—以色列一线走出来的。<br>早期过狩猎-采集生活的现代人为什么走出非洲？非洲到处有兽群，角马、斑马等大兽群在东非草原上奔跑着，而在亚洲打猎，要跑很远才能碰到野兽。在非洲狩猎一般不会被饿死！那人为什么要往非洲之外走呢？这里就有很多社会心理学的、宗教的，或者是早期自然环境变迁的影响，要将这些学科综合在一起研究才能解决人类历史的问题。特别是人类社会有了文化以后，宗教变得尤为重要。我们发现，有些迁徙事件难以理解，比如乌拉尔语系的人群的迁徙。现在芬兰人、爱沙尼亚人都是乌拉尔人，还有很多分布于欧亚大陆北冰洋沿岸以及乌拉尔山区的民族，从瑞典的拉普人到乌拉尔东部的萨摩耶人，还有西伯利亚东北角的尤卡吉尔人，都是乌拉尔人。他们的基因和汉族的基因非常接近，Y 染色体类型是汉族部分类型中分化出来的一个小分支。从历史语言学角度研究发现，乌拉尔语系，例如芬兰语，很多字词的韵母和汉语的对应关系非常明确[1,2]。汉语里读“分”的，在芬兰语里叫“pala”，汉语读“鲲”的，他们读“kala”，存在成系列的对应关系。他们为什么从中国跑到环境严酷的北冰洋沿岸？我们研究发现，极可能是宗教原因。北欧神话体系非常完备，他们的主神是住在北极的天神奥丁。天神必须住在众星环绕的地方，那就是北极，所以他们一路向北到了那里。信仰天神是大多数民族早期的自然信仰，但是北欧的天神体系与中国上古体系相似度特别高。不过经过几千年的历史变迁，天神信仰在中国已经几乎完全改变了，而乌拉尔人群的这一信仰变化没这么大，所以类似中国早期的这些神灵节日他们还在过。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211555873.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.1　驼鹿在中国上古神话体系和中亚草原信仰体系中都具有重要地位。
<br><br>历史问题非常有趣，但是历史越久越模糊，研究的难度越大。为了提高研究的精度，必须不断发展研究方法。而我们的研究方法叫历史人类学。2016年12月，《科学》专门做了一个关于我的长篇报道，报道了我的研究成果、研究经历，还把我的大照片贴在《科学》上面。因为编辑认为我们的研究开创了新的领域，解决了以前不能解决的问题。这个新的学科领域就叫作历史人类学。历史人类学有新的研究范式，跟传统的历史学、人类学都不一样。<br>传统的历史学是从文本出发的。历史学系的老师对文本文献很熟，往往能发现不同文献里讲述同一个历史事件是不一样的。到底哪个文本的讲法是对的，如何分辨？历史学的办法，就是通过分析事件的历史逻辑来判断哪个可信度大。但是这种逻辑的可靠性并非绝对，有时是有出错风险的，因为有些人做事情就是不合逻辑的，而且历史有太多巧合，所以传统历史学的研究很多是依赖于权威学者主观判断的水准。一个年轻人刚入行，看了几本零星的书，他依据的材料少，判断力就差些。一些老学者看过许多别人没有看过的资料，他依据的材料丰富，总结的规律可靠，判断能力就强，或者说他的思维水平就高。这就是为什么文科生要信老师，要信任学术界的权威。但是即使是权威，他对事件判断的准确度又能够达到百分之多少呢？传统的历史学研究精度很难量化。真的量化的话，我觉得不一定达到80%。但是对于理科研究来说，95%的准确度也可能不够，因为还有5%可能是不对的！对数据分析而言，显著性要求是95%以上，但这还不一定满足所有研究的需求，有些研究要求99%，甚至99.999%的可靠性，因为其中的出错风险是不能容忍的，例如法医学的一些研究就属于这类情况。所以自然科学的研究在精度上占有优势。<br>但是传统的自然科学，例如人类学研究，在范式上也有问题。特别是这些研究在语境上太生硬了，描述的语言经常是一段密码，大众听不懂。比如说，你的基因型是 O3α型，这种型上面的 M117位点有突变，然后怎么怎么变异，说明你来自数千年前的某个考古文化。作为一名没有专业背景的读者，听得懂吗？所以说这种描述脱离了常用语境，虽然试图讲历史，但是没有真正讲明白，没有解决历史学的问题。传统的人类学创造了一堆生硬的新名词，比如说把人分成东亚人种、南亚人种、北亚人种，然后这个北亚人种是数万年前开始混合形成的，多少万年前后有竞争。这些东西一般人听着很陌生，北亚人种是什么，怎么从民族分辨谁是北亚人种？不知道！这些都不是历史传统的语境。<br>考古学也是一样。一般把考古学划为文科，但实际上考古学更像是工科。工科最典型的建筑学是造房子，造房子往上造，而考古学是往下挖，还要求挖得规规矩矩，技术要求很严格，每个地层都要分得很清楚，还要画图纸，这完全是工科的事情。至于挖出来东西是什么，解决什么历史问题，这不是工科要解决的问题。所以有些考古学家非常反对用考古材料研究历史，认为做好工科事情就行了，文科的问题交给文科去解决。其实考古学的传统，特别是史前考古，也遇到了跟人类学一样的问题，就是没有在传统语境中研究问题。一个考古文化挖出来以后，很难分析这个考古文化跟其他考古文化有什么关系，也很少考虑这是不是一个大类型中的小类型。研究者往往就根据发现地的村子的名字，或者小镇的地名，给这个文化命名。比如仰韶文化，是在河南仰韶村发现的。这个文化经历了几千年，影响了中原很大范围，还涉及周边地区，甚至到了甘肃最西北的地方。那些地方都曾经发现有与仰韶文化类似的文化类型。那么长的一个时期，那么大的一个地理范围，仰韶文化人群应该是一个很大的族群，他们生活的时期甚至可能是一个朝代。他们是什么族群，他们到底是哪个朝代的，他们的首领是谁，这些问题考古学都没有回答，而是简单地用一个小村子的名字命名，来称其为仰韶文化，回避了所有历史问题。这个新命名的名词脱离了传统的轨迹，没有解决问题反而制造了问题。这种回避大传统的考古学的命名表达，还使得考古学的博物馆展览的展品介绍实质上是与观众认知隔离的。观众在博物馆看到了“马家窑文化彩陶双耳罐”，双耳罐，没有质的信息传递，谁看不出它是两个耳朵啊？彩陶也很清楚，它是用彩色颜料画的，黑一条，红一条，都知道那是彩陶。这个标签提供的唯一信息就是马家窑文化，而那是不够的，因为非考古专业人员不懂马家窑文化是什么。但是，假如我告诉观众，马家窑文化是轩辕黄帝把炎帝的领袖地位取代了以后，炎帝部族逃到西边甘肃形成的炎帝族群的文化，那观众就很容易明白了。这样的描述才是历史语境下的历史表述，而不是在考古专业内的磋磨，当然这需要更多交叉学科的掺入。所以说，我们的考古学的方法，一开始就存在某些先天缺陷。分区、断代、命名的非科学化，把历史信息碎片化了，而碎片化的信息往往是无用的，反而给科学规律发现带来障碍。<br>佛教有“所知障”的说法，但并不是说知识越多越不好。知识多，还需要串起来，博学之后还要切问还要近思，还要把知识全部穿成一条线。我们追求的是道，是规律，不是求零碎的知识。所以《道德经》提倡“为学日益，为道日损”。学问要不断积累增益，才可能归纳出“道”来。而“道”就是“要放弃多余”，要简化，再简化。“大道至简”，科学的规律一定是最精简的，人类进化的规律也一定是符合生物进化的大规律的，而不是特立独行的特殊规律。<br>历史人类学与人类学不同，前者是从文本出发的，以传统的历史记忆为研究材料，讲述的可能还是大家听过的历史故事。通过基因、考古等科学获得的实证，可以把历史上发生过的事研究得相对客观、正确。<br>历史人类学研究历史，最关键的就是实证，而取得实证的关键就是基因。我们要把很多考古问题、历史问题转化为遗传学问题，用基因去解决。曾经有人认为，用基因研究出来的东西也不可靠，例如一些基因分析研究发现这个群体和那个群体近，另一些研究却发现这个群体和另外一个群体近，遗传学研究自己都矛盾。其实这是因为基因太复杂了，并不是随便研究什么基因都能用于解决历史学问题的。<br>人类基因组有23对染色体，还有一个线粒体，总共有约30亿碱基对。这个庞大的基因组有80%跟老鼠是一样的，有98%跟猩猩是一样的。人类基因组里面并不是所有的 DNA 片段都有明确功能，不同片段的重要程度因为功能差异而不同。有的 DNA 片段完全不能变，比如，决定你长两个眼睛一个鼻子的片段跟老鼠的是一样的，如果拿这种片段去研究人与人之间的关系，那就一点解析度都没有。有的片段可以变化，在人与人之间有差异，但是它决定了你是不是抗冻，这类 DNA 片段或基因更多是因适应不同的地理环境而变化，拿来研究人与人之间的关系、研究历史也没有用。在广东人群中，这个基因都是一样的不抗冻类型，和属于哪个民族没有关系；在北方长城以北，人群中这个基因又是完全一样的抗冻类型。这个基因变体的分布是对应于纬度、温度的，完全是适应气候的，能解析的是地理信息，不是历史信息。这类基因非常多，如果用这类基因作一个分析，就会发现，中国人是有南北差异的，且以长江为分界线。人类遗传学的一些早期论文确实结论都是这样。中国人分南北人群，长江以南是南方人群，以北是北方人群，这是一个重要的发现，但是对于历史研究意义并不大。那么研究历史，要选择什么类型的基因片段呢？答案就是，这些基因片段不能受环境的影响，而是直接跟族群迁徙的历史完全对应起来，这样才能解决历史问题。实际上，基因组中大部分没有具体功能的 DNA 片段正符合我们历史研究的要求，因为它们没有具体功能，所以不受环境选择的影响，其中的变异完全被保留，忠实记录了历史分化的程度。另外，历史研究还需要这些片段以很长的区段整段地传下来，稳定地积累一系列的突变，这样才能形成足够长的“段落”，足以通过比较“文本”区分各种个体、家族和群体差异。<br>人类的历史，有的时候关键是家族史。历史的走向往往取决于一个关键人物，或者一个核心家族的兴衰。往往皇家的兴衰就是一个国家的兴衰。拿什么基因片段来研究家族的历史呢？这可就要根据基因片段的遗传模式来决定了。第二章提到，基因组里不同染色体的传递模式是不一样的。大部分染色体是常染色体，常染色体是双系遗传的，每个人的每种常染色体都有一对，一条来自父亲，一条来自母亲。常染色体传给下一代的时候会进行重组。传到最后，常染色体就成为一个极其复杂的历史组合，汇聚了所有祖先的信息。如果研究常染色体，每往上追溯一代，常染色体的贡献祖先就增加一倍。父母一辈两个人，祖父母一辈四个人，曾祖父母一辈就是八个人……如此无限地散发开来。到了一定的历史时期，所有的人都是你的祖先，因为你的常染色体是由很多他们传下来的片段拼起来的。当然这种散发是有一定限度的，并不是线性的单调发散。但是如果所有古代人都是我们的祖先，哪怕是大量古代人是我们的祖先，我们追寻祖先还有什么意义呢？一个不需要回答的问题是没有意义的。研究历史，追溯祖先，不找到我们真正的关键祖先，不回答我们关心的真实问题，就是没有意义的研究，只是发现了一个好玩的现象。我们要找的祖先，并非遗传给我们基因片段的所有古人，而是特定的传给我们家族、语言、文化、信仰的祖先。<br>中国的家族传承是按父系关系组建的，于是就有一条染色体的遗传与家族的传承基本保持一致，那就是父系遗传的 Y 染色体，只有男性才有，所以它肯定来自父亲，而父亲的 Y 染色体肯定来自爷爷。家族不变，Y 染色体就绝对丢不了。而且，前文说过，Y 染色体主干区段基本不与 X 染色体发生重组，其上发生的突变都会流传下来（图2.5）。<br>家族中的 Y 染色体失传的可能性也存在，例如有些人是过继的，或者是被收养的，或者是其他各种特殊情况造成失传的，但是在中国社会中这样的情况比例比较低，所以 Y 染色体是用来研究家族史乃至研究各种族群历史的最重要的材料。有人问，女性的 DNA 就不能用来研究历史吗，母系的历史不要研究吗？母系遗传当然也可以研究。母系遗传的 DNA 片段就是线粒体 DNA，我们可以通过线粒体追溯母系历史，研究母亲的母亲的母亲从哪里来。但是研究历史用不着这些信息。你外婆的外婆姓什么，你知道吗？你感兴趣吗？大部分人都不会问这个问题，因为人类历史基本上是父系社会的传承，出于社会学原因，我们没有必要过多关注母系遗传历史。当然，母系遗传的信息也是非常有用的，在世界范围内的各大区域差异上是有进化学意义的，只是在地区内的民族群体之间没有显著差异。研究发现，民族之间，女性基因交流很普遍，男性基因交流就很少。因为大部分的民族都是父系社会的，母系社会只是偶发现象。人类历史上也是这样，母系社会阶段只是一个假说，到目前为止没有证据可以证明人类社会的发展存在一个普遍性的母系社会阶段。在考古上也从来没有发现过普遍的母系社会。所以通过父系遗传的 Y 染色体，我们就可以追溯到我们家族的祖先，追溯到家族中跟我们同姓的上几代祖先。同姓的两个家族到底是不是来自同一个祖先，不同姓的家族是不是真的没有关系，这些人们常常会遇到的问题，现在可以用 Y 染色体来判断了。<br><br>历史人类学解决的第一个历史问题，就是曹操的身世。我们用曹操家族的基因研究来做个例子，说明基于遗传学的历史研究是怎样开展的。曹操是1800年前三国时候的人物，他的身世似乎很难追踪。曹操的爷爷是个太监，那曹操父亲是从哪里来的？当然，曹操的父亲肯定是养子。曹操的政敌对这个养子的故事就做过各式各样的发挥。袁绍跟曹操小时候要好得很，但是真打起来的时候，却对曹操说，你这家伙，你爸是乞丐携养的，我是四世三公，你有什么资格跟我争夺天下？东吴那边更有高招，专门搜集曹操的花边新闻，编成合集，取名《曹瞒传》，就说，曹操啊，你爸是夏侯家过继来的，所以你们家和这个夏侯家是兄弟关系，现在你的女儿嫁给夏侯家的儿子，夏侯家的女儿嫁给你家儿子……这个故事很火爆，所以流传很广，还被编进了《三国演义》。大众的心理是很追奇的，民众喜欢猎奇，所以曹操怎么辩驳都没用，老百姓不爱听，所以我们从来没看到过史书里有曹操的辟谣。<br>但是《三国志》就没记载曹操父亲是从夏侯家过继来的。曹操家大业大，曹操的爷爷曹腾做了中常侍，就是一人之下万人之上的官员，像他这样的家业怎么可能传给一个外人呢，这不合理。而且对于贵族阶级来说，从外姓过继，这是丢人的事情。其实曹操的爷爷有兄弟四个，他一个人做太监，三个兄弟都做了大官，怎么可能没孩子，用得着从外姓过继吗？但这只是历史逻辑的推理，事实究竟如何？为了弄清真相，我们可以把这个亲子鉴定的历史问题转换成遗传学问题。把曹操的爷爷的基因，与曹操父亲的基因作对比，曹操父亲的 Y 染色体基因应该与曹操后代的基因是一样的。<br>所以首先要找曹操的后代，然后跟曹操的爷爷作一个基因对比，如果两边对上，就可以证明曹操的父亲本来就是老曹家的。2010年，我们把全国的曹姓做了筛选，找到70多个曹姓家族进行 Y 染色体的检测，结果发现全国的曹姓各种各样的 Y 染色体都有，没有一致性。这是一个好现象，因为曹姓本来就是多起源的，应该不一样才对。只有普通曹姓 Y 染色体不一样而曹操的后代 Y 染色体都一样，才能把曹操后代从众多曹姓家族中挑出来。从全国的70多个曹姓中，我们找到有家谱有文献记载的9家曹操的后代，分析结果表明，这9家人的 Y 染色体有8家是一样的，都是 O2-F1462型。这个类型是很罕见的，在人群中1%都不到，如此一致基本不可能是巧合。这说明他们8家真的源于一家[3]。<br>我们又去曹操的爷爷这一辈的墓葬里找来骨头，来鉴定其中的 Y 染色体。曹操的父亲和祖父等长辈都葬在他们的老家谯郡，也就是今天的安徽亳州。听说我们来研究曹操家族，亳州考古所的研究人员都很高兴，但说到骨头，却都犯难了。原来曹操家族墓葬是20世纪70年代挖掘的，那时对保护古迹不够重视，而且挖出来的时候人骨基本都烂了。考古队员说：“当时我们挖墓的时候，看到墓葬形制很漂亮，就保护了起来。很多出土文物搬到博物馆保存着。墓葬的封土就倾倒在边上了，墓葬中残余的骨头也跟封土扔在一起，骨头渣子，都没保存。”但是有位谢书璧老先生，当时是带队挖掘的，回想起当年挖曹操爷爷的弟弟的墓时，看到墓主的两颗牙，亮晶晶的特别好，就留了下来，心想以后科学发达了说不定还能分析出些特别的信息。那两颗牙就用信封装着，放在库房的考古材料里。他翻了半天，发现了30多年前放的这个信封。倒出来一看，两颗牙还是亮晶晶的。我们拿到实验室，在牙齿上钻一个小洞，把骨粉掏出来，做骨粉里的基因测试，发现其中的 Y 染色体果然是 O2-F1462这个类型，跟曹操的后代一模一样[4,5]。<br>我们也做了夏侯家的 Y 染色体，完全不是这个类型，而是 O1类型，不是 O2类型，两者相差很远。这说明，曹操后代跟曹操爷爷辈是一样的，曹操的父亲不是外面捡来的，还是老曹家人。就这样，历史问题的悬案通过自然科学遗传学的手段解决了，这是历史人类学的第一个案例。<br>所以，Y 染色体谱系可以研究历史问题。历史上的两个人，只要找到他们在 Y 染色体谱系树上的位置，就能分析出很多信息。全世界的任何男人都可以在 Y 染色体谱系树上找到位置，所以全世界家族的演化历史都可以分析清楚。很多家族的历史组合成了民族的历史，很多民族的历史组合成了大洲的历史，各个大洲的历史又整合成了全人类的历史。<br><br>中华民族这个大群体，由56个民族组成。每个民族又由许许多多家族构成，在遗传谱系分析中会发现有些家族是跨越民族的。基因的分析会让中华民族多元一体的构架更加清晰。我们要研究中华民族的历史，必须从东亚的各个民族类群的源流分析。<br>民族这个概念比较复杂，是从政治角度，根据国家政策落实的需要，基于共同语言文化特征的心理认同，以及各种复杂社会经济关系来划分的。遗传学研究民族群体的时候，传统上简化为根据语言学分类来区分族群，实际上研究的是一个个语言类群的使用群体，而不是社会意义上的民族。语言学和生物学一样是有体系的，生物学上分界、门、纲、目、科、属、种，语言学上有语系、语族、语支、语种。语系，就相当于生物分类中的“目”的概念，生物学中同一个“目”内的生物有明显的亲缘关系，比如灵长目中的各物种，所以明显有亲缘关系的语言是同一个语系的，彼此间有大量同源词。比如汉藏语系，汉语和藏语的同源词特别多，这两个语言是同源，关系很明确。语族则更进一步，相当于“科”的概念，同一语族内的语种明显是很相似的。比如日耳曼语族，里面的英语、德语、荷兰语、瑞典语，相互之间非常接近。语支相当于“属”的概念，亲缘关系更近。比如羌语支的，内部的语言都是基本可以相互通话的。再往下，语种就是相当于“物种”了。<br>早期的蒙古利亚人种到达东亚以后渐渐分化，形成了9个语系，从南到北分别是：南岛语系、南亚语系、侗傣语系、苗瑶语系、汉藏语系、满蒙语系（阿尔泰语系）、匈羯语系（叶尼塞语系）、乌拉尔语系、古亚语系。匈羯与满蒙是完全不同的族群，很多北方民族说自己祖先是匈奴，可能并不是事实。中国境内已经没有讲匈羯语系语言的民族了。从基因上，可能就已经决定了一个民族讲话有没有声调。根据语言学分析，匈奴讲的语言是有声调的语言。我们分析了匈奴墓葬中的遗骨的基因，分析了和语言相关的基因，都是有声调的突变类型。而现代的满蒙族群的同样基因都没有声调突变，所以满蒙语系都是没有声调的。匈奴人和蒙古人，在其他基因上差距更大，可能个别血统有交流，但是主体上没有传承性。千万不要以为住在一个地方的，就是必然有传承关系的，族群的迁徙是非常频繁的。在蒙古草原上，民族换了很多批，蒙古人是很晚的时候才从黑龙江流域扩张到草原上来，成为主体的。<br>更有意思的是，东亚的语系从词汇相似性上分析，是两两成对的。侗傣语系和南岛语系的词汇很接近，南亚语系和苗瑶语系很接近，汉藏语系和乌拉尔语系很接近，匈羯语系和古亚语系很接近。每一对语系都是其中一个有声调，另一个没声调。这个现象非常奇怪。声调分为调值声调和调形声调两种。调值声调就是根据声音的高低来区分的声调，调形声调就是根据声音的升降形态来区分的声调。全世界范围内，只有东亚地区才有调形声调，带调形声调的语系有汉藏、苗瑶、侗傣、匈羯四个。我们发现，东亚的声调很晚才出现，大概从西周开始才渐渐开始产生声调，而产生声调可能都是受中华文明圈的影响，这种影响源于基因交流[6]。四个有声调的语系的使用者都是在中华文明圈内长期紧密接触、频繁交流的族群。匈奴人就在汉族的北边，交流很多。苗瑶就在汉族的南边，长期住在长江中游。侗傣起源于江浙一带，也长期与汉族互动。而对应的另外四个语系的族群跑到中华文明圈外面去了，所以较少受到中华文明的早期影响，没有搭上声调发生这班车。<br>语系对基因结构的影响很大。上述提到的四对语系的现象，在 Y 染色体分布上也同样能看到。每个族群中有不同的家族组成结构，有不同的 Y 染色体类型，并以独特的比例组合起来。一个族群中有各种男人，当然会有各种 Y 染色体。而不同族群的 Y 染色体的组成配比是不一样的。所以比较不同族群中各种 Y 染色体所占比例，可以计算出族群之间的遗传距离。同一个语族的人群，Y 染色体类型基本一致。比如图5.2中，汉族和藏缅族群的 Y 染色体比例特别接近，每种类型的比例基本一样。所以汉族和藏缅族群是最接近的。所以汉藏语系的分类在基因上也是支持的。满族和蒙古族也是很接近的，几乎一样，这和语言也是对应的。苗瑶和汉藏稍微远一点，南亚和苗瑶比较接近。侗傣和南岛距离汉藏更远。所以族群之间的关系通过这个比对，通过计算分析可以画出一棵树来（图5.3）。中华民族所有族群都是从一个根上出来的，都是同根的，大约都是4万年前从云南进入中国，渐渐分散开来。3万年前分散成苗瑶、侗傣的祖先人群，侗傣又分出南岛，苗瑶祖先2万年前又分出汉藏的祖先人群，汉藏后来又分成汉和藏缅两部分。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211556197.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.2　东亚各族群间 Y 染色体差异。
<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211556031.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.3　中华民族同根。箭头中文字表示各族群祖先进入这片土地的距今大致年数。
<br><br>民族是这样通过分化形成的吗？不是的！分化只是早期狩猎采集人群迁徙扩散的过程，那个时候哪有民族？民族是有了文化以后聚集起来而形成的，因为需要大量的聚集人口来承载民族文化。<br>早期人群散布到中国各地，交流很少，基因差异越来越大。各地的人群分化了几万年，散布在丛林中，形成不了民族，孕育不出文明。文明是后来才产生的，其产生的过程有内在自然规律，需要适宜的环境、足够的经济基础。文明什么时候产生的？很晚。约1.2万年前，冰川期结束以后，一切才有可能。冰川期是不可能有文明的，冰天雪地，食物匮乏，能活下来就不错了，没法养育足够的人口，所以史前文明是不存在的。从1.8万年前开始，冰川渐渐消退，到1.2万年前，气温上升到接近现在的水平，全世界进入温暖美好环境，春天终于来了。对这个春天，从7.4万年前的多峇巨灾开始，生命已经等了几万年。那时候起，动植物繁茂起来，花儿盛开，各种兽群开始繁衍、壮大，我们人类有足够的东西吃了。有了充足的食物，人口就开始增长。所以1.1万年前，全球温带的女性人口开始膨胀。为什么增长的只是女性呢？男性到哪里去了？女性在家里操持家务、采摘果实，相对安全，就活下来了。男性要去打猎，经常会遇到凶猛的野兽或者各种危险的情况，很容易受伤或者死亡，所以男性成员损失很大，人口增长不了。20世纪90年代，我经常去各种偏远地区调查少数民族，也调查过一些狩猎的民族，比如独龙族。直到十几年前，他们还经常狩猎，因此男性成员经常会有损失，村子里男女人口比例特别不均衡。<br>智商在人群中也是正态分布的，大部分人是平凡的，少数人智商比较高，少数人智商偏低。当人口少的时候，智商特别高或者特别低的人就很难出现了，所以群体发明创造会少，社会发展会慢。人口多了，聪明人才会多，创造发明也才多。农业就是各地的聪明人发明的。一万年前，世界若干地区，特别是人口最多的东亚和西亚，陆陆续续出现了农业。农业生产了更多粮食，能养活更多的人，人更多了就有更多的发明，新石器就出现了。吃饱了才有时间磨制新石器，才能把生产工具磨得精细一点。工具更好，农业可以更快发展，粮食多了，人就越来越多。那些拥有农业的族群壮大起来以后，周边族群向他们学习，向他们靠拢，想了解这个族群怎么这么厉害，粮食哪里来的？向农业族群学习，就先要听懂他们的语言，语言就传播开来了。还有文化的传播。打个比方，农业族群的人告诉那些求学者，你要种这个庄稼，有窍门的，先要用人头祭天[7-10]，祷告要有一套咒语，这谷子要用血水泡了再播种，有一整套祭天、祭谷神的流程。于是，文化传播开来了，宗教传播开来了。当时的人，还不知道什么叫科技，什么叫宗教，都混在一起。所以，当世界观、价值观、人生观，整个三观体系在每个圈子内部的人群中都逐渐变成一样的，民族就形成了，他们信同样的宗教，用同样的思想观念，有同样的神话体系，讲同样的语言，语系也就聚合在一起。这就是民族的聚合。<br>前所未有的大量人群聚合在一起，就需要有管理，不然社会就乱了，于是出现了社会规范和规范的裁判者——领导人。领导人的私人财富积累起来以后要传承。到了大约7000年前，四大古文明萌发了。人类文明起源的过程就是一个“人法地，地法天”的过程。气候变暖是第一步“天”的变化，而后物产变丰富是第二步“地”的变化，最后社会发展是第三步“人”的变化。这是一种自然规律，也是社会规律。文明的产生不是神话里说的那样，一个天生神力的人拍拍脑袋就创造出来的。四大古文明的形成，不管是古巴比伦、古埃及还是古印度、古中国，都必须经历这个过程。关于四大古文明的说法目前争议颇多，古巴比伦、古埃及、古印度的文明其实都是密切关联，甚至可能同源的。陈中原教授一直在研究埃及的史前气候变迁，他发现尼罗河流域的生态非常脆弱，文明发展经常中断，然后有人从西亚新月地区移民过来，建立新的社群。所以，古埃及的文明有可能来自古巴比伦。从完全独立起源的文明来看，四大文明应该是东亚文明、地中海文明、美洲文明、内非文明。有人认为，中国只有3000多年的文明史，只能从商代算起。这是中国的史前考古学与历史学严重脱离造成的误解，至少5000多年前中国就已经有了良渚古城这样的宏伟的城市[11,13]，怎么会没有文明？西方学者提出用青铜器作为一个文明的标志，是有问题的[13]。文明形态多种多样，而具体的某种发明有偶然性。按照西方发明的标准去衡量中国，并不客观，逻辑上有问题。<br><br>中国是一个农业大国，中华文明的起源与农业关系密切。<br>不同地方有不同的作物区划，不同的语系实际上发生于不同的作物区，语系的起源跟作物的起源和农业区的起源有关。现在世界各地的语系分布非常不规则（图5.4），交错在一起，很多分布区也不是农业区。但是究其源头，大部分语系是起源于农业区的。有的语系人群只是后期因为气候或政治原因搬迁了。比如苗瑶语系的苗族、瑶族、畲族，散布在南方各地的山区。其中苗族主要分布在贵州地区。贵州在中国地形的第二级阶梯和第三级阶梯之间，受到地壳变动的大量挤压，山地陡峭，种庄稼有点困难，不可能是农业发源地，所以苗族不可能起源于这里。有人说汉族起源于甘肃、青海一带，那也是不可能的。汉族不可能起源于不适合农业的高原地区。人口庞大的古老民族更可能起源于农业发达的大河中下游和三角洲地区。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211557459.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.4　世界主要语系分布示意图。
<br>中国主要有两个农业发源地（图5.5）。中国早期驯化的农作物的种类很多，主要的是大米、小米，其他黄米、大豆、菱角之类的，在各个考古文化区系中被驯化。大米和小米是中国上古的主粮，麦子不是。麦子起源于西亚，大约4100年前才来到中国。北方的小米起源于磁山文化，在桑干河流域，就是丁玲写的《太阳照在桑干河上》的那个桑干河，它从山西大同流到河北涿鹿，从河北流到北京，被称为永定河。我们经常讲，黄河是中华民族的母亲河，那么桑干河就是中华民族的“祖母河”。目前发现的最早的驯化小米，也叫作粟，是在北京郊区永定河边距今一万多年的东胡林遗址，更早可能追溯到山西下川文化[14]。最早的小米就是在桑干河流域驯化的。有粮才有人，有人才有文化。中华民族主体的源头在这里。南方的大米可能是更早驯化的作物，大米养活的人多，而且南方气候好，天气变暖后作物恢复得快。所以，大米（也就是水稻），应该驯化得更早。目前最早的驯化大米发现于湖南南部湘江源头的道县玉蟾岩遗址，距今接近1.5万年，不过证据需要更多分析[15]。明确的大米驯化是在沅江边上，洞庭湖西岸，1.2万年前就开始了，到了七八千年前那里已经有成片的水稻田。洞庭湖周边，古代叫作云梦大泽，一大片的湿地，环境好得不得了，特别适宜种水稻。所以中华文化的主流源头就是这两个：北方的小米和南方的大米。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211557237.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.5　中国两个农业起源点——两个文明源头。
<br>因此中国早期形成了两个文明圈，大米文明圈和小米文明圈。在不同的农业区就形成了不同的文化体系。种大米的区系包括：高庙文化区，从彭头山文化到高庙文化、大溪文化，再到屈家岭、石家河；长江下游的良渚文化区，从河姆渡到马家浜、崧泽、良渚、马桥，这一层层文化更替发展下来；还有北方山东的大汶口文化区，从后李、北辛到大汶口、龙山、岳石，不过大汶口文化之前当地基本不种植水稻。所以南部和东部这三块农业核心区域是种大米的。北方两块区域，仰韶文化区和红山文化区，是种小米的。黄河中游和西辽河流域是种小米的。文化接触往往造成冲撞，大米文明和小米文明，这两个区域和人群文化上必然形成差异，继而冲突对峙。最后战争和妥协使他们融合在一起，形成了中华文明[16]。中华文明是大米和小米融合在一起的，就像一阴一阳融合成了一个完美的太极图。<br><br><br>我们说中国的文明7000年前就起源了，但有学者并不承认。西方有学者提出，古文明的标准之一是看一个地区有没有开始使用青铜器。可青铜器是西亚发明的一种配比精密的合金，一个西亚发明的东西传到哪里，哪里就算有文明，这合理吗？美洲的玛雅文明中没有青铜器，古代旧大陆的文明都没有传到新大陆去，那新大陆就没有文明了吗？当然不是。我们为什么不能拿中国发明的东西，看它传到哪里就说哪里开始有文明？比如说丝绸，丝绸传到哪里，哪里才是有文明，那西方文明就太晚了，1000年都不到。这同样不对。所以，不能拿某个地方发明的一个东西作为标准。我们认为，文明还是要有一种更加普适性的标准。<br>2014年我在联合国大厦做了一次演讲，提出古文明需要一个新的评判体系，并拟定了一个新的文明标准，得到世界各国很多学者专家的认可。我认为，文明的本质是社会规范，遵守特定的完备的社会规范，就是文明。而社会规范可以分出数个元素，例如统治、法律、历法、记录。规范需要有维护者，一个文明形态必须有统治。如果没人管，社会就乱套了，大家想干什么就干什么，那是野蛮，不是文明了。要有法律来规定行为范围，制定等级制度、礼仪制度，法律不一定成文，也可能是习惯法。还要有历法，让人们知道什么时候应该做什么了。“山中无甲子，寒尽不知年”，那就是原始蛮荒。有了历法，能够记日子，这才是文明，所以历法是特别重要的。历法可以通过古代星象图、天文观测遗址等材料去考证，特别是早期的天文台、观象台。从仰韶文化的西水坡大墓中来看[17]，中国至少6000多年前就有星象观测和历法。还要有记录，能够把这些文明的具体规范内容记录下来，流传下去，而记录也不一定非要是严格的文字。<br>如何判断古文明中有“统治”呢？早期的统治就是看有没有领导人——“帝王”，民主的统治是后来的事情。与很多动物群体一样，帝王有最大的生育权，占据最多的社会资源，他生的孩子就多，他的基因谱系扩张就快。通过 Y 染色体作遗传学分析，可以找出早期迅速扩张的谱系，来确定到底有没有帝王，什么时候出现，也可以分析他大概在哪里，尝试把陵墓找出来。早期全国人口不多，但是如果有帝王，他的孩子多，谱系就扩张得特别快，在人群中很容易占比很高。只要把我们现代人的 Y 染色体谱系树画出来，在谱系树上往根部追溯查找，就可以看到那些可能出现的扩张点。<br>把中国男人的 Y 染色体，根据序列差异程度画成一棵进化树（或称谱系树，图5.6）。结果发现，大部分的 Y 染色体谱系都是直线的，没有分叉，说明这一传承的脉络每一代都有一个男孩，才能把 Y 染色体传到现在，这并不容易。如果有一个老爷爷生两个儿子，两个儿子都有 Y 染色体并一直传到现在，这个老爷爷就是树上的一个分叉。在这棵树上面基本找不到三分叉，即使将全国40万个样本放在一起，信息存储量巨大，三分叉也很难找到。但有意思的是，在这棵树上我们看到有三个节点，它们有很多的分叉（图5.6中用三个矩形红块覆盖）。这里是随机抽样的几个样本，证明节点上这三个人生了很多男孩。实际上这三个人可能每人都生了上百个孩子，于是在这几个节点上就散开来了，现在中国的男人里面，他们的后代接近一半。不管什么民族，都有这三个人的后代，即大约一半的人都是这些人的后代[18]。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211558286.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.6　简化的中国人群父系 Y 染色体谱系树。
<br>根据后代的序列差异，即每两个人之间的差异程度，能计算出这人生活的年代。图5.6中标示位点 F11的这个人生活在距今6800年前后，标示位点 F46的人生活在约6500年前，标示位点 M117的人生活在约5400年前。夏朝是4000多年前开始的，而此处都6800年了，完全超出我们的历史认知范畴。但是，传说中，夏商周之前不是有三皇五帝吗？以前大家都说这只是神话传说，但有什么证据证明这些是假的呢？自古传下来的这个说法，是我们传统的语境，传统的历史文本，但是长期以来没有证据证明它是真的，也没有证据证明它是假的。现在，我们需要寻找证据去证明它是真的，或是假的。当前，至少从遗传的证据来看，这个说法并非毫无来由——我们发现中国上古有三个统治者，年代很早。<br><br>上文提到的这些年代，在考古学中有证据吗？从考古遗址里看到，6800年前并没有那么原始，中国最早的城市，就是那时候出现的。我刚才讲到种水稻的湖南常德彭头山，在它附近1000米的地方，出现了中国第一个城市。这个城，规模很大，相当于复旦校园这么大。护城河5米宽，城墙5米高，为什么要造5米高的城墙？守护帝王的三宫六院嘛。城里有皇宫，有祭坛，有王陵。最早的王陵中有一男一女，男的胸前戴着两个像太极图一样的玛瑙璜。这座6800年前的城，现在是湖南常德的一处景区，帝王骨架展示在博物馆里。这种文化叫高庙文化，7800多年前从沅江上游高庙坪上开始产生。高庙文化出土了很多画着八角星的祭祀盘。最近有人发现了水族的水书《连山》，其中的八卦就是八角星的图案。这会不会与伏羲演八卦的传说有关？高庙文化的这一批人群，每年夏秋，春潮退了以后，他们就到下游种水稻；冬春时，他们就到沅江上游。到了晚期，他们不迁了，就在水里筑起高台，建造城堡，定居下来。水涨上来时他们在城里居住，水退下去时他们就到城旁边的地里种水稻。高庙文化是中华文明最早发源的文化，它对中华文化的影响很大[19]。<br>从考古文化发展看，高庙文化沿着长江下去，大约6500年前扩展到山东。山东大汶口文化的陶器纹饰图案与高庙文化一脉相承。前面讲到的 Y 染色体基因 F11在这两个考古文化中占据主流，现代人群中 F11在湖南是最多的，其次是山东，山东闯关东那一批人，又把这基因带到东北去。所以基因分布和历史分布、考古分布是吻合的。我们现在看起来，高庙文化的发展与苗族基因起源、苗文化发展、传播过程是一致的（图5.7）。我们在贵州台江等地详细调查了苗族的基因，苗族有大量 F11类型，苗装上刺绣的凤凰造型与高庙文化的凤凰图案几乎一样，苗族织锦的八角星纹样也与高庙文化八角星徽章几乎一样。F11类型在汉族里面占了五分之一，在苗族里占了一半。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211559287.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.7　湖南常德城头山古城遗址（上左）、统治者墓葬（上右），以及高庙文化图案与苗族刺绣对照（下）。
<br><br>1988年，河南濮阳造水库，挖出一个大墓，发现它的历史很悠久，赶紧叫考古队员来清理。距今6400多年的仰韶文化就此被发现[17]。清理完了，当地想继续造水库。现在濮阳人都说，这个遗址没有保留下来是千古遗憾，着急造水库干吗，缺水你们忍一忍。这么大的遗址，要是留到现在，比秦始皇兵马俑还要壮观，参观的人会有多少？遗址中最核心的部分，已借到国家博物馆作为常设展览。这个墓葬有上千平方米，墓主人在中间，头朝南，脚朝北，旁边用贝壳排了各种造型（图5.8）。东边排了一条青龙，西边排了一头白虎。南边排了七只动物，第一个是一头驼鹿（即犴，井宿），北极地区、黑龙江北部、大兴安岭都有驼鹿；第二个，在驼鹿背上，是一只山羊（鬼宿）；再往上是一头獐（柳宿）、一匹马（星宿）、一头鹿（张宿）、一条蛇（翼宿）；再一个弯弯曲曲像蚯蚓一样的东西（轸宿）。这七只动物就是南方朱雀七宿。这个排布造型是什么概念？就是所谓“二十八星宿”的漫天星图。东方是青龙七宿，西方是白虎七宿，南方是朱雀七宿，北方是玄武七宿北斗星。众星环绕墓主，他是谁？他可能是图5.6中标示位点为 F46的第二个人物，他留下来的后代，在中国人中占了14%。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211559550.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.8　河南濮阳西水坡墓主周围的二十八星宿（上）以及仰韶文化图案（中）与藏缅族群（下）对照。
<br>仰韶文化在中原，很多人说，中原肯定是汉族的老家，仰韶文化肯定是汉文化的祖先。但是且慢，我们要分辨一下仰韶文化的特征。仰韶文化里，出现最多的是太阳纹、水波纹、鱼纹，还有双鱼捧螺、圆圈舞。汉族里面没有把鱼抬到这么高的地位的，鱼不是汉族的象征。谁喜欢鱼，中国那么多民族里谁对鱼最尊重？是羌族、藏族。藏族人是不吃鱼的，鱼是寄存祖先灵魂的地方，鱼是祖宗灵魂变出来的东西，不能吃。藏族人的“吉祥八宝”中就有双鱼图和宝螺图。只有极个别藏族群体受汉族影响，有打鱼的，但那是很少的现象。仰韶文化西迁以后，羌族、彝族、藏族的文化开始发源。他们喜好跳圆圈舞，但是汉族不会跳这些民族的舞蹈，汉族喜欢唱戏。他们的文化，对汉文化有影响，但不是汉文化的主流。这是氐羌文化，羌人是牧羊人，传说中羌人转向农业的一支姓姜。那个时候不用姓，但是姜姓后人对祖先没姓不理解，心想既然我姓姜，那我的爷爷也姓姜啊，不能说自己的爷爷没有姓。所以传说中，第一位炎帝叫姜石年，末代炎帝叫姜参卢。羊是4100年前从西亚传进来的，之前中国根本就没有驯化的羊。<br><br>那么汉族的文化源头在哪儿？图5.6中标示基因位点为 M117的第三个人5400年前生活在辽宁、内蒙古、河北三省区边界地区，曾经的热河省，就是燕山的一脉往北延伸的地方，《山海经》里记载的北次三经。一个现在叫牛河梁的山岗上，5300多年前开始建造金字塔形陵墓，这个中国“金字塔”比埃及金字塔早600年[20,21]。中间金字塔里埋的是这一族群最早的帝王，后面的金字塔里可能埋着他的儿子，再后面山岗的五分之一可能埋着他的孙子。一圈16个人，16座金字塔，16代人，都埋在这个圈里面，排布成天上轩辕星座的形状。北边就是青丘，2018年我去赤峰市敖汉旗探访城子山遗址，那里就是传说中的青丘，在山顶上有200多个五六千年前的祭坛遗迹。山腰间突出一块巨石，惟妙惟肖，是一个狐狸的头（图5.9）。青丘东边就是朝阳谷，有一个半拉山。2015年发现在半拉山东面还有一个比它早二三十年的墓[22]。在中间这位墓主胸前摆了四套仪仗。一面玉璧，代表神权；一顶玉瑁，看过《周礼》的都知道“君礼臣以瑁，臣礼君以圭”，所以玉瑁代表皇权，早期的玉瑁真做得像帽子一样，套在发髻上的形状像马蹄；第三个是熊头玉权杖，代表有熊氏的宗主权；第四个是玉钺，代表军权。神权、皇权、宗主权、军事权，四套权力在他的手上。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211601811.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.9　赤峰敖汉旗城子山遗址天然形成的狐狸头巨石。这里是传说中的青丘。
<br>这个文化就是红山文化，习惯大量用玉器，爱玉是汉族的传统。而且玉器被雕刻成各种龙凤造型，龙凤又是汉族的标记。然后还有玉熊龙，以及各种小玉人。这些小玉人很有意思。它们把手放在胸前，胸部塌进去，小腹部胀起来，丹田胀气、吸气，这是在练气呢。还有的玉人盘腿而坐，造型各异，都在运气。为什么古人玩这个，还重要到雕成小玉人流传下来？因为《黄帝内经》传下来养生的道理，就是讲练气的。道家的这个学说叫黄老之术。黄老道，黄帝开创，老子总结。道家信仰，也是汉族的核心思想。当然，这件事需要更多研究，很可能是真的。金字塔旁还出土了一个神庙，挖出一个女神，胸部丰满，脸相神奇——当时的审美观念跟现在不太一样。有人看了说，那时崇拜女性，是母系社会。但墓葬挖开一看，全是男的，领袖全是男的，不是母系社会。传说中轩辕黄帝拜九天玄女为师，虽然传说是汉代才记录下来的，但上古的民间传说里可能是一直就有的，这个事也很值得探究。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211601046.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.10　辽宁建平牛河梁遗址中央大墓主人遗迹（上），红山文化与汉族对照（下）。
<br><br>中国早期的三皇五帝的历史，能说它没有吗？这些踪迹都在。黄帝、炎帝，轩辕、神农，很可能是后人给他们起的名字。有些人当时有另外的名字，然后后人就用这些名字称呼他们，并尊敬他们，神化他们。三皇是三个大时代，延续几百年，五帝是五个小朝代。所以早期的历史，真的从基因、从考古、从实证的角度，好像看起来没有那么可疑。我们后来再精细地查看遗传谱系时发现，没那么简单，除了这三大祖先以外还有三小，三小以外还有六个更小的。东亚新石器时代有十二个扩张的共同祖先，包括在韩国和日本有两个，在中国境内有十个，比传说中的三皇五帝还多[23]。他们之间的关系，我们现在还没研究清楚，但是迷雾正在渐渐散开的过程中。我们至少发现，颛顼、夏禹跟黄帝对不上，有些历史人物的关系跟《史记》里面记载的有差异。《史记·黄帝本纪》里说，五帝都是黄帝的后代，黄帝是五帝的第一个，从位列三皇变成五帝之一了，而把五帝里面原来第一位的少昊去掉了，为什么呢，因为少昊把黄帝给推翻了，所以他肯定不是黄帝的后代。司马迁希望讲求万世一系，只能有一个祖宗，不能有三个祖宗，更不能有十二个祖宗。只有一个祖宗才叫大一统。这是司马迁的态度。但是我们发现，有些史书里不是这么记录的，《竹书》里就不是这么记载的，以前历史就没有这个说法，到了《史记》才改，司马迁有他的政治目的。以前没证明的东西我们要重新证明，不要去迷信。现在，我们有更多证据去思考“疑古时代”的那些问题。<br><br>有一些学者认为，夏朝不存在，夏朝是古人假造的。但是中华文明探源工程把夏朝之前的尧和舜的陶寺遗址挖出来了[24]，尧舜和商朝之间的夏肯定是存在的，只不过夏在哪里，什么文化层是夏，对此还有争议。从单一的学科，比如仅仅从考古学出发，解决不了这个问题。我们用基因来研究，并且与考古学、历史学结合，与挖出来的疑似夏朝遗迹中的贵族的骨头比对一下，就清楚了。而夏朝遗址在哪里，从哪儿才能得到明确为夏朝贵族的骨骸，目前还没解决，是最需要去调查研究的。<br>我们提出一个可能的假说，接下去就要证明它，这必须把历史问题转化为可以检验的科学问题。夏朝是否存在的问题，实际上就是陶寺文化的尧舜时代（约4300～4000年前）与商朝（约3600～3000年前）之间这400年的广域王朝能否被证明是传说中的四百载夏朝。要证明是夏朝，关键就是研究夏朝统治家族是否符合我们传说中的血统身世的问题。对于夏朝开创者大禹的身世，有两个说法：其一是“禹出西羌”，来自西北；其二是“夏为越后”，来自江浙。这看似矛盾的说法，能否通过历史人类学梳理出头绪来？只要证明夏后氏的渊源符合这两点，就基本能确定夏朝真实存在。<br>历史传说确实很神奇，也留下了很多的蛛丝马迹。5000多年前轩辕黄帝通过涿鹿之战一统天下。而我们看到考古文化中这个年代——5300多年前，也发生了翻天覆地的变化。红山文化从辽西南下，成功扩大了影响范围。这是不是轩辕黄帝南下的结果？黄帝带领红山文化的汉族祖先人群得到了中原，得到了天下。很多地方留下了当时的战场遗迹。中原的仰韶文化庙底沟类型发生显著变化，整个仰韶文化大量西迁至甘肃青海一带，变成马家窑文化。马家窑文化其实就是仰韶文化的余绪，只不过从中原搬到了一个新的地方，然后受到新环境的影响，有一些器物稍微变了一些，以适应那里的环境。然后安徽辉煌了600年的凌家滩文化彻底灭亡了。湖南、湖北大溪文化变成屈家岭文化，那里出现的大量玉器，都向东北看齐。长江下游江浙一带，原来是受凌家滩文化影响的崧泽文化，5300年前变成了良渚文化，出现各种玉器，造型和礼仪也向东北看齐。东北红山文化里代表各种权力的玉钺、玉瑁、玉璧（以半拉山陵墓为代表）等，成了良渚文化中最重要的钺、琮、璧。这种现象在全国很多地区或多或少都出现了。但是，良渚人并不从外地进口玉材，良渚早期的玉器，玉材来自东北的目前只发现一具。良渚的中期遗迹中，有从东北传过来的玉器，但是形状都是方的。后期良渚的礼器，用蛇纹石取代了玉，看来跟东北的关系断绝了。<br>5000多年前，东北的红山文化消失了（华北的延续还在），取而代之的是小河沿文化。小河沿文化的器物都是山东大汶口的纹饰。传说中，山东是少昊氏的地盘，黄帝统一中国的时候没把山东打下来。可能少昊5000年前把黄帝老家给包抄了，变成小河沿文化。这时黄帝式微，统治不了全国各地。后来，黄帝的子孙在江浙的统治也不稳了，可能发生了春秋末年“田代齐姜”式的事情。所以我们研究发现，良渚的贵族不再是红山贵族的 O3型 Y 染色体，而是变成了当地的 O1型[25]。<br>江浙这一带的统治者，很可能就是传说中的颛顼。如今江浙这一带很多人家，家谱上都认颛顼是他们的祖先，和颛顼的文化关系非常密切，这可能是良渚文化所遗留的。史书上说，颛顼是黄帝子孙。颛顼可能并非单指一个人，而是一个帝号，就像炎帝、黄帝，都有很多代。良渚文化的历代首领都可能被称为颛顼。早期的颛顼可能就是黄帝的子孙，后期的颛顼就不见得是黄帝后代（可能发生过“田氏夺齐”式的事件），但颛顼是大禹的祖先，这点应该是明确的，所有书籍都这样记载。颛顼传了几代到鲧，鲧的儿子就是大禹，大禹攫得天下，这一传承可能比较准确。颛顼中的“顼”字，就是玉人头的意思；“颛”字，表示戴着华丽的礼帽。“颛顼”就是戴着大帽子的玉人头（图5.11）。玉人指良渚文化中的人像，当然，更早的是东北红山文化中出现的小玉人，这些小玉人是修道模样的，而在良渚的宗庙里，常见形象是下面刻着兽头的玉琮。历代颛顼的头像被刻在玉琮上，可能就是顼（玉首）的本义。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211602070.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.11　良渚、石家河、齐家、三星堆文化的文物可能代表着颛顼和夏朝的传承。
<br>很多地方都有4500年前五帝时代的玉人头像出土，特别是湖北石家河文化（图5.11）中的玉人头像，跟良渚玉琮造型非常接近——有很夸张的大鼻子大耳朵。这个玉器是上海博物馆的馆藏之宝，这个造型与3600年前的三星堆青铜人头像非常相似，这说明三星堆文物不是从埃及、巴比伦来的，而是我们自己传承下来的。以前有玉石就用玉石来做，后来有了青铜合金，就用青铜来做，都是夏人用于祭祀列祖列宗的。后世说“夏人尚鬼”，就是夏人喜欢把自己的祖宗刻成雕像放在一起的意思。这些雕像从良渚文化，延续到颛顼时代的石家河文化，再到齐家文化。<br>考古中发现，4500年前良渚文化开始北上，进入山东，甚至影响中原。江浙地区的良渚最高统治贵族可能都跑掉了，只剩下留守政府。到4400年前，良渚文化的影响又出现在陕西、甘肃、青海的齐家文化中。齐家文化把马家窑文化取代了。齐家文化中出现大量类似良渚的玉琮、玉璧，还有一些中原的玉刀，所以齐家文化其实可以被称为西北良渚文化。还有一拨人从江浙一带把玉璧带到广东北部韶关一带，产生了广东的良渚文化，叫石硖文化。<br>这些信息连在一起，是什么关系？研究上古历史，就像破案一样。这段历史很可能是这样的：少昊是五帝第一代，少昊打败末代黄帝，进入中原；过了100年，良渚变强北上，打败少昊得了天下，这就是颛顼时代；再过100年，帝喾从安徽河南一带兴起，打败颛顼夺得天下，颛顼没法回到江浙，于是逃到西北，就形成齐家文化；颛顼流亡到西北，经过几代人，传到大禹。传说“夏为越后”即大禹是东南的越人，又说“禹出西羌”，在这个故事里就都能解释：大禹祖籍浙江，生于陕西。对齐家文化的贵族作基因分析，可以判定他们是不是来自浙江良渚文化的后裔。如果再检测一下疑似夏都的二里头文化遗址中的贵族基因，看看他们与齐家文化的贵族是不是同一个家族的，那么夏朝有没有，就真相大白了。所以齐家文化，特别是石峁古城的贵族基因很重要。齐家文化中的老百姓肯定与夏人的贵族不是同一个族群，而是此前的马家窑文化的当地人，其中很多是5300年前从中原跑过去的仰韶文化贵族的后代。<br>“一带一路”上的东西方交流可能开始得非常早，4400～4100年前，因为东西方文化的交流，中国的技术和物资传给西方，西方也传给中国三件宝贝，直接传到西北齐家文化中，那就是麦子、羊、青铜。北方以前一直种小米，收成并不好。引种麦子后，能养活更多人。夏人治水，开发黄河流域的河套地区，建设塞上江南，人口就迅速增长起来，夏人还建了陕北神木的石峁古城和旁边延安峁等好几个古城。齐家文化的石峁很有可能是颛顼流亡政府所在地。所以传说中的大禹治水很可能就是兴修河套水利，水治好后可以耕种大片麦田，邦国才可以兴盛。第二件宝贝羊，羊是特别好的肉食来源。羊比猪好养，大禹的士兵吃了肉食身体强壮，比舜的士兵强多了，可被称为战斗民族。第三件宝贝青铜，是开挂的武器，舜帝的士兵都拿着石制兵器，大禹的士兵拿的是青铜武器，坚固多了。所以大约4000年前，考古文化中发现，石峁的力量从陕北武力扩张，攻下山西南部尧都平阳，就是陶寺遗址[26]。舜帝的时代就此结束。上古的三皇五帝可能是8个朝代，每个朝代有很多帝王。例如尸子说“神农七十世有天下”。舜帝可能也不止一个。舜一世死后葬在陶寺，舜二世被大禹追到湖南南边，被传为“南巡”。史书上说，舜南狩禅位于禹。后来舜二世死在湖南，舜帝的两个妃子（舜二世的母亲）半道上哭死在洞庭湖君山，留下了湘妃的传说。<br>这个故事看来是讲得下去的。有了麦子和羊作为充沛的食物、有了青铜作为锋利武器之后，在大禹治理下，夏人带领着本来被统治的羌人，取得了天下。所以至今很多羌人都认大禹为祖先，也认颛顼为祖先。从那时起羌人的文化变化了，开始养羊，也从那以后叫羌人。《说文解字》说，羌从人从羊。科技考古人员筛选西北地区考古遗址中的麦粒，测定最早的麦粒就是4100年前的[27,28]。4000多年前就建立了传说中的夏朝，这个年代框架，考古结果与历史记载完全对应。所以我们认为夏后氏的起源，最早从良渚开始，4500年前进入中原，开始五帝中第二个朝代颛顼，4400年前流亡到西北，定都石峁开始齐家文化。然后在4000年前又进入中原，建立了夏朝。在二里头的宫城里挖出的最精美的一件青铜重器就是绿松石镶嵌的青铜龙。龙的造型非常奇特，有很夸张的铲刀形面部，大鼻子，小眼睛。同样的造型从来没有在其他考古遗址中发现过，但是2019年，在陕北的石峁古城中挖出的石雕的龙纹，与二里头绿松石青铜龙的造型几乎一模一样（图5.11）。这又是夏人来自西北的一项证据。这是把历史和考古结合在一起的推理。<br>4400年前颛顼朝被灭，留在江浙良渚文化老家的贵族必然受到打击，良渚文化因此灭亡。统治者向南逃亡到了广东，产生石硖文化。后来这一人群又演化成历史上的百越民族，就是现在的侗傣语系族群。包括老挝、泰国、印度阿萨姆邦的主体民族都是侗傣语系的。实际上泰国常用的龙的造型也是大鼻子小眼睛的，与石峁和二里头的龙可能是同一起源（图5.11）。所以很多少数民族并不是各地独立形成的，而是从中华文明的核心因为历史原因分化出来的，甚至可能是统治者的直接后代。羌人可能是涿鹿之战后失势的炎帝的后人，跑到西北变成藏缅语族的氐羌族群。苗族可能是太昊伏羲氏的血裔，在涿鹿之战中失势，从江淮湖广地区跑到湘西山里去了。侗傣民族是颛顼失去天下后部分贵族跑到南方后变成的。所以民族不是一个个孤立的群体，中华民族本来是一家子，后来因为各种历史政治原因流散开。涿鹿之战以后，红山文化的原始汉族就融合成更大的汉族，没有走掉留在中原的仰韶文化民众不会变成羌族，而是融入汉族。山东大汶口文化的少昊是太昊伏羲的后代，少昊统治中原的时候，民众也融入了汉族。结果在汉族里，伏羲的后代占的比例最高，达19%，黄帝的后人（O3-M117）比伏羲的（O3-F11）还少一点。<br>夏朝末年时，商灭夏不仅仅是一场战役。刚开始商汤灭夏，夏人就逃到南方长江流域的安徽，史称“夏都南巢”。南夏北商对峙了大约300年。直到后来商人可能有了战马，才把南夏灭了，夏人又逃到了四川，把宗庙搬到那里，所以留下了三星堆那么多精美的青铜重器。这就是《三字经》里的“四百载，迁夏社”。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211602880.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.12　广西发现的新石器时代赤璋。这是华夏六礼中礼南方的重器，证明广西邕江流域百越族群在当时已在华夏文明圈内。（实物藏于广西民间收藏博物馆）
<br>历史人类学的研究，将不同领域的丰富证据结合在一起。我们不妨给中国的族群演化作一小结。<br><br>历史人类学的研究，将不同领域的丰富证据结合在一起。我们不妨给中国的族群演化作一小结。<br><br>约8000～6000年前，桑干河流域的小米种植人群渐渐增多，向外扩张寻找更多可耕地。约8000年前向南扩散的人群延伸到了黄河流域，发展出裴李岗—仰韶文化。约6000年前向北扩散的人群延伸到西辽河流域，征服了当地的赵宝沟文化人群，发展成了红山文化。汉藏语系的分化就此开始。南下人群奠定了藏缅语族基础，北上人群奠定了汉语族基础。在红山文化最高规格墓葬人骨中，检测到了汉族最主流的 Y 染色体类型 O-M117。<br>约5400年前，气候转冷，红山文化人群南下，跨过涿鹿的桑干河，可能渐次征服了各地人群，民族发生大融合。因此，约5300年前，中国各地的文化多发生巨变。中原的仰韶文化西迁，征服了甘青的大地湾文化，形成马家窑文化。湖广的大溪文化变成屈家岭文化。江浙的崧泽文化变成良渚文化。各地多开始如红山文化一样大量用玉，也出现了许多同样的刻符。仰韶西迁使汉藏语系最终分化。汉语族成为东部主流，藏缅语族成为西部主流。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211603415.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.13　第十一棵进化树——汉藏语系演化。图中数字为距今大致年数。
<br>约4400年前，可能由于来自良渚的齐家文化入侵，马家窑文化终结，部分藏缅人群南迁，发生了第一次分化。约3600年前的夏商之战，约3000年前的商周之战，以及约2600年前的秦羌之战，使藏缅语族分化成了6个主要的语支。<br>春秋战国的诸侯割据，使汉语族分化成了秦、齐、楚3个主要语支。<br><br>汉语5000多年前就已南下，并深度影响了长江流域的上古苗瑶语和侗傣语，但早期汉语并未成为长江流域的主流。秦汉时期，对南方的征伐使汉语渐渐强势。而后来中原的历次战乱使汉族大量人口南下，将其基因与语言文化带到了江南和岭南。特别是东晋、晚唐、南宋的三次大规模南迁，使汉族的南方基因组与北方基因组基本趋同，而显著不同于周边少数民族。虽然历史上北方的汉族人群受到外族影响巨大，但在人口比例上汉族占绝对优势，因此汉族各地基因组基本一致，特别是 Y 染色体。<br>迁到各地的人群因为交流沟通减少，语言渐渐演变分化，形成诸多方言语种。目前汉语的各主要方言基本是因五代十国时期的割据分化形成的。五代的中原地区形成了官话，北汉形成了晋语，吴越形成了吴语，南唐形成了赣语与客家话，闽国形成了诸闽语，南汉形成了粤语，楚国形成了湘语……<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211604648.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.14　第十二棵进化树——汉语族语言演化。
<br>各方言因为移民来源不同，分属于秦、齐、楚三个语支。秦语支丢失了浊辅音，却保留了复杂的韵母系统。楚语支保留了复杂的浊辅音，而丢失了双元音韵母和复杂的韵尾。齐语支有其独特的演化方向，最有特色的是翘舌音变成了普通舌尖塞音（d, t）。<br>汉族虽然内部有诸多差异，但相对外族在基因、语言、文化诸多方面保持了最大的一致性，更由于政权统一时间长，内部认同强，因此稳定地维持为同一个民族。<br><br>江浙地区自古有着独特的文化区系。史前为原始南岛-侗傣语的分布区，经历了马家浜文化、崧泽文化、良渚文化、钱山漾文化、马桥文化等考古时期。至今，上海南郊与丽水若干地区方言还保留了侗傣语特色的缩气塞音与大量侗傣语词汇。<br>春秋时期越国民众普遍使用侗傣语，当时留下的《越人歌》就记录了这种语言，上层则开始使用吴国从西部带来的汉语楚方言。越王政令《维甲令》（维甲修内矛赤鸡稽繇方舟航买仪尘治须虑亟怒纷纷士击高文习之于夷宿之于莱致之于单）中就出现了越汉词汇夹用的现象。这可能是现代吴语的起源。<br>秦汉以后，北方汉族陆续迁入，吴方言渐渐稳定并分化。从北向南的迁徙，在江浙内部形成两条路线，各分化出三个方言区。西线为宣州—婺州—处衢，东线为太湖—台州—东瓯，东线方言之间甚至没有明确的分界线。<br>北方移民增多也使吴语中的汉源词汇渐渐增多，但这种词汇的改换始终基于早期楚方言的语音——简化的韵母结构。可能由于江浙经济发展较快，特别是南宋以来，吴语发音倾向音节短促以提高效率，因此把所有延长发音时间的双元音和鼻音韵尾都简化成更多样的单元音或腭化音。在北部吴语（太湖方言区）中，这一现象尤为突出，甚至进一步取消了影响音节时长的舒声与入声的音长区分，也取消了入声的塞音韵尾的必需性，代之以不同的元音，使元音音位大幅增加。上海奉贤区的偒傣话甚至发展出了20个元音音位，成为世界上元音最多的语言。偒傣话书面形式是发展自楚篆的锦带书。<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211604308.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>
图5.15　第十三棵进化树——吴语语言演化。
<br>前文讲述的涿鹿之战与禹夏起源，都是假说，有的证据多一点，有的证据少一点，但是我们可以大胆假设，给出中华民族早期历史的一个版本，这个版本不一定全对，尽管目前的证据似乎充分，但我们还是要继续小心求证。对于早期历史，文字记载也是有的，但是很多古文字学家对文字记载有较多争议。例如，在疑似尧的墓葬里挖出一个陶器，上面写着两个字“大尧”。“尧”字争议比较少，但是那个“大”字争议较多。早期文字不可能从商朝才开始，文字在商朝时已经很成熟了。但是我们发现，早在5300年前，全国各个文化区域，不管湖南、湖北，还是浙江、中原，刻画的符号都是一样的，这跟仓颉造字的传说很可能是一致的。我们传说中的历史很可能都是真的，但需要运用各种研究手段，搞清我们的历史。<br>有人认为“炎黄”不能讲，“中华”也不能讲，担心一说炎黄，会影响一些少数民族的情绪。其实少数民族也是炎黄的后代，比如维吾尔族里有10%炎黄后代的基因，藏族里面一半的基因都是炎帝神农传下来的，另外，苗族、羌族，很多人都是伏羲炎黄后代。其实西南一些外国人也是炎黄子孙。中华民族的血缘纽带是剪不断的，从来不是想象出来的，而是切切实实存在的。<br><br>
<br>
高晶一．汉语与乌拉尔语言同源关系概论[M]//张维佳．地域文化与中国语言．北京：商务印书馆，2014,36-90.

<br>
Gao J Y. On etymology of Sinitic, Indo-European and Uralic terms for‘star’ supported by regular sound correspondences[J]. Archaeoastronomy and Ancient Technologies,2020,8（2）:29-40.

<br>
Wang C C, Yan S, Hou Z, et al. Present Y chromosomes reveal the ancestry of Emperor Cao Cao of 1,800 years ago[J]. Journal of Human Genetics,2012,57:216-218.

<br>
Wang C C, Yan S, Yao C, et al. Ancient DNA of Emperor CAO Cao's granduncle matches those of his present descendants[J]. Journal of Human Genetics, 2013, 58:238-239.

<br>
文少卿，王传超，敖雪，等．古DNA证据支持曹操的父系遗传类型属于单倍群O2-F1462[J]．人类学学报，2016,35（4）:617-625.

<br>
徐丹，傅京起．语言接触与语言变异[M]．北京：商务印书馆，2019,69-96.

<br>
宋兆麟．民族学中的人头祭与有关的考古资料[J]．广西民族研究，1986, （1）:66-77.

<br>
金汉波．史前至商周时期的人头崇拜及其相关问题[J]．民俗研究，2005, （4）:89-111.

<br>
孟鸥．从卜辞看商代的人祭之法[J]．青岛大学师范学院学报，2000,17（4）:23-32.

<br>
王胜华．西盟佤族的猎头习俗与头颅崇拜[J]．中国文化，1994, （1）:71-77.

<br>
赵晔．良渚文明的圣地[M]．杭州：杭州出版社，2013.

<br>
刘斌，余靖静，曾奇琦．五千年良渚王国[M]．杭州：浙江少年儿童出版社，2019.

<br>
易华．从玉帛古国到干戈王“國”[J]．甘肃社会科学，2017, （6）:62-68.

<br>
侯毅．从东胡林遗址发现看京晋冀地区农业文明的起源[J]．首都师范大学学报：社会科学版，2007, （1）:25-28.

<br>
张文绪，袁家荣．湖南道县玉蟾岩古栽培稻的初步研究[J]．作物学报，1998, （4）:416.

<br>
张明华．抚胸玉立人姿式意义暨红山文化南下之探讨[J]．上海博物馆集刊，2005, （10）:411-422.

<br>
南海森．濮阳西水坡[M]．郑州：中州古籍出版社，2012.

<br>
Yan S, Wang C C, Zheng H X, et al. Y Chromosomes of 40% Chinese Descend from Three Neolithic Super-Grandfathers[J]. PLoS ONE,2014,9（8）: e105691.

<br>
贺刚．湘西史前遗存与中国古史传说[M]．长沙：岳麓书社，2013.

<br>
辽宁省文物考古研究所．牛河梁：红山文化遗址发掘报告（1983～2003年度）[M]．北京：文物出版社，2012.

<br>
郭大顺．牛河梁红山文化遗址与玉器精粹[M]．北京：文物出版社，1997.

<br>
熊增珑，樊圣英，李道新，等．辽宁朝阳市半拉山红山文化墓地[J]．考古，2017, （7）:18-30+2.

<br>
Wen S Q, Tong X Z, Li H. Y-chromosome-based genetic pattern in East Asia affected by Neolithic transition[J]. Quaternary International,2006,426:50-55.

<br>
中国社会科学院考古研究所，山西省临汾市文物局．襄汾陶寺：1978～1985年发掘报告[M]．北京：文物出版社，2015.

<br>
Li H, Huang Y, Mustavich L F, et al. Y chromosomes of Prehistoric People along the Yangtze River[J]. Human Genetics,2007,122:383-388.

<br>
徐峰．石峁与陶寺考古发现的初步比较[J]．文博，2014, （1）:18-22.

<br>
Lister D L, Jones H, Oliveira H R, et al. Barley heads east: genetic analyses reveal routes of spread through diverse Eurasian landscapes[J]. PLoS ONE, 2018, 13 （7）: e0196652.

<br>
Liu X, Lister D L, Zhao Z, et al. Journey to the east: diverse routes and variable flowering times for wheat and barley en route to prehistoric China[J]. PLoS ONE,2017, 12（11）: e0187405.

]]></description><link>书籍\人类起源与迁徙之谜（第五章）.html</link><guid isPermaLink="false">书籍/人类起源与迁徙之谜（第五章）.md</guid><pubDate>Mon, 21 Oct 2024 08:10:17 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211555873.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202410211555873.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[牛刀小试]]></title><description><![CDATA[ 
 <br><br>假设，某种特定的癌症的患病率为0.1%(0.001)。有一个简易的方法能够检查出是否患上这种癌症:患上这种癌症的人中有95%(0.95)的概率被诊断为阳性。但另一方面，健康人群也有2%(0.02)的可能性被误诊为阳性。那么，如果在这个检查中被诊断为阳性的时候，实际患上这种癌症的概率为多少呢?<br>解：假设在人群中患病为 ，那么不患病就是 ，那么<br><br>在患病的情况下被诊断为阳性为 ，没患病且被诊断为阳性为 。那么，根据贝叶斯定律：<br><br>那么就等于(0.95×0.001)/((0.95×0.001)+0.02×0.999)<br><br>
假设夫妻俩的第一个孩子是女儿。那么，接下来生的孩子依然是女儿的概率为多少？
<br>第一，从统计学观点来看，生男生女的比率并不是各占一半的。事实上，生男孩的比率会稍微高一点。在日本，生男生女的概率比约为 51:49。即使具体比率上有所差别，但“男孩的概率高一些”这一特性，是全世界共通的。不管原因如何，在生物学上，男女的出生率有着其固有的结构，因此，不能说这种现象与投硬币有着同等的概率。<br>第二，那位读者的医生朋友观察的是“关于多数夫妻生下来的多数孩子的样本统计”，而不是“针对某对特定的夫妻所生的孩子进行的统计”。即使人类整体在统计时呈现出 51:49这样稳定的比率，但某一对特定夫妻所生孩子是男还是女的问题上，并不一定遵循这个比率。这对夫妻有其固有的特性，因此也不能否定是否存在“生女孩稍微容易一点”或“生男孩稍微容易一点”这种性向的可能性。<br>标准统计学（又称内曼－皮尔逊统计学）在阐明全人类范围内的男女例这一性向问题时是有效的，但不能用来解答“特定的某一对夫妻更容易生男孩还是女孩”的问题。这是因为，如果不使用达到一定程度的大量数据，就不能运用标准统计学来推断，关于这一点，在第8讲中会进行详细的解说。理由是，对于某一对特定的夫妻，他们所生的孩子数量，并不足以用来进行统计验证；而且，在生下大量的孩子的过程中，随着年龄的增长，身体条件也会发生变化。]]></description><link>书籍\统计学关我什么事：生活中的极简统计学.html</link><guid isPermaLink="false">书籍/统计学关我什么事：生活中的极简统计学.md</guid><pubDate>Thu, 20 Jun 2024 12:44:14 GMT</pubDate></item><item><title><![CDATA[序言]]></title><description><![CDATA[<a class="tag" href="?query=tag:excalidraw" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#excalidraw</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> <a class="tag" href="?query=tag:想法" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#想法</a> 
 <br><br>动植物也是研究古代人群的迁徙的好材料。<br>
比如<br>
<br>C3 植物是指二氧化碳在光合作用中直接固定成为三碳化合物，即3-磷酸甘油酸：如小麦、大米、玉米、大豆等。
<br>C4 植物则是指二氧化碳在光合作用中首先被固定成为四碳化合物， 即草酸：玉米、甘蔗、高粱等。<br>
这2种物质分别在不同的农作物中不同。<br>
微量同位元素的应用：人活着的时候，会从环境和食物中吸收特定的元素，死了之后，骨骼、牙齿还有茎秆会留下元素信息。通过这些信息就可以知道所吃的食物的营养成分。
<br><br>魏敦瑞在北京周口店，距今77万年。附近发现了<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>，魏敦瑞提出了巨人假说，但是后来被证明是错误的。但是他对于<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>的猜测：<a data-href="多中心假说" href="术语\多中心假说.html" class="internal-link" target="_self" rel="noopener nofollow">多中心假说</a>，后来演变成为了<a data-href="连续进化附带杂交假说" href="术语\连续进化附带杂交假说.html" class="internal-link" target="_self" rel="noopener nofollow">连续进化附带杂交假说</a>。<br>距今330万年，东非的肯尼亚境内，发现了石制品。<br>
中国境内的石制品落后于非洲，这证明了中国境内独立进化？<br>
但是后来分子生物学的技术不认为是这样。<br><br>线粒体夏娃距今14万年。<br>
Y染色体亚当距今6万年。在科伊桑人。<br>
所以<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>不是祖先。<br><br><br>基因学者认为：各地的智人都源自非洲，并非是各地的直立人进化而来的。<br>
就连在<a data-href="云南" href="术语\云南.html" class="internal-link" target="_self" rel="noopener nofollow">云南</a>发现的<a data-href="元谋人" href="术语\元谋人.html" class="internal-link" target="_self" rel="noopener nofollow">元谋人</a>，应该也被错误估计了时间。<br><br>为什么亚洲人有<a data-href="铲形门齿" href="术语\铲形门齿.html" class="internal-link" target="_self" rel="noopener nofollow">铲形门齿</a>？<br>
原因是，这个基因是决定腺体分泌的。<br><br>为什么周口店的<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>没有留下后代？也许是因为环境因素导致的。也有可能是<a data-href="塔岛技术悲剧" href="术语\塔岛技术悲剧.html" class="internal-link" target="_self" rel="noopener nofollow">塔岛技术悲剧</a>。还有可能近亲繁殖导致的致病遗传。<br>
比如在中国境内，距今10万年-4万年没有发现任何古人类遗迹，可能是它们灭绝了。<br><br><br>正如<a data-href="格鲁吉亚人" href="术语\格鲁吉亚人.html" class="internal-link" target="_self" rel="noopener nofollow">格鲁吉亚人</a>人类可能是第一批走出非洲的直立人。<br>
后来又发现了<a data-href="海德堡人" href="术语\海德堡人.html" class="internal-link" target="_self" rel="noopener nofollow">海德堡人</a>。可以算是一种晚期直立人。<br>总之，在现代智人还没有冲出非洲的时候，外面至少有3种古人类生活着：<br>
<br><a data-href="尼安德特人（Neanderthal）" href="术语\尼安德特人（neanderthal）.html" class="internal-link" target="_self" rel="noopener nofollow">尼安德特人（Neanderthal）</a>
<br><a data-href="丹尼索瓦人（Denisovan）" href="术语\丹尼索瓦人（denisovan）.html" class="internal-link" target="_self" rel="noopener nofollow">丹尼索瓦人（Denisovan）</a>
<br><a data-href="直立人" href="直立人" class="internal-link" target="_self" rel="noopener nofollow">直立人</a>
<br>认为大概距今40万年，<a data-href="尼安德特人（Neanderthal）" href="术语\尼安德特人（neanderthal）.html" class="internal-link" target="_self" rel="noopener nofollow">尼安德特人（Neanderthal）</a>和<a data-href="丹尼索瓦人（Denisovan）" href="术语\丹尼索瓦人（denisovan）.html" class="internal-link" target="_self" rel="noopener nofollow">丹尼索瓦人（Denisovan）</a>分道扬镳，前者走向了欧洲，后者走向了亚洲。<br><br><br>这其实也有运气的成分，例如：<br><br>内蒙古曾经有一个很大的湖泊，叫做<a data-href="居延海" href="术语\居延海.html" class="internal-link" target="_self" rel="noopener nofollow">居延海</a>。<br>在<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>遗址的附近，有一个<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>遗址。距今大概4.2万年-3,85万年，很可能是海岸暴走族的后代。而且<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>与亚马孙人的遗传关系很近。但是<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>最终灭绝了。<br>更有趣的是，在<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>附近，有<a data-href="山顶洞人" href="术语\山顶洞人.html" class="internal-link" target="_self" rel="noopener nofollow">山顶洞人</a>遗址，它们生活在3.4万年-2.9万年，但是很不幸，在战火中消失，应该是草原狩猎族的后代。<br><br>
<br>3万年前，海岸暴走族也开始北上，也可能走了2条路。

<br>从海岸到华南
<br>从河流到<a data-href="云南" href="术语\云南.html" class="internal-link" target="_self" rel="noopener nofollow">云南</a>


<br>以<a data-href="山顶洞人" href="术语\山顶洞人.html" class="internal-link" target="_self" rel="noopener nofollow">山顶洞人</a>为代表的这群人，从<a data-href="华北地区" href="术语\华北地区.html" class="internal-link" target="_self" rel="noopener nofollow">华北地区</a>到了<a data-href="黄河流域" href="黄河流域" class="internal-link" target="_self" rel="noopener nofollow">黄河流域</a>。开始逐渐发扬光大。但是好景不长，距今2.5万年，进入了<a data-href="末次盛冰期 (Last Glacial Maximum,LGM)" href="术语\末次盛冰期-(last-glacial-maximum,lgm).html" class="internal-link" target="_self" rel="noopener nofollow">末次盛冰期 (Last Glacial Maximum,LGM)</a>，它们可能向南迁徙。
<br>经过了一段时间的迁徙和混合，中国内陆形成了2批人。干脆直接用组合1和组合2来代替。<br>
<br>组合1在<a data-href="中国北方" href="术语\中国北方.html" class="internal-link" target="_self" rel="noopener nofollow">中国北方</a>现代人的频率高
<br>组合2在<a data-href="中国南方" href="术语\中国南方.html" class="internal-link" target="_self" rel="noopener nofollow">中国南方</a>现代人的频率高<br>
但是！这2种组合的人并不是分别从<a data-href="中国南方" href="术语\中国南方.html" class="internal-link" target="_self" rel="noopener nofollow">中国南方</a>和<a data-href="中国北方" href="术语\中国北方.html" class="internal-link" target="_self" rel="noopener nofollow">中国北方</a>进入中国的。<br>
科学家认为，他们都是从南方进入的。只是组合2在南方扩展比较早，而组合1到达北方之后才迅速扩张。<br>
而当时的那群草原狩猎族和他们的后代<a data-href="山顶洞人" href="术语\山顶洞人.html" class="internal-link" target="_self" rel="noopener nofollow">山顶洞人</a>可能不容乐观。
<br>根据目前的研究，基因多样性从多到少：<br>
南方非汉族人群＞男方汉族＞北方汉族＞北方非汉族人群。<br>
根据对牙齿的描述，可以分为<a data-href="巽他型牙齿" href="术语\巽他型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">巽他型牙齿</a>和<a data-href="中国型牙齿" href="术语\中国型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">中国型牙齿</a>。<br><br>可能<a data-href="巽他型牙齿" href="术语\巽他型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">巽他型牙齿</a>就是组合2，<a data-href="中国型牙齿" href="术语\中国型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">中国型牙齿</a>就是组合1。<br>后来出现了<a data-href="遗传漂变（genetic drift）" href="术语\遗传漂变（genetic-drift）.html" class="internal-link" target="_self" rel="noopener nofollow">遗传漂变（genetic drift）</a>。<br><br><br><a data-href="贾湖遗址" href="术语\贾湖遗址.html" class="internal-link" target="_self" rel="noopener nofollow">贾湖遗址</a>距今9000-7800年，在河南舞阳，这里有骨笛存在。<a data-href="新仙女木期（Younger Dryas）" href="术语\新仙女木期（younger-dryas）.html" class="internal-link" target="_self" rel="noopener nofollow">新仙女木期（Younger Dryas）</a>的降温事件使得食物变成了很大的问题，于是智人开始采集。<br>我们需要明确一个观点：古人在有吃有喝的情况下不会选择去吃不好吃的农作，而会选择吃肉。<br>粟和黍的起源地可能不一样。粟最早出现在<a data-href="中国北方" href="术语\中国北方.html" class="internal-link" target="_self" rel="noopener nofollow">中国北方</a>的深林草原过度地带，而黍在稍微偏南一点的地方，例如<a data-href="太行山" href="太行山" class="internal-link" target="_self" rel="noopener nofollow">太行山</a>和<a data-href="燕山山脉" href="燕山山脉" class="internal-link" target="_self" rel="noopener nofollow">燕山山脉</a>。<br>
河北南部有一个<a data-href="磁山遗址" href="术语\磁山遗址.html" class="internal-link" target="_self" rel="noopener nofollow">磁山遗址</a>。换句话说，万年前的<a data-href="中国北方" href="术语\中国北方.html" class="internal-link" target="_self" rel="noopener nofollow">中国北方</a>古人吃黄米，后来才吃小米。<br>
农作物的驯化是一个漫长的过程。<br>
<a data-href="菽（Glycine max）" href="术语\菽（glycine-max）.html" class="internal-link" target="_self" rel="noopener nofollow">菽（Glycine max）</a>也是北方的另一个重要农作物。<br>
英国考古学家发现在距今<br>
<br>7000年的欧洲到黑海到东欧和中欧的多个地点都发现了<a data-href="黍（Panicum miliaceum）" href="术语\黍（panicum-miliaceum）.html" class="internal-link" target="_self" rel="noopener nofollow">黍（Panicum miliaceum）</a>，
<br>距今5000年欧洲又发现了<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>，距今3000年<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>种植明显增加，可能是经过草原传到了欧洲。
<br>韩国距今4500年发现了<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>，日本距今4000年发现了<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>。<br>
有可能<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>传播到印度。
<br><br>在中国长江流域的湖南的<a data-href="城头山遗址" href="城头山遗址" class="internal-link" target="_self" rel="noopener nofollow">城头山遗址</a>，发现了8000年前的的人工栽培<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>，以及灌溉系统。<br>
在江西北部的<a data-href="吊桶环遗址" href="吊桶环遗址" class="internal-link" target="_self" rel="noopener nofollow">吊桶环遗址</a>，发现了距今12000年前的野生<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>和10000年前的栽培<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>硅石。<br>
湖南北部的<a data-href="玉蟾岩遗址" href="玉蟾岩遗址" class="internal-link" target="_self" rel="noopener nofollow">玉蟾岩遗址</a>，发现了18000年和14000年前的驯化<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>，为世界最早。<br>
所以，我们认为，<a data-href="中国南方" href="术语\中国南方.html" class="internal-link" target="_self" rel="noopener nofollow">中国南方</a>才是<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>的起源地。<br><a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>分为2种：<br><br>最早出现的应该是<a data-href="粳稻" href="术语\粳稻.html" class="internal-link" target="_self" rel="noopener nofollow">粳稻</a>，出现在<a data-href="河姆渡遗址" href="术语\河姆渡遗址.html" class="internal-link" target="_self" rel="noopener nofollow">河姆渡遗址</a>，后来在印度恒河流域发现了<a data-href="籼稻" href="术语\籼稻.html" class="internal-link" target="_self" rel="noopener nofollow">籼稻</a>，之后中国的<a data-href="粳稻" href="术语\粳稻.html" class="internal-link" target="_self" rel="noopener nofollow">粳稻</a>传入印度，与<a data-href="籼稻" href="术语\籼稻.html" class="internal-link" target="_self" rel="noopener nofollow">籼稻</a>杂交，完成了驯化。<br>但是<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>真正开始大规模成为食物应该是在<a data-href="良渚文化" href="术语\良渚文化.html" class="internal-link" target="_self" rel="noopener nofollow">良渚文化</a>。之后向四面八方辐射。<br><br><a data-href="贾湖遗址" href="术语\贾湖遗址.html" class="internal-link" target="_self" rel="noopener nofollow">贾湖遗址</a>有饲养猪，吃猪肉的习惯。距今8000年前，就在<a data-href="黄河流域" href="黄河流域" class="internal-link" target="_self" rel="noopener nofollow">黄河流域</a>和<a data-href="长江流域" href="长江流域" class="internal-link" target="_self" rel="noopener nofollow">长江流域</a>，出现了家猪饲养。<br><a data-href="黄河流域" href="黄河流域" class="internal-link" target="_self" rel="noopener nofollow">黄河流域</a>的家猪应该是单一驯化中心起源。应该在<a data-href="贾湖遗址" href="术语\贾湖遗址.html" class="internal-link" target="_self" rel="noopener nofollow">贾湖遗址</a>附近。<br>
由于<a data-href="长江流域" href="长江流域" class="internal-link" target="_self" rel="noopener nofollow">长江流域</a>水产资源丰富，养猪始终不多，所以在5000年前，形成了南鱼北猪的局面。<br>后来，<a data-href="良渚文化" href="术语\良渚文化.html" class="internal-link" target="_self" rel="noopener nofollow">良渚文化</a>打破了这种局面，养猪开始多了起来，多半是因为人口多了，鱼类不满足了。<br><br>天时：全新世大暖期。也被称为<a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a>温暖期。<br>
地利：当时的黄土高原自然环境优越，非常适合农作物生长。<br>
人和：农具的发明和野生品种的驯化。<br>这段时间中国大地上面人口爆炸式增长。<br>
<a href=".?query=tag:excalidraw" class="tag" target="_blank" rel="noopener nofollow">#excalidraw</a> <a data-href="5000-4000年中原文化" href="5000-4000年中原文化" class="internal-link" target="_self" rel="noopener nofollow">5000-4000年中原文化</a>。<br>但是在农耕文明这段期间，大规模人群之间的基因交流并没有增加，因为农民要长期守着自己的田地。<br>古人类开荒的时候，可能选择放火烧山，变成耕地，二氧化碳上升，温室效应，温度升高，农作物吸收二氧化碳，产量上升。<br>
但是农业的发展也造成了疾病的传播，因为长期聚集在一个地方。和家禽混居。<br><br><br><a data-href="石峁遗址" href="术语\石峁遗址.html" class="internal-link" target="_self" rel="noopener nofollow">石峁遗址</a>距今4300年，延续了300年左右之后遗弃。<br>
充满了军事和暴力的色彩。而且还有壁画和起稿线。<br>
而且还有一些血腥的祭祀活动，以女性居多。<br>
他们应该是在防御来自北方草原的敌人。<br><br>冰河时代，祖先已经开始捕猎野马了。<br>
最早的马可能是<a data-href="哈萨克斯坦" href="哈萨克斯坦" class="internal-link" target="_self" rel="noopener nofollow">哈萨克斯坦</a>的，距今6000-5500年。<br>
距今5000年，骑马的游牧群体就开始向欧亚大陆四面八方扩散。草原人群开始突袭农耕社会的村镇。<br>
认为：游牧是从农耕社会衍生出来的而不是渔猎方式过渡来的，毕竟，驯化大型动物需要以农耕的粮食为基础。<br>中国的马是在<a data-href="商朝" href="商朝" class="internal-link" target="_self" rel="noopener nofollow">商朝</a>晚期突然出现的，距今3000年。<br>
换句话说，中国缺乏把野马驯化的过程。<br>那么，我们可以回答，<a data-href="石峁遗址" href="术语\石峁遗址.html" class="internal-link" target="_self" rel="noopener nofollow">石峁遗址</a>的古城是在防御日益强大的游牧民族。<br>
证据：<a data-href="石峁遗址" href="术语\石峁遗址.html" class="internal-link" target="_self" rel="noopener nofollow">石峁遗址</a>的头骨和其他区域的古人遗骨有明显的区别，说明不是自家人。祭祀坑里面的死者与<a data-href="夏家店下层文化" href="夏家店下层文化" class="internal-link" target="_self" rel="noopener nofollow">夏家店下层文化</a>的关系比较接近，证明他们的家乡在草原。<br>那么<a data-href="石峁遗址" href="术语\石峁遗址.html" class="internal-link" target="_self" rel="noopener nofollow">石峁遗址</a>的自家人是什么呢？证据显示，其与<a data-href="陶寺文化" href="术语\陶寺文化.html" class="internal-link" target="_self" rel="noopener nofollow">陶寺文化</a>人群关系密切，这些人群大概是<a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a>温暖期中的农耕社会在人口膨胀之后的人群。<br><br><a data-href="西亚" href="西亚" class="internal-link" target="_self" rel="noopener nofollow">西亚</a>地区是冶金术最早的地区，因为那里有最早的红铜制品。距今9000年。<br>
距今6000年的时候，砷铜出现在<a data-href="西亚" href="西亚" class="internal-link" target="_self" rel="noopener nofollow">西亚</a>的<a data-href="安纳托利亚农业人群（Anatolian farmer）" href="术语\安纳托利亚农业人群（anatolian-farmer）.html" class="internal-link" target="_self" rel="noopener nofollow">安纳托利亚农业人群（Anatolian farmer）</a>高原并风靡周边。<br>
距今5000年的古巴比伦，出现了青铜。<br>
在<a data-href="甘肃" href="甘肃" class="internal-link" target="_self" rel="noopener nofollow">甘肃</a>中南部发现了一把青铜刀，距今5000年，但是只有一把，所以可能是带过来的。<br>真正的冶炼应该是在4000年前的<a data-href="甘肃" href="甘肃" class="internal-link" target="_self" rel="noopener nofollow">甘肃</a>中东部和<a data-href="青海" href="青海" class="internal-link" target="_self" rel="noopener nofollow">青海</a>东北部一级<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a>和<a data-href="宁夏" href="术语\宁夏.html" class="internal-link" target="_self" rel="noopener nofollow">宁夏</a>的<a data-href="齐家文化" href="术语\齐家文化.html" class="internal-link" target="_self" rel="noopener nofollow">齐家文化</a>。<br>
后来中原出现了更强大的<a data-href="二里头文化" href="术语\二里头文化.html" class="internal-link" target="_self" rel="noopener nofollow">二里头文化</a>，在青铜更进一步。青铜只配贵族享有，周朝的时候，建立了公侯伯子男等。<a data-href="齐家文化" href="术语\齐家文化.html" class="internal-link" target="_self" rel="noopener nofollow">齐家文化</a>可能对<a data-href="二里头文化" href="术语\二里头文化.html" class="internal-link" target="_self" rel="noopener nofollow">二里头文化</a>有影响。<br>但是<a data-href="二里头文化" href="术语\二里头文化.html" class="internal-link" target="_self" rel="noopener nofollow">二里头文化</a>并没有把青铜器作为武器。<br>青铜是怎么传入中国的？可能是通过新疆，也可能是通过草原。考古学家认为，多半是通过的草原。<br><br><a data-href="中华文明西来说" href="中华文明西来说" class="internal-link" target="_self" rel="noopener nofollow">中华文明西来说</a>和<a data-href="中华文明本土说" href="术语\中华文明本土说.html" class="internal-link" target="_self" rel="noopener nofollow">中华文明本土说</a>：发现中华的彩陶文化的出现，有东面早，西面晚的现象，这个不符合西来说。<br>
但是从另一个方面讲，在距今8000年的西亚，就已经出现了彩陶，根据地图，越是原理伊朗、伊拉克，彩陶时间越晚。<br>为什么呢？因为路线错了。虽然中国的彩陶受到了西方的激发，但是最早不是从<a data-href="新疆" href="术语\新疆.html" class="internal-link" target="_self" rel="noopener nofollow">新疆</a>和<a data-href="甘肃" href="甘肃" class="internal-link" target="_self" rel="noopener nofollow">甘肃</a>过来的，而是从草原来的。从<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a>经过<a data-href="阴山山脉" href="阴山山脉" class="internal-link" target="_self" rel="noopener nofollow">阴山山脉</a>南下而来。<br>古代有<a data-href="居延海" href="术语\居延海.html" class="internal-link" target="_self" rel="noopener nofollow">居延海</a>和<a data-href="额济纳河（黑河，弱水）" href="术语\额济纳河（黑河，弱水）.html" class="internal-link" target="_self" rel="noopener nofollow">额济纳河（黑河，弱水）</a>，沟通了南北，沟通了<a data-href="青藏高原" href="术语\青藏高原.html" class="internal-link" target="_self" rel="noopener nofollow">青藏高原</a>北部，<a data-href="甘肃" href="甘肃" class="internal-link" target="_self" rel="noopener nofollow">甘肃</a><a data-href="河西走廊" href="河西走廊" class="internal-link" target="_self" rel="noopener nofollow">河西走廊</a>以及<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a>，当时动植物资源丰富。<br>青铜和彩陶的路线很像，都是从<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a>南下然后分成东西。<br><a data-href="二里头遗址" href="术语\二里头遗址.html" class="internal-link" target="_self" rel="noopener nofollow">二里头遗址</a>还有海北，上面有穿孔。刚开始的时候，人们认为其时中国的近海，但是却揭示这些贝壳属于暖水品种，生活在印度洋和中国南海的热带还与，绝对不会再中国的东海。所以应该是印度经过欧亚草原最终向南抵达中原。<br><br>最早的<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>起源<a data-href="西亚" href="西亚" class="internal-link" target="_self" rel="noopener nofollow">西亚</a>，距今10000年。距今7000年，小麦就达到了中亚，但是迟迟没有进入<a data-href="东亚" href="东亚" class="internal-link" target="_self" rel="noopener nofollow">东亚</a>。原因在于<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>的植物特性。它比较适合地中海气候，但是中国是季风气候，所以迟迟没有进入东亚。<br>
最终，在距今5200年进入了<a data-href="新疆" href="术语\新疆.html" class="internal-link" target="_self" rel="noopener nofollow">新疆</a>的<a data-href="阿尔泰山脉" href="术语\阿尔泰山脉.html" class="internal-link" target="_self" rel="noopener nofollow">阿尔泰山脉</a>。<br>
总之，距今3000年小麦一定已经传入了中原。<br>
传入中原之后，迅速发展。<br><a data-href="渭河" href="渭河" class="internal-link" target="_self" rel="noopener nofollow">渭河</a>是黄河的重要分支，是重要的农耕区。东周时期，<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>的产量非常高。秦汉时期，奠定了南稻北麦的局面。<br><br>距今10000年，绵羊就出现在了西亚。距今5200年，草原有了绵羊。所以大概是5000年前从草原传输带过来的。<br>牛包括：<br>
<br>黄牛

<br>普通牛：距今11000年在<a data-href="西亚" href="西亚" class="internal-link" target="_self" rel="noopener nofollow">西亚</a>北驯服。
<br>瘤牛：<a data-href="南亚" href="南亚" class="internal-link" target="_self" rel="noopener nofollow">南亚</a>的<a data-href="印度" href="印度" class="internal-link" target="_self" rel="noopener nofollow">印度</a>河谷驯化的，距今9000年左右。


<br>牦牛：略晚于普通牛，在<a data-href="中亚" href="中亚" class="internal-link" target="_self" rel="noopener nofollow">中亚</a>被驯服。
<br>水牛
<br>距今4500年，<a data-href="黄河流域" href="黄河流域" class="internal-link" target="_self" rel="noopener nofollow">黄河流域</a>出现了黄牛的迹象。因此有可能是欧亚草原过来的。中华大地上的第一批黄牛吃碳四类，也就是<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>和<a data-href="黍（Panicum miliaceum）" href="术语\黍（panicum-miliaceum）.html" class="internal-link" target="_self" rel="noopener nofollow">黍（Panicum miliaceum）</a>。<br>
黄牛除了通过欧亚草原传输带，还通过云南。<br><br><a data-href="中华文明本土说" href="术语\中华文明本土说.html" class="internal-link" target="_self" rel="noopener nofollow">中华文明本土说</a>有一个极端，<a data-href="中华文明西来说" href="中华文明西来说" class="internal-link" target="_self" rel="noopener nofollow">中华文明西来说</a>有另一个极端。要么认为一些物种的进出就决定了文明的走向，要么犯了<a data-href="塔岛技术悲剧" href="术语\塔岛技术悲剧.html" class="internal-link" target="_self" rel="noopener nofollow">塔岛技术悲剧</a>的原理。<br>远古文明的衰落应该2方面的原因：<br>
<br>人口增长
<br>气候变异
<br>最终导致了满天星斗到月明星稀。<br><br>认为最早的中国就是<a data-href="二里头文化" href="术语\二里头文化.html" class="internal-link" target="_self" rel="noopener nofollow">二里头文化</a>。<br>
苏美尔人最初是外来户，白手起家，以宗教来树立权威。<br>
这一点有点像<a data-href="良渚文化" href="术语\良渚文化.html" class="internal-link" target="_self" rel="noopener nofollow">良渚文化</a>。<br><br><br><a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>位于<a data-href="新疆" href="术语\新疆.html" class="internal-link" target="_self" rel="noopener nofollow">新疆</a>的<a data-href="塔里木盆地（Tarim Basin）" href="术语\塔里木盆地（tarim-basin）.html" class="internal-link" target="_self" rel="noopener nofollow">塔里木盆地（Tarim Basin）</a>罗布泊附近，最早于1927年发现，但是因为战争，直到2000年。<br><a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>距今大约3980-3540年。可能是罗布泊的最早定居者。她的母系基因认为，欧亚大陆东部的人群，例如<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a><a data-href="贝加尔湖" href="术语\贝加尔湖.html" class="internal-link" target="_self" rel="noopener nofollow">贝加尔湖</a>一带，与西亚的人群在欧亚草原带相遇混合，之后南下，进入了罗布泊。<br>此外，<a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>的人群还有少量的<a data-href="南亚" href="南亚" class="internal-link" target="_self" rel="noopener nofollow">南亚</a><a data-href="中亚" href="中亚" class="internal-link" target="_self" rel="noopener nofollow">中亚</a><a data-href="伊朗" href="伊朗" class="internal-link" target="_self" rel="noopener nofollow">伊朗</a>地区的基因。<br>
<br>早期的基因型不多，但是慢慢地母系多样性越来越复杂。
<br>但是父系基因型却不同，普遍属于欧亚大陆西部的基因型，而且早期和晚期没什么区别。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 是联姻？还是掠夺？
<br><a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>有<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>和<a data-href="黍（Panicum miliaceum）" href="术语\黍（panicum-miliaceum）.html" class="internal-link" target="_self" rel="noopener nofollow">黍（Panicum miliaceum）</a>以及<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>，还有牛奶。这说明他们的西亚的基因使得其能够乳糖耐受。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 乳糖酶基因是否帮助这些人免疫力增强以至于拥有更大的影响力？<br>无论如何，<a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>的人群指向了另一批人，那就是<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>人群。<br>但是提到<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>，就必须先知道<a data-href="颜那亚文化（Yamna culture）" href="术语\颜那亚文化（yamna-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">颜那亚文化（Yamna culture）</a>。<br>距今5600年，草原带西部，诞生了<a data-href="颜那亚文化（Yamna culture）" href="术语\颜那亚文化（yamna-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">颜那亚文化（Yamna culture）</a>，这批草原人群掌握了青铜技术。这个文化的特点就是喜欢<a data-href="竖穴墓" href="术语\竖穴墓.html" class="internal-link" target="_self" rel="noopener nofollow">竖穴墓</a>。就是从地面直接向下挖出墓室，埋葬死者。<br>
距今4000年，<a data-href="颜那亚文化（Yamna culture）" href="术语\颜那亚文化（yamna-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">颜那亚文化（Yamna culture）</a>人群扩散到欧亚草原东部，在<a data-href="贝加尔湖" href="术语\贝加尔湖.html" class="internal-link" target="_self" rel="noopener nofollow">贝加尔湖</a>附近。<br><a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>人群应该继承了<a data-href="颜那亚文化（Yamna culture）" href="术语\颜那亚文化（yamna-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">颜那亚文化（Yamna culture）</a>的西方基因加上东方的基因，在<a data-href="阿尔泰山脉" href="术语\阿尔泰山脉.html" class="internal-link" target="_self" rel="noopener nofollow">阿尔泰山脉</a>附近诞生了。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202311142228377.png" referrerpolicy="no-referrer"><br><br><a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>人群面临来自西方和东方的威胁。<br><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202311142228255.png" referrerpolicy="no-referrer"><br><a data-href="奥库涅夫文化（Okunev Culture）" href="术语\奥库涅夫文化（okunev-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">奥库涅夫文化（Okunev Culture）</a>活跃在<a data-href="米努辛斯克盆地" href="米努辛斯克盆地" class="internal-link" target="_self" rel="noopener nofollow">米努辛斯克盆地</a>，位于南西伯利亚，靠近<a data-href="新疆" href="术语\新疆.html" class="internal-link" target="_self" rel="noopener nofollow">新疆</a>和<a data-href="内蒙古" href="内蒙古" class="internal-link" target="_self" rel="noopener nofollow">内蒙古</a>。相对比较易守难攻，是草原文化的摇篮。它们压制住了<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>人群，但是遇到了<a data-href="四坝文化（Siba culture）" href="术语\四坝文化（siba-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">四坝文化（Siba culture）</a>人群。<br>后来，距今3400年，<a data-href="米努辛斯克盆地" href="米努辛斯克盆地" class="internal-link" target="_self" rel="noopener nofollow">米努辛斯克盆地</a>孕育了强大的<a data-href="卡拉苏克文化（Karasuk culture）" href="术语\卡拉苏克文化（karasuk-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">卡拉苏克文化（Karasuk culture）</a>，后者居然与商朝有交流。而<a data-href="卡拉苏克文化（Karasuk culture）" href="术语\卡拉苏克文化（karasuk-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">卡拉苏克文化（Karasuk culture）</a>的强盛导致了商朝的灭亡。<br><a data-href="卡拉苏克文化（Karasuk culture）" href="术语\卡拉苏克文化（karasuk-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">卡拉苏克文化（Karasuk culture）</a>扩张，导致原本生活在<a data-href="阿尔泰山脉" href="术语\阿尔泰山脉.html" class="internal-link" target="_self" rel="noopener nofollow">阿尔泰山脉</a>附近的部落南下寻找生存空间，就挤压了古代<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>的生存空间，于是<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>只能够朝<a data-href="河西走廊" href="河西走廊" class="internal-link" target="_self" rel="noopener nofollow">河西走廊</a>传导，影响了<a data-href="甘肃" href="甘肃" class="internal-link" target="_self" rel="noopener nofollow">甘肃</a>和<a data-href="陕西" href="陕西" class="internal-link" target="_self" rel="noopener nofollow">陕西</a>的部落，其中就有周人。<br><br>周人只能往东南方向走，后来商朝与东夷作战，导致后方空虚，被周人偷家。<br>
从<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>南下直到周人克商朝，逃避不是办法。<br><br>古代世界的2大阵营，农耕社会和游牧社会，在4000-3000年前形成，比如雅利安人大扩张，灭了古印度，灭了古希腊。<br>后来东周“烽火戏诸侯”导致了自己的灭亡，其实体现了游牧民族的强大。<br>诗经·采薇：靡室靡家，猃狁之故，不遑启居，猃狁之故。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a>  猃狁是什么民族？<br>历史上的<a data-href="山戎" href="术语\山戎.html" class="internal-link" target="_self" rel="noopener nofollow">山戎</a>，也就是夏家店上层文化，被齐桓公给灭了。之后该地区形成了东胡。<br><br>距今4000年-2000年，欧亚大陆上面冲突不断。<br>有三种模式：<br>
<br>填空模式：哪里有空白，哪里就被占据。这种模式，一块新空间的种群基因是与他们的故乡种群一致，比较容易追踪。比如<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>南下就是这种模式。
<br>群体替代模式：当没有空白的时候，强大的族群回去消灭驱赶弱小的族群。例如<a data-href="奥库涅夫文化（Okunev Culture）" href="术语\奥库涅夫文化（okunev-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">奥库涅夫文化（Okunev Culture）</a>驱赶<a data-href="切穆尔切克文化（克尔木齐文化，Chemurchek）" href="术语\切穆尔切克文化（克尔木齐文化，chemurchek）.html" class="internal-link" target="_self" rel="noopener nofollow">切穆尔切克文化（克尔木齐文化，Chemurchek）</a>群体，<a data-href="卡拉苏克文化（Karasuk culture）" href="术语\卡拉苏克文化（karasuk-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">卡拉苏克文化（Karasuk culture）</a>驱赶<a data-href="阿尔泰山脉" href="术语\阿尔泰山脉.html" class="internal-link" target="_self" rel="noopener nofollow">阿尔泰山脉</a>部落的情况。
<br>上层更换模式：各个族群都有很多人口，有所了解。一个区域的基因会很复杂，底层广大群众的基因型不变，而上层统治者的基因型改变。变化的不是生物基因，而是文化基因。
<br><br><br><a data-href="尼雅遗址" href="尼雅遗址" class="internal-link" target="_self" rel="noopener nofollow">尼雅遗址</a>位于新疆的<a data-href="塔克拉玛干沙漠" href="塔克拉玛干沙漠" class="internal-link" target="_self" rel="noopener nofollow">塔克拉玛干沙漠</a>，距今2200-1500年。从西汉一直到南北朝。是古代中国，古印度，古希腊，古波斯的罕见交汇点。<br>
在该处的一个成年男性的DNA进行检测，发现其属于欧亚大陆西部的基因型，与伊朗和西亚很近。属于<a data-href="精绝国" href="精绝国" class="internal-link" target="_self" rel="noopener nofollow">精绝国</a>。<br><a data-href="于阗国" href="于阗国" class="internal-link" target="_self" rel="noopener nofollow">于阗国</a>遗址距今2200-1700年。从西汉到三国两晋。与<a data-href="尼雅遗址" href="尼雅遗址" class="internal-link" target="_self" rel="noopener nofollow">尼雅遗址</a>人群接近。<a data-href="于阗国" href="于阗国" class="internal-link" target="_self" rel="noopener nofollow">于阗国</a>与<a data-href="精绝国" href="精绝国" class="internal-link" target="_self" rel="noopener nofollow">精绝国</a>都属于<a data-href="塔里木盆地（Tarim Basin）" href="术语\塔里木盆地（tarim-basin）.html" class="internal-link" target="_self" rel="noopener nofollow">塔里木盆地（Tarim Basin）</a>的<a data-href="昆仑山" href="昆仑山" class="internal-link" target="_self" rel="noopener nofollow">昆仑山</a>下区域。<a data-href="于阗国" href="于阗国" class="internal-link" target="_self" rel="noopener nofollow">于阗国</a>曾经吞并了<a data-href="精绝国" href="精绝国" class="internal-link" target="_self" rel="noopener nofollow">精绝国</a>。<br><br>在<a data-href="精绝国" href="精绝国" class="internal-link" target="_self" rel="noopener nofollow">精绝国</a>里面找到了“五星出东方，利中国讨南羌”。<br>当时的<a data-href="昆仑山" href="昆仑山" class="internal-link" target="_self" rel="noopener nofollow">昆仑山</a>又被叫做南山。<br>
南羌大概就是<a data-href="昆仑山" href="昆仑山" class="internal-link" target="_self" rel="noopener nofollow">昆仑山</a>的游牧民族。南山羌可能是古代羌人的后代。古代<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人的基因应该是欧亚大陆东部的基因加上了小<a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>。<br><br><a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>的族缘是个谜。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 今天的技术能不能解决这个问题？<br>匈奴是很典型的上层更换模式。<a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>的基因型有欧亚大陆东部的成分，也有西部的成分，直到现在都无法归类。语言学家也一直争论，究竟是蒙古语系统还是突厥语系统。<br>
在秦朝，西边是<a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>，中间是<a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>，东边是<a data-href="东胡" href="东胡" class="internal-link" target="_self" rel="noopener nofollow">东胡</a>。<br>
<a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>可能是<a data-href="卡拉苏克文化（Karasuk culture）" href="术语\卡拉苏克文化（karasuk-culture）.html" class="internal-link" target="_self" rel="noopener nofollow">卡拉苏克文化（Karasuk culture）</a>的后代。而且<a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>可能与<a data-href="吐火罗语(Tocharian language)" href="术语\吐火罗语(tocharian-language).html" class="internal-link" target="_self" rel="noopener nofollow">吐火罗语(Tocharian language)</a>有很大的关系。<br><a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>被匈奴打败，一分为二。分为：<br>
<br>大月氏：西迁到中亚，建立了<a data-href="贵霜国" href="贵霜国" class="internal-link" target="_self" rel="noopener nofollow">贵霜国</a>。
<br>小月氏：从伊犁向南走入<a data-href="昆仑山" href="昆仑山" class="internal-link" target="_self" rel="noopener nofollow">昆仑山</a>，与当地的族群融合，成为了南山羌。
<br><a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>还打败了<a data-href="东胡" href="东胡" class="internal-link" target="_self" rel="noopener nofollow">东胡</a>，<a data-href="东胡" href="东胡" class="internal-link" target="_self" rel="noopener nofollow">东胡</a>向北躲进了鲜卑山，向南躲进了乌桓山。<br><br>虽然<a data-href="小河墓地（Xiaohe site）" href="术语\小河墓地（xiaohe-site）.html" class="internal-link" target="_self" rel="noopener nofollow">小河墓地（Xiaohe site）</a>的人群也生活在罗布泊，但是距今有4000-3500年，至于是不是<a data-href="楼兰国" href="楼兰国" class="internal-link" target="_self" rel="noopener nofollow">楼兰国</a>的前辈就不知道了。<br><a data-href="楼兰国" href="楼兰国" class="internal-link" target="_self" rel="noopener nofollow">楼兰国</a>最早是一封信中，距今2100年，本来是受到<a data-href="月氏国" href="术语\月氏国.html" class="internal-link" target="_self" rel="noopener nofollow">月氏国</a>统治，后来被<a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>统治。<br>但是到东晋时期，楼兰因为环境恶化就不再适合人居住了。<br><a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>和汉朝阻断了草原之路，于是丝绸之路变成了唯一的通道。<br><br><a data-href="黄河流域" href="黄河流域" class="internal-link" target="_self" rel="noopener nofollow">黄河流域</a>上游有一条<a data-href="湟水" href="湟水" class="internal-link" target="_self" rel="noopener nofollow">湟水</a>，曾是风水宝地。发现了距今3000年的<a data-href="卡约文化" href="卡约文化" class="internal-link" target="_self" rel="noopener nofollow">卡约文化</a>遗址。应该是属于古老的<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人的文化。它们的母系遗传特征很像<a data-href="云南" href="术语\云南.html" class="internal-link" target="_self" rel="noopener nofollow">云南</a>的现代<a data-href="普米族" href="普米族" class="internal-link" target="_self" rel="noopener nofollow">普米族</a>。<br><a data-href="湟水" href="湟水" class="internal-link" target="_self" rel="noopener nofollow">湟水</a>的另一处距今1900-1700年的遗址中也发现，与现代汉族，云南的<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>和<a data-href="纳西族" href="纳西族" class="internal-link" target="_self" rel="noopener nofollow">纳西族</a>也很像。<br>这说明<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人不是一个单一的民族，而是一个统称：<br>
<br>有一部分<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人融入了秦汉，
<br>但是另外一部分远走高飞，形成了现代的<a data-href="彝族" href="彝族" class="internal-link" target="_self" rel="noopener nofollow">彝族</a>，<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>，<a data-href="哈尼族" href="哈尼族" class="internal-link" target="_self" rel="noopener nofollow">哈尼族</a>，<a data-href="纳西族" href="纳西族" class="internal-link" target="_self" rel="noopener nofollow">纳西族</a><a data-href="普米族" href="普米族" class="internal-link" target="_self" rel="noopener nofollow">普米族</a>等等。
<br>还有一部分<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>去了<a data-href="青藏高原" href="术语\青藏高原.html" class="internal-link" target="_self" rel="noopener nofollow">青藏高原</a>。
<br>考古学发现：<a data-href="2019 NSR 新石器时代的粟（作）农（民）通过采用大麦农业为青藏高原的永久定居做出了贡献" href="文献及报道\文献\2024年阅读\1-6月\2019-nsr-新石器时代的粟（作）农（民）通过采用大麦农业为青藏高原的永久定居做出了贡献.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="👍" aria-label="👍" data-icon="👍" aria-hidden="true" style="transform: translateY(0px);"></span>2019 NSR 新石器时代的粟（作）农（民）通过采用大麦农业为青藏高原的永久定居做出了贡献</a><img class="emoji" draggable="false" alt="👍" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f44d.svg" height="18px" style="max-width: 100%;">，距今5200年-3600年前，携带<a data-href="粟（millet）" href="术语\粟（millet）.html" class="internal-link" target="_self" rel="noopener nofollow">粟（millet）</a>和<a data-href="黍（Panicum miliaceum）" href="术语\黍（panicum-miliaceum）.html" class="internal-link" target="_self" rel="noopener nofollow">黍（Panicum miliaceum）</a>的农民采用了抗寒的<a data-href="大麦" href="大麦" class="internal-link" target="_self" rel="noopener nofollow">大麦</a>，进入了<a data-href="青藏高原" href="术语\青藏高原.html" class="internal-link" target="_self" rel="noopener nofollow">青藏高原</a>。而且这群人应当与<a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a>人群很有关系。<br>距今3300年之前，<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>祖先人群：<br>
<br><a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人
<br><a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a>人群
<br>再往后，距今2000年，<a data-href="青藏高原" href="术语\青藏高原.html" class="internal-link" target="_self" rel="noopener nofollow">青藏高原</a>的人群经历了一次大规模的扩散，奠定了现代<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>人的基因型分布。<br>青藏高原在10000年-7000年前以及4000-3000年前经历过2次大规模的增长。<br>总而言之，<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>的祖先应该是<a data-href="仰韶文化" href="术语\仰韶文化.html" class="internal-link" target="_self" rel="noopener nofollow">仰韶文化</a>人群，融合了<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>人乃至中亚，北亚的少量族。到吐蕃时期，就已经和现在的藏族人群相似了。<br>但是并不知道<a data-href="丹尼索瓦人（Denisovan）" href="术语\丹尼索瓦人（denisovan）.html" class="internal-link" target="_self" rel="noopener nofollow">丹尼索瓦人（Denisovan）</a>的EPAS基因究竟是哪个人群带上去的？ <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 有没有办法解决这个问题？<br><a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>将这种特殊的基因传给了<a data-href="夏尔巴人" href="术语\夏尔巴人.html" class="internal-link" target="_self" rel="noopener nofollow">夏尔巴人</a>，基因研究发现，<a data-href="夏尔巴人" href="术语\夏尔巴人.html" class="internal-link" target="_self" rel="noopener nofollow">夏尔巴人</a>应该是<a data-href="藏族" href="术语\藏族.html" class="internal-link" target="_self" rel="noopener nofollow">藏族</a>人的一个比较晚的分支，距今大概1500年，当时正好是吐蕃诞生的前夕，高原上征讨严重。<br><br><br>生物学家利用5000-750年前的8个古代人群，分别是中原古代人群，山西<a data-href="陶寺文化" href="术语\陶寺文化.html" class="internal-link" target="_self" rel="noopener nofollow">陶寺文化</a>人群，秦始皇人群，西北古代，东北古代，古代<a data-href="鲜卑" href="鲜卑" class="internal-link" target="_self" rel="noopener nofollow">鲜卑</a>，蒙古<a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>人群。发现：<br>
<br>所有的古代人群都能够在当今的汉族人中找到共享的单倍型。说明都与现代汉人的祖先有关系。
<br>中原古代人群与现代北方汉族人群的基因型共享比例最高，其次分别是西北古代，<a data-href="陶寺文化" href="术语\陶寺文化.html" class="internal-link" target="_self" rel="noopener nofollow">陶寺文化</a>人群，东北古代，<a data-href="鲜卑" href="鲜卑" class="internal-link" target="_self" rel="noopener nofollow">鲜卑</a>和<a data-href="匈奴" href="术语\匈奴.html" class="internal-link" target="_self" rel="noopener nofollow">匈奴</a>。距离越近，年代越近，贡献越大，反之越小。
<br>有些考古学家认为，华和夏分别代表不同的古代人群。北方草原族群南下，南方族群北上，最终在中原相遇。认为华夏民族的但僧就是<a data-href="克里奥尔化" href="克里奥尔化" class="internal-link" target="_self" rel="noopener nofollow">克里奥尔化</a>的产物。<br><br>分子生物学家采集了148名客家男子的基因，发现，与<a data-href="汉族" href="汉族" class="internal-link" target="_self" rel="noopener nofollow">汉族</a>，<a data-href="畬族" href="畬族" class="internal-link" target="_self" rel="noopener nofollow">畬族</a>，<a data-href="侗族" href="侗族" class="internal-link" target="_self" rel="noopener nofollow">侗族</a>很像。他们大多数来自<a data-href="中原地区" href="术语\中原地区.html" class="internal-link" target="_self" rel="noopener nofollow">中原地区</a>。也有来自其他地方的。<br>
线粒体DNA分析发现，不同的客家族群的母系遗传不同。<br>对于<a data-href="汉族" href="汉族" class="internal-link" target="_self" rel="noopener nofollow">汉族</a>，从<a data-href="Y染色体" href="Y染色体" class="internal-link" target="_self" rel="noopener nofollow">Y染色体</a>来看：南方汉族男性和北方汉族男性的分化不明显。应该是北方汉族男性南迁形成的；<br>
从线粒体DNA来看：南方汉族女性和北方汉族女性分化很大，南方汉族的女性中来自南方其他族群的女性基因比继承北方汉族女性祖先的基因更多一些。<br>中国历史上南迁有3次：<br>
<br>西晋灭亡前后
<br>唐朝安史之乱
<br>辽金侵宋
<br>南迁以男性为主。<br><br><a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>大规模西南，应该是在秦穆公时期，他向西扩张，导致<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>奔逃到<a data-href="四川" href="四川" class="internal-link" target="_self" rel="noopener nofollow">四川</a><a data-href="云南" href="术语\云南.html" class="internal-link" target="_self" rel="noopener nofollow">云南</a>等地。而且根据线粒体DNA的数据，南迁的时候应该不存在性别差异。<br>由于女性可以通过嫁入，男性很难融入，所以形成了男客女主的局面。<br><br>在<a data-href="氐羌（古）" href="术语\氐羌（古）.html" class="internal-link" target="_self" rel="noopener nofollow">氐羌（古）</a>南迁之前，南方就已经有了一些文化，例如湖南的<a data-href="梅山文化（蚩尤文化）" href="术语\梅山文化（蚩尤文化）.html" class="internal-link" target="_self" rel="noopener nofollow">梅山文化（蚩尤文化）</a>。<br>比较大的文化就是<a data-href="百越族群" href="术语\百越族群.html" class="internal-link" target="_self" rel="noopener nofollow">百越族群</a>，位于浙江上海等地；还有<a data-href="百濮族群" href="百濮族群" class="internal-link" target="_self" rel="noopener nofollow">百濮族群</a>，在<a data-href="中国西南" href="术语\中国西南.html" class="internal-link" target="_self" rel="noopener nofollow">中国西南</a>地区。<br>也许<a data-href="百越族群" href="术语\百越族群.html" class="internal-link" target="_self" rel="noopener nofollow">百越族群</a>含有更多的海岸暴走族的基因，而<a data-href="百濮族群" href="百濮族群" class="internal-link" target="_self" rel="noopener nofollow">百濮族群</a>含有内陆迁徙的基因。<br>而且现代东北亚人和<a data-href="日本" href="术语\日本.html" class="internal-link" target="_self" rel="noopener nofollow">日本</a>人体内有与<a data-href="百越族群" href="术语\百越族群.html" class="internal-link" target="_self" rel="noopener nofollow">百越族群</a>相似的基因，说明可能是海岸暴走族带过去的。<br>另外，<a data-href="台湾" href="术语\台湾.html" class="internal-link" target="_self" rel="noopener nofollow">台湾</a>古人应该来自<a data-href="中国南方" href="术语\中国南方.html" class="internal-link" target="_self" rel="noopener nofollow">中国南方</a>，因为台湾的一些少数民族体内有一种特殊基因，这种基因广泛分布于中国大陆包括<a data-href="陶寺文化" href="术语\陶寺文化.html" class="internal-link" target="_self" rel="noopener nofollow">陶寺文化</a>群体内还有沿海汉族以及<a data-href="苗瑶语（Hmong–Mien languages）" href="术语\苗瑶语（hmong–mien-languages）.html" class="internal-link" target="_self" rel="noopener nofollow">苗瑶语（Hmong–Mien languages）</a>群体。但是在东南亚体内没有。这种基因出现在距今2万6千年，应该是智人进入中国大陆后出现的。<br>南下族群的<a data-href="Y染色体" href="Y染色体" class="internal-link" target="_self" rel="noopener nofollow">Y染色体</a>与语言有明显的同步现象，但是<a data-href="线粒体DNA（mtDNA）" href="术语\线粒体dna（mtdna）.html" class="internal-link" target="_self" rel="noopener nofollow">线粒体DNA（mtDNA）</a>却缺乏这种现象，这正是本章所探讨的男客女主的观点。语言和文化石不同族群男性固守的根脉，很难融入其他群体，但是不同族群的女性却可以跨越族群融合。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 这或许可以解释为什么现在大多数研究没有涉及<a data-href="线粒体DNA（mtDNA）" href="术语\线粒体dna（mtdna）.html" class="internal-link" target="_self" rel="noopener nofollow">线粒体DNA（mtDNA）</a>与语言的相关性。<br><br><br>科学家进行基因分析，发现南岛人来自东亚。<br><a data-href="拉皮塔文化" href="术语\拉皮塔文化.html" class="internal-link" target="_self" rel="noopener nofollow">拉皮塔文化</a>是大洋洲及太平洋地区的一种<a data-href="新石器时代（Neolithic）" href="术语\新石器时代（neolithic）.html" class="internal-link" target="_self" rel="noopener nofollow">新石器时代（Neolithic）</a>文化，分布<a data-href="美拉尼西亚" href="术语\美拉尼西亚.html" class="internal-link" target="_self" rel="noopener nofollow">美拉尼西亚</a>群岛等。距今3500年左右。学者们把这种文化的扩张称作快车模式。认为，<a data-href="台湾" href="术语\台湾.html" class="internal-link" target="_self" rel="noopener nofollow">台湾</a>是<a data-href="南岛语系（Austroasiatic language family）" href="术语\南岛语系（austroasiatic-language-family）.html" class="internal-link" target="_self" rel="noopener nofollow">南岛语系（Austroasiatic language family）</a>重要的发源地。<br><a data-href="南岛语系（Austroasiatic language family）" href="术语\南岛语系（austroasiatic-language-family）.html" class="internal-link" target="_self" rel="noopener nofollow">南岛语系（Austroasiatic language family）</a>人群祖先应该是生活在中国的浙江一带，单后据今5900年开始向南方走。<a data-href="原南岛人的形成：来自父系创始人谱系修订后的见解" href="文献及报道\文献\2023年阅读\原南岛人的形成：来自父系创始人谱系修订后的见解.html" class="internal-link" target="_self" rel="noopener nofollow">原南岛人的形成：来自父系创始人谱系修订后的见解</a>。<a data-href="早期南岛人：进出台湾" href="文献及报道\文献\2023年阅读\早期南岛人：进出台湾.html" class="internal-link" target="_self" rel="noopener nofollow">早期南岛人：进出台湾</a>。<br>在中国的江浙一带的<a data-href="壮侗语（Kra–Dai languages）" href="术语\壮侗语（kra–dai-languages）.html" class="internal-link" target="_self" rel="noopener nofollow">壮侗语（Kra–Dai languages）</a>族群一部分进入<a data-href="台湾" href="术语\台湾.html" class="internal-link" target="_self" rel="noopener nofollow">台湾</a>，然后南下<a data-href="菲律宾" href="菲律宾" class="internal-link" target="_self" rel="noopener nofollow">菲律宾</a>和<a data-href="印度尼西亚" href="印度尼西亚" class="internal-link" target="_self" rel="noopener nofollow">印度尼西亚</a>，形成了<a data-href="拉皮塔文化" href="术语\拉皮塔文化.html" class="internal-link" target="_self" rel="noopener nofollow">拉皮塔文化</a>，然后向太平洋扩散；另一部分进入<a data-href="中南半岛" href="中南半岛" class="internal-link" target="_self" rel="noopener nofollow">中南半岛</a>，再进入<a data-href="印度尼西亚" href="印度尼西亚" class="internal-link" target="_self" rel="noopener nofollow">印度尼西亚</a>的西部群岛，形成<a data-href="马来人群" href="马来人群" class="internal-link" target="_self" rel="noopener nofollow">马来人群</a>。距今2300年，马来人群与古巴布亚人进行交流，然后向太平洋扩散。<br><a data-href="南岛语系（Austroasiatic language family）" href="术语\南岛语系（austroasiatic-language-family）.html" class="internal-link" target="_self" rel="noopener nofollow">南岛语系（Austroasiatic language family）</a>群体向东南亚，其实是衣锦还乡，因为本身东亚人有部分就是海岸暴走族的后代，他们到达东南亚，那里也有海岸暴走族的的后代。<br><br>日本在3万年前就有人类活动的迹象了。那个时期刚好是<a data-href="山顶洞人" href="术语\山顶洞人.html" class="internal-link" target="_self" rel="noopener nofollow">山顶洞人</a>活动的日子，有可能是冰河时代迁移过来的草原狩猎族。但是没找到遗骸。<br>距今1万8千年在琉球群岛发现了古人类但是没有基因测序。<br>距今1万6千年，日本列岛来了人群，因为可以制造草绳样的花纹的陶器，称为<a data-href="绳文人" href="术语\绳文人.html" class="internal-link" target="_self" rel="noopener nofollow">绳文人</a>。陆续出土的<a data-href="绳文人" href="术语\绳文人.html" class="internal-link" target="_self" rel="noopener nofollow">绳文人</a>证明了，在日本列岛相对封闭的环境是独立进化的。<br>距今2300年，<a data-href="弥生人" href="术语\弥生人.html" class="internal-link" target="_self" rel="noopener nofollow">弥生人</a>出现了。<a data-href="绳文人" href="术语\绳文人.html" class="internal-link" target="_self" rel="noopener nofollow">绳文人</a>主要是<a data-href="巽他型牙齿" href="术语\巽他型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">巽他型牙齿</a>，而<a data-href="弥生人" href="术语\弥生人.html" class="internal-link" target="_self" rel="noopener nofollow">弥生人</a>主要是<a data-href="中国型牙齿" href="术语\中国型牙齿.html" class="internal-link" target="_self" rel="noopener nofollow">中国型牙齿</a>。<br>关于<a data-href="弥生人" href="术语\弥生人.html" class="internal-link" target="_self" rel="noopener nofollow">弥生人</a>究竟来自中国南方还是北方，也一直有争论。因为他们是种植<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>的，而<a data-href="稻（Oryza sativa L.）" href="术语\稻（oryza-sativa-l.）.html" class="internal-link" target="_self" rel="noopener nofollow">稻（Oryza sativa L.）</a>没有经过朝鲜半岛，所以有可能是直接跨海到达日本。但是因为他们也种植小麦，这段时期长江流域没有<a data-href="小麦" href="小麦" class="internal-link" target="_self" rel="noopener nofollow">小麦</a>，所以应该是中国北方输入的。 <a href=".?query=tag:想法" class="tag" target="_blank" rel="noopener nofollow">#想法</a> 究竟来自哪里，或是两者兼有？<br><br><a data-href="美洲" href="术语\美洲.html" class="internal-link" target="_self" rel="noopener nofollow">美洲</a>原住民应该是通过<a data-href="白令海峡（Bering Strait）" href="术语\白令海峡（bering-strait）.html" class="internal-link" target="_self" rel="noopener nofollow">白令海峡（Bering Strait）</a>进入美洲的，<a data-href="2023 CR 线粒体基因组证明两次扩张事件以及母系祖先从中国北部沿海向美洲和日本的扩散" href="文献及报道\文献\2024年阅读\1-6月\2023-cr-线粒体基因组证明两次扩张事件以及母系祖先从中国北部沿海向美洲和日本的扩散.html" class="internal-link" target="_self" rel="noopener nofollow"><span class="iconize-icon-in-link" title="😀" aria-label="😀" data-icon="😀" aria-hidden="true" style="transform: translateY(0px);"></span>2023 CR 线粒体基因组证明两次扩张事件以及母系祖先从中国北部沿海向美洲和日本的扩散</a><img class="emoji" draggable="false" alt="😀" src="https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f600.svg" height="18px" style="max-width: 100%;">。<br>
应该是草原狩猎族的后代。<br>
但是美洲的原住民应该不止草原狩猎族，而应该还有海岸暴走族，因为<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>和<a data-href="南美洲（South America）" href="术语\南美洲（south-america）.html" class="internal-link" target="_self" rel="noopener nofollow">南美洲（South America）</a>的<a data-href="亚马孙人" href="亚马孙人" class="internal-link" target="_self" rel="noopener nofollow">亚马孙人</a>很像，另外，<a data-href="亚马孙人" href="亚马孙人" class="internal-link" target="_self" rel="noopener nofollow">亚马孙人</a>居然和大洋洲的原住民有2%的基因。说明海岸暴走族后来变成了<a data-href="田园洞人" href="术语\田园洞人.html" class="internal-link" target="_self" rel="noopener nofollow">田园洞人</a>，大洋洲原住民，以及<a data-href="南美洲（South America）" href="术语\南美洲（south-america）.html" class="internal-link" target="_self" rel="noopener nofollow">南美洲（South America）</a>部分原住民。<br><br><br>最远古的一层是第零祖先层， 也就是直立人的时代。中华大地最早的直立人来自非洲， 他们经由西亚、中亚一路而来。他们来到东亚的时候，可能已经步入直立人发展的后期了， 而且他们很可能是从北方地区进入中国的，周口店<a data-href="北京猿人" href="术语\北京猿人.html" class="internal-link" target="_self" rel="noopener nofollow">北京猿人</a>就是中国直立人的典型代表， 甚至可能是最早的那批中国古人类之一。他们不是我们的直系祖先， 但他们的到来本身就表示， 即使在古人类石器技术如此落后的时代，他们也能够跨越亚洲大陆山重水复的地理阻隔。中华大地从有古人类的那一刻起， 就与全世界建立了联系， 只是联系有时如滔滔洪流，有时又细若游丝。<br>东亚的直立人时代， 即是欧亚大陆东西方的基因与技术交流细若游丝的日子。东亚直立人群体仿佛陷入了一个巨大的孤岛中，与外界特别是欧亚非大陆的西部严重缺乏交流， 几乎只能依靠最初带过来的粗糙石器技术， 加以若干本土化技术改进， 在几十万年中孤独地艰难生活， 最终他们全部消失在历史的长河， 他们的基因之河断流了。正是因为他们的基因没有留传给我们，所以我用“第零祖先层”来描述他们。<br>
之后从距今5万年前起，东亚古人类进入第一祖先层， 那是智人扩张的时代。东亚的智人扩张时代是从南方拉开序幕的，然后南方与北方的现代智人祖先联袂上演了跷跷板式的迁徙大戏，中华大地变得热闹起来。智人个体和群体的智慧赋予他们卓越的迁徙能力，不论是面对广阔的大海还是宏伟的山川， 他们都有足够的技术和勇气去跨越阻隔，打破孤岛之魔咒。<br>在智人扩张时代，大陆上的地理阻隔已经失效，面对近乎空空荡荡的原野、丛林、山岭甚至近海岛屿，古老祖先的迁徙奏响了“填空模式”， 智人的基因之河肆意漫流，冲向地球上每个适宜生存的角落。具体到中华大地上，他们或沿着海岸线暴走，从热带的南海海岸北进到温带的渤海湾，并深入陆地；或循着草原带的野兽脚印狩猎而来， 从西到东席卷北方的山岗与平原……<br>严酷的盛冰期从距今2.65万年前一直持续到距今1.9万年前，这是我们古老祖先艰难的一段萎缩期。北方古老人群损失惨重，也许有少部分人群向南转移退却；南方古老人群则利用海平面降低带来的大陆与岛屿连成一片的有利条件，休养生息，养精蓄锐。<br>熬过了盛冰期后，第二祖先层掀开了， 在南方蛰伏了数千年的祖先们抓住温暖间冰期的大好机会，挥动着长矛向北挺进，追逐繁盛的野生动物，采摘满山的野果野菜。第二祖先层的古人类迁徙方向，主要是自南向北的， 他们的扩张过程反映到基因型上， 就造成了今日南方人群基因型多于北方人群基因型的面貌。<br>从百万年前的第零祖先层到第一祖先层、第二祖先层，我们的祖先基本上以狩猎、采集、捕鱼为生， 这样的谋生手段驱动着他们走南闯北。从距今1万年前开始，我们的祖先进入了第三祖先层，他们的谋生手段有了翻天覆地的变化——农耕时代来临了。<br>东亚的农耕时代绚丽且魔幻，不论南方还是北方， 中华大地似乎被施加了丰饶的魔法， 魔法棒点拨之处， 各种农作物和驯化动物纷纷涌现。那高超的魔法师不是别人，正是我们智慧的祖先， 第一批从事农业的祖先。他们在末次冰期“黎明前的黑暗”中对动植物驯化的各种尝试，在冰期结束后激发了改天换地的农业革命。<br>农耕时代也是人口暴增的时代，伴随着粮食产量激增， 大地上的人口快速增长， 中华大地上的人们第一次感受到了什么叫拥挤。一拨又一拨的人群带上小米、黄米或大米种子，赶着猪、狗、鸡、鸭， 前往适合农耕的新天地，把东西南北广阔的大地变成农耕魔法的舞台。<br>农耕人口相对于狩猎采集人口在数量上具有压倒性优势，反映在基因型种类上也是压倒性优势。从基因扩张的角度讲，这个时代的人群迁徙主要是以“群体替代模式”展开的，尤其是农耕群体对于狩猎采集人群的替代，虽然彼此同属于现代智人之列。所以， 农耕的传播意味着农耕人口基因的广泛传播和对少量的狩猎采集人群基因的吸收。<br>距今4 000年前以来的较为晚近的祖先们，可以统一归入第四祖先层， 那是属于文化基因传播的时代。东亚的文化基因时代在文明的钟鼓礼乐声中翩翩而至。我们的祖先以中华大地丰厚的农耕社会为基底， 海纳百川地接纳了来自大陆西方的各种物产与技术，尤其是对国家制度建立与维系至关重要的青铜技术、马车技术，然后再加以创新，终于奏响了中华大地独特的文明古国序曲。<br>文明的序曲并非中原地区的独奏，而是大江南北各个文明政权的合奏。每个政权固然是以某个族群为核心建立起来的，但在族群交融如此频繁和充分的环境中，真正区分不同政权的并非族群的生物基因， 而是他们的文化基因。生物基因混杂的群体拥有近似的文化认同， 从而构建了文化共同体——部族或宗族、政权或国家。他们之间的协调与竞争，已经不再只是生物基因的交流，而是文化基因的交流与碰撞。<br>在第四祖先层的早期，由于草原带西部接触一些重要技术的时间略早，率先崛起， 因此生物基因和文化基因的流动、传输有自西向东的趋势。一旦东亚地区吸收了来自西方的文化元素，整合了内部之后，趋势就发生了逆转，以中原的秦汉王朝和东方草原的匈奴为代表的东亚古代帝国力量又开始自东向西扩张，把东亚人群的生物基因和文化基因输入了西北地区。<br>与此同时，北方地区的汉人群体及其他区域性的群体如南岛语系族群的祖先人群，以庞大的农业人口优势向四周扩散，尤其是向南方地区挺进， 把本群体的生物基因和文化基因撒向南国， 撒向东亚和东南亚的岛屿，甚至跨海扬帆至星辰之下的大洋深处。<br>以上便是对积累于历史长河中的祖先层序的粗线条回顾， 是各个领域的学者们从祖先以及动植物基因中读出的“无字史记”的内容梗概。翻阅由基因写就的这本无字史记，中华大地上祖先的迁徙史跌宕起伏、波澜壮阔，我们这些后人能够从这本天成之作中得到什么启示呢?<br>
<br>首先，我们不是东亚细亚的孤儿， 我们的祖先、我们的文明都是交融的产物。
<br>虽然仍有部分学者坚持中国人祖先的本土起源或曰连续进化附带杂交， 坚持中华文明的单纯的本土起源， 但通过古代各个人群的基因比较， 以及动植物基因的研究可以推断，中国人的祖先最初来自非洲， 然后四面八方各个古代人群融合在一起， 形成了中国人；中华文明既是我们的祖先立足东亚沃土辛勤耕耘、智慧创造的结晶，也是大量外界的物产、技术乃至各种思想输入东亚，落地生根结出的硕果。那些我们现代人所熟知的大洲之间的边界、国家之间的边界， 都是迟至几百年以来的近代逐渐定型的人为界线，并不存在于古代人群迁徙的路途上。古代人群并不会用近代、现代的界线“画地为牢”， 限制他们的基因交流和文化交流。<br>
<br>其次，基因趋于天下大同，文化迈向美美与共。
<br>人口数量和人口密度是决定古代各个人群基因交流的关键因素。直立人时代人口稀少，部落规模也很小， 很多部落会在自然环境波动中自生自灭，他们比周围的灵长类如猩猩、猴子群体强不了太多。因此，远距离的基因交流基本上是不存在的，散落在大陆各个角落的直立人大体上独立进化。认为各个大洲的直立人会迈着同样的进化步伐变成相同的现代智人的观点，不太可能是事实。<br>　<br>
当现代智人走出非洲向全世界扩张时，人口数量比直立人时代明显更多了， 迁徙能力卓越的智人一路上甚至与尼安德特人、丹尼索瓦人进行杂交， 更不用说现代智人不同群体间的杂交了。当农耕传播时代来临时， 相比之前的狩猎采集时代，人口数量有了几十倍甚至上百倍的增加。不同族群间的基因交流更为频繁，昔日因人烟稀少而相对独立进化的各个人群，此时彼此交换着自己的基因型，于是不同人群之间在基因型上逐渐你中有我、我中有你，假以时日，全世界的人类在基因上将趋于天下大同。虽然人群与人群之间仍然会有分野， 但这种分野主要是各自文化基因的差异， 而非生物基因的差别。<br>　<br>
3. 最后，基因属于生命科学， 祖先属于文化情感， 在认识祖先与自我的路途上， 理智与情感都是需要的。<br>　<br>
科学是理性、理智的， 不论人们愿意接受也好，不愿意接受也罢，科学用证据说话， 揭示出祖先的秘史。没有科学成果的支撑，我们连自己的祖先是谁都搞不清楚。对祖先最大的不敬，莫过于认错了自己的祖先是谁，所拜非本尊。要清楚地知晓我们的直系祖先是谁， 生命科学是最重要的工具， 那隐藏在细胞中的肉眼不可见的基因上， 镌刻着祖先的生命信息，那些信息把我们与祖先之间的血脉真正联系了起来。<br>　<br>
中国人自古以来就是非常尊敬祖先的人群，商周的甲骨文中包括大量祭祀祖先的内容。可以想见，在商周之前的很长时间里，中华大地上的人们就已经建立起了对于祖先的尊崇意识和祭拜仪式。这既是古人面对危险重重的外界提高族群凝聚力的有效方式， 也是给身处凡世间的自我一个确定性的“锚”，能够让渺小的个体知晓自己在历史时空长河中所处的位置，获得自己作为个体的存在感和价值感。<br>中国乃至东亚文明圈悠久的祖先崇拜传统，源自古老的农耕聚落结构， 是中国人独特的重要文化基因， 区别于欧美古代文明圈流行的宗教传统。时至今日，对祖先的尊崇乃至祭拜， 仍然是很多中国人重要的情感表达方式， 是人们生活与生命中难以割舍的精神园地。<br>
面对祖先， 我们接受基因证据， 我们也珍视精神传统。<br>但是， 我们应该反对任何以基因之名打造的民族主义和民族歧视， 那些论调并没有科学依据。<br>所有的地球人都是现代智人， 有着 14万年前的共同祖母——线粒体夏娃，有着6万年前的共同祖父——Y染色体亚当。对今天的中国人来说，所有人的共同祖先不会早于3万年前。要追溯绝大多数中国人的共同祖先， 也许只需要把时钟向过去回拨几千年就能成功找到。<br>也许有细心的读者会注意到， 本书尽量避免使用白色人种、蒙古人种、雅利安人种、尼格罗人种这类有着种族味道甚至种族歧视味道的词， 除非引用早期学者的观点的需要。因为这些词混淆了现代民族概念与古代人群称谓， 它们往往是臆想出来的名词，并无基因科学的支持， 对我们理解人类历史造成了干扰。本书在描述古代人群时， 更多地用中性的大洲和大洲方位的地域名称来表示人群，比如“东亚人群”“草原带西部人群”。本书有时也会使用“氐羌族群”“藏缅语系族群”这类包含民族名称的词， 这些词已被学者长期使用，它们要么是对古代人群的描述， 不会引起古今混淆，要么是语言学的中性描述， 不会引起种族歧视。<br>必须强调， 今天我们所谈论的民族，是近代民族国家兴起后才出现的概念， 是一种想象的共同体，是文化基因的产物。基因告诉我们，早在1万年前农耕时代来临时，生物基因意义上的族群界限就开始变得模糊，人群与人群之间的基因交流变得越来越频繁。以“民族”这个几百年前学者构想出来的文化概念来向远古反向推演，认为人类几万年的迁徙历史中， 数以千计的族群之间有着明确的基因阻隔，族群之间有着本质的基因差别， 进而判定族群与族群在智力、文化上有高低之分，这些观点是毫无科学基础的臆想。<br>例如历史上， 汉人有自己的族群认同， 匈奴人有自己的族群认同， 羌人也有自己的族群认同， 这种认同正是不同族群通过对各自祖先、神灵的界定和崇敬来强化的情感。这是历史形成的情感，并没有多大的基因依据。民族主义与族群情感之间的本质区别在于，前者宣扬对外歧视， 后者追求自我欣赏。<br>族群内部的自我欣赏只要不滑向对外歧视，就无伤大雅、无可厚非。只是我们应当警惕这种自我欣赏的潜在危险——族群滑向塔岛技术悲剧。在复活节岛的传说中，第一批移民中有一位建筑师努库·柯胡，他是移民中唯一会建造房屋的人， 跟他学习的移民都没有掌握建筑精髓，特别是没有学会如何盖屋顶，于是在努库·柯胡死后，复活节岛上的房子很容易被大风吹掉屋顶。离开故乡驶向复活节岛的航行中，努库·柯胡没有带上他的妻子同行，到了复活节岛后， 移民却无力再回到故乡。因此，每当日落的时候，建筑师努库·柯胡就很悲伤，因为太阳落下的位置， 正是故乡和妻子所在的方向。<br>由于地理阻隔，复活节岛、塔岛、日本列岛、澳大利亚大陆乃至中华大地、美洲大陆，在远古时代都曾经一度形成塔岛技术悲剧，令深陷其中的古人类、古代族群生计艰难。站在21世纪科技如此发达的今天，我们丝毫不用担心自然界的地理阻隔会把中华大地与世界的其他部分再次分开。但我们要警惕， 那些族群内部自我欣赏所形成的某些消极的文化基因， 会在族群与族群之间制造新的阻隔。<br>比如， 以独立自主产生的本土人群进化、本土文明起源为骄傲，以吸收外来人群基因和文化元素为耻辱， 这种本族群“情感迷恋”是对纯正血统的迷信， 是对自身文化的自大， 在本族群、本国度与外部世界间人为制造障碍。<br>另一种让群体陷入封闭状态的文化基因是迷信丛林法则或曰黑暗丛林法则。这类法则的信徒们认定，他人就是地狱，别的文化和文明是自己必然的敌人， 为了保护自己， 要杜绝与其他文化、文明的接触。历史确实存在一定的丛林法则现象， 比如汉朝、匈奴对于西北地区弱小文化的冲击、欧洲殖民者对于美洲原住民文明的伤害和瓦解。但人类迁徙历史浩荡的主流是彼此开放而非封闭，东亚直立人因被动的封闭而陷入石器技术停滞的境地，中原各文化因为主动的开放，吸纳了大量外来物产、技术乃至思想，站上了古代文明的舞台中央⋯⋯随着农耕时代后人口增长和族群融合，黑暗丛林法则早已失效，因为越封闭，越落后，越无法阻挡打破封闭的力量。<br>一旦拒绝外界交流，每一个人就变成了孤岛， 每一个族群、每一个国家都会变成孤岛。几万年来，人类通过生物基因的突变和扩张、文化基因的创造和交融，最终摆脱了个人的孤岛、族群的孤岛，避免了塔岛技术悲剧。我们需要拥抱那些让各个族群能够平等地走到一起的文化基因， 抵制那些分裂各个族群，在各个族群间制造歧视链的文化基因。<br>本书动笔于2019年年末到 2020年年初新冠肺炎疫情暴发的时期， 疫情迫使各个国家彼此关闭了国境通道， 各个社区限制了人员进出。一种病毒能够感染几乎所有国家的所有族群，这生动地验证了， 全人类在生物基因上差异非常小， 天南地北的族群早已有了充分的基因融合， 只是这验证的方式过于苦难。<br>在疫情期间， 世界各地的很多有识之士都呼吁全球合作， 共同抗疫。被迫暂时隔离的人们通过互联网举办了诸如“同一个世界：共同宅家”(One World: Together at Home)免费音乐会一类的活动,为陷入困境的世界和个人投下希望的光亮；各国科学家也在共享着对于病毒本身和抗病毒药物研究的信息⋯⋯这正是由祖先创造、历史传承的积极、开放的文化基因在发挥作用， 我们都将受益于这样的文化基因打造的人文环境。<br>“四海一家”的信条， 经由一代又一代祖先的传承， 已经写入你我的生物基因里，让它也写入你我的文化基因中吧。<br>今天的人类已经登上了月球， 还没有冲出地月系统。从这个角度看，我们仍然处于地月系统的孤岛上。正如百万年中人类祖先在地球表面的持续探索，人类向太空探索的脚步也从未停止，有朝一日，宇航员必将冲出地月系统，踏上火星和其他星球的表面，书写人类基因的扩张史记新章节，不输祖先曾有的勇气和英名。<br>那将是地球生命基因的荣光，那将是全体人类的荣光。]]></description><link>书籍\无字史记.html</link><guid isPermaLink="false">书籍/无字史记.md</guid><pubDate>Tue, 20 Aug 2024 12:44:13 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202311142228377.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202311142228377.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[西西弗神话]]></title><description><![CDATA[ 
 <br>阅读哲学随笔，有时候就像在做推理，因为每一个简单词汇背后，可能都隐藏着能引导你触摸到真理的庞大信息。就像《西西弗神话》，这个看似简单而不能再简单的名字。<br>西西弗（Sisyphus）是希腊神话里一个永生永世都在做着苦役的人物。诸神惩罚西西弗，命令他将一块巨石推向山顶。但每当巨石即将到达山顶时，它就会重新滚回山脚，西西弗永远都不可能完成诸神的命令，只能一直重复这个无意义的死循环。<br>所以许多人的第一印象，都会觉得这也许只是一本神话故事。<br>但《西西弗神话》其实还有一个经常被人忽略掉的副标题「论荒诞」。<br>书名完整的含义其实是：「用神话人物西西弗作为象征，分析与解释关于荒诞的一切。」<br>在加缪看来，当代生活里的每一个人，都是西西弗。西西弗的生活是荒诞的，我们也是，理解了西西弗，你就理解了有关荒诞的一切。<br>那么问题来了，为什么我们一定要理解「荒诞」？<br><br>荒诞（absurdism），也译作荒谬，它起初只是一个源自拉丁语的音乐术语，指旋律的不和谐。从17世纪起，也有一些哲学家开始用这个词来形容一种生活中不合逻辑、不合情理的离奇感受。<br>但接下来的两个多世纪，整个世界陷入了一种前所未有的动荡不安，狂飙式的科技进步、经济大萧条、世界大战……那些曾经被认为是“真理”的价值观被一次又一次的全球事件所颠覆、习惯、风俗、社会、思想……一切坚固的东西都烟消云散了。<br>整个人类的精神世界陷入了一种全体性的迷茫，直到今天，依旧如此。<br>加缪觉得，这种迷茫就是荒诞，他说：<br>整个世界都是荒诞的，没有人能逃离荒诞，当你感受到荒诞时，你就会觉得生活痛苦而无意义。<br>但为什么我们无法逃离荒诞呢？加缪在《西西弗神话》里举出了三种荒诞产生的原因：<br><br>《西西弗神话》里有这样一段话：<br>起床、乘电车、在办公室或工厂干四小时，吃饭，乘电车，再干四小时，吃饭，睡觉，而且星期一、星期二、星期三、星期四、星期五和星期六，大部分的时间里，这条路走得相当顺畅。不过有一天，突然萌生了‘为什么’的疑问，在这种带有惊讶色彩的厌倦中，一切就开始了。<br>生活就是一种循环，无论你选择的是哪一种生活。<br>但没完没了的循环终究会让人心生厌倦，终有一天你会心生疑惑：&nbsp;「为什么我的生活是这样的？」，然后你就会开始质疑造成这种生活状态的一切，从而心生痛苦，觉得生活没有意义。<br><br>人从出生就踏上了走向死亡的必然旅程，没有人能够体验死亡，我们只能够通过听闻见识别人的死亡去理解它，但只要我们去想象死亡，我们自然就会心生恐惧与沉痛，我们如此地渴望着生，可我们终有一死，死亡让人意识到生命的脆弱，人生的痛苦。<br><br>我们的认知和生活存在一种永恒的阻碍，我们越是希望理性、明晰地理解生活，就越发现自己其实完全不了解生活。<br>我们的心智和理性都是有限的，越读书，就越发现自己无知，越试图理解，就越发现自己完全不理解，久而久之这会让人对生活产生一种非常可怕的陌生感，感觉不到自己是生活的一部分。<br>在加缪看来，之所以每个人都无法从荒诞中逃离，因为我们生来希望追求理性，渴望长生，期盼生活有意义，想要生活在一个和谐安宁的社会里，向往是非分明，期盼善恶报应。<br>但战争、疫情、经济危机、自然破坏、贫富不均、性别冲突、价值观崩塌……那些在我们生活中时刻发生的事情，一遍又一遍地在提醒我们——<br>人天性所向往的东西，偏偏都是不存在与世界上的，所以我们一定会在某个时刻体悟到荒诞，然后就会觉得自己孤独、无依无靠，和生活完全分离，像是被全世界所流放，痛苦不堪。<br>但如果《西西弗神话》只是告诉我们“荒诞”究竟是怎么一回事儿，那倒也没有多少推荐的必要了，就像加缪自己说的：「我感兴趣的，主要不在于发现种种荒诞，而是荒诞产生的结果」。<br>因为荒诞已经是每个人在生活中必然会有的状态，所以他更想通过这本书告诉我们的，是人究竟应该如何面对荒诞？<br>加缪有一句名言：「真正严肃的哲学命题只有一个，那便是自杀。」在他看来，面对荒诞只有两种态度，自杀或者反抗。<br>自杀有两种，一种叫做生理的自杀，荒诞使人对现实生活充满绝望，但人无法逃脱荒诞，就像人无法逃离自己的皮肤，荒诞使得他承认自己不理解生活、被生活所超越了，承认自己的所作所为都不再有意义，这个世界上没有任何的深刻理由能够支持自己再活下去。所以那些深陷于荒诞的痛苦中不可自拔的普通人，只能采用最决绝的自我毁灭，才能离开荒诞。<br>另一种，叫做哲学的自杀，有些人意识到了现实的荒诞，并因之痛苦不堪时，所以他们将逃离荒诞的希望寄托于宗教或者来世。无论是海格德尔、舍斯托夫、雅斯贝斯还是胡塞尔，这些伟大哲学家们面对荒诞时，要么是试图用一种伟大的思想来否认荒诞的存在，要么索性就将解决荒诞的方式交给了神来解决。但加缪根本不认同这一点，他说：「纵观各种存在哲学，我看到它们无一例外地都号召我逃跑。 」<br><br>加缪觉得，荒诞让人意识到人生的无意义、生存的无价值，但是越是如此，人就越应该去体验、去接受、不寄希望于虚无缥缈的信仰，立足当下，用行动去为生活创造意义。<br>这便是反抗。<br>加缪在《西西弗神话》里把这种敢于承认荒诞，并且反抗荒诞的人，称之为「荒诞人」。<br>他在书里举了很多例子来说明「荒诞人」到底是怎么样的一种人？<br>荒诞人是清醒的人，他既不会掩盖回避种种荒诞的现实，也不会将自己的关注点投向上帝或者天神，面对无可避免的命运，他不会屈从，而是努力抗争，并且肩负起抗争过程中的苦痛。<br>荒诞人也是自由的人，因为荒诞让人意识到，来世或者天国都是不存在的，这反而能够让人彻底打破精神枷锁，世界上没有全知全能的上帝为人安排好了生活的一切轨迹，人是独立的，人的一切意义都是在当下的生活中，依靠着自己的行动来创造的，除了当下这片荒诞的天地，人什么都没有。<br>最重要的，荒诞人是无比激情的人，因为荒诞意味着人唯一的出路就是现在，应该尽情地以火热的激情感受人生，不追求获得最好，而是获得最多，尽量感受和体验丰富多彩、多种多样的人生。<br>在加缪看来，永生永世都在推着巨石向山顶前行的西西弗，是「荒诞人」的最高典型，可以称为是&nbsp;「荒诞英雄」。<br>西西弗艰难地将巨石推上山又眼看着它滚下去的时候，是其命运最残酷的时刻，但是这也是他最为清醒地意识到自我命运的时候。<br>加缪认为，此时的西西弗已经高于了它的命运，他把神明赋予他的荒诞命运变成了自己安排的事情，将自己视为命运的主人，从而将人的尊严和生命的意义淋漓尽致地表现了出来。<br>「他的命运属于自己。他的那块巨石是他的事」。<br>西西弗清楚地认识到了自己的荒诞处境，他清楚地知道自己努力推上山的巨石一定会滑落，但是他却选择在此用肩膀抗住荒诞的巨石，并勇于承担一切，这就是他的反抗。<br>这种清醒的行为使得原本天神降下的惩罚，变成了西西弗自己选择的英雄行为，当他开始清醒地推动石头的时候，他的命运就不再属于众神，而是属于自己。<br>西西弗欣然地接受了残酷和荒谬的命运;&nbsp;他知道劳动是徒劳的，不会产生任何结果，他也明白自己不存在未来；他的快乐源于他对自己命运的认可、源于对诸神的蔑视、源于对荒谬的抗争、源于他搬起石头那一刻生发的自豪感。<br><br>天神的惩罚是希望让西西弗感到所有的努力都是徒劳，感到生命不存在任何的意义和价值，这就是荒诞最让人恐惧的地方，但西西弗明知荒诞无意义，却坚定地一次又一次地推动巨石，因为他根本就不会把荒诞视作是与自己的存在格格不入的东西。<br>西西弗，还有那些如西西弗一般的「荒诞人」们，公开承认荒诞就是自己的命运，他们不为永恒做任何事情，只在有限的生命里，自由地、充满激情地进行着反抗，生活越没有意义，他们就越觉得生活值得过。<br>「荒诞人有这样的优势，知道所有的王国都是虚幻的，他们心知肚明，这就是他们的全部伟大之所在…被剥夺了希望，不等于绝望，大地的火焰完全抵得上天国的芳香」。我们每个人其实都是荒诞世界里的西西弗，无时无刻地推动着各种各样的巨石，登顶，又重重滑落。<br>他，就是我们每个人命运的象征。<br>但西西弗的命运是绝对单一的，他永远只能在神话的世界里周转往复，可我们却真实地拥有比他更无限的可能。<br>在如此极端的条件下，西西弗依然可以通过自己的反抗，在荒诞世界里拥有自由，那到底还有什么理由能够阻挡我们成为那个清醒、自由又充满激情的「荒诞人」呢？<br>世界或许是悲观的，但人永远应该是乐观的，我们永远都不应该试图回避或者默默忍受生活中所遇到的种种困难，因为这意味着我们否定了生命的意义。<br>我们必须义无反顾地去反抗荒诞，而生活中一切幸福、价值与美好的体验，都会在这种反抗的过程里，灿烂升起。<br>这就是加缪想要通过《西西弗神话》这本书告诉我们的：<br>「推石上山这场搏斗本身，就足以充实一颗人心。」]]></description><link>书籍\西西弗神话.html</link><guid isPermaLink="false">书籍/西西弗神话.md</guid><pubDate>Thu, 20 Jun 2024 12:44:16 GMT</pubDate></item><item><title><![CDATA[中国少数民族的形成]]></title><description><![CDATA[ 
 <br><br>从中国少数民族形成的起点，即从发生学角度方面考察，中国少数民族的形成大体有两种类型和途径：一种是<a data-href="原生型民族" href="术语\原生型民族.html" class="internal-link" target="_self" rel="noopener nofollow">原生型民族</a>，一种是<a data-href="融生型民族" href="术语\融生型民族.html" class="internal-link" target="_self" rel="noopener nofollow">融生型民族</a>。<br>所谓原生型民族，主要是指其民族形成起点是在氏族、部落、部落联盟的基础上，经历了原始社会末期的发展阶段，发展成为民族共同体。这里所说原始社会末期的发展阶段，并不是指整个人类的原始社会发展历程，也不是指作为中国历史上中原地区原始社会末期哪个时代，而是具体指我国某个地区和某个民族自身发展的区域性的原始社会发展末期。譬如11、12世纪，就中国中原和大部分地区来说，已经处于比较发达的封建社会的历史时期，然而蒙古地区的一些地方的人群，却仍然处于原始社会末期。再如我国黑龙江地区和云南某些地区，一直到我国近代，原始社会才出现瓦解的局面。<br>因此，原生型民族并不是指其形成时间的早迟，而是指其形成的起点，是在原始社会发展的末期阶段，是在氏族、部落、部落联盟的基础上形成为民族。这类少数民族在我国民族中是比较多的。例如：苗族、羌族、藏族、维吾尔族、蒙古族以及云南、东北等地的许多民族，都经历了氏族、部落、部落联盟最后进人了民族共同体的阶段。<br>所谓融生型民族，主要是指由已经成为民族的众多民族的成分，由于某种原因发生融合，而形成的民族。其民族形成的起点是比较高的社会发展阶段，其民族共同体的发展并未经历过原始社会的氏族、胞族、部落以及奴隶社会等社会发展阶段。就我国来说，封建社会发展阶段漫长，这类民族共同体基本上都产生于这个历史时期。其中如回族、东乡族、保安族、撒拉族、土族、哈萨克族、乌孜别克族等众多民族。<br>如何确认我国少数民族族源的方法问题，实际上在我国少数民族研究中，已经基本解决。族源的确定要避免主观臆测和想象，要有客观依据。<br>
<br>首先，由于大多数少数民族与汉族和历代中央王朝有密切关系和交往，而我国历代“正史”和其他文献也都有记述少数民族社会活动的传统，因此在我国典籍中，对少数民族的记载是很多的，其中不乏关于民族族源的历史资料，因此汉文历史典籍，是我国绝大多数少数民族族源的重要根据；
<br>其次，我国一些少数民族如蒙古族、藏族、壮族以及历史上的突厥、回等都有自己的文字和有关文献，这些也都是确定这些少数民族族源的基本资料，把少数民族的历史文献资料与汉族文献资料进行比较研究，那更是研究其族源的最好根据；
<br>再次，当前使用的语言学谱系分类法，是研究民族间语言关系亲疏的重要手段，通过对语言关系的研究来确定民族之间的亲疏关系，特别是族源关系，也是十分有效的方法；
<br>最后，通过社会调查，从民间流传的各种传说、各民族习俗、社会生活等方面进行分析比较研究，也是确认民族族源的重要方法。此外，现代基因技术的发展和在民族学中的应用，对研究一个民族的族源，也有重要的参考价值。
目前也许应该通过分子生物学的证据进行考究，这也是一个新的方法。

<br><br>
<br>多源性特征：我国现代绝大多数民族，从族源上说都是多源的。而且从其发展过程来说，它的每一个发展阶段，每一个源头也都是多源的。不同阶段的多源，不同源头的多源。从这个意义上说，任何一个民族都是在融合、同化、分化中产生和发展的，纯粹血统、纯粹体征、纯粹文化的民族是不存在的。例如藏族，从7世纪初，开始作为一个稳定的民族共同体登上了中国多民族的历史舞台。在此以前，就他的族源来说，最基础的部分是西藏地区的土著原始部落，他们早就在西藏地区生息并进行开发，这些土著原始部落的存在，已为西藏地区的考古所证实。根据《唐书》记载，羌族的一支发羌，于战国末年，辗转由东向西南迁徙，进人青藏高原，成为藏族的重要族源之一。在松赞干布时期，又统了西藏高原的附国、大小羊同、象雄、苏毗等部落，从而正式形成了民族共同体。藏族形成后，在北上、东进、西下的过程中，又有大批羌、氏、吐谷浑、西南夷和西域民族成分加人，特别是有大量汉族成分加人，使藏族成为辉煌一时的古代强大民族。藏族的大部分成分，都是较稳定的，如羌、氏、吐谷浑以及汉族等，他们本身也是由许多民族成分融合而成的，因此可以说，藏族族源不仅是多源的，而且其族源的各部分也都是来自多源民族共同体的成分。
<br>地域性特征：我国现代大多数民族，一般来说都有自己祖祖辈辈长期生存生活的区域，而其族源一般都与自己区域内古代活动过的民族有一定的传承关系，因此一个民族的族源，往往带有很强的地域特征。例如满族，作为一个稳定的民族共同体，形成于16世纪末17世纪初，以白山黑水著称的东北地区是他们的发源地。其族源，一般来说，基本上来自在这一地区曾经活动过的各族，具体来说，他们的族源既直接来源于女真，也有契丹、娄、肃慎等东北各族的成分。再如蒙古族，形成于12世纪末13世纪初的蒙古高原，在成吉思汗统一蒙古高原各部时，就将操不同语言，有不同文化的各部统合到了蒙古部之中，形成了蒙古族。这些操不同语言，有不同文化传统、历史渊源的各部，实际上是蒙古高原上从匈奴以来的各古代民族的遗部，蒙古族完全可以说是历史上长期生活于蒙古高原上的北方各族的集大成者。在民族族源地域性特征方面还有一种情况，即古代的一些民族大迁徙到新地域之后，与当地民族融合，产生出一种新的民族。
<br>主源性特征：虽然绝大多数民族在族源上表现为多源性特征，但同时每个民族的族源的成分，在形成该民族中也都有主次之分，即该民族的诸多族源中，明显表现出主源的痕迹。在古代民族中，如吐谷浑族，在其族源中虽有大量的羌族、氏族以及西南夷的成分，但其主源则明显是鲜卑族的成分，这主要表现在其文化的传承、语言的族属、族内结构、族源认同等方面，更多地反映出鲜卑族的特征。羌族成分的特征在吐谷浑族中虽也突出，但比起鲜卑族的特征来，显然并不占主要地位。其他现代民族如维吾尔族，其族源虽然在西域地区吸收了大量古代西域民族的成分，但是从其历史发展、体貌特征、习俗文化、文化传承等方面看，其族源的主要成分仍是回鹃。再如蒙古族，其族源成分中几乎吸纳了北方的古代各民族成分，但就其族源的最主要的成分，直接出于东胡的室韦，特别是蒙兀室韦。因此，每个民族的族源的主要成分，是可以认识的，这也是民族学研究的一项重要任务。
<br>文化继承特征：在分析民族族源的成分时，有许多因素可以作为根据，其中如历史记载、口碑传说、经济文化类型、共同地域范围等，都可以提供定的根据，来确认一个民族的族源成分。而其中比较明显的一个重要因素就是文化，包括语言、习俗、口碑传说、宗教等。特别是语言因素，由于语言、文字在一个民族中影响最普遍，与一个民族的思维方式、感情传递、心理状态、相互交往都有最密切的关系，因此是最不易变动、最稳定的因素，从而也成为判断和剖析一个民族族源的重要根据。以上这些是我国少数民族（包括汉族在内），在民族族源、民族形成方面的一些特点。根据上述一些看法，我们认为我国各少数民族（除了朝鲜族和俄罗斯族外）在族源的关系上，可以用下面这个图表来作一个大体的表述。
]]></description><link>书籍\中国少数民族通史.html</link><guid isPermaLink="false">书籍/中国少数民族通史.md</guid><pubDate>Wed, 26 Jun 2024 08:14:13 GMT</pubDate></item><item><title><![CDATA[英文期刊投稿中的简写与状态解读]]></title><description><![CDATA[ 
 <br><br>投稿后，进入审稿阶段，可以看到一些奇怪的缩写，现将其小结如下：  <br>Authors：作者，不用多说了。  <br>EIC： Editors in Chief，主编，权力最大，有稿件最终决定权。  <br>AE：Associate Editors，副编辑，（就是文章发表后在首页第一栏下方的contributing editor），对你的稿件来说，此人非常重要&nbsp;，他会在审稿人意见的基础上对文章作个综合评价后，给主编一个recommendation。一般主编都会按照AE的意见写最终的decision letter。  <br>ADM：Administrator，相当于编辑部的执行编辑（Managing Editor），你会发现编辑部给你的信大都是他写给你的。 他是编辑部里和你最接近的人，给你分配稿件号（Edit the manuscript ID number），修改各种投稿状态和日期（Edit the submission date）。  <br>Reviewers： 审稿人。（Article要求两个审稿人＋AE，总共三个人审。communication只有一个人审，这个人或者是编辑部指定的审稿人，或者是AE）<br>典型的审稿流程如下：author&nbsp;<a data-tooltip-position="top" aria-label="https://so.csdn.net/so/search?q=submit&amp;spm=1001.2101.3001.7020" rel="noopener nofollow" class="external-link" href="https://so.csdn.net/so/search?q=submit&amp;spm=1001.2101.3001.7020" target="_blank">submit</a>&nbsp;--》&nbsp;Admin Checks and Passes to EIC --》&nbsp;EIC Assigns to Associate Editor --》AE Invites and /or Assigns Reviewers --》Reviewers Score --》AE Makes Final Decision<br><br>
<br>awaiting admin. procession&nbsp; 一般3－4天后就会安排主编。
<br>awaiting reviewer assignment&nbsp;&nbsp;等待指定审稿人。 主编在选择审稿人，等待审稿人回复是否同意审稿。一般在一周以内。看审稿人回复速度。
<br>awaiting reviewer scores&nbsp;&nbsp;等待审稿人审稿意见。一般会有一个审稿限期。但是审稿人觉得时间时间不够，可以写信给主编要求延长审稿期限。这个时间长短要取决于审稿人是否有空看你的文章，还要看他是否守时。
<br>awaiting AE assignment&nbsp;等待AE的指派。编辑部在选择/联系AE。一般1－3天左右。
<br>awaiting AE recommendation&nbsp;等待AE的推荐。 一般AE比较守时。但基本都是期限的最后一两天才给结果。
<br>awaiting EIC decision&nbsp;－激动人心的时刻。 等待主编的决定。一般3－4天。
<br>如果communication的话，就没有第4和第5步。
<br><br>
<br>&nbsp;Submitted to Journal&nbsp;当上传结束后，显示的状态是Submitted to Journal，这个状态是自然形成的无需处理。
<br>With editor&nbsp;如果在投稿的时候没有要求选择编辑，就先到主编那里，主编会分派给别的编辑。这当中就会有另两个状态：&nbsp;① Editor assigned　编辑分； ② Editor Declined Invitation　编辑拒绝邀请，这时主编不得不将投稿文章重新分派给其它编辑。
<br>Reviewer(s) invited&nbsp;说明编辑已接手处理，正在邀请审稿人中。有时该过程会持续很长时间，如果其中原因是编辑一直没有找到合适的审稿人，这时投稿者可以向编辑推荐审稿人。
<br>Under&nbsp;review&nbsp;审稿人的意见已上传，说明审稿人已接受审稿，正在审稿中，这应该是一个漫长的等待（期刊通常会限定审稿人审稿时间，一般为一个月左右）。当然前面各步骤也可能很慢的，要看编辑的处理情况。如果被邀请审稿人不想审，就会decline，编辑会重新邀请别的审稿人。
<br>required&nbsp;review&nbsp;completed&nbsp;审稿结束，等编辑处理，该过程短则几天，长则无期，科学堂 有一篇文章出现required&nbsp;review&nbsp;completed状态已近一个月了，还是没有消息。
<br>Decision in Process&nbsp;到了这一步就快要有结果了，编辑开始考虑是给修改还是直接拒，当然也有可能直接接受的，但可能性很小，呵呵。
<br>Minor revision/Major revision&nbsp;小修/大修，这个时候可以稍微庆祝一下了，因为有修改就有可能。具体怎么改就不多说了，谦虚谨慎是不可少的（因为修改后一般会再发给审稿人看，所以一定要 细心的回答每一个审稿人的每一个问题，态度要谦逊，要让审稿人觉得他提的每个问题都很有水准的，然后针对他的问题，一个一个的做出答复，能修改的就修改， 不能修改的给出理由，而且都要列出来，文章的哪一段哪一行修改了最好都说出来，记住：给审稿人减少麻烦就是给你自己减少麻烦！另注：&nbsp;有时，审稿人会在修改意见里隐讳里说出要你仔细阅读某几篇文献，这时可要注意了，其中某些文章可能就是评审者自己发表的，这时你最好在你的修改稿中加以引用），修改后被拒绝的例子也多不胜数的。
<br>Revision Submitted to Journal&nbsp;修改后重新提交，等待编辑审理。
<br>Accepted&nbsp;如果不要再审，只是小修改，编辑看后会马上显示这个状态，但如果要再审也会有上面的部分状态。一步会比较快，但也有慢的。看杂志的。
<br>Rejected&nbsp;相信大家见了Rejected，都会很郁闷。但也不要太灰心，耐心将评审意见看完，一般评审者会给出有益的建议，相信看后你会有所收获。
<br><br><br>
<br>－Accept
<br>&nbsp;-&nbsp;accept after minor revision( 小修后接收，withour re-review不需要再送审)
<br>－reconsideration after major revision.(大修之后再送审，即要再经过审稿流程3－6)
<br>－reject and resubmit&nbsp;(论文现在状态不能接受，但可以修改后重新再投。要重新经过审稿流程1－6)
<br>－reject&nbsp;（没希望了，改投把)
]]></description><link>home\英文期刊投稿中的简写与状态解读.html</link><guid isPermaLink="false">Home/英文期刊投稿中的简写与状态解读.md</guid><pubDate>Thu, 20 Jun 2024 13:46:14 GMT</pubDate></item><item><title><![CDATA[伦理填报]]></title><description><![CDATA[ 
 <br><br>
<br><a data-tooltip-position="top" aria-label="http://ctmsec.cd120.com/" rel="noopener nofollow" class="external-link" href="http://ctmsec.cd120.com/" target="_blank">打开官网</a>
<br>http://ctmsec.cd120.com/
复制<br>
<br>登陆账号：
<br>账号：20229186
密码：Hgl2wmg@1314
复制<br>
<br>点击右上角：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231018522.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>选择研究者发起的临床研究备案及伦理申报。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231018154.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>按照下图的模板进行填写，注意项目名称、研究例数、内容摘要填写自己的，人类遗传资源选择唾液。<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231015320.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>填报之后下载附件，附件应该具备如下：<br>
<img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231021189.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;">
<br>附件打印填写，然后扫描上传。
<br><br>这是国家超算成都中心的链接，你们几个都去熟悉一下界面和操作。软件安装在固定位置/public/home/heguanglin/biosoftware，软连接到/public/home/heguanglin/biosoftware/bin。]]></description><link>home\注意事项.html</link><guid isPermaLink="false">Home/注意事项.md</guid><pubDate>Wed, 04 Sep 2024 08:53:33 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231018522.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/202401231018522.png"&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[共享文档]]></title><description><![CDATA[ 
 <br>这是我的主页（也是我的思维领域）
当然，你也可以访问我的其他主页：<br>
<a data-tooltip-position="top" aria-label="https://www.yuque.com/wusheng-s0bue" rel="noopener nofollow" class="external-link" href="https://www.yuque.com/wusheng-s0bue" target="_blank">梧之声 · 语雀 (yuque.com)</a><br>
<a data-tooltip-position="top" aria-label="https://blog.csdn.net/2301_79082700?type=blog" rel="noopener nofollow" class="external-link" href="https://blog.csdn.net/2301_79082700?type=blog" target="_blank">梧叶之声-CSDN博客</a>
<br>我的邮件地址： <a data-tooltip-position="top" aria-label="mailto:giantlinlinlin@gmail.com" rel="noopener nofollow" class="external-link" href="mailto:giantlinlinlin@gmail.com" target="_blank">giantlinlinlin@gmail.com</a>
如果笔记中的链接失效可以发邮件给我（尽管我不一定会看）。
<br><br>以下文档通过 OneDrive 共享无需登录即可查看。<br><br>
<br><a data-tooltip-position="top" aria-label="https://1drv.ms/x/s!AnGqDjyiZ5t_hOhzEJ74MDF4ouS5Dg?e=ah4wGP" rel="noopener nofollow" class="external-link" href="https://1drv.ms/x/s!AnGqDjyiZ5t_hOhzEJ74MDF4ouS5Dg?e=ah4wGP" target="_blank">线粒体DNA系统发育树.xlsx</a>
<br><a data-tooltip-position="top" aria-label="https://1drv.ms/x/s!AnGqDjyiZ5t_hOht4Vqit6qWxKUv0Q?e=6tDTEY" rel="noopener nofollow" class="external-link" href="https://1drv.ms/x/s!AnGqDjyiZ5t_hOht4Vqit6qWxKUv0Q?e=6tDTEY" target="_blank">古代mtDNA收集.xlsx</a>
<br><br>
<br><a data-tooltip-position="top" aria-label="https://1drv.ms/x/s!AnGqDjyiZ5t_hOhs88iCTd5wGt_sUg?e=V04daO" rel="noopener nofollow" class="external-link" href="https://1drv.ms/x/s!AnGqDjyiZ5t_hOhs88iCTd5wGt_sUg?e=V04daO" target="_blank">基础信息速查表.xlsx</a>
<br><a data-tooltip-position="top" aria-label="https://1drv.ms/w/s!AnGqDjyiZ5t_hZNKOojOum8txaSM5g?e=9G51kH" rel="noopener nofollow" class="external-link" href="https://1drv.ms/w/s!AnGqDjyiZ5t_hZNKOojOum8txaSM5g?e=9G51kH" target="_blank">课题组邮箱和推荐审稿人.docx</a>
<br><br>Cake is a lie.<br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211539.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211548.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211555.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br><img alt="image.png" src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211605.png" referrerpolicy="no-referrer" style="width: 500px; max-width: 100%;"><br>花园里有什么？  <br>你不能从这里学到任何东西  
除非你多次尝试。熟能生巧。
<br>我把自己的笔记分享到这里
包括但不限于R语言、生物信息学以及一些你可能不感兴趣的内容。
<br>Important
以下学科：马克思主义哲学、逻辑学、宗教学、伦理学、、经济统计学、财政学、税收学、法学、民法学、刑法学、行政法学、商法学、国际法学、教育学、学前教育学、小学教育学、特殊教育学、教育技术学、中国语言文学、外国语言文学、比较文学与世界文学、新闻传播学、中国史、世界史、史学理论及史学史、考古学、数学、物理学、化学、生物科学、地理科学、机械工程、电子科学与技术、计算机科学与技术、土木工程、化学工程与技术、环境科学与工程、、植物保护学、林学、畜牧学、水产养殖学、基础医学、临床医学、口腔医学、公共卫生与预防医学、中药学、药学、军事学、美术学、设计学、戏剧与影视学、数字人文学科、计算机辅助工程、有限元分析、计算流体力学、生物电脑学、物理电脑学、化学电脑学……<br>
几乎都没有。  
<br>
Σ(⊙▽⊙"a你不会还在看吧？
]]></description><link>home\homepage.html</link><guid isPermaLink="false">Home/Homepage.md</guid><pubDate>Thu, 12 Sep 2024 13:21:09 GMT</pubDate><enclosure url="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211539.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src="https://picturerealm.oss-cn-chengdu.aliyuncs.com/obsidian/20240616211539.png"&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>